<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />
<title>lunar_lander_DQN</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<style type="text/css">
    /*!
*
* Twitter Bootstrap
*
*/
/*!
 * Bootstrap v3.3.7 (http://getbootstrap.com)
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 */
/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
html {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block;
  vertical-align: baseline;
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a {
  background-color: transparent;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit;
  font: inherit;
  margin: 0;
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"],
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button;
  cursor: pointer;
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box;
  padding: 0;
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0;
  padding: 0;
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
/*! Source: https://github.com/h5bp/html5-boilerplate/blob/master/src/css/main.css */
@media print {
  *,
  *:before,
  *:after {
    background: transparent !important;
    color: #000 !important;
    box-shadow: none !important;
    text-shadow: none !important;
  }
  a,
  a:visited {
    text-decoration: underline;
  }
  a[href]:after {
    content: " (" attr(href) ")";
  }
  abbr[title]:after {
    content: " (" attr(title) ")";
  }
  a[href^="#"]:after,
  a[href^="javascript:"]:after {
    content: "";
  }
  pre,
  blockquote {
    border: 1px solid #999;
    page-break-inside: avoid;
  }
  thead {
    display: table-header-group;
  }
  tr,
  img {
    page-break-inside: avoid;
  }
  img {
    max-width: 100% !important;
  }
  p,
  h2,
  h3 {
    orphans: 3;
    widows: 3;
  }
  h2,
  h3 {
    page-break-after: avoid;
  }
  .navbar {
    display: none;
  }
  .btn > .caret,
  .dropup > .btn > .caret {
    border-top-color: #000 !important;
  }
  .label {
    border: 1px solid #000;
  }
  .table {
    border-collapse: collapse !important;
  }
  .table td,
  .table th {
    background-color: #fff !important;
  }
  .table-bordered th,
  .table-bordered td {
    border: 1px solid #ddd !important;
  }
}
@font-face {
  font-family: 'Glyphicons Halflings';
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot');
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot?#iefix') format('embedded-opentype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff2') format('woff2'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff') format('woff'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.ttf') format('truetype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.svg#glyphicons_halflingsregular') format('svg');
}
.glyphicon {
  position: relative;
  top: 1px;
  display: inline-block;
  font-family: 'Glyphicons Halflings';
  font-style: normal;
  font-weight: normal;
  line-height: 1;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.glyphicon-asterisk:before {
  content: "\002a";
}
.glyphicon-plus:before {
  content: "\002b";
}
.glyphicon-euro:before,
.glyphicon-eur:before {
  content: "\20ac";
}
.glyphicon-minus:before {
  content: "\2212";
}
.glyphicon-cloud:before {
  content: "\2601";
}
.glyphicon-envelope:before {
  content: "\2709";
}
.glyphicon-pencil:before {
  content: "\270f";
}
.glyphicon-glass:before {
  content: "\e001";
}
.glyphicon-music:before {
  content: "\e002";
}
.glyphicon-search:before {
  content: "\e003";
}
.glyphicon-heart:before {
  content: "\e005";
}
.glyphicon-star:before {
  content: "\e006";
}
.glyphicon-star-empty:before {
  content: "\e007";
}
.glyphicon-user:before {
  content: "\e008";
}
.glyphicon-film:before {
  content: "\e009";
}
.glyphicon-th-large:before {
  content: "\e010";
}
.glyphicon-th:before {
  content: "\e011";
}
.glyphicon-th-list:before {
  content: "\e012";
}
.glyphicon-ok:before {
  content: "\e013";
}
.glyphicon-remove:before {
  content: "\e014";
}
.glyphicon-zoom-in:before {
  content: "\e015";
}
.glyphicon-zoom-out:before {
  content: "\e016";
}
.glyphicon-off:before {
  content: "\e017";
}
.glyphicon-signal:before {
  content: "\e018";
}
.glyphicon-cog:before {
  content: "\e019";
}
.glyphicon-trash:before {
  content: "\e020";
}
.glyphicon-home:before {
  content: "\e021";
}
.glyphicon-file:before {
  content: "\e022";
}
.glyphicon-time:before {
  content: "\e023";
}
.glyphicon-road:before {
  content: "\e024";
}
.glyphicon-download-alt:before {
  content: "\e025";
}
.glyphicon-download:before {
  content: "\e026";
}
.glyphicon-upload:before {
  content: "\e027";
}
.glyphicon-inbox:before {
  content: "\e028";
}
.glyphicon-play-circle:before {
  content: "\e029";
}
.glyphicon-repeat:before {
  content: "\e030";
}
.glyphicon-refresh:before {
  content: "\e031";
}
.glyphicon-list-alt:before {
  content: "\e032";
}
.glyphicon-lock:before {
  content: "\e033";
}
.glyphicon-flag:before {
  content: "\e034";
}
.glyphicon-headphones:before {
  content: "\e035";
}
.glyphicon-volume-off:before {
  content: "\e036";
}
.glyphicon-volume-down:before {
  content: "\e037";
}
.glyphicon-volume-up:before {
  content: "\e038";
}
.glyphicon-qrcode:before {
  content: "\e039";
}
.glyphicon-barcode:before {
  content: "\e040";
}
.glyphicon-tag:before {
  content: "\e041";
}
.glyphicon-tags:before {
  content: "\e042";
}
.glyphicon-book:before {
  content: "\e043";
}
.glyphicon-bookmark:before {
  content: "\e044";
}
.glyphicon-print:before {
  content: "\e045";
}
.glyphicon-camera:before {
  content: "\e046";
}
.glyphicon-font:before {
  content: "\e047";
}
.glyphicon-bold:before {
  content: "\e048";
}
.glyphicon-italic:before {
  content: "\e049";
}
.glyphicon-text-height:before {
  content: "\e050";
}
.glyphicon-text-width:before {
  content: "\e051";
}
.glyphicon-align-left:before {
  content: "\e052";
}
.glyphicon-align-center:before {
  content: "\e053";
}
.glyphicon-align-right:before {
  content: "\e054";
}
.glyphicon-align-justify:before {
  content: "\e055";
}
.glyphicon-list:before {
  content: "\e056";
}
.glyphicon-indent-left:before {
  content: "\e057";
}
.glyphicon-indent-right:before {
  content: "\e058";
}
.glyphicon-facetime-video:before {
  content: "\e059";
}
.glyphicon-picture:before {
  content: "\e060";
}
.glyphicon-map-marker:before {
  content: "\e062";
}
.glyphicon-adjust:before {
  content: "\e063";
}
.glyphicon-tint:before {
  content: "\e064";
}
.glyphicon-edit:before {
  content: "\e065";
}
.glyphicon-share:before {
  content: "\e066";
}
.glyphicon-check:before {
  content: "\e067";
}
.glyphicon-move:before {
  content: "\e068";
}
.glyphicon-step-backward:before {
  content: "\e069";
}
.glyphicon-fast-backward:before {
  content: "\e070";
}
.glyphicon-backward:before {
  content: "\e071";
}
.glyphicon-play:before {
  content: "\e072";
}
.glyphicon-pause:before {
  content: "\e073";
}
.glyphicon-stop:before {
  content: "\e074";
}
.glyphicon-forward:before {
  content: "\e075";
}
.glyphicon-fast-forward:before {
  content: "\e076";
}
.glyphicon-step-forward:before {
  content: "\e077";
}
.glyphicon-eject:before {
  content: "\e078";
}
.glyphicon-chevron-left:before {
  content: "\e079";
}
.glyphicon-chevron-right:before {
  content: "\e080";
}
.glyphicon-plus-sign:before {
  content: "\e081";
}
.glyphicon-minus-sign:before {
  content: "\e082";
}
.glyphicon-remove-sign:before {
  content: "\e083";
}
.glyphicon-ok-sign:before {
  content: "\e084";
}
.glyphicon-question-sign:before {
  content: "\e085";
}
.glyphicon-info-sign:before {
  content: "\e086";
}
.glyphicon-screenshot:before {
  content: "\e087";
}
.glyphicon-remove-circle:before {
  content: "\e088";
}
.glyphicon-ok-circle:before {
  content: "\e089";
}
.glyphicon-ban-circle:before {
  content: "\e090";
}
.glyphicon-arrow-left:before {
  content: "\e091";
}
.glyphicon-arrow-right:before {
  content: "\e092";
}
.glyphicon-arrow-up:before {
  content: "\e093";
}
.glyphicon-arrow-down:before {
  content: "\e094";
}
.glyphicon-share-alt:before {
  content: "\e095";
}
.glyphicon-resize-full:before {
  content: "\e096";
}
.glyphicon-resize-small:before {
  content: "\e097";
}
.glyphicon-exclamation-sign:before {
  content: "\e101";
}
.glyphicon-gift:before {
  content: "\e102";
}
.glyphicon-leaf:before {
  content: "\e103";
}
.glyphicon-fire:before {
  content: "\e104";
}
.glyphicon-eye-open:before {
  content: "\e105";
}
.glyphicon-eye-close:before {
  content: "\e106";
}
.glyphicon-warning-sign:before {
  content: "\e107";
}
.glyphicon-plane:before {
  content: "\e108";
}
.glyphicon-calendar:before {
  content: "\e109";
}
.glyphicon-random:before {
  content: "\e110";
}
.glyphicon-comment:before {
  content: "\e111";
}
.glyphicon-magnet:before {
  content: "\e112";
}
.glyphicon-chevron-up:before {
  content: "\e113";
}
.glyphicon-chevron-down:before {
  content: "\e114";
}
.glyphicon-retweet:before {
  content: "\e115";
}
.glyphicon-shopping-cart:before {
  content: "\e116";
}
.glyphicon-folder-close:before {
  content: "\e117";
}
.glyphicon-folder-open:before {
  content: "\e118";
}
.glyphicon-resize-vertical:before {
  content: "\e119";
}
.glyphicon-resize-horizontal:before {
  content: "\e120";
}
.glyphicon-hdd:before {
  content: "\e121";
}
.glyphicon-bullhorn:before {
  content: "\e122";
}
.glyphicon-bell:before {
  content: "\e123";
}
.glyphicon-certificate:before {
  content: "\e124";
}
.glyphicon-thumbs-up:before {
  content: "\e125";
}
.glyphicon-thumbs-down:before {
  content: "\e126";
}
.glyphicon-hand-right:before {
  content: "\e127";
}
.glyphicon-hand-left:before {
  content: "\e128";
}
.glyphicon-hand-up:before {
  content: "\e129";
}
.glyphicon-hand-down:before {
  content: "\e130";
}
.glyphicon-circle-arrow-right:before {
  content: "\e131";
}
.glyphicon-circle-arrow-left:before {
  content: "\e132";
}
.glyphicon-circle-arrow-up:before {
  content: "\e133";
}
.glyphicon-circle-arrow-down:before {
  content: "\e134";
}
.glyphicon-globe:before {
  content: "\e135";
}
.glyphicon-wrench:before {
  content: "\e136";
}
.glyphicon-tasks:before {
  content: "\e137";
}
.glyphicon-filter:before {
  content: "\e138";
}
.glyphicon-briefcase:before {
  content: "\e139";
}
.glyphicon-fullscreen:before {
  content: "\e140";
}
.glyphicon-dashboard:before {
  content: "\e141";
}
.glyphicon-paperclip:before {
  content: "\e142";
}
.glyphicon-heart-empty:before {
  content: "\e143";
}
.glyphicon-link:before {
  content: "\e144";
}
.glyphicon-phone:before {
  content: "\e145";
}
.glyphicon-pushpin:before {
  content: "\e146";
}
.glyphicon-usd:before {
  content: "\e148";
}
.glyphicon-gbp:before {
  content: "\e149";
}
.glyphicon-sort:before {
  content: "\e150";
}
.glyphicon-sort-by-alphabet:before {
  content: "\e151";
}
.glyphicon-sort-by-alphabet-alt:before {
  content: "\e152";
}
.glyphicon-sort-by-order:before {
  content: "\e153";
}
.glyphicon-sort-by-order-alt:before {
  content: "\e154";
}
.glyphicon-sort-by-attributes:before {
  content: "\e155";
}
.glyphicon-sort-by-attributes-alt:before {
  content: "\e156";
}
.glyphicon-unchecked:before {
  content: "\e157";
}
.glyphicon-expand:before {
  content: "\e158";
}
.glyphicon-collapse-down:before {
  content: "\e159";
}
.glyphicon-collapse-up:before {
  content: "\e160";
}
.glyphicon-log-in:before {
  content: "\e161";
}
.glyphicon-flash:before {
  content: "\e162";
}
.glyphicon-log-out:before {
  content: "\e163";
}
.glyphicon-new-window:before {
  content: "\e164";
}
.glyphicon-record:before {
  content: "\e165";
}
.glyphicon-save:before {
  content: "\e166";
}
.glyphicon-open:before {
  content: "\e167";
}
.glyphicon-saved:before {
  content: "\e168";
}
.glyphicon-import:before {
  content: "\e169";
}
.glyphicon-export:before {
  content: "\e170";
}
.glyphicon-send:before {
  content: "\e171";
}
.glyphicon-floppy-disk:before {
  content: "\e172";
}
.glyphicon-floppy-saved:before {
  content: "\e173";
}
.glyphicon-floppy-remove:before {
  content: "\e174";
}
.glyphicon-floppy-save:before {
  content: "\e175";
}
.glyphicon-floppy-open:before {
  content: "\e176";
}
.glyphicon-credit-card:before {
  content: "\e177";
}
.glyphicon-transfer:before {
  content: "\e178";
}
.glyphicon-cutlery:before {
  content: "\e179";
}
.glyphicon-header:before {
  content: "\e180";
}
.glyphicon-compressed:before {
  content: "\e181";
}
.glyphicon-earphone:before {
  content: "\e182";
}
.glyphicon-phone-alt:before {
  content: "\e183";
}
.glyphicon-tower:before {
  content: "\e184";
}
.glyphicon-stats:before {
  content: "\e185";
}
.glyphicon-sd-video:before {
  content: "\e186";
}
.glyphicon-hd-video:before {
  content: "\e187";
}
.glyphicon-subtitles:before {
  content: "\e188";
}
.glyphicon-sound-stereo:before {
  content: "\e189";
}
.glyphicon-sound-dolby:before {
  content: "\e190";
}
.glyphicon-sound-5-1:before {
  content: "\e191";
}
.glyphicon-sound-6-1:before {
  content: "\e192";
}
.glyphicon-sound-7-1:before {
  content: "\e193";
}
.glyphicon-copyright-mark:before {
  content: "\e194";
}
.glyphicon-registration-mark:before {
  content: "\e195";
}
.glyphicon-cloud-download:before {
  content: "\e197";
}
.glyphicon-cloud-upload:before {
  content: "\e198";
}
.glyphicon-tree-conifer:before {
  content: "\e199";
}
.glyphicon-tree-deciduous:before {
  content: "\e200";
}
.glyphicon-cd:before {
  content: "\e201";
}
.glyphicon-save-file:before {
  content: "\e202";
}
.glyphicon-open-file:before {
  content: "\e203";
}
.glyphicon-level-up:before {
  content: "\e204";
}
.glyphicon-copy:before {
  content: "\e205";
}
.glyphicon-paste:before {
  content: "\e206";
}
.glyphicon-alert:before {
  content: "\e209";
}
.glyphicon-equalizer:before {
  content: "\e210";
}
.glyphicon-king:before {
  content: "\e211";
}
.glyphicon-queen:before {
  content: "\e212";
}
.glyphicon-pawn:before {
  content: "\e213";
}
.glyphicon-bishop:before {
  content: "\e214";
}
.glyphicon-knight:before {
  content: "\e215";
}
.glyphicon-baby-formula:before {
  content: "\e216";
}
.glyphicon-tent:before {
  content: "\26fa";
}
.glyphicon-blackboard:before {
  content: "\e218";
}
.glyphicon-bed:before {
  content: "\e219";
}
.glyphicon-apple:before {
  content: "\f8ff";
}
.glyphicon-erase:before {
  content: "\e221";
}
.glyphicon-hourglass:before {
  content: "\231b";
}
.glyphicon-lamp:before {
  content: "\e223";
}
.glyphicon-duplicate:before {
  content: "\e224";
}
.glyphicon-piggy-bank:before {
  content: "\e225";
}
.glyphicon-scissors:before {
  content: "\e226";
}
.glyphicon-bitcoin:before {
  content: "\e227";
}
.glyphicon-btc:before {
  content: "\e227";
}
.glyphicon-xbt:before {
  content: "\e227";
}
.glyphicon-yen:before {
  content: "\00a5";
}
.glyphicon-jpy:before {
  content: "\00a5";
}
.glyphicon-ruble:before {
  content: "\20bd";
}
.glyphicon-rub:before {
  content: "\20bd";
}
.glyphicon-scale:before {
  content: "\e230";
}
.glyphicon-ice-lolly:before {
  content: "\e231";
}
.glyphicon-ice-lolly-tasted:before {
  content: "\e232";
}
.glyphicon-education:before {
  content: "\e233";
}
.glyphicon-option-horizontal:before {
  content: "\e234";
}
.glyphicon-option-vertical:before {
  content: "\e235";
}
.glyphicon-menu-hamburger:before {
  content: "\e236";
}
.glyphicon-modal-window:before {
  content: "\e237";
}
.glyphicon-oil:before {
  content: "\e238";
}
.glyphicon-grain:before {
  content: "\e239";
}
.glyphicon-sunglasses:before {
  content: "\e240";
}
.glyphicon-text-size:before {
  content: "\e241";
}
.glyphicon-text-color:before {
  content: "\e242";
}
.glyphicon-text-background:before {
  content: "\e243";
}
.glyphicon-object-align-top:before {
  content: "\e244";
}
.glyphicon-object-align-bottom:before {
  content: "\e245";
}
.glyphicon-object-align-horizontal:before {
  content: "\e246";
}
.glyphicon-object-align-left:before {
  content: "\e247";
}
.glyphicon-object-align-vertical:before {
  content: "\e248";
}
.glyphicon-object-align-right:before {
  content: "\e249";
}
.glyphicon-triangle-right:before {
  content: "\e250";
}
.glyphicon-triangle-left:before {
  content: "\e251";
}
.glyphicon-triangle-bottom:before {
  content: "\e252";
}
.glyphicon-triangle-top:before {
  content: "\e253";
}
.glyphicon-console:before {
  content: "\e254";
}
.glyphicon-superscript:before {
  content: "\e255";
}
.glyphicon-subscript:before {
  content: "\e256";
}
.glyphicon-menu-left:before {
  content: "\e257";
}
.glyphicon-menu-right:before {
  content: "\e258";
}
.glyphicon-menu-down:before {
  content: "\e259";
}
.glyphicon-menu-up:before {
  content: "\e260";
}
* {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
*:before,
*:after {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
html {
  font-size: 10px;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
}
body {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 1.42857143;
  color: #000;
  background-color: #fff;
}
input,
button,
select,
textarea {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}
a {
  color: #337ab7;
  text-decoration: none;
}
a:hover,
a:focus {
  color: #23527c;
  text-decoration: underline;
}
a:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
figure {
  margin: 0;
}
img {
  vertical-align: middle;
}
.img-responsive,
.thumbnail > img,
.thumbnail a > img,
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  display: block;
  max-width: 100%;
  height: auto;
}
.img-rounded {
  border-radius: 3px;
}
.img-thumbnail {
  padding: 4px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: all 0.2s ease-in-out;
  -o-transition: all 0.2s ease-in-out;
  transition: all 0.2s ease-in-out;
  display: inline-block;
  max-width: 100%;
  height: auto;
}
.img-circle {
  border-radius: 50%;
}
hr {
  margin-top: 18px;
  margin-bottom: 18px;
  border: 0;
  border-top: 1px solid #eeeeee;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  margin: -1px;
  padding: 0;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
[role="button"] {
  cursor: pointer;
}
h1,
h2,
h3,
h4,
h5,
h6,
.h1,
.h2,
.h3,
.h4,
.h5,
.h6 {
  font-family: inherit;
  font-weight: 500;
  line-height: 1.1;
  color: inherit;
}
h1 small,
h2 small,
h3 small,
h4 small,
h5 small,
h6 small,
.h1 small,
.h2 small,
.h3 small,
.h4 small,
.h5 small,
.h6 small,
h1 .small,
h2 .small,
h3 .small,
h4 .small,
h5 .small,
h6 .small,
.h1 .small,
.h2 .small,
.h3 .small,
.h4 .small,
.h5 .small,
.h6 .small {
  font-weight: normal;
  line-height: 1;
  color: #777777;
}
h1,
.h1,
h2,
.h2,
h3,
.h3 {
  margin-top: 18px;
  margin-bottom: 9px;
}
h1 small,
.h1 small,
h2 small,
.h2 small,
h3 small,
.h3 small,
h1 .small,
.h1 .small,
h2 .small,
.h2 .small,
h3 .small,
.h3 .small {
  font-size: 65%;
}
h4,
.h4,
h5,
.h5,
h6,
.h6 {
  margin-top: 9px;
  margin-bottom: 9px;
}
h4 small,
.h4 small,
h5 small,
.h5 small,
h6 small,
.h6 small,
h4 .small,
.h4 .small,
h5 .small,
.h5 .small,
h6 .small,
.h6 .small {
  font-size: 75%;
}
h1,
.h1 {
  font-size: 33px;
}
h2,
.h2 {
  font-size: 27px;
}
h3,
.h3 {
  font-size: 23px;
}
h4,
.h4 {
  font-size: 17px;
}
h5,
.h5 {
  font-size: 13px;
}
h6,
.h6 {
  font-size: 12px;
}
p {
  margin: 0 0 9px;
}
.lead {
  margin-bottom: 18px;
  font-size: 14px;
  font-weight: 300;
  line-height: 1.4;
}
@media (min-width: 768px) {
  .lead {
    font-size: 19.5px;
  }
}
small,
.small {
  font-size: 92%;
}
mark,
.mark {
  background-color: #fcf8e3;
  padding: .2em;
}
.text-left {
  text-align: left;
}
.text-right {
  text-align: right;
}
.text-center {
  text-align: center;
}
.text-justify {
  text-align: justify;
}
.text-nowrap {
  white-space: nowrap;
}
.text-lowercase {
  text-transform: lowercase;
}
.text-uppercase {
  text-transform: uppercase;
}
.text-capitalize {
  text-transform: capitalize;
}
.text-muted {
  color: #777777;
}
.text-primary {
  color: #337ab7;
}
a.text-primary:hover,
a.text-primary:focus {
  color: #286090;
}
.text-success {
  color: #3c763d;
}
a.text-success:hover,
a.text-success:focus {
  color: #2b542c;
}
.text-info {
  color: #31708f;
}
a.text-info:hover,
a.text-info:focus {
  color: #245269;
}
.text-warning {
  color: #8a6d3b;
}
a.text-warning:hover,
a.text-warning:focus {
  color: #66512c;
}
.text-danger {
  color: #a94442;
}
a.text-danger:hover,
a.text-danger:focus {
  color: #843534;
}
.bg-primary {
  color: #fff;
  background-color: #337ab7;
}
a.bg-primary:hover,
a.bg-primary:focus {
  background-color: #286090;
}
.bg-success {
  background-color: #dff0d8;
}
a.bg-success:hover,
a.bg-success:focus {
  background-color: #c1e2b3;
}
.bg-info {
  background-color: #d9edf7;
}
a.bg-info:hover,
a.bg-info:focus {
  background-color: #afd9ee;
}
.bg-warning {
  background-color: #fcf8e3;
}
a.bg-warning:hover,
a.bg-warning:focus {
  background-color: #f7ecb5;
}
.bg-danger {
  background-color: #f2dede;
}
a.bg-danger:hover,
a.bg-danger:focus {
  background-color: #e4b9b9;
}
.page-header {
  padding-bottom: 8px;
  margin: 36px 0 18px;
  border-bottom: 1px solid #eeeeee;
}
ul,
ol {
  margin-top: 0;
  margin-bottom: 9px;
}
ul ul,
ol ul,
ul ol,
ol ol {
  margin-bottom: 0;
}
.list-unstyled {
  padding-left: 0;
  list-style: none;
}
.list-inline {
  padding-left: 0;
  list-style: none;
  margin-left: -5px;
}
.list-inline > li {
  display: inline-block;
  padding-left: 5px;
  padding-right: 5px;
}
dl {
  margin-top: 0;
  margin-bottom: 18px;
}
dt,
dd {
  line-height: 1.42857143;
}
dt {
  font-weight: bold;
}
dd {
  margin-left: 0;
}
@media (min-width: 541px) {
  .dl-horizontal dt {
    float: left;
    width: 160px;
    clear: left;
    text-align: right;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }
  .dl-horizontal dd {
    margin-left: 180px;
  }
}
abbr[title],
abbr[data-original-title] {
  cursor: help;
  border-bottom: 1px dotted #777777;
}
.initialism {
  font-size: 90%;
  text-transform: uppercase;
}
blockquote {
  padding: 9px 18px;
  margin: 0 0 18px;
  font-size: inherit;
  border-left: 5px solid #eeeeee;
}
blockquote p:last-child,
blockquote ul:last-child,
blockquote ol:last-child {
  margin-bottom: 0;
}
blockquote footer,
blockquote small,
blockquote .small {
  display: block;
  font-size: 80%;
  line-height: 1.42857143;
  color: #777777;
}
blockquote footer:before,
blockquote small:before,
blockquote .small:before {
  content: '\2014 \00A0';
}
.blockquote-reverse,
blockquote.pull-right {
  padding-right: 15px;
  padding-left: 0;
  border-right: 5px solid #eeeeee;
  border-left: 0;
  text-align: right;
}
.blockquote-reverse footer:before,
blockquote.pull-right footer:before,
.blockquote-reverse small:before,
blockquote.pull-right small:before,
.blockquote-reverse .small:before,
blockquote.pull-right .small:before {
  content: '';
}
.blockquote-reverse footer:after,
blockquote.pull-right footer:after,
.blockquote-reverse small:after,
blockquote.pull-right small:after,
.blockquote-reverse .small:after,
blockquote.pull-right .small:after {
  content: '\00A0 \2014';
}
address {
  margin-bottom: 18px;
  font-style: normal;
  line-height: 1.42857143;
}
code,
kbd,
pre,
samp {
  font-family: monospace;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 2px;
}
kbd {
  padding: 2px 4px;
  font-size: 90%;
  color: #888;
  background-color: transparent;
  border-radius: 1px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
}
kbd kbd {
  padding: 0;
  font-size: 100%;
  font-weight: bold;
  box-shadow: none;
}
pre {
  display: block;
  padding: 8.5px;
  margin: 0 0 9px;
  font-size: 12px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  color: #333333;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 2px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  white-space: pre-wrap;
  background-color: transparent;
  border-radius: 0;
}
.pre-scrollable {
  max-height: 340px;
  overflow-y: scroll;
}
.container {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
@media (min-width: 768px) {
  .container {
    width: 768px;
  }
}
@media (min-width: 992px) {
  .container {
    width: 940px;
  }
}
@media (min-width: 1200px) {
  .container {
    width: 1140px;
  }
}
.container-fluid {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
.row {
  margin-left: 0px;
  margin-right: 0px;
}
.col-xs-1, .col-sm-1, .col-md-1, .col-lg-1, .col-xs-2, .col-sm-2, .col-md-2, .col-lg-2, .col-xs-3, .col-sm-3, .col-md-3, .col-lg-3, .col-xs-4, .col-sm-4, .col-md-4, .col-lg-4, .col-xs-5, .col-sm-5, .col-md-5, .col-lg-5, .col-xs-6, .col-sm-6, .col-md-6, .col-lg-6, .col-xs-7, .col-sm-7, .col-md-7, .col-lg-7, .col-xs-8, .col-sm-8, .col-md-8, .col-lg-8, .col-xs-9, .col-sm-9, .col-md-9, .col-lg-9, .col-xs-10, .col-sm-10, .col-md-10, .col-lg-10, .col-xs-11, .col-sm-11, .col-md-11, .col-lg-11, .col-xs-12, .col-sm-12, .col-md-12, .col-lg-12 {
  position: relative;
  min-height: 1px;
  padding-left: 0px;
  padding-right: 0px;
}
.col-xs-1, .col-xs-2, .col-xs-3, .col-xs-4, .col-xs-5, .col-xs-6, .col-xs-7, .col-xs-8, .col-xs-9, .col-xs-10, .col-xs-11, .col-xs-12 {
  float: left;
}
.col-xs-12 {
  width: 100%;
}
.col-xs-11 {
  width: 91.66666667%;
}
.col-xs-10 {
  width: 83.33333333%;
}
.col-xs-9 {
  width: 75%;
}
.col-xs-8 {
  width: 66.66666667%;
}
.col-xs-7 {
  width: 58.33333333%;
}
.col-xs-6 {
  width: 50%;
}
.col-xs-5 {
  width: 41.66666667%;
}
.col-xs-4 {
  width: 33.33333333%;
}
.col-xs-3 {
  width: 25%;
}
.col-xs-2 {
  width: 16.66666667%;
}
.col-xs-1 {
  width: 8.33333333%;
}
.col-xs-pull-12 {
  right: 100%;
}
.col-xs-pull-11 {
  right: 91.66666667%;
}
.col-xs-pull-10 {
  right: 83.33333333%;
}
.col-xs-pull-9 {
  right: 75%;
}
.col-xs-pull-8 {
  right: 66.66666667%;
}
.col-xs-pull-7 {
  right: 58.33333333%;
}
.col-xs-pull-6 {
  right: 50%;
}
.col-xs-pull-5 {
  right: 41.66666667%;
}
.col-xs-pull-4 {
  right: 33.33333333%;
}
.col-xs-pull-3 {
  right: 25%;
}
.col-xs-pull-2 {
  right: 16.66666667%;
}
.col-xs-pull-1 {
  right: 8.33333333%;
}
.col-xs-pull-0 {
  right: auto;
}
.col-xs-push-12 {
  left: 100%;
}
.col-xs-push-11 {
  left: 91.66666667%;
}
.col-xs-push-10 {
  left: 83.33333333%;
}
.col-xs-push-9 {
  left: 75%;
}
.col-xs-push-8 {
  left: 66.66666667%;
}
.col-xs-push-7 {
  left: 58.33333333%;
}
.col-xs-push-6 {
  left: 50%;
}
.col-xs-push-5 {
  left: 41.66666667%;
}
.col-xs-push-4 {
  left: 33.33333333%;
}
.col-xs-push-3 {
  left: 25%;
}
.col-xs-push-2 {
  left: 16.66666667%;
}
.col-xs-push-1 {
  left: 8.33333333%;
}
.col-xs-push-0 {
  left: auto;
}
.col-xs-offset-12 {
  margin-left: 100%;
}
.col-xs-offset-11 {
  margin-left: 91.66666667%;
}
.col-xs-offset-10 {
  margin-left: 83.33333333%;
}
.col-xs-offset-9 {
  margin-left: 75%;
}
.col-xs-offset-8 {
  margin-left: 66.66666667%;
}
.col-xs-offset-7 {
  margin-left: 58.33333333%;
}
.col-xs-offset-6 {
  margin-left: 50%;
}
.col-xs-offset-5 {
  margin-left: 41.66666667%;
}
.col-xs-offset-4 {
  margin-left: 33.33333333%;
}
.col-xs-offset-3 {
  margin-left: 25%;
}
.col-xs-offset-2 {
  margin-left: 16.66666667%;
}
.col-xs-offset-1 {
  margin-left: 8.33333333%;
}
.col-xs-offset-0 {
  margin-left: 0%;
}
@media (min-width: 768px) {
  .col-sm-1, .col-sm-2, .col-sm-3, .col-sm-4, .col-sm-5, .col-sm-6, .col-sm-7, .col-sm-8, .col-sm-9, .col-sm-10, .col-sm-11, .col-sm-12 {
    float: left;
  }
  .col-sm-12 {
    width: 100%;
  }
  .col-sm-11 {
    width: 91.66666667%;
  }
  .col-sm-10 {
    width: 83.33333333%;
  }
  .col-sm-9 {
    width: 75%;
  }
  .col-sm-8 {
    width: 66.66666667%;
  }
  .col-sm-7 {
    width: 58.33333333%;
  }
  .col-sm-6 {
    width: 50%;
  }
  .col-sm-5 {
    width: 41.66666667%;
  }
  .col-sm-4 {
    width: 33.33333333%;
  }
  .col-sm-3 {
    width: 25%;
  }
  .col-sm-2 {
    width: 16.66666667%;
  }
  .col-sm-1 {
    width: 8.33333333%;
  }
  .col-sm-pull-12 {
    right: 100%;
  }
  .col-sm-pull-11 {
    right: 91.66666667%;
  }
  .col-sm-pull-10 {
    right: 83.33333333%;
  }
  .col-sm-pull-9 {
    right: 75%;
  }
  .col-sm-pull-8 {
    right: 66.66666667%;
  }
  .col-sm-pull-7 {
    right: 58.33333333%;
  }
  .col-sm-pull-6 {
    right: 50%;
  }
  .col-sm-pull-5 {
    right: 41.66666667%;
  }
  .col-sm-pull-4 {
    right: 33.33333333%;
  }
  .col-sm-pull-3 {
    right: 25%;
  }
  .col-sm-pull-2 {
    right: 16.66666667%;
  }
  .col-sm-pull-1 {
    right: 8.33333333%;
  }
  .col-sm-pull-0 {
    right: auto;
  }
  .col-sm-push-12 {
    left: 100%;
  }
  .col-sm-push-11 {
    left: 91.66666667%;
  }
  .col-sm-push-10 {
    left: 83.33333333%;
  }
  .col-sm-push-9 {
    left: 75%;
  }
  .col-sm-push-8 {
    left: 66.66666667%;
  }
  .col-sm-push-7 {
    left: 58.33333333%;
  }
  .col-sm-push-6 {
    left: 50%;
  }
  .col-sm-push-5 {
    left: 41.66666667%;
  }
  .col-sm-push-4 {
    left: 33.33333333%;
  }
  .col-sm-push-3 {
    left: 25%;
  }
  .col-sm-push-2 {
    left: 16.66666667%;
  }
  .col-sm-push-1 {
    left: 8.33333333%;
  }
  .col-sm-push-0 {
    left: auto;
  }
  .col-sm-offset-12 {
    margin-left: 100%;
  }
  .col-sm-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-sm-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-sm-offset-9 {
    margin-left: 75%;
  }
  .col-sm-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-sm-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-sm-offset-6 {
    margin-left: 50%;
  }
  .col-sm-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-sm-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-sm-offset-3 {
    margin-left: 25%;
  }
  .col-sm-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-sm-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-sm-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 992px) {
  .col-md-1, .col-md-2, .col-md-3, .col-md-4, .col-md-5, .col-md-6, .col-md-7, .col-md-8, .col-md-9, .col-md-10, .col-md-11, .col-md-12 {
    float: left;
  }
  .col-md-12 {
    width: 100%;
  }
  .col-md-11 {
    width: 91.66666667%;
  }
  .col-md-10 {
    width: 83.33333333%;
  }
  .col-md-9 {
    width: 75%;
  }
  .col-md-8 {
    width: 66.66666667%;
  }
  .col-md-7 {
    width: 58.33333333%;
  }
  .col-md-6 {
    width: 50%;
  }
  .col-md-5 {
    width: 41.66666667%;
  }
  .col-md-4 {
    width: 33.33333333%;
  }
  .col-md-3 {
    width: 25%;
  }
  .col-md-2 {
    width: 16.66666667%;
  }
  .col-md-1 {
    width: 8.33333333%;
  }
  .col-md-pull-12 {
    right: 100%;
  }
  .col-md-pull-11 {
    right: 91.66666667%;
  }
  .col-md-pull-10 {
    right: 83.33333333%;
  }
  .col-md-pull-9 {
    right: 75%;
  }
  .col-md-pull-8 {
    right: 66.66666667%;
  }
  .col-md-pull-7 {
    right: 58.33333333%;
  }
  .col-md-pull-6 {
    right: 50%;
  }
  .col-md-pull-5 {
    right: 41.66666667%;
  }
  .col-md-pull-4 {
    right: 33.33333333%;
  }
  .col-md-pull-3 {
    right: 25%;
  }
  .col-md-pull-2 {
    right: 16.66666667%;
  }
  .col-md-pull-1 {
    right: 8.33333333%;
  }
  .col-md-pull-0 {
    right: auto;
  }
  .col-md-push-12 {
    left: 100%;
  }
  .col-md-push-11 {
    left: 91.66666667%;
  }
  .col-md-push-10 {
    left: 83.33333333%;
  }
  .col-md-push-9 {
    left: 75%;
  }
  .col-md-push-8 {
    left: 66.66666667%;
  }
  .col-md-push-7 {
    left: 58.33333333%;
  }
  .col-md-push-6 {
    left: 50%;
  }
  .col-md-push-5 {
    left: 41.66666667%;
  }
  .col-md-push-4 {
    left: 33.33333333%;
  }
  .col-md-push-3 {
    left: 25%;
  }
  .col-md-push-2 {
    left: 16.66666667%;
  }
  .col-md-push-1 {
    left: 8.33333333%;
  }
  .col-md-push-0 {
    left: auto;
  }
  .col-md-offset-12 {
    margin-left: 100%;
  }
  .col-md-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-md-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-md-offset-9 {
    margin-left: 75%;
  }
  .col-md-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-md-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-md-offset-6 {
    margin-left: 50%;
  }
  .col-md-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-md-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-md-offset-3 {
    margin-left: 25%;
  }
  .col-md-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-md-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-md-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 1200px) {
  .col-lg-1, .col-lg-2, .col-lg-3, .col-lg-4, .col-lg-5, .col-lg-6, .col-lg-7, .col-lg-8, .col-lg-9, .col-lg-10, .col-lg-11, .col-lg-12 {
    float: left;
  }
  .col-lg-12 {
    width: 100%;
  }
  .col-lg-11 {
    width: 91.66666667%;
  }
  .col-lg-10 {
    width: 83.33333333%;
  }
  .col-lg-9 {
    width: 75%;
  }
  .col-lg-8 {
    width: 66.66666667%;
  }
  .col-lg-7 {
    width: 58.33333333%;
  }
  .col-lg-6 {
    width: 50%;
  }
  .col-lg-5 {
    width: 41.66666667%;
  }
  .col-lg-4 {
    width: 33.33333333%;
  }
  .col-lg-3 {
    width: 25%;
  }
  .col-lg-2 {
    width: 16.66666667%;
  }
  .col-lg-1 {
    width: 8.33333333%;
  }
  .col-lg-pull-12 {
    right: 100%;
  }
  .col-lg-pull-11 {
    right: 91.66666667%;
  }
  .col-lg-pull-10 {
    right: 83.33333333%;
  }
  .col-lg-pull-9 {
    right: 75%;
  }
  .col-lg-pull-8 {
    right: 66.66666667%;
  }
  .col-lg-pull-7 {
    right: 58.33333333%;
  }
  .col-lg-pull-6 {
    right: 50%;
  }
  .col-lg-pull-5 {
    right: 41.66666667%;
  }
  .col-lg-pull-4 {
    right: 33.33333333%;
  }
  .col-lg-pull-3 {
    right: 25%;
  }
  .col-lg-pull-2 {
    right: 16.66666667%;
  }
  .col-lg-pull-1 {
    right: 8.33333333%;
  }
  .col-lg-pull-0 {
    right: auto;
  }
  .col-lg-push-12 {
    left: 100%;
  }
  .col-lg-push-11 {
    left: 91.66666667%;
  }
  .col-lg-push-10 {
    left: 83.33333333%;
  }
  .col-lg-push-9 {
    left: 75%;
  }
  .col-lg-push-8 {
    left: 66.66666667%;
  }
  .col-lg-push-7 {
    left: 58.33333333%;
  }
  .col-lg-push-6 {
    left: 50%;
  }
  .col-lg-push-5 {
    left: 41.66666667%;
  }
  .col-lg-push-4 {
    left: 33.33333333%;
  }
  .col-lg-push-3 {
    left: 25%;
  }
  .col-lg-push-2 {
    left: 16.66666667%;
  }
  .col-lg-push-1 {
    left: 8.33333333%;
  }
  .col-lg-push-0 {
    left: auto;
  }
  .col-lg-offset-12 {
    margin-left: 100%;
  }
  .col-lg-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-lg-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-lg-offset-9 {
    margin-left: 75%;
  }
  .col-lg-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-lg-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-lg-offset-6 {
    margin-left: 50%;
  }
  .col-lg-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-lg-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-lg-offset-3 {
    margin-left: 25%;
  }
  .col-lg-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-lg-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-lg-offset-0 {
    margin-left: 0%;
  }
}
table {
  background-color: transparent;
}
caption {
  padding-top: 8px;
  padding-bottom: 8px;
  color: #777777;
  text-align: left;
}
th {
  text-align: left;
}
.table {
  width: 100%;
  max-width: 100%;
  margin-bottom: 18px;
}
.table > thead > tr > th,
.table > tbody > tr > th,
.table > tfoot > tr > th,
.table > thead > tr > td,
.table > tbody > tr > td,
.table > tfoot > tr > td {
  padding: 8px;
  line-height: 1.42857143;
  vertical-align: top;
  border-top: 1px solid #ddd;
}
.table > thead > tr > th {
  vertical-align: bottom;
  border-bottom: 2px solid #ddd;
}
.table > caption + thead > tr:first-child > th,
.table > colgroup + thead > tr:first-child > th,
.table > thead:first-child > tr:first-child > th,
.table > caption + thead > tr:first-child > td,
.table > colgroup + thead > tr:first-child > td,
.table > thead:first-child > tr:first-child > td {
  border-top: 0;
}
.table > tbody + tbody {
  border-top: 2px solid #ddd;
}
.table .table {
  background-color: #fff;
}
.table-condensed > thead > tr > th,
.table-condensed > tbody > tr > th,
.table-condensed > tfoot > tr > th,
.table-condensed > thead > tr > td,
.table-condensed > tbody > tr > td,
.table-condensed > tfoot > tr > td {
  padding: 5px;
}
.table-bordered {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > tbody > tr > th,
.table-bordered > tfoot > tr > th,
.table-bordered > thead > tr > td,
.table-bordered > tbody > tr > td,
.table-bordered > tfoot > tr > td {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > thead > tr > td {
  border-bottom-width: 2px;
}
.table-striped > tbody > tr:nth-of-type(odd) {
  background-color: #f9f9f9;
}
.table-hover > tbody > tr:hover {
  background-color: #f5f5f5;
}
table col[class*="col-"] {
  position: static;
  float: none;
  display: table-column;
}
table td[class*="col-"],
table th[class*="col-"] {
  position: static;
  float: none;
  display: table-cell;
}
.table > thead > tr > td.active,
.table > tbody > tr > td.active,
.table > tfoot > tr > td.active,
.table > thead > tr > th.active,
.table > tbody > tr > th.active,
.table > tfoot > tr > th.active,
.table > thead > tr.active > td,
.table > tbody > tr.active > td,
.table > tfoot > tr.active > td,
.table > thead > tr.active > th,
.table > tbody > tr.active > th,
.table > tfoot > tr.active > th {
  background-color: #f5f5f5;
}
.table-hover > tbody > tr > td.active:hover,
.table-hover > tbody > tr > th.active:hover,
.table-hover > tbody > tr.active:hover > td,
.table-hover > tbody > tr:hover > .active,
.table-hover > tbody > tr.active:hover > th {
  background-color: #e8e8e8;
}
.table > thead > tr > td.success,
.table > tbody > tr > td.success,
.table > tfoot > tr > td.success,
.table > thead > tr > th.success,
.table > tbody > tr > th.success,
.table > tfoot > tr > th.success,
.table > thead > tr.success > td,
.table > tbody > tr.success > td,
.table > tfoot > tr.success > td,
.table > thead > tr.success > th,
.table > tbody > tr.success > th,
.table > tfoot > tr.success > th {
  background-color: #dff0d8;
}
.table-hover > tbody > tr > td.success:hover,
.table-hover > tbody > tr > th.success:hover,
.table-hover > tbody > tr.success:hover > td,
.table-hover > tbody > tr:hover > .success,
.table-hover > tbody > tr.success:hover > th {
  background-color: #d0e9c6;
}
.table > thead > tr > td.info,
.table > tbody > tr > td.info,
.table > tfoot > tr > td.info,
.table > thead > tr > th.info,
.table > tbody > tr > th.info,
.table > tfoot > tr > th.info,
.table > thead > tr.info > td,
.table > tbody > tr.info > td,
.table > tfoot > tr.info > td,
.table > thead > tr.info > th,
.table > tbody > tr.info > th,
.table > tfoot > tr.info > th {
  background-color: #d9edf7;
}
.table-hover > tbody > tr > td.info:hover,
.table-hover > tbody > tr > th.info:hover,
.table-hover > tbody > tr.info:hover > td,
.table-hover > tbody > tr:hover > .info,
.table-hover > tbody > tr.info:hover > th {
  background-color: #c4e3f3;
}
.table > thead > tr > td.warning,
.table > tbody > tr > td.warning,
.table > tfoot > tr > td.warning,
.table > thead > tr > th.warning,
.table > tbody > tr > th.warning,
.table > tfoot > tr > th.warning,
.table > thead > tr.warning > td,
.table > tbody > tr.warning > td,
.table > tfoot > tr.warning > td,
.table > thead > tr.warning > th,
.table > tbody > tr.warning > th,
.table > tfoot > tr.warning > th {
  background-color: #fcf8e3;
}
.table-hover > tbody > tr > td.warning:hover,
.table-hover > tbody > tr > th.warning:hover,
.table-hover > tbody > tr.warning:hover > td,
.table-hover > tbody > tr:hover > .warning,
.table-hover > tbody > tr.warning:hover > th {
  background-color: #faf2cc;
}
.table > thead > tr > td.danger,
.table > tbody > tr > td.danger,
.table > tfoot > tr > td.danger,
.table > thead > tr > th.danger,
.table > tbody > tr > th.danger,
.table > tfoot > tr > th.danger,
.table > thead > tr.danger > td,
.table > tbody > tr.danger > td,
.table > tfoot > tr.danger > td,
.table > thead > tr.danger > th,
.table > tbody > tr.danger > th,
.table > tfoot > tr.danger > th {
  background-color: #f2dede;
}
.table-hover > tbody > tr > td.danger:hover,
.table-hover > tbody > tr > th.danger:hover,
.table-hover > tbody > tr.danger:hover > td,
.table-hover > tbody > tr:hover > .danger,
.table-hover > tbody > tr.danger:hover > th {
  background-color: #ebcccc;
}
.table-responsive {
  overflow-x: auto;
  min-height: 0.01%;
}
@media screen and (max-width: 767px) {
  .table-responsive {
    width: 100%;
    margin-bottom: 13.5px;
    overflow-y: hidden;
    -ms-overflow-style: -ms-autohiding-scrollbar;
    border: 1px solid #ddd;
  }
  .table-responsive > .table {
    margin-bottom: 0;
  }
  .table-responsive > .table > thead > tr > th,
  .table-responsive > .table > tbody > tr > th,
  .table-responsive > .table > tfoot > tr > th,
  .table-responsive > .table > thead > tr > td,
  .table-responsive > .table > tbody > tr > td,
  .table-responsive > .table > tfoot > tr > td {
    white-space: nowrap;
  }
  .table-responsive > .table-bordered {
    border: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:first-child,
  .table-responsive > .table-bordered > tbody > tr > th:first-child,
  .table-responsive > .table-bordered > tfoot > tr > th:first-child,
  .table-responsive > .table-bordered > thead > tr > td:first-child,
  .table-responsive > .table-bordered > tbody > tr > td:first-child,
  .table-responsive > .table-bordered > tfoot > tr > td:first-child {
    border-left: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:last-child,
  .table-responsive > .table-bordered > tbody > tr > th:last-child,
  .table-responsive > .table-bordered > tfoot > tr > th:last-child,
  .table-responsive > .table-bordered > thead > tr > td:last-child,
  .table-responsive > .table-bordered > tbody > tr > td:last-child,
  .table-responsive > .table-bordered > tfoot > tr > td:last-child {
    border-right: 0;
  }
  .table-responsive > .table-bordered > tbody > tr:last-child > th,
  .table-responsive > .table-bordered > tfoot > tr:last-child > th,
  .table-responsive > .table-bordered > tbody > tr:last-child > td,
  .table-responsive > .table-bordered > tfoot > tr:last-child > td {
    border-bottom: 0;
  }
}
fieldset {
  padding: 0;
  margin: 0;
  border: 0;
  min-width: 0;
}
legend {
  display: block;
  width: 100%;
  padding: 0;
  margin-bottom: 18px;
  font-size: 19.5px;
  line-height: inherit;
  color: #333333;
  border: 0;
  border-bottom: 1px solid #e5e5e5;
}
label {
  display: inline-block;
  max-width: 100%;
  margin-bottom: 5px;
  font-weight: bold;
}
input[type="search"] {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
input[type="radio"],
input[type="checkbox"] {
  margin: 4px 0 0;
  margin-top: 1px \9;
  line-height: normal;
}
input[type="file"] {
  display: block;
}
input[type="range"] {
  display: block;
  width: 100%;
}
select[multiple],
select[size] {
  height: auto;
}
input[type="file"]:focus,
input[type="radio"]:focus,
input[type="checkbox"]:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
output {
  display: block;
  padding-top: 7px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
}
.form-control {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
}
.form-control:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.form-control::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.form-control:-ms-input-placeholder {
  color: #999;
}
.form-control::-webkit-input-placeholder {
  color: #999;
}
.form-control::-ms-expand {
  border: 0;
  background-color: transparent;
}
.form-control[disabled],
.form-control[readonly],
fieldset[disabled] .form-control {
  background-color: #eeeeee;
  opacity: 1;
}
.form-control[disabled],
fieldset[disabled] .form-control {
  cursor: not-allowed;
}
textarea.form-control {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: none;
}
@media screen and (-webkit-min-device-pixel-ratio: 0) {
  input[type="date"].form-control,
  input[type="time"].form-control,
  input[type="datetime-local"].form-control,
  input[type="month"].form-control {
    line-height: 32px;
  }
  input[type="date"].input-sm,
  input[type="time"].input-sm,
  input[type="datetime-local"].input-sm,
  input[type="month"].input-sm,
  .input-group-sm input[type="date"],
  .input-group-sm input[type="time"],
  .input-group-sm input[type="datetime-local"],
  .input-group-sm input[type="month"] {
    line-height: 30px;
  }
  input[type="date"].input-lg,
  input[type="time"].input-lg,
  input[type="datetime-local"].input-lg,
  input[type="month"].input-lg,
  .input-group-lg input[type="date"],
  .input-group-lg input[type="time"],
  .input-group-lg input[type="datetime-local"],
  .input-group-lg input[type="month"] {
    line-height: 45px;
  }
}
.form-group {
  margin-bottom: 15px;
}
.radio,
.checkbox {
  position: relative;
  display: block;
  margin-top: 10px;
  margin-bottom: 10px;
}
.radio label,
.checkbox label {
  min-height: 18px;
  padding-left: 20px;
  margin-bottom: 0;
  font-weight: normal;
  cursor: pointer;
}
.radio input[type="radio"],
.radio-inline input[type="radio"],
.checkbox input[type="checkbox"],
.checkbox-inline input[type="checkbox"] {
  position: absolute;
  margin-left: -20px;
  margin-top: 4px \9;
}
.radio + .radio,
.checkbox + .checkbox {
  margin-top: -5px;
}
.radio-inline,
.checkbox-inline {
  position: relative;
  display: inline-block;
  padding-left: 20px;
  margin-bottom: 0;
  vertical-align: middle;
  font-weight: normal;
  cursor: pointer;
}
.radio-inline + .radio-inline,
.checkbox-inline + .checkbox-inline {
  margin-top: 0;
  margin-left: 10px;
}
input[type="radio"][disabled],
input[type="checkbox"][disabled],
input[type="radio"].disabled,
input[type="checkbox"].disabled,
fieldset[disabled] input[type="radio"],
fieldset[disabled] input[type="checkbox"] {
  cursor: not-allowed;
}
.radio-inline.disabled,
.checkbox-inline.disabled,
fieldset[disabled] .radio-inline,
fieldset[disabled] .checkbox-inline {
  cursor: not-allowed;
}
.radio.disabled label,
.checkbox.disabled label,
fieldset[disabled] .radio label,
fieldset[disabled] .checkbox label {
  cursor: not-allowed;
}
.form-control-static {
  padding-top: 7px;
  padding-bottom: 7px;
  margin-bottom: 0;
  min-height: 31px;
}
.form-control-static.input-lg,
.form-control-static.input-sm {
  padding-left: 0;
  padding-right: 0;
}
.input-sm {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-sm {
  height: 30px;
  line-height: 30px;
}
textarea.input-sm,
select[multiple].input-sm {
  height: auto;
}
.form-group-sm .form-control {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.form-group-sm select.form-control {
  height: 30px;
  line-height: 30px;
}
.form-group-sm textarea.form-control,
.form-group-sm select[multiple].form-control {
  height: auto;
}
.form-group-sm .form-control-static {
  height: 30px;
  min-height: 30px;
  padding: 6px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.input-lg {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-lg {
  height: 45px;
  line-height: 45px;
}
textarea.input-lg,
select[multiple].input-lg {
  height: auto;
}
.form-group-lg .form-control {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.form-group-lg select.form-control {
  height: 45px;
  line-height: 45px;
}
.form-group-lg textarea.form-control,
.form-group-lg select[multiple].form-control {
  height: auto;
}
.form-group-lg .form-control-static {
  height: 45px;
  min-height: 35px;
  padding: 11px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.has-feedback {
  position: relative;
}
.has-feedback .form-control {
  padding-right: 40px;
}
.form-control-feedback {
  position: absolute;
  top: 0;
  right: 0;
  z-index: 2;
  display: block;
  width: 32px;
  height: 32px;
  line-height: 32px;
  text-align: center;
  pointer-events: none;
}
.input-lg + .form-control-feedback,
.input-group-lg + .form-control-feedback,
.form-group-lg .form-control + .form-control-feedback {
  width: 45px;
  height: 45px;
  line-height: 45px;
}
.input-sm + .form-control-feedback,
.input-group-sm + .form-control-feedback,
.form-group-sm .form-control + .form-control-feedback {
  width: 30px;
  height: 30px;
  line-height: 30px;
}
.has-success .help-block,
.has-success .control-label,
.has-success .radio,
.has-success .checkbox,
.has-success .radio-inline,
.has-success .checkbox-inline,
.has-success.radio label,
.has-success.checkbox label,
.has-success.radio-inline label,
.has-success.checkbox-inline label {
  color: #3c763d;
}
.has-success .form-control {
  border-color: #3c763d;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-success .form-control:focus {
  border-color: #2b542c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
}
.has-success .input-group-addon {
  color: #3c763d;
  border-color: #3c763d;
  background-color: #dff0d8;
}
.has-success .form-control-feedback {
  color: #3c763d;
}
.has-warning .help-block,
.has-warning .control-label,
.has-warning .radio,
.has-warning .checkbox,
.has-warning .radio-inline,
.has-warning .checkbox-inline,
.has-warning.radio label,
.has-warning.checkbox label,
.has-warning.radio-inline label,
.has-warning.checkbox-inline label {
  color: #8a6d3b;
}
.has-warning .form-control {
  border-color: #8a6d3b;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-warning .form-control:focus {
  border-color: #66512c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
}
.has-warning .input-group-addon {
  color: #8a6d3b;
  border-color: #8a6d3b;
  background-color: #fcf8e3;
}
.has-warning .form-control-feedback {
  color: #8a6d3b;
}
.has-error .help-block,
.has-error .control-label,
.has-error .radio,
.has-error .checkbox,
.has-error .radio-inline,
.has-error .checkbox-inline,
.has-error.radio label,
.has-error.checkbox label,
.has-error.radio-inline label,
.has-error.checkbox-inline label {
  color: #a94442;
}
.has-error .form-control {
  border-color: #a94442;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-error .form-control:focus {
  border-color: #843534;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
}
.has-error .input-group-addon {
  color: #a94442;
  border-color: #a94442;
  background-color: #f2dede;
}
.has-error .form-control-feedback {
  color: #a94442;
}
.has-feedback label ~ .form-control-feedback {
  top: 23px;
}
.has-feedback label.sr-only ~ .form-control-feedback {
  top: 0;
}
.help-block {
  display: block;
  margin-top: 5px;
  margin-bottom: 10px;
  color: #404040;
}
@media (min-width: 768px) {
  .form-inline .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .form-inline .form-control-static {
    display: inline-block;
  }
  .form-inline .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .form-inline .input-group .input-group-addon,
  .form-inline .input-group .input-group-btn,
  .form-inline .input-group .form-control {
    width: auto;
  }
  .form-inline .input-group > .form-control {
    width: 100%;
  }
  .form-inline .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio,
  .form-inline .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio label,
  .form-inline .checkbox label {
    padding-left: 0;
  }
  .form-inline .radio input[type="radio"],
  .form-inline .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .form-inline .has-feedback .form-control-feedback {
    top: 0;
  }
}
.form-horizontal .radio,
.form-horizontal .checkbox,
.form-horizontal .radio-inline,
.form-horizontal .checkbox-inline {
  margin-top: 0;
  margin-bottom: 0;
  padding-top: 7px;
}
.form-horizontal .radio,
.form-horizontal .checkbox {
  min-height: 25px;
}
.form-horizontal .form-group {
  margin-left: 0px;
  margin-right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .control-label {
    text-align: right;
    margin-bottom: 0;
    padding-top: 7px;
  }
}
.form-horizontal .has-feedback .form-control-feedback {
  right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .form-group-lg .control-label {
    padding-top: 11px;
    font-size: 17px;
  }
}
@media (min-width: 768px) {
  .form-horizontal .form-group-sm .control-label {
    padding-top: 6px;
    font-size: 12px;
  }
}
.btn {
  display: inline-block;
  margin-bottom: 0;
  font-weight: normal;
  text-align: center;
  vertical-align: middle;
  touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  white-space: nowrap;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  border-radius: 2px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #333;
  text-decoration: none;
}
.btn:active,
.btn.active {
  outline: 0;
  background-image: none;
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  opacity: 0.65;
  filter: alpha(opacity=65);
  -webkit-box-shadow: none;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.btn-default:focus,
.btn-default.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.btn-default:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active:hover,
.btn-default.active:hover,
.open > .dropdown-toggle.btn-default:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.open > .dropdown-toggle.btn-default:focus,
.btn-default:active.focus,
.btn-default.active.focus,
.open > .dropdown-toggle.btn-default.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  background-image: none;
}
.btn-default.disabled:hover,
.btn-default[disabled]:hover,
fieldset[disabled] .btn-default:hover,
.btn-default.disabled:focus,
.btn-default[disabled]:focus,
fieldset[disabled] .btn-default:focus,
.btn-default.disabled.focus,
.btn-default[disabled].focus,
fieldset[disabled] .btn-default.focus {
  background-color: #fff;
  border-color: #ccc;
}
.btn-default .badge {
  color: #fff;
  background-color: #333;
}
.btn-primary {
  color: #fff;
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary:focus,
.btn-primary.focus {
  color: #fff;
  background-color: #286090;
  border-color: #122b40;
}
.btn-primary:hover {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active:hover,
.btn-primary.active:hover,
.open > .dropdown-toggle.btn-primary:hover,
.btn-primary:active:focus,
.btn-primary.active:focus,
.open > .dropdown-toggle.btn-primary:focus,
.btn-primary:active.focus,
.btn-primary.active.focus,
.open > .dropdown-toggle.btn-primary.focus {
  color: #fff;
  background-color: #204d74;
  border-color: #122b40;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  background-image: none;
}
.btn-primary.disabled:hover,
.btn-primary[disabled]:hover,
fieldset[disabled] .btn-primary:hover,
.btn-primary.disabled:focus,
.btn-primary[disabled]:focus,
fieldset[disabled] .btn-primary:focus,
.btn-primary.disabled.focus,
.btn-primary[disabled].focus,
fieldset[disabled] .btn-primary.focus {
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary .badge {
  color: #337ab7;
  background-color: #fff;
}
.btn-success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success:focus,
.btn-success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.btn-success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active:hover,
.btn-success.active:hover,
.open > .dropdown-toggle.btn-success:hover,
.btn-success:active:focus,
.btn-success.active:focus,
.open > .dropdown-toggle.btn-success:focus,
.btn-success:active.focus,
.btn-success.active.focus,
.open > .dropdown-toggle.btn-success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  background-image: none;
}
.btn-success.disabled:hover,
.btn-success[disabled]:hover,
fieldset[disabled] .btn-success:hover,
.btn-success.disabled:focus,
.btn-success[disabled]:focus,
fieldset[disabled] .btn-success:focus,
.btn-success.disabled.focus,
.btn-success[disabled].focus,
fieldset[disabled] .btn-success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.btn-info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info:focus,
.btn-info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.btn-info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active:hover,
.btn-info.active:hover,
.open > .dropdown-toggle.btn-info:hover,
.btn-info:active:focus,
.btn-info.active:focus,
.open > .dropdown-toggle.btn-info:focus,
.btn-info:active.focus,
.btn-info.active.focus,
.open > .dropdown-toggle.btn-info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  background-image: none;
}
.btn-info.disabled:hover,
.btn-info[disabled]:hover,
fieldset[disabled] .btn-info:hover,
.btn-info.disabled:focus,
.btn-info[disabled]:focus,
fieldset[disabled] .btn-info:focus,
.btn-info.disabled.focus,
.btn-info[disabled].focus,
fieldset[disabled] .btn-info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.btn-warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning:focus,
.btn-warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.btn-warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active:hover,
.btn-warning.active:hover,
.open > .dropdown-toggle.btn-warning:hover,
.btn-warning:active:focus,
.btn-warning.active:focus,
.open > .dropdown-toggle.btn-warning:focus,
.btn-warning:active.focus,
.btn-warning.active.focus,
.open > .dropdown-toggle.btn-warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  background-image: none;
}
.btn-warning.disabled:hover,
.btn-warning[disabled]:hover,
fieldset[disabled] .btn-warning:hover,
.btn-warning.disabled:focus,
.btn-warning[disabled]:focus,
fieldset[disabled] .btn-warning:focus,
.btn-warning.disabled.focus,
.btn-warning[disabled].focus,
fieldset[disabled] .btn-warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.btn-danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger:focus,
.btn-danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.btn-danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active:hover,
.btn-danger.active:hover,
.open > .dropdown-toggle.btn-danger:hover,
.btn-danger:active:focus,
.btn-danger.active:focus,
.open > .dropdown-toggle.btn-danger:focus,
.btn-danger:active.focus,
.btn-danger.active.focus,
.open > .dropdown-toggle.btn-danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  background-image: none;
}
.btn-danger.disabled:hover,
.btn-danger[disabled]:hover,
fieldset[disabled] .btn-danger:hover,
.btn-danger.disabled:focus,
.btn-danger[disabled]:focus,
fieldset[disabled] .btn-danger:focus,
.btn-danger.disabled.focus,
.btn-danger[disabled].focus,
fieldset[disabled] .btn-danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger .badge {
  color: #d9534f;
  background-color: #fff;
}
.btn-link {
  color: #337ab7;
  font-weight: normal;
  border-radius: 0;
}
.btn-link,
.btn-link:active,
.btn-link.active,
.btn-link[disabled],
fieldset[disabled] .btn-link {
  background-color: transparent;
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn-link,
.btn-link:hover,
.btn-link:focus,
.btn-link:active {
  border-color: transparent;
}
.btn-link:hover,
.btn-link:focus {
  color: #23527c;
  text-decoration: underline;
  background-color: transparent;
}
.btn-link[disabled]:hover,
fieldset[disabled] .btn-link:hover,
.btn-link[disabled]:focus,
fieldset[disabled] .btn-link:focus {
  color: #777777;
  text-decoration: none;
}
.btn-lg,
.btn-group-lg > .btn {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.btn-sm,
.btn-group-sm > .btn {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-xs,
.btn-group-xs > .btn {
  padding: 1px 5px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-block {
  display: block;
  width: 100%;
}
.btn-block + .btn-block {
  margin-top: 5px;
}
input[type="submit"].btn-block,
input[type="reset"].btn-block,
input[type="button"].btn-block {
  width: 100%;
}
.fade {
  opacity: 0;
  -webkit-transition: opacity 0.15s linear;
  -o-transition: opacity 0.15s linear;
  transition: opacity 0.15s linear;
}
.fade.in {
  opacity: 1;
}
.collapse {
  display: none;
}
.collapse.in {
  display: block;
}
tr.collapse.in {
  display: table-row;
}
tbody.collapse.in {
  display: table-row-group;
}
.collapsing {
  position: relative;
  height: 0;
  overflow: hidden;
  -webkit-transition-property: height, visibility;
  transition-property: height, visibility;
  -webkit-transition-duration: 0.35s;
  transition-duration: 0.35s;
  -webkit-transition-timing-function: ease;
  transition-timing-function: ease;
}
.caret {
  display: inline-block;
  width: 0;
  height: 0;
  margin-left: 2px;
  vertical-align: middle;
  border-top: 4px dashed;
  border-top: 4px solid \9;
  border-right: 4px solid transparent;
  border-left: 4px solid transparent;
}
.dropup,
.dropdown {
  position: relative;
}
.dropdown-toggle:focus {
  outline: 0;
}
.dropdown-menu {
  position: absolute;
  top: 100%;
  left: 0;
  z-index: 1000;
  display: none;
  float: left;
  min-width: 160px;
  padding: 5px 0;
  margin: 2px 0 0;
  list-style: none;
  font-size: 13px;
  text-align: left;
  background-color: #fff;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.15);
  border-radius: 2px;
  -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  background-clip: padding-box;
}
.dropdown-menu.pull-right {
  right: 0;
  left: auto;
}
.dropdown-menu .divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: normal;
  line-height: 1.42857143;
  color: #333333;
  white-space: nowrap;
}
.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  text-decoration: none;
  color: #262626;
  background-color: #f5f5f5;
}
.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #fff;
  text-decoration: none;
  outline: 0;
  background-color: #337ab7;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #777777;
}
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
  cursor: not-allowed;
}
.open > .dropdown-menu {
  display: block;
}
.open > a {
  outline: 0;
}
.dropdown-menu-right {
  left: auto;
  right: 0;
}
.dropdown-menu-left {
  left: 0;
  right: auto;
}
.dropdown-header {
  display: block;
  padding: 3px 20px;
  font-size: 12px;
  line-height: 1.42857143;
  color: #777777;
  white-space: nowrap;
}
.dropdown-backdrop {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  top: 0;
  z-index: 990;
}
.pull-right > .dropdown-menu {
  right: 0;
  left: auto;
}
.dropup .caret,
.navbar-fixed-bottom .dropdown .caret {
  border-top: 0;
  border-bottom: 4px dashed;
  border-bottom: 4px solid \9;
  content: "";
}
.dropup .dropdown-menu,
.navbar-fixed-bottom .dropdown .dropdown-menu {
  top: auto;
  bottom: 100%;
  margin-bottom: 2px;
}
@media (min-width: 541px) {
  .navbar-right .dropdown-menu {
    left: auto;
    right: 0;
  }
  .navbar-right .dropdown-menu-left {
    left: 0;
    right: auto;
  }
}
.btn-group,
.btn-group-vertical {
  position: relative;
  display: inline-block;
  vertical-align: middle;
}
.btn-group > .btn,
.btn-group-vertical > .btn {
  position: relative;
  float: left;
}
.btn-group > .btn:hover,
.btn-group-vertical > .btn:hover,
.btn-group > .btn:focus,
.btn-group-vertical > .btn:focus,
.btn-group > .btn:active,
.btn-group-vertical > .btn:active,
.btn-group > .btn.active,
.btn-group-vertical > .btn.active {
  z-index: 2;
}
.btn-group .btn + .btn,
.btn-group .btn + .btn-group,
.btn-group .btn-group + .btn,
.btn-group .btn-group + .btn-group {
  margin-left: -1px;
}
.btn-toolbar {
  margin-left: -5px;
}
.btn-toolbar .btn,
.btn-toolbar .btn-group,
.btn-toolbar .input-group {
  float: left;
}
.btn-toolbar > .btn,
.btn-toolbar > .btn-group,
.btn-toolbar > .input-group {
  margin-left: 5px;
}
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-left: 8px;
  padding-right: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-left: 12px;
  padding-right: 12px;
}
.btn-group.open .dropdown-toggle {
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn .caret {
  margin-left: 0;
}
.btn-lg .caret {
  border-width: 5px 5px 0;
  border-bottom-width: 0;
}
.dropup .btn-lg .caret {
  border-width: 0 5px 5px;
}
.btn-group-vertical > .btn,
.btn-group-vertical > .btn-group,
.btn-group-vertical > .btn-group > .btn {
  display: block;
  float: none;
  width: 100%;
  max-width: 100%;
}
.btn-group-vertical > .btn-group > .btn {
  float: none;
}
.btn-group-vertical > .btn + .btn,
.btn-group-vertical > .btn + .btn-group,
.btn-group-vertical > .btn-group + .btn,
.btn-group-vertical > .btn-group + .btn-group {
  margin-top: -1px;
  margin-left: 0;
}
.btn-group-vertical > .btn:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.btn-group-vertical > .btn:first-child:not(:last-child) {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn:last-child:not(:first-child) {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
.btn-group-vertical > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.btn-group-justified {
  display: table;
  width: 100%;
  table-layout: fixed;
  border-collapse: separate;
}
.btn-group-justified > .btn,
.btn-group-justified > .btn-group {
  float: none;
  display: table-cell;
  width: 1%;
}
.btn-group-justified > .btn-group .btn {
  width: 100%;
}
.btn-group-justified > .btn-group .dropdown-menu {
  left: auto;
}
[data-toggle="buttons"] > .btn input[type="radio"],
[data-toggle="buttons"] > .btn-group > .btn input[type="radio"],
[data-toggle="buttons"] > .btn input[type="checkbox"],
[data-toggle="buttons"] > .btn-group > .btn input[type="checkbox"] {
  position: absolute;
  clip: rect(0, 0, 0, 0);
  pointer-events: none;
}
.input-group {
  position: relative;
  display: table;
  border-collapse: separate;
}
.input-group[class*="col-"] {
  float: none;
  padding-left: 0;
  padding-right: 0;
}
.input-group .form-control {
  position: relative;
  z-index: 2;
  float: left;
  width: 100%;
  margin-bottom: 0;
}
.input-group .form-control:focus {
  z-index: 3;
}
.input-group-lg > .form-control,
.input-group-lg > .input-group-addon,
.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-group-lg > .form-control,
select.input-group-lg > .input-group-addon,
select.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  line-height: 45px;
}
textarea.input-group-lg > .form-control,
textarea.input-group-lg > .input-group-addon,
textarea.input-group-lg > .input-group-btn > .btn,
select[multiple].input-group-lg > .form-control,
select[multiple].input-group-lg > .input-group-addon,
select[multiple].input-group-lg > .input-group-btn > .btn {
  height: auto;
}
.input-group-sm > .form-control,
.input-group-sm > .input-group-addon,
.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-group-sm > .form-control,
select.input-group-sm > .input-group-addon,
select.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  line-height: 30px;
}
textarea.input-group-sm > .form-control,
textarea.input-group-sm > .input-group-addon,
textarea.input-group-sm > .input-group-btn > .btn,
select[multiple].input-group-sm > .form-control,
select[multiple].input-group-sm > .input-group-addon,
select[multiple].input-group-sm > .input-group-btn > .btn {
  height: auto;
}
.input-group-addon,
.input-group-btn,
.input-group .form-control {
  display: table-cell;
}
.input-group-addon:not(:first-child):not(:last-child),
.input-group-btn:not(:first-child):not(:last-child),
.input-group .form-control:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.input-group-addon,
.input-group-btn {
  width: 1%;
  white-space: nowrap;
  vertical-align: middle;
}
.input-group-addon {
  padding: 6px 12px;
  font-size: 13px;
  font-weight: normal;
  line-height: 1;
  color: #555555;
  text-align: center;
  background-color: #eeeeee;
  border: 1px solid #ccc;
  border-radius: 2px;
}
.input-group-addon.input-sm {
  padding: 5px 10px;
  font-size: 12px;
  border-radius: 1px;
}
.input-group-addon.input-lg {
  padding: 10px 16px;
  font-size: 17px;
  border-radius: 3px;
}
.input-group-addon input[type="radio"],
.input-group-addon input[type="checkbox"] {
  margin-top: 0;
}
.input-group .form-control:first-child,
.input-group-addon:first-child,
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group > .btn,
.input-group-btn:first-child > .dropdown-toggle,
.input-group-btn:last-child > .btn:not(:last-child):not(.dropdown-toggle),
.input-group-btn:last-child > .btn-group:not(:last-child) > .btn {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.input-group-addon:first-child {
  border-right: 0;
}
.input-group .form-control:last-child,
.input-group-addon:last-child,
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group > .btn,
.input-group-btn:last-child > .dropdown-toggle,
.input-group-btn:first-child > .btn:not(:first-child),
.input-group-btn:first-child > .btn-group:not(:first-child) > .btn {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.input-group-addon:last-child {
  border-left: 0;
}
.input-group-btn {
  position: relative;
  font-size: 0;
  white-space: nowrap;
}
.input-group-btn > .btn {
  position: relative;
}
.input-group-btn > .btn + .btn {
  margin-left: -1px;
}
.input-group-btn > .btn:hover,
.input-group-btn > .btn:focus,
.input-group-btn > .btn:active {
  z-index: 2;
}
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group {
  margin-right: -1px;
}
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group {
  z-index: 2;
  margin-left: -1px;
}
.nav {
  margin-bottom: 0;
  padding-left: 0;
  list-style: none;
}
.nav > li {
  position: relative;
  display: block;
}
.nav > li > a {
  position: relative;
  display: block;
  padding: 10px 15px;
}
.nav > li > a:hover,
.nav > li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.nav > li.disabled > a {
  color: #777777;
}
.nav > li.disabled > a:hover,
.nav > li.disabled > a:focus {
  color: #777777;
  text-decoration: none;
  background-color: transparent;
  cursor: not-allowed;
}
.nav .open > a,
.nav .open > a:hover,
.nav .open > a:focus {
  background-color: #eeeeee;
  border-color: #337ab7;
}
.nav .nav-divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.nav > li > a > img {
  max-width: none;
}
.nav-tabs {
  border-bottom: 1px solid #ddd;
}
.nav-tabs > li {
  float: left;
  margin-bottom: -1px;
}
.nav-tabs > li > a {
  margin-right: 2px;
  line-height: 1.42857143;
  border: 1px solid transparent;
  border-radius: 2px 2px 0 0;
}
.nav-tabs > li > a:hover {
  border-color: #eeeeee #eeeeee #ddd;
}
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a:focus {
  color: #555555;
  background-color: #fff;
  border: 1px solid #ddd;
  border-bottom-color: transparent;
  cursor: default;
}
.nav-tabs.nav-justified {
  width: 100%;
  border-bottom: 0;
}
.nav-tabs.nav-justified > li {
  float: none;
}
.nav-tabs.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-tabs.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-tabs.nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs.nav-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs.nav-justified > .active > a,
.nav-tabs.nav-justified > .active > a:hover,
.nav-tabs.nav-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs.nav-justified > .active > a,
  .nav-tabs.nav-justified > .active > a:hover,
  .nav-tabs.nav-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.nav-pills > li {
  float: left;
}
.nav-pills > li > a {
  border-radius: 2px;
}
.nav-pills > li + li {
  margin-left: 2px;
}
.nav-pills > li.active > a,
.nav-pills > li.active > a:hover,
.nav-pills > li.active > a:focus {
  color: #fff;
  background-color: #337ab7;
}
.nav-stacked > li {
  float: none;
}
.nav-stacked > li + li {
  margin-top: 2px;
  margin-left: 0;
}
.nav-justified {
  width: 100%;
}
.nav-justified > li {
  float: none;
}
.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs-justified {
  border-bottom: 0;
}
.nav-tabs-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs-justified > .active > a,
.nav-tabs-justified > .active > a:hover,
.nav-tabs-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs-justified > .active > a,
  .nav-tabs-justified > .active > a:hover,
  .nav-tabs-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.tab-content > .tab-pane {
  display: none;
}
.tab-content > .active {
  display: block;
}
.nav-tabs .dropdown-menu {
  margin-top: -1px;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar {
  position: relative;
  min-height: 30px;
  margin-bottom: 18px;
  border: 1px solid transparent;
}
@media (min-width: 541px) {
  .navbar {
    border-radius: 2px;
  }
}
@media (min-width: 541px) {
  .navbar-header {
    float: left;
  }
}
.navbar-collapse {
  overflow-x: visible;
  padding-right: 0px;
  padding-left: 0px;
  border-top: 1px solid transparent;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1);
  -webkit-overflow-scrolling: touch;
}
.navbar-collapse.in {
  overflow-y: auto;
}
@media (min-width: 541px) {
  .navbar-collapse {
    width: auto;
    border-top: 0;
    box-shadow: none;
  }
  .navbar-collapse.collapse {
    display: block !important;
    height: auto !important;
    padding-bottom: 0;
    overflow: visible !important;
  }
  .navbar-collapse.in {
    overflow-y: visible;
  }
  .navbar-fixed-top .navbar-collapse,
  .navbar-static-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    padding-left: 0;
    padding-right: 0;
  }
}
.navbar-fixed-top .navbar-collapse,
.navbar-fixed-bottom .navbar-collapse {
  max-height: 340px;
}
@media (max-device-width: 540px) and (orientation: landscape) {
  .navbar-fixed-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    max-height: 200px;
  }
}
.container > .navbar-header,
.container-fluid > .navbar-header,
.container > .navbar-collapse,
.container-fluid > .navbar-collapse {
  margin-right: 0px;
  margin-left: 0px;
}
@media (min-width: 541px) {
  .container > .navbar-header,
  .container-fluid > .navbar-header,
  .container > .navbar-collapse,
  .container-fluid > .navbar-collapse {
    margin-right: 0;
    margin-left: 0;
  }
}
.navbar-static-top {
  z-index: 1000;
  border-width: 0 0 1px;
}
@media (min-width: 541px) {
  .navbar-static-top {
    border-radius: 0;
  }
}
.navbar-fixed-top,
.navbar-fixed-bottom {
  position: fixed;
  right: 0;
  left: 0;
  z-index: 1030;
}
@media (min-width: 541px) {
  .navbar-fixed-top,
  .navbar-fixed-bottom {
    border-radius: 0;
  }
}
.navbar-fixed-top {
  top: 0;
  border-width: 0 0 1px;
}
.navbar-fixed-bottom {
  bottom: 0;
  margin-bottom: 0;
  border-width: 1px 0 0;
}
.navbar-brand {
  float: left;
  padding: 6px 0px;
  font-size: 17px;
  line-height: 18px;
  height: 30px;
}
.navbar-brand:hover,
.navbar-brand:focus {
  text-decoration: none;
}
.navbar-brand > img {
  display: block;
}
@media (min-width: 541px) {
  .navbar > .container .navbar-brand,
  .navbar > .container-fluid .navbar-brand {
    margin-left: 0px;
  }
}
.navbar-toggle {
  position: relative;
  float: right;
  margin-right: 0px;
  padding: 9px 10px;
  margin-top: -2px;
  margin-bottom: -2px;
  background-color: transparent;
  background-image: none;
  border: 1px solid transparent;
  border-radius: 2px;
}
.navbar-toggle:focus {
  outline: 0;
}
.navbar-toggle .icon-bar {
  display: block;
  width: 22px;
  height: 2px;
  border-radius: 1px;
}
.navbar-toggle .icon-bar + .icon-bar {
  margin-top: 4px;
}
@media (min-width: 541px) {
  .navbar-toggle {
    display: none;
  }
}
.navbar-nav {
  margin: 3px 0px;
}
.navbar-nav > li > a {
  padding-top: 10px;
  padding-bottom: 10px;
  line-height: 18px;
}
@media (max-width: 540px) {
  .navbar-nav .open .dropdown-menu {
    position: static;
    float: none;
    width: auto;
    margin-top: 0;
    background-color: transparent;
    border: 0;
    box-shadow: none;
  }
  .navbar-nav .open .dropdown-menu > li > a,
  .navbar-nav .open .dropdown-menu .dropdown-header {
    padding: 5px 15px 5px 25px;
  }
  .navbar-nav .open .dropdown-menu > li > a {
    line-height: 18px;
  }
  .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-nav .open .dropdown-menu > li > a:focus {
    background-image: none;
  }
}
@media (min-width: 541px) {
  .navbar-nav {
    float: left;
    margin: 0;
  }
  .navbar-nav > li {
    float: left;
  }
  .navbar-nav > li > a {
    padding-top: 6px;
    padding-bottom: 6px;
  }
}
.navbar-form {
  margin-left: 0px;
  margin-right: 0px;
  padding: 10px 0px;
  border-top: 1px solid transparent;
  border-bottom: 1px solid transparent;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  margin-top: -1px;
  margin-bottom: -1px;
}
@media (min-width: 768px) {
  .navbar-form .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .navbar-form .form-control-static {
    display: inline-block;
  }
  .navbar-form .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .navbar-form .input-group .input-group-addon,
  .navbar-form .input-group .input-group-btn,
  .navbar-form .input-group .form-control {
    width: auto;
  }
  .navbar-form .input-group > .form-control {
    width: 100%;
  }
  .navbar-form .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio,
  .navbar-form .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio label,
  .navbar-form .checkbox label {
    padding-left: 0;
  }
  .navbar-form .radio input[type="radio"],
  .navbar-form .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .navbar-form .has-feedback .form-control-feedback {
    top: 0;
  }
}
@media (max-width: 540px) {
  .navbar-form .form-group {
    margin-bottom: 5px;
  }
  .navbar-form .form-group:last-child {
    margin-bottom: 0;
  }
}
@media (min-width: 541px) {
  .navbar-form {
    width: auto;
    border: 0;
    margin-left: 0;
    margin-right: 0;
    padding-top: 0;
    padding-bottom: 0;
    -webkit-box-shadow: none;
    box-shadow: none;
  }
}
.navbar-nav > li > .dropdown-menu {
  margin-top: 0;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar-fixed-bottom .navbar-nav > li > .dropdown-menu {
  margin-bottom: 0;
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.navbar-btn {
  margin-top: -1px;
  margin-bottom: -1px;
}
.navbar-btn.btn-sm {
  margin-top: 0px;
  margin-bottom: 0px;
}
.navbar-btn.btn-xs {
  margin-top: 4px;
  margin-bottom: 4px;
}
.navbar-text {
  margin-top: 6px;
  margin-bottom: 6px;
}
@media (min-width: 541px) {
  .navbar-text {
    float: left;
    margin-left: 0px;
    margin-right: 0px;
  }
}
@media (min-width: 541px) {
  .navbar-left {
    float: left !important;
    float: left;
  }
  .navbar-right {
    float: right !important;
    float: right;
    margin-right: 0px;
  }
  .navbar-right ~ .navbar-right {
    margin-right: 0;
  }
}
.navbar-default {
  background-color: #f8f8f8;
  border-color: #e7e7e7;
}
.navbar-default .navbar-brand {
  color: #777;
}
.navbar-default .navbar-brand:hover,
.navbar-default .navbar-brand:focus {
  color: #5e5e5e;
  background-color: transparent;
}
.navbar-default .navbar-text {
  color: #777;
}
.navbar-default .navbar-nav > li > a {
  color: #777;
}
.navbar-default .navbar-nav > li > a:hover,
.navbar-default .navbar-nav > li > a:focus {
  color: #333;
  background-color: transparent;
}
.navbar-default .navbar-nav > .active > a,
.navbar-default .navbar-nav > .active > a:hover,
.navbar-default .navbar-nav > .active > a:focus {
  color: #555;
  background-color: #e7e7e7;
}
.navbar-default .navbar-nav > .disabled > a,
.navbar-default .navbar-nav > .disabled > a:hover,
.navbar-default .navbar-nav > .disabled > a:focus {
  color: #ccc;
  background-color: transparent;
}
.navbar-default .navbar-toggle {
  border-color: #ddd;
}
.navbar-default .navbar-toggle:hover,
.navbar-default .navbar-toggle:focus {
  background-color: #ddd;
}
.navbar-default .navbar-toggle .icon-bar {
  background-color: #888;
}
.navbar-default .navbar-collapse,
.navbar-default .navbar-form {
  border-color: #e7e7e7;
}
.navbar-default .navbar-nav > .open > a,
.navbar-default .navbar-nav > .open > a:hover,
.navbar-default .navbar-nav > .open > a:focus {
  background-color: #e7e7e7;
  color: #555;
}
@media (max-width: 540px) {
  .navbar-default .navbar-nav .open .dropdown-menu > li > a {
    color: #777;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #333;
    background-color: transparent;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #555;
    background-color: #e7e7e7;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #ccc;
    background-color: transparent;
  }
}
.navbar-default .navbar-link {
  color: #777;
}
.navbar-default .navbar-link:hover {
  color: #333;
}
.navbar-default .btn-link {
  color: #777;
}
.navbar-default .btn-link:hover,
.navbar-default .btn-link:focus {
  color: #333;
}
.navbar-default .btn-link[disabled]:hover,
fieldset[disabled] .navbar-default .btn-link:hover,
.navbar-default .btn-link[disabled]:focus,
fieldset[disabled] .navbar-default .btn-link:focus {
  color: #ccc;
}
.navbar-inverse {
  background-color: #222;
  border-color: #080808;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-brand:hover,
.navbar-inverse .navbar-brand:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-text {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a:hover,
.navbar-inverse .navbar-nav > li > a:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-nav > .active > a,
.navbar-inverse .navbar-nav > .active > a:hover,
.navbar-inverse .navbar-nav > .active > a:focus {
  color: #fff;
  background-color: #080808;
}
.navbar-inverse .navbar-nav > .disabled > a,
.navbar-inverse .navbar-nav > .disabled > a:hover,
.navbar-inverse .navbar-nav > .disabled > a:focus {
  color: #444;
  background-color: transparent;
}
.navbar-inverse .navbar-toggle {
  border-color: #333;
}
.navbar-inverse .navbar-toggle:hover,
.navbar-inverse .navbar-toggle:focus {
  background-color: #333;
}
.navbar-inverse .navbar-toggle .icon-bar {
  background-color: #fff;
}
.navbar-inverse .navbar-collapse,
.navbar-inverse .navbar-form {
  border-color: #101010;
}
.navbar-inverse .navbar-nav > .open > a,
.navbar-inverse .navbar-nav > .open > a:hover,
.navbar-inverse .navbar-nav > .open > a:focus {
  background-color: #080808;
  color: #fff;
}
@media (max-width: 540px) {
  .navbar-inverse .navbar-nav .open .dropdown-menu > .dropdown-header {
    border-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu .divider {
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a {
    color: #9d9d9d;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #fff;
    background-color: transparent;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #fff;
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #444;
    background-color: transparent;
  }
}
.navbar-inverse .navbar-link {
  color: #9d9d9d;
}
.navbar-inverse .navbar-link:hover {
  color: #fff;
}
.navbar-inverse .btn-link {
  color: #9d9d9d;
}
.navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link:focus {
  color: #fff;
}
.navbar-inverse .btn-link[disabled]:hover,
fieldset[disabled] .navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link[disabled]:focus,
fieldset[disabled] .navbar-inverse .btn-link:focus {
  color: #444;
}
.breadcrumb {
  padding: 8px 15px;
  margin-bottom: 18px;
  list-style: none;
  background-color: #f5f5f5;
  border-radius: 2px;
}
.breadcrumb > li {
  display: inline-block;
}
.breadcrumb > li + li:before {
  content: "/\00a0";
  padding: 0 5px;
  color: #5e5e5e;
}
.breadcrumb > .active {
  color: #777777;
}
.pagination {
  display: inline-block;
  padding-left: 0;
  margin: 18px 0;
  border-radius: 2px;
}
.pagination > li {
  display: inline;
}
.pagination > li > a,
.pagination > li > span {
  position: relative;
  float: left;
  padding: 6px 12px;
  line-height: 1.42857143;
  text-decoration: none;
  color: #337ab7;
  background-color: #fff;
  border: 1px solid #ddd;
  margin-left: -1px;
}
.pagination > li:first-child > a,
.pagination > li:first-child > span {
  margin-left: 0;
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.pagination > li:last-child > a,
.pagination > li:last-child > span {
  border-bottom-right-radius: 2px;
  border-top-right-radius: 2px;
}
.pagination > li > a:hover,
.pagination > li > span:hover,
.pagination > li > a:focus,
.pagination > li > span:focus {
  z-index: 2;
  color: #23527c;
  background-color: #eeeeee;
  border-color: #ddd;
}
.pagination > .active > a,
.pagination > .active > span,
.pagination > .active > a:hover,
.pagination > .active > span:hover,
.pagination > .active > a:focus,
.pagination > .active > span:focus {
  z-index: 3;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
  cursor: default;
}
.pagination > .disabled > span,
.pagination > .disabled > span:hover,
.pagination > .disabled > span:focus,
.pagination > .disabled > a,
.pagination > .disabled > a:hover,
.pagination > .disabled > a:focus {
  color: #777777;
  background-color: #fff;
  border-color: #ddd;
  cursor: not-allowed;
}
.pagination-lg > li > a,
.pagination-lg > li > span {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.pagination-lg > li:first-child > a,
.pagination-lg > li:first-child > span {
  border-bottom-left-radius: 3px;
  border-top-left-radius: 3px;
}
.pagination-lg > li:last-child > a,
.pagination-lg > li:last-child > span {
  border-bottom-right-radius: 3px;
  border-top-right-radius: 3px;
}
.pagination-sm > li > a,
.pagination-sm > li > span {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.pagination-sm > li:first-child > a,
.pagination-sm > li:first-child > span {
  border-bottom-left-radius: 1px;
  border-top-left-radius: 1px;
}
.pagination-sm > li:last-child > a,
.pagination-sm > li:last-child > span {
  border-bottom-right-radius: 1px;
  border-top-right-radius: 1px;
}
.pager {
  padding-left: 0;
  margin: 18px 0;
  list-style: none;
  text-align: center;
}
.pager li {
  display: inline;
}
.pager li > a,
.pager li > span {
  display: inline-block;
  padding: 5px 14px;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 15px;
}
.pager li > a:hover,
.pager li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.pager .next > a,
.pager .next > span {
  float: right;
}
.pager .previous > a,
.pager .previous > span {
  float: left;
}
.pager .disabled > a,
.pager .disabled > a:hover,
.pager .disabled > a:focus,
.pager .disabled > span {
  color: #777777;
  background-color: #fff;
  cursor: not-allowed;
}
.label {
  display: inline;
  padding: .2em .6em .3em;
  font-size: 75%;
  font-weight: bold;
  line-height: 1;
  color: #fff;
  text-align: center;
  white-space: nowrap;
  vertical-align: baseline;
  border-radius: .25em;
}
a.label:hover,
a.label:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.label:empty {
  display: none;
}
.btn .label {
  position: relative;
  top: -1px;
}
.label-default {
  background-color: #777777;
}
.label-default[href]:hover,
.label-default[href]:focus {
  background-color: #5e5e5e;
}
.label-primary {
  background-color: #337ab7;
}
.label-primary[href]:hover,
.label-primary[href]:focus {
  background-color: #286090;
}
.label-success {
  background-color: #5cb85c;
}
.label-success[href]:hover,
.label-success[href]:focus {
  background-color: #449d44;
}
.label-info {
  background-color: #5bc0de;
}
.label-info[href]:hover,
.label-info[href]:focus {
  background-color: #31b0d5;
}
.label-warning {
  background-color: #f0ad4e;
}
.label-warning[href]:hover,
.label-warning[href]:focus {
  background-color: #ec971f;
}
.label-danger {
  background-color: #d9534f;
}
.label-danger[href]:hover,
.label-danger[href]:focus {
  background-color: #c9302c;
}
.badge {
  display: inline-block;
  min-width: 10px;
  padding: 3px 7px;
  font-size: 12px;
  font-weight: bold;
  color: #fff;
  line-height: 1;
  vertical-align: middle;
  white-space: nowrap;
  text-align: center;
  background-color: #777777;
  border-radius: 10px;
}
.badge:empty {
  display: none;
}
.btn .badge {
  position: relative;
  top: -1px;
}
.btn-xs .badge,
.btn-group-xs > .btn .badge {
  top: 0;
  padding: 1px 5px;
}
a.badge:hover,
a.badge:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.list-group-item.active > .badge,
.nav-pills > .active > a > .badge {
  color: #337ab7;
  background-color: #fff;
}
.list-group-item > .badge {
  float: right;
}
.list-group-item > .badge + .badge {
  margin-right: 5px;
}
.nav-pills > li > a > .badge {
  margin-left: 3px;
}
.jumbotron {
  padding-top: 30px;
  padding-bottom: 30px;
  margin-bottom: 30px;
  color: inherit;
  background-color: #eeeeee;
}
.jumbotron h1,
.jumbotron .h1 {
  color: inherit;
}
.jumbotron p {
  margin-bottom: 15px;
  font-size: 20px;
  font-weight: 200;
}
.jumbotron > hr {
  border-top-color: #d5d5d5;
}
.container .jumbotron,
.container-fluid .jumbotron {
  border-radius: 3px;
  padding-left: 0px;
  padding-right: 0px;
}
.jumbotron .container {
  max-width: 100%;
}
@media screen and (min-width: 768px) {
  .jumbotron {
    padding-top: 48px;
    padding-bottom: 48px;
  }
  .container .jumbotron,
  .container-fluid .jumbotron {
    padding-left: 60px;
    padding-right: 60px;
  }
  .jumbotron h1,
  .jumbotron .h1 {
    font-size: 59px;
  }
}
.thumbnail {
  display: block;
  padding: 4px;
  margin-bottom: 18px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: border 0.2s ease-in-out;
  -o-transition: border 0.2s ease-in-out;
  transition: border 0.2s ease-in-out;
}
.thumbnail > img,
.thumbnail a > img {
  margin-left: auto;
  margin-right: auto;
}
a.thumbnail:hover,
a.thumbnail:focus,
a.thumbnail.active {
  border-color: #337ab7;
}
.thumbnail .caption {
  padding: 9px;
  color: #000;
}
.alert {
  padding: 15px;
  margin-bottom: 18px;
  border: 1px solid transparent;
  border-radius: 2px;
}
.alert h4 {
  margin-top: 0;
  color: inherit;
}
.alert .alert-link {
  font-weight: bold;
}
.alert > p,
.alert > ul {
  margin-bottom: 0;
}
.alert > p + p {
  margin-top: 5px;
}
.alert-dismissable,
.alert-dismissible {
  padding-right: 35px;
}
.alert-dismissable .close,
.alert-dismissible .close {
  position: relative;
  top: -2px;
  right: -21px;
  color: inherit;
}
.alert-success {
  background-color: #dff0d8;
  border-color: #d6e9c6;
  color: #3c763d;
}
.alert-success hr {
  border-top-color: #c9e2b3;
}
.alert-success .alert-link {
  color: #2b542c;
}
.alert-info {
  background-color: #d9edf7;
  border-color: #bce8f1;
  color: #31708f;
}
.alert-info hr {
  border-top-color: #a6e1ec;
}
.alert-info .alert-link {
  color: #245269;
}
.alert-warning {
  background-color: #fcf8e3;
  border-color: #faebcc;
  color: #8a6d3b;
}
.alert-warning hr {
  border-top-color: #f7e1b5;
}
.alert-warning .alert-link {
  color: #66512c;
}
.alert-danger {
  background-color: #f2dede;
  border-color: #ebccd1;
  color: #a94442;
}
.alert-danger hr {
  border-top-color: #e4b9c0;
}
.alert-danger .alert-link {
  color: #843534;
}
@-webkit-keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
@keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
.progress {
  overflow: hidden;
  height: 18px;
  margin-bottom: 18px;
  background-color: #f5f5f5;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
}
.progress-bar {
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 18px;
  color: #fff;
  text-align: center;
  background-color: #337ab7;
  -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  -webkit-transition: width 0.6s ease;
  -o-transition: width 0.6s ease;
  transition: width 0.6s ease;
}
.progress-striped .progress-bar,
.progress-bar-striped {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-size: 40px 40px;
}
.progress.active .progress-bar,
.progress-bar.active {
  -webkit-animation: progress-bar-stripes 2s linear infinite;
  -o-animation: progress-bar-stripes 2s linear infinite;
  animation: progress-bar-stripes 2s linear infinite;
}
.progress-bar-success {
  background-color: #5cb85c;
}
.progress-striped .progress-bar-success {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-info {
  background-color: #5bc0de;
}
.progress-striped .progress-bar-info {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-warning {
  background-color: #f0ad4e;
}
.progress-striped .progress-bar-warning {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-danger {
  background-color: #d9534f;
}
.progress-striped .progress-bar-danger {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.media {
  margin-top: 15px;
}
.media:first-child {
  margin-top: 0;
}
.media,
.media-body {
  zoom: 1;
  overflow: hidden;
}
.media-body {
  width: 10000px;
}
.media-object {
  display: block;
}
.media-object.img-thumbnail {
  max-width: none;
}
.media-right,
.media > .pull-right {
  padding-left: 10px;
}
.media-left,
.media > .pull-left {
  padding-right: 10px;
}
.media-left,
.media-right,
.media-body {
  display: table-cell;
  vertical-align: top;
}
.media-middle {
  vertical-align: middle;
}
.media-bottom {
  vertical-align: bottom;
}
.media-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.media-list {
  padding-left: 0;
  list-style: none;
}
.list-group {
  margin-bottom: 20px;
  padding-left: 0;
}
.list-group-item {
  position: relative;
  display: block;
  padding: 10px 15px;
  margin-bottom: -1px;
  background-color: #fff;
  border: 1px solid #ddd;
}
.list-group-item:first-child {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
}
.list-group-item:last-child {
  margin-bottom: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
a.list-group-item,
button.list-group-item {
  color: #555;
}
a.list-group-item .list-group-item-heading,
button.list-group-item .list-group-item-heading {
  color: #333;
}
a.list-group-item:hover,
button.list-group-item:hover,
a.list-group-item:focus,
button.list-group-item:focus {
  text-decoration: none;
  color: #555;
  background-color: #f5f5f5;
}
button.list-group-item {
  width: 100%;
  text-align: left;
}
.list-group-item.disabled,
.list-group-item.disabled:hover,
.list-group-item.disabled:focus {
  background-color: #eeeeee;
  color: #777777;
  cursor: not-allowed;
}
.list-group-item.disabled .list-group-item-heading,
.list-group-item.disabled:hover .list-group-item-heading,
.list-group-item.disabled:focus .list-group-item-heading {
  color: inherit;
}
.list-group-item.disabled .list-group-item-text,
.list-group-item.disabled:hover .list-group-item-text,
.list-group-item.disabled:focus .list-group-item-text {
  color: #777777;
}
.list-group-item.active,
.list-group-item.active:hover,
.list-group-item.active:focus {
  z-index: 2;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.list-group-item.active .list-group-item-heading,
.list-group-item.active:hover .list-group-item-heading,
.list-group-item.active:focus .list-group-item-heading,
.list-group-item.active .list-group-item-heading > small,
.list-group-item.active:hover .list-group-item-heading > small,
.list-group-item.active:focus .list-group-item-heading > small,
.list-group-item.active .list-group-item-heading > .small,
.list-group-item.active:hover .list-group-item-heading > .small,
.list-group-item.active:focus .list-group-item-heading > .small {
  color: inherit;
}
.list-group-item.active .list-group-item-text,
.list-group-item.active:hover .list-group-item-text,
.list-group-item.active:focus .list-group-item-text {
  color: #c7ddef;
}
.list-group-item-success {
  color: #3c763d;
  background-color: #dff0d8;
}
a.list-group-item-success,
button.list-group-item-success {
  color: #3c763d;
}
a.list-group-item-success .list-group-item-heading,
button.list-group-item-success .list-group-item-heading {
  color: inherit;
}
a.list-group-item-success:hover,
button.list-group-item-success:hover,
a.list-group-item-success:focus,
button.list-group-item-success:focus {
  color: #3c763d;
  background-color: #d0e9c6;
}
a.list-group-item-success.active,
button.list-group-item-success.active,
a.list-group-item-success.active:hover,
button.list-group-item-success.active:hover,
a.list-group-item-success.active:focus,
button.list-group-item-success.active:focus {
  color: #fff;
  background-color: #3c763d;
  border-color: #3c763d;
}
.list-group-item-info {
  color: #31708f;
  background-color: #d9edf7;
}
a.list-group-item-info,
button.list-group-item-info {
  color: #31708f;
}
a.list-group-item-info .list-group-item-heading,
button.list-group-item-info .list-group-item-heading {
  color: inherit;
}
a.list-group-item-info:hover,
button.list-group-item-info:hover,
a.list-group-item-info:focus,
button.list-group-item-info:focus {
  color: #31708f;
  background-color: #c4e3f3;
}
a.list-group-item-info.active,
button.list-group-item-info.active,
a.list-group-item-info.active:hover,
button.list-group-item-info.active:hover,
a.list-group-item-info.active:focus,
button.list-group-item-info.active:focus {
  color: #fff;
  background-color: #31708f;
  border-color: #31708f;
}
.list-group-item-warning {
  color: #8a6d3b;
  background-color: #fcf8e3;
}
a.list-group-item-warning,
button.list-group-item-warning {
  color: #8a6d3b;
}
a.list-group-item-warning .list-group-item-heading,
button.list-group-item-warning .list-group-item-heading {
  color: inherit;
}
a.list-group-item-warning:hover,
button.list-group-item-warning:hover,
a.list-group-item-warning:focus,
button.list-group-item-warning:focus {
  color: #8a6d3b;
  background-color: #faf2cc;
}
a.list-group-item-warning.active,
button.list-group-item-warning.active,
a.list-group-item-warning.active:hover,
button.list-group-item-warning.active:hover,
a.list-group-item-warning.active:focus,
button.list-group-item-warning.active:focus {
  color: #fff;
  background-color: #8a6d3b;
  border-color: #8a6d3b;
}
.list-group-item-danger {
  color: #a94442;
  background-color: #f2dede;
}
a.list-group-item-danger,
button.list-group-item-danger {
  color: #a94442;
}
a.list-group-item-danger .list-group-item-heading,
button.list-group-item-danger .list-group-item-heading {
  color: inherit;
}
a.list-group-item-danger:hover,
button.list-group-item-danger:hover,
a.list-group-item-danger:focus,
button.list-group-item-danger:focus {
  color: #a94442;
  background-color: #ebcccc;
}
a.list-group-item-danger.active,
button.list-group-item-danger.active,
a.list-group-item-danger.active:hover,
button.list-group-item-danger.active:hover,
a.list-group-item-danger.active:focus,
button.list-group-item-danger.active:focus {
  color: #fff;
  background-color: #a94442;
  border-color: #a94442;
}
.list-group-item-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.list-group-item-text {
  margin-bottom: 0;
  line-height: 1.3;
}
.panel {
  margin-bottom: 18px;
  background-color: #fff;
  border: 1px solid transparent;
  border-radius: 2px;
  -webkit-box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
}
.panel-body {
  padding: 15px;
}
.panel-heading {
  padding: 10px 15px;
  border-bottom: 1px solid transparent;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel-heading > .dropdown .dropdown-toggle {
  color: inherit;
}
.panel-title {
  margin-top: 0;
  margin-bottom: 0;
  font-size: 15px;
  color: inherit;
}
.panel-title > a,
.panel-title > small,
.panel-title > .small,
.panel-title > small > a,
.panel-title > .small > a {
  color: inherit;
}
.panel-footer {
  padding: 10px 15px;
  background-color: #f5f5f5;
  border-top: 1px solid #ddd;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .list-group,
.panel > .panel-collapse > .list-group {
  margin-bottom: 0;
}
.panel > .list-group .list-group-item,
.panel > .panel-collapse > .list-group .list-group-item {
  border-width: 1px 0;
  border-radius: 0;
}
.panel > .list-group:first-child .list-group-item:first-child,
.panel > .panel-collapse > .list-group:first-child .list-group-item:first-child {
  border-top: 0;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .list-group:last-child .list-group-item:last-child,
.panel > .panel-collapse > .list-group:last-child .list-group-item:last-child {
  border-bottom: 0;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .panel-heading + .panel-collapse > .list-group .list-group-item:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.panel-heading + .list-group .list-group-item:first-child {
  border-top-width: 0;
}
.list-group + .panel-footer {
  border-top-width: 0;
}
.panel > .table,
.panel > .table-responsive > .table,
.panel > .panel-collapse > .table {
  margin-bottom: 0;
}
.panel > .table caption,
.panel > .table-responsive > .table caption,
.panel > .panel-collapse > .table caption {
  padding-left: 15px;
  padding-right: 15px;
}
.panel > .table:first-child,
.panel > .table-responsive:first-child > .table:first-child {
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child {
  border-top-left-radius: 1px;
  border-top-right-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:first-child {
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:last-child {
  border-top-right-radius: 1px;
}
.panel > .table:last-child,
.panel > .table-responsive:last-child > .table:last-child {
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child {
  border-bottom-left-radius: 1px;
  border-bottom-right-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:first-child {
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:last-child {
  border-bottom-right-radius: 1px;
}
.panel > .panel-body + .table,
.panel > .panel-body + .table-responsive,
.panel > .table + .panel-body,
.panel > .table-responsive + .panel-body {
  border-top: 1px solid #ddd;
}
.panel > .table > tbody:first-child > tr:first-child th,
.panel > .table > tbody:first-child > tr:first-child td {
  border-top: 0;
}
.panel > .table-bordered,
.panel > .table-responsive > .table-bordered {
  border: 0;
}
.panel > .table-bordered > thead > tr > th:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:first-child,
.panel > .table-bordered > tbody > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:first-child,
.panel > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-bordered > thead > tr > td:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:first-child,
.panel > .table-bordered > tbody > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:first-child,
.panel > .table-bordered > tfoot > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:first-child {
  border-left: 0;
}
.panel > .table-bordered > thead > tr > th:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:last-child,
.panel > .table-bordered > tbody > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:last-child,
.panel > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-bordered > thead > tr > td:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:last-child,
.panel > .table-bordered > tbody > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:last-child,
.panel > .table-bordered > tfoot > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:last-child {
  border-right: 0;
}
.panel > .table-bordered > thead > tr:first-child > td,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > td,
.panel > .table-bordered > tbody > tr:first-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > td,
.panel > .table-bordered > thead > tr:first-child > th,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > th,
.panel > .table-bordered > tbody > tr:first-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > th {
  border-bottom: 0;
}
.panel > .table-bordered > tbody > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > td,
.panel > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-bordered > tbody > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > th,
.panel > .table-bordered > tfoot > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > th {
  border-bottom: 0;
}
.panel > .table-responsive {
  border: 0;
  margin-bottom: 0;
}
.panel-group {
  margin-bottom: 18px;
}
.panel-group .panel {
  margin-bottom: 0;
  border-radius: 2px;
}
.panel-group .panel + .panel {
  margin-top: 5px;
}
.panel-group .panel-heading {
  border-bottom: 0;
}
.panel-group .panel-heading + .panel-collapse > .panel-body,
.panel-group .panel-heading + .panel-collapse > .list-group {
  border-top: 1px solid #ddd;
}
.panel-group .panel-footer {
  border-top: 0;
}
.panel-group .panel-footer + .panel-collapse .panel-body {
  border-bottom: 1px solid #ddd;
}
.panel-default {
  border-color: #ddd;
}
.panel-default > .panel-heading {
  color: #333333;
  background-color: #f5f5f5;
  border-color: #ddd;
}
.panel-default > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ddd;
}
.panel-default > .panel-heading .badge {
  color: #f5f5f5;
  background-color: #333333;
}
.panel-default > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ddd;
}
.panel-primary {
  border-color: #337ab7;
}
.panel-primary > .panel-heading {
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.panel-primary > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #337ab7;
}
.panel-primary > .panel-heading .badge {
  color: #337ab7;
  background-color: #fff;
}
.panel-primary > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #337ab7;
}
.panel-success {
  border-color: #d6e9c6;
}
.panel-success > .panel-heading {
  color: #3c763d;
  background-color: #dff0d8;
  border-color: #d6e9c6;
}
.panel-success > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #d6e9c6;
}
.panel-success > .panel-heading .badge {
  color: #dff0d8;
  background-color: #3c763d;
}
.panel-success > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #d6e9c6;
}
.panel-info {
  border-color: #bce8f1;
}
.panel-info > .panel-heading {
  color: #31708f;
  background-color: #d9edf7;
  border-color: #bce8f1;
}
.panel-info > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #bce8f1;
}
.panel-info > .panel-heading .badge {
  color: #d9edf7;
  background-color: #31708f;
}
.panel-info > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #bce8f1;
}
.panel-warning {
  border-color: #faebcc;
}
.panel-warning > .panel-heading {
  color: #8a6d3b;
  background-color: #fcf8e3;
  border-color: #faebcc;
}
.panel-warning > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #faebcc;
}
.panel-warning > .panel-heading .badge {
  color: #fcf8e3;
  background-color: #8a6d3b;
}
.panel-warning > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #faebcc;
}
.panel-danger {
  border-color: #ebccd1;
}
.panel-danger > .panel-heading {
  color: #a94442;
  background-color: #f2dede;
  border-color: #ebccd1;
}
.panel-danger > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ebccd1;
}
.panel-danger > .panel-heading .badge {
  color: #f2dede;
  background-color: #a94442;
}
.panel-danger > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ebccd1;
}
.embed-responsive {
  position: relative;
  display: block;
  height: 0;
  padding: 0;
  overflow: hidden;
}
.embed-responsive .embed-responsive-item,
.embed-responsive iframe,
.embed-responsive embed,
.embed-responsive object,
.embed-responsive video {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  height: 100%;
  width: 100%;
  border: 0;
}
.embed-responsive-16by9 {
  padding-bottom: 56.25%;
}
.embed-responsive-4by3 {
  padding-bottom: 75%;
}
.well {
  min-height: 20px;
  padding: 19px;
  margin-bottom: 20px;
  background-color: #f5f5f5;
  border: 1px solid #e3e3e3;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
}
.well blockquote {
  border-color: #ddd;
  border-color: rgba(0, 0, 0, 0.15);
}
.well-lg {
  padding: 24px;
  border-radius: 3px;
}
.well-sm {
  padding: 9px;
  border-radius: 1px;
}
.close {
  float: right;
  font-size: 19.5px;
  font-weight: bold;
  line-height: 1;
  color: #000;
  text-shadow: 0 1px 0 #fff;
  opacity: 0.2;
  filter: alpha(opacity=20);
}
.close:hover,
.close:focus {
  color: #000;
  text-decoration: none;
  cursor: pointer;
  opacity: 0.5;
  filter: alpha(opacity=50);
}
button.close {
  padding: 0;
  cursor: pointer;
  background: transparent;
  border: 0;
  -webkit-appearance: none;
}
.modal-open {
  overflow: hidden;
}
.modal {
  display: none;
  overflow: hidden;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1050;
  -webkit-overflow-scrolling: touch;
  outline: 0;
}
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, -25%);
  -ms-transform: translate(0, -25%);
  -o-transform: translate(0, -25%);
  transform: translate(0, -25%);
  -webkit-transition: -webkit-transform 0.3s ease-out;
  -moz-transition: -moz-transform 0.3s ease-out;
  -o-transition: -o-transform 0.3s ease-out;
  transition: transform 0.3s ease-out;
}
.modal.in .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
.modal-open .modal {
  overflow-x: hidden;
  overflow-y: auto;
}
.modal-dialog {
  position: relative;
  width: auto;
  margin: 10px;
}
.modal-content {
  position: relative;
  background-color: #fff;
  border: 1px solid #999;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  background-clip: padding-box;
  outline: 0;
}
.modal-backdrop {
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1040;
  background-color: #000;
}
.modal-backdrop.fade {
  opacity: 0;
  filter: alpha(opacity=0);
}
.modal-backdrop.in {
  opacity: 0.5;
  filter: alpha(opacity=50);
}
.modal-header {
  padding: 15px;
  border-bottom: 1px solid #e5e5e5;
}
.modal-header .close {
  margin-top: -2px;
}
.modal-title {
  margin: 0;
  line-height: 1.42857143;
}
.modal-body {
  position: relative;
  padding: 15px;
}
.modal-footer {
  padding: 15px;
  text-align: right;
  border-top: 1px solid #e5e5e5;
}
.modal-footer .btn + .btn {
  margin-left: 5px;
  margin-bottom: 0;
}
.modal-footer .btn-group .btn + .btn {
  margin-left: -1px;
}
.modal-footer .btn-block + .btn-block {
  margin-left: 0;
}
.modal-scrollbar-measure {
  position: absolute;
  top: -9999px;
  width: 50px;
  height: 50px;
  overflow: scroll;
}
@media (min-width: 768px) {
  .modal-dialog {
    width: 600px;
    margin: 30px auto;
  }
  .modal-content {
    -webkit-box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
  }
  .modal-sm {
    width: 300px;
  }
}
@media (min-width: 992px) {
  .modal-lg {
    width: 900px;
  }
}
.tooltip {
  position: absolute;
  z-index: 1070;
  display: block;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 12px;
  opacity: 0;
  filter: alpha(opacity=0);
}
.tooltip.in {
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.tooltip.top {
  margin-top: -3px;
  padding: 5px 0;
}
.tooltip.right {
  margin-left: 3px;
  padding: 0 5px;
}
.tooltip.bottom {
  margin-top: 3px;
  padding: 5px 0;
}
.tooltip.left {
  margin-left: -3px;
  padding: 0 5px;
}
.tooltip-inner {
  max-width: 200px;
  padding: 3px 8px;
  color: #fff;
  text-align: center;
  background-color: #000;
  border-radius: 2px;
}
.tooltip-arrow {
  position: absolute;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.tooltip.top .tooltip-arrow {
  bottom: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-left .tooltip-arrow {
  bottom: 0;
  right: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-right .tooltip-arrow {
  bottom: 0;
  left: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.right .tooltip-arrow {
  top: 50%;
  left: 0;
  margin-top: -5px;
  border-width: 5px 5px 5px 0;
  border-right-color: #000;
}
.tooltip.left .tooltip-arrow {
  top: 50%;
  right: 0;
  margin-top: -5px;
  border-width: 5px 0 5px 5px;
  border-left-color: #000;
}
.tooltip.bottom .tooltip-arrow {
  top: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-left .tooltip-arrow {
  top: 0;
  right: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-right .tooltip-arrow {
  top: 0;
  left: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.popover {
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1060;
  display: none;
  max-width: 276px;
  padding: 1px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 13px;
  background-color: #fff;
  background-clip: padding-box;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
}
.popover.top {
  margin-top: -10px;
}
.popover.right {
  margin-left: 10px;
}
.popover.bottom {
  margin-top: 10px;
}
.popover.left {
  margin-left: -10px;
}
.popover-title {
  margin: 0;
  padding: 8px 14px;
  font-size: 13px;
  background-color: #f7f7f7;
  border-bottom: 1px solid #ebebeb;
  border-radius: 2px 2px 0 0;
}
.popover-content {
  padding: 9px 14px;
}
.popover > .arrow,
.popover > .arrow:after {
  position: absolute;
  display: block;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.popover > .arrow {
  border-width: 11px;
}
.popover > .arrow:after {
  border-width: 10px;
  content: "";
}
.popover.top > .arrow {
  left: 50%;
  margin-left: -11px;
  border-bottom-width: 0;
  border-top-color: #999999;
  border-top-color: rgba(0, 0, 0, 0.25);
  bottom: -11px;
}
.popover.top > .arrow:after {
  content: " ";
  bottom: 1px;
  margin-left: -10px;
  border-bottom-width: 0;
  border-top-color: #fff;
}
.popover.right > .arrow {
  top: 50%;
  left: -11px;
  margin-top: -11px;
  border-left-width: 0;
  border-right-color: #999999;
  border-right-color: rgba(0, 0, 0, 0.25);
}
.popover.right > .arrow:after {
  content: " ";
  left: 1px;
  bottom: -10px;
  border-left-width: 0;
  border-right-color: #fff;
}
.popover.bottom > .arrow {
  left: 50%;
  margin-left: -11px;
  border-top-width: 0;
  border-bottom-color: #999999;
  border-bottom-color: rgba(0, 0, 0, 0.25);
  top: -11px;
}
.popover.bottom > .arrow:after {
  content: " ";
  top: 1px;
  margin-left: -10px;
  border-top-width: 0;
  border-bottom-color: #fff;
}
.popover.left > .arrow {
  top: 50%;
  right: -11px;
  margin-top: -11px;
  border-right-width: 0;
  border-left-color: #999999;
  border-left-color: rgba(0, 0, 0, 0.25);
}
.popover.left > .arrow:after {
  content: " ";
  right: 1px;
  border-right-width: 0;
  border-left-color: #fff;
  bottom: -10px;
}
.carousel {
  position: relative;
}
.carousel-inner {
  position: relative;
  overflow: hidden;
  width: 100%;
}
.carousel-inner > .item {
  display: none;
  position: relative;
  -webkit-transition: 0.6s ease-in-out left;
  -o-transition: 0.6s ease-in-out left;
  transition: 0.6s ease-in-out left;
}
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  line-height: 1;
}
@media all and (transform-3d), (-webkit-transform-3d) {
  .carousel-inner > .item {
    -webkit-transition: -webkit-transform 0.6s ease-in-out;
    -moz-transition: -moz-transform 0.6s ease-in-out;
    -o-transition: -o-transform 0.6s ease-in-out;
    transition: transform 0.6s ease-in-out;
    -webkit-backface-visibility: hidden;
    -moz-backface-visibility: hidden;
    backface-visibility: hidden;
    -webkit-perspective: 1000px;
    -moz-perspective: 1000px;
    perspective: 1000px;
  }
  .carousel-inner > .item.next,
  .carousel-inner > .item.active.right {
    -webkit-transform: translate3d(100%, 0, 0);
    transform: translate3d(100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.prev,
  .carousel-inner > .item.active.left {
    -webkit-transform: translate3d(-100%, 0, 0);
    transform: translate3d(-100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.next.left,
  .carousel-inner > .item.prev.right,
  .carousel-inner > .item.active {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    left: 0;
  }
}
.carousel-inner > .active,
.carousel-inner > .next,
.carousel-inner > .prev {
  display: block;
}
.carousel-inner > .active {
  left: 0;
}
.carousel-inner > .next,
.carousel-inner > .prev {
  position: absolute;
  top: 0;
  width: 100%;
}
.carousel-inner > .next {
  left: 100%;
}
.carousel-inner > .prev {
  left: -100%;
}
.carousel-inner > .next.left,
.carousel-inner > .prev.right {
  left: 0;
}
.carousel-inner > .active.left {
  left: -100%;
}
.carousel-inner > .active.right {
  left: 100%;
}
.carousel-control {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 15%;
  opacity: 0.5;
  filter: alpha(opacity=50);
  font-size: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
  background-color: rgba(0, 0, 0, 0);
}
.carousel-control.left {
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#80000000', endColorstr='#00000000', GradientType=1);
}
.carousel-control.right {
  left: auto;
  right: 0;
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#00000000', endColorstr='#80000000', GradientType=1);
}
.carousel-control:hover,
.carousel-control:focus {
  outline: 0;
  color: #fff;
  text-decoration: none;
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.carousel-control .icon-prev,
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-left,
.carousel-control .glyphicon-chevron-right {
  position: absolute;
  top: 50%;
  margin-top: -10px;
  z-index: 5;
  display: inline-block;
}
.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  left: 50%;
  margin-left: -10px;
}
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  right: 50%;
  margin-right: -10px;
}
.carousel-control .icon-prev,
.carousel-control .icon-next {
  width: 20px;
  height: 20px;
  line-height: 1;
  font-family: serif;
}
.carousel-control .icon-prev:before {
  content: '\2039';
}
.carousel-control .icon-next:before {
  content: '\203a';
}
.carousel-indicators {
  position: absolute;
  bottom: 10px;
  left: 50%;
  z-index: 15;
  width: 60%;
  margin-left: -30%;
  padding-left: 0;
  list-style: none;
  text-align: center;
}
.carousel-indicators li {
  display: inline-block;
  width: 10px;
  height: 10px;
  margin: 1px;
  text-indent: -999px;
  border: 1px solid #fff;
  border-radius: 10px;
  cursor: pointer;
  background-color: #000 \9;
  background-color: rgba(0, 0, 0, 0);
}
.carousel-indicators .active {
  margin: 0;
  width: 12px;
  height: 12px;
  background-color: #fff;
}
.carousel-caption {
  position: absolute;
  left: 15%;
  right: 15%;
  bottom: 20px;
  z-index: 10;
  padding-top: 20px;
  padding-bottom: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
}
.carousel-caption .btn {
  text-shadow: none;
}
@media screen and (min-width: 768px) {
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-prev,
  .carousel-control .icon-next {
    width: 30px;
    height: 30px;
    margin-top: -10px;
    font-size: 30px;
  }
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .icon-prev {
    margin-left: -10px;
  }
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-next {
    margin-right: -10px;
  }
  .carousel-caption {
    left: 20%;
    right: 20%;
    padding-bottom: 30px;
  }
  .carousel-indicators {
    bottom: 20px;
  }
}
.clearfix:before,
.clearfix:after,
.dl-horizontal dd:before,
.dl-horizontal dd:after,
.container:before,
.container:after,
.container-fluid:before,
.container-fluid:after,
.row:before,
.row:after,
.form-horizontal .form-group:before,
.form-horizontal .form-group:after,
.btn-toolbar:before,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:before,
.btn-group-vertical > .btn-group:after,
.nav:before,
.nav:after,
.navbar:before,
.navbar:after,
.navbar-header:before,
.navbar-header:after,
.navbar-collapse:before,
.navbar-collapse:after,
.pager:before,
.pager:after,
.panel-body:before,
.panel-body:after,
.modal-header:before,
.modal-header:after,
.modal-footer:before,
.modal-footer:after,
.item_buttons:before,
.item_buttons:after {
  content: " ";
  display: table;
}
.clearfix:after,
.dl-horizontal dd:after,
.container:after,
.container-fluid:after,
.row:after,
.form-horizontal .form-group:after,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:after,
.nav:after,
.navbar:after,
.navbar-header:after,
.navbar-collapse:after,
.pager:after,
.panel-body:after,
.modal-header:after,
.modal-footer:after,
.item_buttons:after {
  clear: both;
}
.center-block {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.pull-right {
  float: right !important;
}
.pull-left {
  float: left !important;
}
.hide {
  display: none !important;
}
.show {
  display: block !important;
}
.invisible {
  visibility: hidden;
}
.text-hide {
  font: 0/0 a;
  color: transparent;
  text-shadow: none;
  background-color: transparent;
  border: 0;
}
.hidden {
  display: none !important;
}
.affix {
  position: fixed;
}
@-ms-viewport {
  width: device-width;
}
.visible-xs,
.visible-sm,
.visible-md,
.visible-lg {
  display: none !important;
}
.visible-xs-block,
.visible-xs-inline,
.visible-xs-inline-block,
.visible-sm-block,
.visible-sm-inline,
.visible-sm-inline-block,
.visible-md-block,
.visible-md-inline,
.visible-md-inline-block,
.visible-lg-block,
.visible-lg-inline,
.visible-lg-inline-block {
  display: none !important;
}
@media (max-width: 767px) {
  .visible-xs {
    display: block !important;
  }
  table.visible-xs {
    display: table !important;
  }
  tr.visible-xs {
    display: table-row !important;
  }
  th.visible-xs,
  td.visible-xs {
    display: table-cell !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-block {
    display: block !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline {
    display: inline !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm {
    display: block !important;
  }
  table.visible-sm {
    display: table !important;
  }
  tr.visible-sm {
    display: table-row !important;
  }
  th.visible-sm,
  td.visible-sm {
    display: table-cell !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-block {
    display: block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline {
    display: inline !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md {
    display: block !important;
  }
  table.visible-md {
    display: table !important;
  }
  tr.visible-md {
    display: table-row !important;
  }
  th.visible-md,
  td.visible-md {
    display: table-cell !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-block {
    display: block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline {
    display: inline !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg {
    display: block !important;
  }
  table.visible-lg {
    display: table !important;
  }
  tr.visible-lg {
    display: table-row !important;
  }
  th.visible-lg,
  td.visible-lg {
    display: table-cell !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-block {
    display: block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline {
    display: inline !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline-block {
    display: inline-block !important;
  }
}
@media (max-width: 767px) {
  .hidden-xs {
    display: none !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .hidden-sm {
    display: none !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .hidden-md {
    display: none !important;
  }
}
@media (min-width: 1200px) {
  .hidden-lg {
    display: none !important;
  }
}
.visible-print {
  display: none !important;
}
@media print {
  .visible-print {
    display: block !important;
  }
  table.visible-print {
    display: table !important;
  }
  tr.visible-print {
    display: table-row !important;
  }
  th.visible-print,
  td.visible-print {
    display: table-cell !important;
  }
}
.visible-print-block {
  display: none !important;
}
@media print {
  .visible-print-block {
    display: block !important;
  }
}
.visible-print-inline {
  display: none !important;
}
@media print {
  .visible-print-inline {
    display: inline !important;
  }
}
.visible-print-inline-block {
  display: none !important;
}
@media print {
  .visible-print-inline-block {
    display: inline-block !important;
  }
}
@media print {
  .hidden-print {
    display: none !important;
  }
}
/*!
*
* Font Awesome
*
*/
/*!
 *  Font Awesome 4.2.0 by @davegandy - http://fontawesome.io - @fontawesome
 *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License)
 */
/* FONT PATH
 * -------------------------- */
@font-face {
  font-family: 'FontAwesome';
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?v=4.2.0');
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?#iefix&v=4.2.0') format('embedded-opentype'), url('../components/font-awesome/fonts/fontawesome-webfont.woff?v=4.2.0') format('woff'), url('../components/font-awesome/fonts/fontawesome-webfont.ttf?v=4.2.0') format('truetype'), url('../components/font-awesome/fonts/fontawesome-webfont.svg?v=4.2.0#fontawesomeregular') format('svg');
  font-weight: normal;
  font-style: normal;
}
.fa {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
/* makes the font 33% larger relative to the icon container */
.fa-lg {
  font-size: 1.33333333em;
  line-height: 0.75em;
  vertical-align: -15%;
}
.fa-2x {
  font-size: 2em;
}
.fa-3x {
  font-size: 3em;
}
.fa-4x {
  font-size: 4em;
}
.fa-5x {
  font-size: 5em;
}
.fa-fw {
  width: 1.28571429em;
  text-align: center;
}
.fa-ul {
  padding-left: 0;
  margin-left: 2.14285714em;
  list-style-type: none;
}
.fa-ul > li {
  position: relative;
}
.fa-li {
  position: absolute;
  left: -2.14285714em;
  width: 2.14285714em;
  top: 0.14285714em;
  text-align: center;
}
.fa-li.fa-lg {
  left: -1.85714286em;
}
.fa-border {
  padding: .2em .25em .15em;
  border: solid 0.08em #eee;
  border-radius: .1em;
}
.pull-right {
  float: right;
}
.pull-left {
  float: left;
}
.fa.pull-left {
  margin-right: .3em;
}
.fa.pull-right {
  margin-left: .3em;
}
.fa-spin {
  -webkit-animation: fa-spin 2s infinite linear;
  animation: fa-spin 2s infinite linear;
}
@-webkit-keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
@keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
.fa-rotate-90 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=1);
  -webkit-transform: rotate(90deg);
  -ms-transform: rotate(90deg);
  transform: rotate(90deg);
}
.fa-rotate-180 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=2);
  -webkit-transform: rotate(180deg);
  -ms-transform: rotate(180deg);
  transform: rotate(180deg);
}
.fa-rotate-270 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=3);
  -webkit-transform: rotate(270deg);
  -ms-transform: rotate(270deg);
  transform: rotate(270deg);
}
.fa-flip-horizontal {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1);
  -webkit-transform: scale(-1, 1);
  -ms-transform: scale(-1, 1);
  transform: scale(-1, 1);
}
.fa-flip-vertical {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1);
  -webkit-transform: scale(1, -1);
  -ms-transform: scale(1, -1);
  transform: scale(1, -1);
}
:root .fa-rotate-90,
:root .fa-rotate-180,
:root .fa-rotate-270,
:root .fa-flip-horizontal,
:root .fa-flip-vertical {
  filter: none;
}
.fa-stack {
  position: relative;
  display: inline-block;
  width: 2em;
  height: 2em;
  line-height: 2em;
  vertical-align: middle;
}
.fa-stack-1x,
.fa-stack-2x {
  position: absolute;
  left: 0;
  width: 100%;
  text-align: center;
}
.fa-stack-1x {
  line-height: inherit;
}
.fa-stack-2x {
  font-size: 2em;
}
.fa-inverse {
  color: #fff;
}
/* Font Awesome uses the Unicode Private Use Area (PUA) to ensure screen
   readers do not read off random characters that represent icons */
.fa-glass:before {
  content: "\f000";
}
.fa-music:before {
  content: "\f001";
}
.fa-search:before {
  content: "\f002";
}
.fa-envelope-o:before {
  content: "\f003";
}
.fa-heart:before {
  content: "\f004";
}
.fa-star:before {
  content: "\f005";
}
.fa-star-o:before {
  content: "\f006";
}
.fa-user:before {
  content: "\f007";
}
.fa-film:before {
  content: "\f008";
}
.fa-th-large:before {
  content: "\f009";
}
.fa-th:before {
  content: "\f00a";
}
.fa-th-list:before {
  content: "\f00b";
}
.fa-check:before {
  content: "\f00c";
}
.fa-remove:before,
.fa-close:before,
.fa-times:before {
  content: "\f00d";
}
.fa-search-plus:before {
  content: "\f00e";
}
.fa-search-minus:before {
  content: "\f010";
}
.fa-power-off:before {
  content: "\f011";
}
.fa-signal:before {
  content: "\f012";
}
.fa-gear:before,
.fa-cog:before {
  content: "\f013";
}
.fa-trash-o:before {
  content: "\f014";
}
.fa-home:before {
  content: "\f015";
}
.fa-file-o:before {
  content: "\f016";
}
.fa-clock-o:before {
  content: "\f017";
}
.fa-road:before {
  content: "\f018";
}
.fa-download:before {
  content: "\f019";
}
.fa-arrow-circle-o-down:before {
  content: "\f01a";
}
.fa-arrow-circle-o-up:before {
  content: "\f01b";
}
.fa-inbox:before {
  content: "\f01c";
}
.fa-play-circle-o:before {
  content: "\f01d";
}
.fa-rotate-right:before,
.fa-repeat:before {
  content: "\f01e";
}
.fa-refresh:before {
  content: "\f021";
}
.fa-list-alt:before {
  content: "\f022";
}
.fa-lock:before {
  content: "\f023";
}
.fa-flag:before {
  content: "\f024";
}
.fa-headphones:before {
  content: "\f025";
}
.fa-volume-off:before {
  content: "\f026";
}
.fa-volume-down:before {
  content: "\f027";
}
.fa-volume-up:before {
  content: "\f028";
}
.fa-qrcode:before {
  content: "\f029";
}
.fa-barcode:before {
  content: "\f02a";
}
.fa-tag:before {
  content: "\f02b";
}
.fa-tags:before {
  content: "\f02c";
}
.fa-book:before {
  content: "\f02d";
}
.fa-bookmark:before {
  content: "\f02e";
}
.fa-print:before {
  content: "\f02f";
}
.fa-camera:before {
  content: "\f030";
}
.fa-font:before {
  content: "\f031";
}
.fa-bold:before {
  content: "\f032";
}
.fa-italic:before {
  content: "\f033";
}
.fa-text-height:before {
  content: "\f034";
}
.fa-text-width:before {
  content: "\f035";
}
.fa-align-left:before {
  content: "\f036";
}
.fa-align-center:before {
  content: "\f037";
}
.fa-align-right:before {
  content: "\f038";
}
.fa-align-justify:before {
  content: "\f039";
}
.fa-list:before {
  content: "\f03a";
}
.fa-dedent:before,
.fa-outdent:before {
  content: "\f03b";
}
.fa-indent:before {
  content: "\f03c";
}
.fa-video-camera:before {
  content: "\f03d";
}
.fa-photo:before,
.fa-image:before,
.fa-picture-o:before {
  content: "\f03e";
}
.fa-pencil:before {
  content: "\f040";
}
.fa-map-marker:before {
  content: "\f041";
}
.fa-adjust:before {
  content: "\f042";
}
.fa-tint:before {
  content: "\f043";
}
.fa-edit:before,
.fa-pencil-square-o:before {
  content: "\f044";
}
.fa-share-square-o:before {
  content: "\f045";
}
.fa-check-square-o:before {
  content: "\f046";
}
.fa-arrows:before {
  content: "\f047";
}
.fa-step-backward:before {
  content: "\f048";
}
.fa-fast-backward:before {
  content: "\f049";
}
.fa-backward:before {
  content: "\f04a";
}
.fa-play:before {
  content: "\f04b";
}
.fa-pause:before {
  content: "\f04c";
}
.fa-stop:before {
  content: "\f04d";
}
.fa-forward:before {
  content: "\f04e";
}
.fa-fast-forward:before {
  content: "\f050";
}
.fa-step-forward:before {
  content: "\f051";
}
.fa-eject:before {
  content: "\f052";
}
.fa-chevron-left:before {
  content: "\f053";
}
.fa-chevron-right:before {
  content: "\f054";
}
.fa-plus-circle:before {
  content: "\f055";
}
.fa-minus-circle:before {
  content: "\f056";
}
.fa-times-circle:before {
  content: "\f057";
}
.fa-check-circle:before {
  content: "\f058";
}
.fa-question-circle:before {
  content: "\f059";
}
.fa-info-circle:before {
  content: "\f05a";
}
.fa-crosshairs:before {
  content: "\f05b";
}
.fa-times-circle-o:before {
  content: "\f05c";
}
.fa-check-circle-o:before {
  content: "\f05d";
}
.fa-ban:before {
  content: "\f05e";
}
.fa-arrow-left:before {
  content: "\f060";
}
.fa-arrow-right:before {
  content: "\f061";
}
.fa-arrow-up:before {
  content: "\f062";
}
.fa-arrow-down:before {
  content: "\f063";
}
.fa-mail-forward:before,
.fa-share:before {
  content: "\f064";
}
.fa-expand:before {
  content: "\f065";
}
.fa-compress:before {
  content: "\f066";
}
.fa-plus:before {
  content: "\f067";
}
.fa-minus:before {
  content: "\f068";
}
.fa-asterisk:before {
  content: "\f069";
}
.fa-exclamation-circle:before {
  content: "\f06a";
}
.fa-gift:before {
  content: "\f06b";
}
.fa-leaf:before {
  content: "\f06c";
}
.fa-fire:before {
  content: "\f06d";
}
.fa-eye:before {
  content: "\f06e";
}
.fa-eye-slash:before {
  content: "\f070";
}
.fa-warning:before,
.fa-exclamation-triangle:before {
  content: "\f071";
}
.fa-plane:before {
  content: "\f072";
}
.fa-calendar:before {
  content: "\f073";
}
.fa-random:before {
  content: "\f074";
}
.fa-comment:before {
  content: "\f075";
}
.fa-magnet:before {
  content: "\f076";
}
.fa-chevron-up:before {
  content: "\f077";
}
.fa-chevron-down:before {
  content: "\f078";
}
.fa-retweet:before {
  content: "\f079";
}
.fa-shopping-cart:before {
  content: "\f07a";
}
.fa-folder:before {
  content: "\f07b";
}
.fa-folder-open:before {
  content: "\f07c";
}
.fa-arrows-v:before {
  content: "\f07d";
}
.fa-arrows-h:before {
  content: "\f07e";
}
.fa-bar-chart-o:before,
.fa-bar-chart:before {
  content: "\f080";
}
.fa-twitter-square:before {
  content: "\f081";
}
.fa-facebook-square:before {
  content: "\f082";
}
.fa-camera-retro:before {
  content: "\f083";
}
.fa-key:before {
  content: "\f084";
}
.fa-gears:before,
.fa-cogs:before {
  content: "\f085";
}
.fa-comments:before {
  content: "\f086";
}
.fa-thumbs-o-up:before {
  content: "\f087";
}
.fa-thumbs-o-down:before {
  content: "\f088";
}
.fa-star-half:before {
  content: "\f089";
}
.fa-heart-o:before {
  content: "\f08a";
}
.fa-sign-out:before {
  content: "\f08b";
}
.fa-linkedin-square:before {
  content: "\f08c";
}
.fa-thumb-tack:before {
  content: "\f08d";
}
.fa-external-link:before {
  content: "\f08e";
}
.fa-sign-in:before {
  content: "\f090";
}
.fa-trophy:before {
  content: "\f091";
}
.fa-github-square:before {
  content: "\f092";
}
.fa-upload:before {
  content: "\f093";
}
.fa-lemon-o:before {
  content: "\f094";
}
.fa-phone:before {
  content: "\f095";
}
.fa-square-o:before {
  content: "\f096";
}
.fa-bookmark-o:before {
  content: "\f097";
}
.fa-phone-square:before {
  content: "\f098";
}
.fa-twitter:before {
  content: "\f099";
}
.fa-facebook:before {
  content: "\f09a";
}
.fa-github:before {
  content: "\f09b";
}
.fa-unlock:before {
  content: "\f09c";
}
.fa-credit-card:before {
  content: "\f09d";
}
.fa-rss:before {
  content: "\f09e";
}
.fa-hdd-o:before {
  content: "\f0a0";
}
.fa-bullhorn:before {
  content: "\f0a1";
}
.fa-bell:before {
  content: "\f0f3";
}
.fa-certificate:before {
  content: "\f0a3";
}
.fa-hand-o-right:before {
  content: "\f0a4";
}
.fa-hand-o-left:before {
  content: "\f0a5";
}
.fa-hand-o-up:before {
  content: "\f0a6";
}
.fa-hand-o-down:before {
  content: "\f0a7";
}
.fa-arrow-circle-left:before {
  content: "\f0a8";
}
.fa-arrow-circle-right:before {
  content: "\f0a9";
}
.fa-arrow-circle-up:before {
  content: "\f0aa";
}
.fa-arrow-circle-down:before {
  content: "\f0ab";
}
.fa-globe:before {
  content: "\f0ac";
}
.fa-wrench:before {
  content: "\f0ad";
}
.fa-tasks:before {
  content: "\f0ae";
}
.fa-filter:before {
  content: "\f0b0";
}
.fa-briefcase:before {
  content: "\f0b1";
}
.fa-arrows-alt:before {
  content: "\f0b2";
}
.fa-group:before,
.fa-users:before {
  content: "\f0c0";
}
.fa-chain:before,
.fa-link:before {
  content: "\f0c1";
}
.fa-cloud:before {
  content: "\f0c2";
}
.fa-flask:before {
  content: "\f0c3";
}
.fa-cut:before,
.fa-scissors:before {
  content: "\f0c4";
}
.fa-copy:before,
.fa-files-o:before {
  content: "\f0c5";
}
.fa-paperclip:before {
  content: "\f0c6";
}
.fa-save:before,
.fa-floppy-o:before {
  content: "\f0c7";
}
.fa-square:before {
  content: "\f0c8";
}
.fa-navicon:before,
.fa-reorder:before,
.fa-bars:before {
  content: "\f0c9";
}
.fa-list-ul:before {
  content: "\f0ca";
}
.fa-list-ol:before {
  content: "\f0cb";
}
.fa-strikethrough:before {
  content: "\f0cc";
}
.fa-underline:before {
  content: "\f0cd";
}
.fa-table:before {
  content: "\f0ce";
}
.fa-magic:before {
  content: "\f0d0";
}
.fa-truck:before {
  content: "\f0d1";
}
.fa-pinterest:before {
  content: "\f0d2";
}
.fa-pinterest-square:before {
  content: "\f0d3";
}
.fa-google-plus-square:before {
  content: "\f0d4";
}
.fa-google-plus:before {
  content: "\f0d5";
}
.fa-money:before {
  content: "\f0d6";
}
.fa-caret-down:before {
  content: "\f0d7";
}
.fa-caret-up:before {
  content: "\f0d8";
}
.fa-caret-left:before {
  content: "\f0d9";
}
.fa-caret-right:before {
  content: "\f0da";
}
.fa-columns:before {
  content: "\f0db";
}
.fa-unsorted:before,
.fa-sort:before {
  content: "\f0dc";
}
.fa-sort-down:before,
.fa-sort-desc:before {
  content: "\f0dd";
}
.fa-sort-up:before,
.fa-sort-asc:before {
  content: "\f0de";
}
.fa-envelope:before {
  content: "\f0e0";
}
.fa-linkedin:before {
  content: "\f0e1";
}
.fa-rotate-left:before,
.fa-undo:before {
  content: "\f0e2";
}
.fa-legal:before,
.fa-gavel:before {
  content: "\f0e3";
}
.fa-dashboard:before,
.fa-tachometer:before {
  content: "\f0e4";
}
.fa-comment-o:before {
  content: "\f0e5";
}
.fa-comments-o:before {
  content: "\f0e6";
}
.fa-flash:before,
.fa-bolt:before {
  content: "\f0e7";
}
.fa-sitemap:before {
  content: "\f0e8";
}
.fa-umbrella:before {
  content: "\f0e9";
}
.fa-paste:before,
.fa-clipboard:before {
  content: "\f0ea";
}
.fa-lightbulb-o:before {
  content: "\f0eb";
}
.fa-exchange:before {
  content: "\f0ec";
}
.fa-cloud-download:before {
  content: "\f0ed";
}
.fa-cloud-upload:before {
  content: "\f0ee";
}
.fa-user-md:before {
  content: "\f0f0";
}
.fa-stethoscope:before {
  content: "\f0f1";
}
.fa-suitcase:before {
  content: "\f0f2";
}
.fa-bell-o:before {
  content: "\f0a2";
}
.fa-coffee:before {
  content: "\f0f4";
}
.fa-cutlery:before {
  content: "\f0f5";
}
.fa-file-text-o:before {
  content: "\f0f6";
}
.fa-building-o:before {
  content: "\f0f7";
}
.fa-hospital-o:before {
  content: "\f0f8";
}
.fa-ambulance:before {
  content: "\f0f9";
}
.fa-medkit:before {
  content: "\f0fa";
}
.fa-fighter-jet:before {
  content: "\f0fb";
}
.fa-beer:before {
  content: "\f0fc";
}
.fa-h-square:before {
  content: "\f0fd";
}
.fa-plus-square:before {
  content: "\f0fe";
}
.fa-angle-double-left:before {
  content: "\f100";
}
.fa-angle-double-right:before {
  content: "\f101";
}
.fa-angle-double-up:before {
  content: "\f102";
}
.fa-angle-double-down:before {
  content: "\f103";
}
.fa-angle-left:before {
  content: "\f104";
}
.fa-angle-right:before {
  content: "\f105";
}
.fa-angle-up:before {
  content: "\f106";
}
.fa-angle-down:before {
  content: "\f107";
}
.fa-desktop:before {
  content: "\f108";
}
.fa-laptop:before {
  content: "\f109";
}
.fa-tablet:before {
  content: "\f10a";
}
.fa-mobile-phone:before,
.fa-mobile:before {
  content: "\f10b";
}
.fa-circle-o:before {
  content: "\f10c";
}
.fa-quote-left:before {
  content: "\f10d";
}
.fa-quote-right:before {
  content: "\f10e";
}
.fa-spinner:before {
  content: "\f110";
}
.fa-circle:before {
  content: "\f111";
}
.fa-mail-reply:before,
.fa-reply:before {
  content: "\f112";
}
.fa-github-alt:before {
  content: "\f113";
}
.fa-folder-o:before {
  content: "\f114";
}
.fa-folder-open-o:before {
  content: "\f115";
}
.fa-smile-o:before {
  content: "\f118";
}
.fa-frown-o:before {
  content: "\f119";
}
.fa-meh-o:before {
  content: "\f11a";
}
.fa-gamepad:before {
  content: "\f11b";
}
.fa-keyboard-o:before {
  content: "\f11c";
}
.fa-flag-o:before {
  content: "\f11d";
}
.fa-flag-checkered:before {
  content: "\f11e";
}
.fa-terminal:before {
  content: "\f120";
}
.fa-code:before {
  content: "\f121";
}
.fa-mail-reply-all:before,
.fa-reply-all:before {
  content: "\f122";
}
.fa-star-half-empty:before,
.fa-star-half-full:before,
.fa-star-half-o:before {
  content: "\f123";
}
.fa-location-arrow:before {
  content: "\f124";
}
.fa-crop:before {
  content: "\f125";
}
.fa-code-fork:before {
  content: "\f126";
}
.fa-unlink:before,
.fa-chain-broken:before {
  content: "\f127";
}
.fa-question:before {
  content: "\f128";
}
.fa-info:before {
  content: "\f129";
}
.fa-exclamation:before {
  content: "\f12a";
}
.fa-superscript:before {
  content: "\f12b";
}
.fa-subscript:before {
  content: "\f12c";
}
.fa-eraser:before {
  content: "\f12d";
}
.fa-puzzle-piece:before {
  content: "\f12e";
}
.fa-microphone:before {
  content: "\f130";
}
.fa-microphone-slash:before {
  content: "\f131";
}
.fa-shield:before {
  content: "\f132";
}
.fa-calendar-o:before {
  content: "\f133";
}
.fa-fire-extinguisher:before {
  content: "\f134";
}
.fa-rocket:before {
  content: "\f135";
}
.fa-maxcdn:before {
  content: "\f136";
}
.fa-chevron-circle-left:before {
  content: "\f137";
}
.fa-chevron-circle-right:before {
  content: "\f138";
}
.fa-chevron-circle-up:before {
  content: "\f139";
}
.fa-chevron-circle-down:before {
  content: "\f13a";
}
.fa-html5:before {
  content: "\f13b";
}
.fa-css3:before {
  content: "\f13c";
}
.fa-anchor:before {
  content: "\f13d";
}
.fa-unlock-alt:before {
  content: "\f13e";
}
.fa-bullseye:before {
  content: "\f140";
}
.fa-ellipsis-h:before {
  content: "\f141";
}
.fa-ellipsis-v:before {
  content: "\f142";
}
.fa-rss-square:before {
  content: "\f143";
}
.fa-play-circle:before {
  content: "\f144";
}
.fa-ticket:before {
  content: "\f145";
}
.fa-minus-square:before {
  content: "\f146";
}
.fa-minus-square-o:before {
  content: "\f147";
}
.fa-level-up:before {
  content: "\f148";
}
.fa-level-down:before {
  content: "\f149";
}
.fa-check-square:before {
  content: "\f14a";
}
.fa-pencil-square:before {
  content: "\f14b";
}
.fa-external-link-square:before {
  content: "\f14c";
}
.fa-share-square:before {
  content: "\f14d";
}
.fa-compass:before {
  content: "\f14e";
}
.fa-toggle-down:before,
.fa-caret-square-o-down:before {
  content: "\f150";
}
.fa-toggle-up:before,
.fa-caret-square-o-up:before {
  content: "\f151";
}
.fa-toggle-right:before,
.fa-caret-square-o-right:before {
  content: "\f152";
}
.fa-euro:before,
.fa-eur:before {
  content: "\f153";
}
.fa-gbp:before {
  content: "\f154";
}
.fa-dollar:before,
.fa-usd:before {
  content: "\f155";
}
.fa-rupee:before,
.fa-inr:before {
  content: "\f156";
}
.fa-cny:before,
.fa-rmb:before,
.fa-yen:before,
.fa-jpy:before {
  content: "\f157";
}
.fa-ruble:before,
.fa-rouble:before,
.fa-rub:before {
  content: "\f158";
}
.fa-won:before,
.fa-krw:before {
  content: "\f159";
}
.fa-bitcoin:before,
.fa-btc:before {
  content: "\f15a";
}
.fa-file:before {
  content: "\f15b";
}
.fa-file-text:before {
  content: "\f15c";
}
.fa-sort-alpha-asc:before {
  content: "\f15d";
}
.fa-sort-alpha-desc:before {
  content: "\f15e";
}
.fa-sort-amount-asc:before {
  content: "\f160";
}
.fa-sort-amount-desc:before {
  content: "\f161";
}
.fa-sort-numeric-asc:before {
  content: "\f162";
}
.fa-sort-numeric-desc:before {
  content: "\f163";
}
.fa-thumbs-up:before {
  content: "\f164";
}
.fa-thumbs-down:before {
  content: "\f165";
}
.fa-youtube-square:before {
  content: "\f166";
}
.fa-youtube:before {
  content: "\f167";
}
.fa-xing:before {
  content: "\f168";
}
.fa-xing-square:before {
  content: "\f169";
}
.fa-youtube-play:before {
  content: "\f16a";
}
.fa-dropbox:before {
  content: "\f16b";
}
.fa-stack-overflow:before {
  content: "\f16c";
}
.fa-instagram:before {
  content: "\f16d";
}
.fa-flickr:before {
  content: "\f16e";
}
.fa-adn:before {
  content: "\f170";
}
.fa-bitbucket:before {
  content: "\f171";
}
.fa-bitbucket-square:before {
  content: "\f172";
}
.fa-tumblr:before {
  content: "\f173";
}
.fa-tumblr-square:before {
  content: "\f174";
}
.fa-long-arrow-down:before {
  content: "\f175";
}
.fa-long-arrow-up:before {
  content: "\f176";
}
.fa-long-arrow-left:before {
  content: "\f177";
}
.fa-long-arrow-right:before {
  content: "\f178";
}
.fa-apple:before {
  content: "\f179";
}
.fa-windows:before {
  content: "\f17a";
}
.fa-android:before {
  content: "\f17b";
}
.fa-linux:before {
  content: "\f17c";
}
.fa-dribbble:before {
  content: "\f17d";
}
.fa-skype:before {
  content: "\f17e";
}
.fa-foursquare:before {
  content: "\f180";
}
.fa-trello:before {
  content: "\f181";
}
.fa-female:before {
  content: "\f182";
}
.fa-male:before {
  content: "\f183";
}
.fa-gittip:before {
  content: "\f184";
}
.fa-sun-o:before {
  content: "\f185";
}
.fa-moon-o:before {
  content: "\f186";
}
.fa-archive:before {
  content: "\f187";
}
.fa-bug:before {
  content: "\f188";
}
.fa-vk:before {
  content: "\f189";
}
.fa-weibo:before {
  content: "\f18a";
}
.fa-renren:before {
  content: "\f18b";
}
.fa-pagelines:before {
  content: "\f18c";
}
.fa-stack-exchange:before {
  content: "\f18d";
}
.fa-arrow-circle-o-right:before {
  content: "\f18e";
}
.fa-arrow-circle-o-left:before {
  content: "\f190";
}
.fa-toggle-left:before,
.fa-caret-square-o-left:before {
  content: "\f191";
}
.fa-dot-circle-o:before {
  content: "\f192";
}
.fa-wheelchair:before {
  content: "\f193";
}
.fa-vimeo-square:before {
  content: "\f194";
}
.fa-turkish-lira:before,
.fa-try:before {
  content: "\f195";
}
.fa-plus-square-o:before {
  content: "\f196";
}
.fa-space-shuttle:before {
  content: "\f197";
}
.fa-slack:before {
  content: "\f198";
}
.fa-envelope-square:before {
  content: "\f199";
}
.fa-wordpress:before {
  content: "\f19a";
}
.fa-openid:before {
  content: "\f19b";
}
.fa-institution:before,
.fa-bank:before,
.fa-university:before {
  content: "\f19c";
}
.fa-mortar-board:before,
.fa-graduation-cap:before {
  content: "\f19d";
}
.fa-yahoo:before {
  content: "\f19e";
}
.fa-google:before {
  content: "\f1a0";
}
.fa-reddit:before {
  content: "\f1a1";
}
.fa-reddit-square:before {
  content: "\f1a2";
}
.fa-stumbleupon-circle:before {
  content: "\f1a3";
}
.fa-stumbleupon:before {
  content: "\f1a4";
}
.fa-delicious:before {
  content: "\f1a5";
}
.fa-digg:before {
  content: "\f1a6";
}
.fa-pied-piper:before {
  content: "\f1a7";
}
.fa-pied-piper-alt:before {
  content: "\f1a8";
}
.fa-drupal:before {
  content: "\f1a9";
}
.fa-joomla:before {
  content: "\f1aa";
}
.fa-language:before {
  content: "\f1ab";
}
.fa-fax:before {
  content: "\f1ac";
}
.fa-building:before {
  content: "\f1ad";
}
.fa-child:before {
  content: "\f1ae";
}
.fa-paw:before {
  content: "\f1b0";
}
.fa-spoon:before {
  content: "\f1b1";
}
.fa-cube:before {
  content: "\f1b2";
}
.fa-cubes:before {
  content: "\f1b3";
}
.fa-behance:before {
  content: "\f1b4";
}
.fa-behance-square:before {
  content: "\f1b5";
}
.fa-steam:before {
  content: "\f1b6";
}
.fa-steam-square:before {
  content: "\f1b7";
}
.fa-recycle:before {
  content: "\f1b8";
}
.fa-automobile:before,
.fa-car:before {
  content: "\f1b9";
}
.fa-cab:before,
.fa-taxi:before {
  content: "\f1ba";
}
.fa-tree:before {
  content: "\f1bb";
}
.fa-spotify:before {
  content: "\f1bc";
}
.fa-deviantart:before {
  content: "\f1bd";
}
.fa-soundcloud:before {
  content: "\f1be";
}
.fa-database:before {
  content: "\f1c0";
}
.fa-file-pdf-o:before {
  content: "\f1c1";
}
.fa-file-word-o:before {
  content: "\f1c2";
}
.fa-file-excel-o:before {
  content: "\f1c3";
}
.fa-file-powerpoint-o:before {
  content: "\f1c4";
}
.fa-file-photo-o:before,
.fa-file-picture-o:before,
.fa-file-image-o:before {
  content: "\f1c5";
}
.fa-file-zip-o:before,
.fa-file-archive-o:before {
  content: "\f1c6";
}
.fa-file-sound-o:before,
.fa-file-audio-o:before {
  content: "\f1c7";
}
.fa-file-movie-o:before,
.fa-file-video-o:before {
  content: "\f1c8";
}
.fa-file-code-o:before {
  content: "\f1c9";
}
.fa-vine:before {
  content: "\f1ca";
}
.fa-codepen:before {
  content: "\f1cb";
}
.fa-jsfiddle:before {
  content: "\f1cc";
}
.fa-life-bouy:before,
.fa-life-buoy:before,
.fa-life-saver:before,
.fa-support:before,
.fa-life-ring:before {
  content: "\f1cd";
}
.fa-circle-o-notch:before {
  content: "\f1ce";
}
.fa-ra:before,
.fa-rebel:before {
  content: "\f1d0";
}
.fa-ge:before,
.fa-empire:before {
  content: "\f1d1";
}
.fa-git-square:before {
  content: "\f1d2";
}
.fa-git:before {
  content: "\f1d3";
}
.fa-hacker-news:before {
  content: "\f1d4";
}
.fa-tencent-weibo:before {
  content: "\f1d5";
}
.fa-qq:before {
  content: "\f1d6";
}
.fa-wechat:before,
.fa-weixin:before {
  content: "\f1d7";
}
.fa-send:before,
.fa-paper-plane:before {
  content: "\f1d8";
}
.fa-send-o:before,
.fa-paper-plane-o:before {
  content: "\f1d9";
}
.fa-history:before {
  content: "\f1da";
}
.fa-circle-thin:before {
  content: "\f1db";
}
.fa-header:before {
  content: "\f1dc";
}
.fa-paragraph:before {
  content: "\f1dd";
}
.fa-sliders:before {
  content: "\f1de";
}
.fa-share-alt:before {
  content: "\f1e0";
}
.fa-share-alt-square:before {
  content: "\f1e1";
}
.fa-bomb:before {
  content: "\f1e2";
}
.fa-soccer-ball-o:before,
.fa-futbol-o:before {
  content: "\f1e3";
}
.fa-tty:before {
  content: "\f1e4";
}
.fa-binoculars:before {
  content: "\f1e5";
}
.fa-plug:before {
  content: "\f1e6";
}
.fa-slideshare:before {
  content: "\f1e7";
}
.fa-twitch:before {
  content: "\f1e8";
}
.fa-yelp:before {
  content: "\f1e9";
}
.fa-newspaper-o:before {
  content: "\f1ea";
}
.fa-wifi:before {
  content: "\f1eb";
}
.fa-calculator:before {
  content: "\f1ec";
}
.fa-paypal:before {
  content: "\f1ed";
}
.fa-google-wallet:before {
  content: "\f1ee";
}
.fa-cc-visa:before {
  content: "\f1f0";
}
.fa-cc-mastercard:before {
  content: "\f1f1";
}
.fa-cc-discover:before {
  content: "\f1f2";
}
.fa-cc-amex:before {
  content: "\f1f3";
}
.fa-cc-paypal:before {
  content: "\f1f4";
}
.fa-cc-stripe:before {
  content: "\f1f5";
}
.fa-bell-slash:before {
  content: "\f1f6";
}
.fa-bell-slash-o:before {
  content: "\f1f7";
}
.fa-trash:before {
  content: "\f1f8";
}
.fa-copyright:before {
  content: "\f1f9";
}
.fa-at:before {
  content: "\f1fa";
}
.fa-eyedropper:before {
  content: "\f1fb";
}
.fa-paint-brush:before {
  content: "\f1fc";
}
.fa-birthday-cake:before {
  content: "\f1fd";
}
.fa-area-chart:before {
  content: "\f1fe";
}
.fa-pie-chart:before {
  content: "\f200";
}
.fa-line-chart:before {
  content: "\f201";
}
.fa-lastfm:before {
  content: "\f202";
}
.fa-lastfm-square:before {
  content: "\f203";
}
.fa-toggle-off:before {
  content: "\f204";
}
.fa-toggle-on:before {
  content: "\f205";
}
.fa-bicycle:before {
  content: "\f206";
}
.fa-bus:before {
  content: "\f207";
}
.fa-ioxhost:before {
  content: "\f208";
}
.fa-angellist:before {
  content: "\f209";
}
.fa-cc:before {
  content: "\f20a";
}
.fa-shekel:before,
.fa-sheqel:before,
.fa-ils:before {
  content: "\f20b";
}
.fa-meanpath:before {
  content: "\f20c";
}
/*!
*
* IPython base
*
*/
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
code {
  color: #000;
}
pre {
  font-size: inherit;
  line-height: inherit;
}
label {
  font-weight: normal;
}
/* Make the page background atleast 100% the height of the view port */
/* Make the page itself atleast 70% the height of the view port */
.border-box-sizing {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.corner-all {
  border-radius: 2px;
}
.no-padding {
  padding: 0px;
}
/* Flexible box model classes */
/* Taken from Alex Russell http://infrequently.org/2009/08/css-3-progress/ */
/* This file is a compatability layer.  It allows the usage of flexible box 
model layouts accross multiple browsers, including older browsers.  The newest,
universal implementation of the flexible box model is used when available (see
`Modern browsers` comments below).  Browsers that are known to implement this 
new spec completely include:

    Firefox 28.0+
    Chrome 29.0+
    Internet Explorer 11+ 
    Opera 17.0+

Browsers not listed, including Safari, are supported via the styling under the
`Old browsers` comments below.
*/
.hbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
.hbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.vbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
.vbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.hbox.reverse,
.vbox.reverse,
.reverse {
  /* Old browsers */
  -webkit-box-direction: reverse;
  -moz-box-direction: reverse;
  box-direction: reverse;
  /* Modern browsers */
  flex-direction: row-reverse;
}
.hbox.box-flex0,
.vbox.box-flex0,
.box-flex0 {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
  width: auto;
}
.hbox.box-flex1,
.vbox.box-flex1,
.box-flex1 {
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex,
.vbox.box-flex,
.box-flex {
  /* Old browsers */
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex2,
.vbox.box-flex2,
.box-flex2 {
  /* Old browsers */
  -webkit-box-flex: 2;
  -moz-box-flex: 2;
  box-flex: 2;
  /* Modern browsers */
  flex: 2;
}
.box-group1 {
  /*  Deprecated */
  -webkit-box-flex-group: 1;
  -moz-box-flex-group: 1;
  box-flex-group: 1;
}
.box-group2 {
  /* Deprecated */
  -webkit-box-flex-group: 2;
  -moz-box-flex-group: 2;
  box-flex-group: 2;
}
.hbox.start,
.vbox.start,
.start {
  /* Old browsers */
  -webkit-box-pack: start;
  -moz-box-pack: start;
  box-pack: start;
  /* Modern browsers */
  justify-content: flex-start;
}
.hbox.end,
.vbox.end,
.end {
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
}
.hbox.center,
.vbox.center,
.center {
  /* Old browsers */
  -webkit-box-pack: center;
  -moz-box-pack: center;
  box-pack: center;
  /* Modern browsers */
  justify-content: center;
}
.hbox.baseline,
.vbox.baseline,
.baseline {
  /* Old browsers */
  -webkit-box-pack: baseline;
  -moz-box-pack: baseline;
  box-pack: baseline;
  /* Modern browsers */
  justify-content: baseline;
}
.hbox.stretch,
.vbox.stretch,
.stretch {
  /* Old browsers */
  -webkit-box-pack: stretch;
  -moz-box-pack: stretch;
  box-pack: stretch;
  /* Modern browsers */
  justify-content: stretch;
}
.hbox.align-start,
.vbox.align-start,
.align-start {
  /* Old browsers */
  -webkit-box-align: start;
  -moz-box-align: start;
  box-align: start;
  /* Modern browsers */
  align-items: flex-start;
}
.hbox.align-end,
.vbox.align-end,
.align-end {
  /* Old browsers */
  -webkit-box-align: end;
  -moz-box-align: end;
  box-align: end;
  /* Modern browsers */
  align-items: flex-end;
}
.hbox.align-center,
.vbox.align-center,
.align-center {
  /* Old browsers */
  -webkit-box-align: center;
  -moz-box-align: center;
  box-align: center;
  /* Modern browsers */
  align-items: center;
}
.hbox.align-baseline,
.vbox.align-baseline,
.align-baseline {
  /* Old browsers */
  -webkit-box-align: baseline;
  -moz-box-align: baseline;
  box-align: baseline;
  /* Modern browsers */
  align-items: baseline;
}
.hbox.align-stretch,
.vbox.align-stretch,
.align-stretch {
  /* Old browsers */
  -webkit-box-align: stretch;
  -moz-box-align: stretch;
  box-align: stretch;
  /* Modern browsers */
  align-items: stretch;
}
div.error {
  margin: 2em;
  text-align: center;
}
div.error > h1 {
  font-size: 500%;
  line-height: normal;
}
div.error > p {
  font-size: 200%;
  line-height: normal;
}
div.traceback-wrapper {
  text-align: left;
  max-width: 800px;
  margin: auto;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
body {
  background-color: #fff;
  /* This makes sure that the body covers the entire window and needs to
       be in a different element than the display: box in wrapper below */
  position: absolute;
  left: 0px;
  right: 0px;
  top: 0px;
  bottom: 0px;
  overflow: visible;
}
body > #header {
  /* Initially hidden to prevent FLOUC */
  display: none;
  background-color: #fff;
  /* Display over codemirror */
  position: relative;
  z-index: 100;
}
body > #header #header-container {
  padding-bottom: 5px;
  padding-top: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
body > #header .header-bar {
  width: 100%;
  height: 1px;
  background: #e7e7e7;
  margin-bottom: -1px;
}
@media print {
  body > #header {
    display: none !important;
  }
}
#header-spacer {
  width: 100%;
  visibility: hidden;
}
@media print {
  #header-spacer {
    display: none;
  }
}
#ipython_notebook {
  padding-left: 0px;
  padding-top: 1px;
  padding-bottom: 1px;
}
@media (max-width: 991px) {
  #ipython_notebook {
    margin-left: 10px;
  }
}
[dir="rtl"] #ipython_notebook {
  float: right !important;
}
#noscript {
  width: auto;
  padding-top: 16px;
  padding-bottom: 16px;
  text-align: center;
  font-size: 22px;
  color: red;
  font-weight: bold;
}
#ipython_notebook img {
  height: 28px;
}
#site {
  width: 100%;
  display: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  overflow: auto;
}
@media print {
  #site {
    height: auto !important;
  }
}
/* Smaller buttons */
.ui-button .ui-button-text {
  padding: 0.2em 0.8em;
  font-size: 77%;
}
input.ui-button {
  padding: 0.3em 0.9em;
}
span#login_widget {
  float: right;
}
span#login_widget > .button,
#logout {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button:focus,
#logout:focus,
span#login_widget > .button.focus,
#logout.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
span#login_widget > .button:hover,
#logout:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active:hover,
#logout:active:hover,
span#login_widget > .button.active:hover,
#logout.active:hover,
.open > .dropdown-togglespan#login_widget > .button:hover,
.open > .dropdown-toggle#logout:hover,
span#login_widget > .button:active:focus,
#logout:active:focus,
span#login_widget > .button.active:focus,
#logout.active:focus,
.open > .dropdown-togglespan#login_widget > .button:focus,
.open > .dropdown-toggle#logout:focus,
span#login_widget > .button:active.focus,
#logout:active.focus,
span#login_widget > .button.active.focus,
#logout.active.focus,
.open > .dropdown-togglespan#login_widget > .button.focus,
.open > .dropdown-toggle#logout.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  background-image: none;
}
span#login_widget > .button.disabled:hover,
#logout.disabled:hover,
span#login_widget > .button[disabled]:hover,
#logout[disabled]:hover,
fieldset[disabled] span#login_widget > .button:hover,
fieldset[disabled] #logout:hover,
span#login_widget > .button.disabled:focus,
#logout.disabled:focus,
span#login_widget > .button[disabled]:focus,
#logout[disabled]:focus,
fieldset[disabled] span#login_widget > .button:focus,
fieldset[disabled] #logout:focus,
span#login_widget > .button.disabled.focus,
#logout.disabled.focus,
span#login_widget > .button[disabled].focus,
#logout[disabled].focus,
fieldset[disabled] span#login_widget > .button.focus,
fieldset[disabled] #logout.focus {
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button .badge,
#logout .badge {
  color: #fff;
  background-color: #333;
}
.nav-header {
  text-transform: none;
}
#header > span {
  margin-top: 10px;
}
.modal_stretch .modal-dialog {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  min-height: 80vh;
}
.modal_stretch .modal-dialog .modal-body {
  max-height: calc(100vh - 200px);
  overflow: auto;
  flex: 1;
}
@media (min-width: 768px) {
  .modal .modal-dialog {
    width: 700px;
  }
}
@media (min-width: 768px) {
  select.form-control {
    margin-left: 12px;
    margin-right: 12px;
  }
}
/*!
*
* IPython auth
*
*/
.center-nav {
  display: inline-block;
  margin-bottom: -4px;
}
/*!
*
* IPython tree view
*
*/
/* We need an invisible input field on top of the sentense*/
/* "Drag file onto the list ..." */
.alternate_upload {
  background-color: none;
  display: inline;
}
.alternate_upload.form {
  padding: 0;
  margin: 0;
}
.alternate_upload input.fileinput {
  text-align: center;
  vertical-align: middle;
  display: inline;
  opacity: 0;
  z-index: 2;
  width: 12ex;
  margin-right: -12ex;
}
.alternate_upload .btn-upload {
  height: 22px;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
[dir="rtl"] #tabs li {
  float: right;
}
ul#tabs {
  margin-bottom: 4px;
}
[dir="rtl"] ul#tabs {
  margin-right: 0px;
}
ul#tabs a {
  padding-top: 6px;
  padding-bottom: 4px;
}
ul.breadcrumb a:focus,
ul.breadcrumb a:hover {
  text-decoration: none;
}
ul.breadcrumb i.icon-home {
  font-size: 16px;
  margin-right: 4px;
}
ul.breadcrumb span {
  color: #5e5e5e;
}
.list_toolbar {
  padding: 4px 0 4px 0;
  vertical-align: middle;
}
.list_toolbar .tree-buttons {
  padding-top: 1px;
}
[dir="rtl"] .list_toolbar .tree-buttons {
  float: left !important;
}
[dir="rtl"] .list_toolbar .pull-right {
  padding-top: 1px;
  float: left !important;
}
[dir="rtl"] .list_toolbar .pull-left {
  float: right !important;
}
.dynamic-buttons {
  padding-top: 3px;
  display: inline-block;
}
.list_toolbar [class*="span"] {
  min-height: 24px;
}
.list_header {
  font-weight: bold;
  background-color: #EEE;
}
.list_placeholder {
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
}
.list_container {
  margin-top: 4px;
  margin-bottom: 20px;
  border: 1px solid #ddd;
  border-radius: 2px;
}
.list_container > div {
  border-bottom: 1px solid #ddd;
}
.list_container > div:hover .list-item {
  background-color: red;
}
.list_container > div:last-child {
  border: none;
}
.list_item:hover .list_item {
  background-color: #ddd;
}
.list_item a {
  text-decoration: none;
}
.list_item:hover {
  background-color: #fafafa;
}
.list_header > div,
.list_item > div {
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
.list_header > div input,
.list_item > div input {
  margin-right: 7px;
  margin-left: 14px;
  vertical-align: baseline;
  line-height: 22px;
  position: relative;
  top: -1px;
}
.list_header > div .item_link,
.list_item > div .item_link {
  margin-left: -1px;
  vertical-align: baseline;
  line-height: 22px;
}
.new-file input[type=checkbox] {
  visibility: hidden;
}
.item_name {
  line-height: 22px;
  height: 24px;
}
.item_icon {
  font-size: 14px;
  color: #5e5e5e;
  margin-right: 7px;
  margin-left: 7px;
  line-height: 22px;
  vertical-align: baseline;
}
.item_buttons {
  line-height: 1em;
  margin-left: -5px;
}
.item_buttons .btn,
.item_buttons .btn-group,
.item_buttons .input-group {
  float: left;
}
.item_buttons > .btn,
.item_buttons > .btn-group,
.item_buttons > .input-group {
  margin-left: 5px;
}
.item_buttons .btn {
  min-width: 13ex;
}
.item_buttons .running-indicator {
  padding-top: 4px;
  color: #5cb85c;
}
.item_buttons .kernel-name {
  padding-top: 4px;
  color: #5bc0de;
  margin-right: 7px;
  float: left;
}
.toolbar_info {
  height: 24px;
  line-height: 24px;
}
.list_item input:not([type=checkbox]) {
  padding-top: 3px;
  padding-bottom: 3px;
  height: 22px;
  line-height: 14px;
  margin: 0px;
}
.highlight_text {
  color: blue;
}
#project_name {
  display: inline-block;
  padding-left: 7px;
  margin-left: -2px;
}
#project_name > .breadcrumb {
  padding: 0px;
  margin-bottom: 0px;
  background-color: transparent;
  font-weight: bold;
}
#tree-selector {
  padding-right: 0px;
}
[dir="rtl"] #tree-selector a {
  float: right;
}
#button-select-all {
  min-width: 50px;
}
#select-all {
  margin-left: 7px;
  margin-right: 2px;
}
.menu_icon {
  margin-right: 2px;
}
.tab-content .row {
  margin-left: 0px;
  margin-right: 0px;
}
.folder_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f114";
}
.folder_icon:before.pull-left {
  margin-right: .3em;
}
.folder_icon:before.pull-right {
  margin-left: .3em;
}
.notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
}
.notebook_icon:before.pull-left {
  margin-right: .3em;
}
.notebook_icon:before.pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
  color: #5cb85c;
}
.running_notebook_icon:before.pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.pull-right {
  margin-left: .3em;
}
.file_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f016";
  position: relative;
  top: -2px;
}
.file_icon:before.pull-left {
  margin-right: .3em;
}
.file_icon:before.pull-right {
  margin-left: .3em;
}
#notebook_toolbar .pull-right {
  padding-top: 0px;
  margin-right: -1px;
}
ul#new-menu {
  left: auto;
  right: 0;
}
[dir="rtl"] #new-menu {
  text-align: right;
}
.kernel-menu-icon {
  padding-right: 12px;
  width: 24px;
  content: "\f096";
}
.kernel-menu-icon:before {
  content: "\f096";
}
.kernel-menu-icon-current:before {
  content: "\f00c";
}
#tab_content {
  padding-top: 20px;
}
#running .panel-group .panel {
  margin-top: 3px;
  margin-bottom: 1em;
}
#running .panel-group .panel .panel-heading {
  background-color: #EEE;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
#running .panel-group .panel .panel-heading a:focus,
#running .panel-group .panel .panel-heading a:hover {
  text-decoration: none;
}
#running .panel-group .panel .panel-body {
  padding: 0px;
}
#running .panel-group .panel .panel-body .list_container {
  margin-top: 0px;
  margin-bottom: 0px;
  border: 0px;
  border-radius: 0px;
}
#running .panel-group .panel .panel-body .list_container .list_item {
  border-bottom: 1px solid #ddd;
}
#running .panel-group .panel .panel-body .list_container .list_item:last-child {
  border-bottom: 0px;
}
[dir="rtl"] #running .col-sm-8 {
  float: right !important;
}
.delete-button {
  display: none;
}
.duplicate-button {
  display: none;
}
.rename-button {
  display: none;
}
.shutdown-button {
  display: none;
}
.dynamic-instructions {
  display: inline-block;
  padding-top: 4px;
}
/*!
*
* IPython text editor webapp
*
*/
.selected-keymap i.fa {
  padding: 0px 5px;
}
.selected-keymap i.fa:before {
  content: "\f00c";
}
#mode-menu {
  overflow: auto;
  max-height: 20em;
}
.edit_app #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.edit_app #menubar .navbar {
  /* Use a negative 1 bottom margin, so the border overlaps the border of the
    header */
  margin-bottom: -1px;
}
.dirty-indicator {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator.pull-left {
  margin-right: .3em;
}
.dirty-indicator.pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-dirty.pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-clean.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f00c";
}
.dirty-indicator-clean:before.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.pull-right {
  margin-left: .3em;
}
#filename {
  font-size: 16pt;
  display: table;
  padding: 0px 5px;
}
#current-mode {
  padding-left: 5px;
  padding-right: 5px;
}
#texteditor-backdrop {
  padding-top: 20px;
  padding-bottom: 20px;
}
@media not print {
  #texteditor-backdrop {
    background-color: #EEE;
  }
}
@media print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container {
    padding: 0px;
    background-color: #fff;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area .rendered_html table {
  margin-left: 0;
  margin-right: 0;
}
div.output_area .rendered_html img {
  margin-left: 0;
  margin-right: 0;
}
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}
.rendered_html em {
  font-style: italic;
}
.rendered_html strong {
  font-weight: bold;
}
.rendered_html u {
  text-decoration: underline;
}
.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}
.rendered_html h1 {
  font-size: 185.7%;
  margin: 1.08em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h2 {
  font-size: 157.1%;
  margin: 1.27em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h3 {
  font-size: 128.6%;
  margin: 1.55em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h4 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h5 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h6 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul {
  list-style: disc;
  margin: 0em 2em;
  padding-left: 0px;
}
.rendered_html ul ul {
  list-style: square;
  margin: 0em 2em;
}
.rendered_html ul ul ul {
  list-style: circle;
  margin: 0em 2em;
}
.rendered_html ol {
  list-style: decimal;
  margin: 0em 2em;
  padding-left: 0px;
}
.rendered_html ol ol {
  list-style: upper-alpha;
  margin: 0em 2em;
}
.rendered_html ol ol ol {
  list-style: lower-alpha;
  margin: 0em 2em;
}
.rendered_html ol ol ol ol {
  list-style: lower-roman;
  margin: 0em 2em;
}
.rendered_html ol ol ol ol ol {
  list-style: decimal;
  margin: 0em 2em;
}
.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}
.rendered_html hr {
  color: black;
  background-color: black;
}
.rendered_html pre {
  margin: 1em 2em;
}
.rendered_html pre,
.rendered_html code {
  border: 0;
  background-color: #fff;
  color: #000;
  font-size: 100%;
  padding: 0px;
}
.rendered_html blockquote {
  margin: 1em 2em;
}
.rendered_html table {
  margin-left: auto;
  margin-right: auto;
  border: 1px solid black;
  border-collapse: collapse;
}
.rendered_html tr,
.rendered_html th,
.rendered_html td {
  border: 1px solid black;
  border-collapse: collapse;
  margin: 1em 2em;
}
.rendered_html td,
.rendered_html th {
  text-align: left;
  vertical-align: middle;
  padding: 4px;
}
.rendered_html th {
  font-weight: bold;
}
.rendered_html * + table {
  margin-top: 1em;
}
.rendered_html p {
  text-align: left;
}
.rendered_html * + p {
  margin-top: 1em;
}
.rendered_html img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,
.rendered_html svg {
  max-width: 100%;
  height: auto;
}
.rendered_html img.unconfined,
.rendered_html svg.unconfined {
  max-width: none;
}
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered .rendered_html {
  overflow-x: auto;
  overflow-y: hidden;
}
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
/*!
*
* IPython notebook webapp
*
*/
@media (max-width: 767px) {
  .notebook_app {
    padding-left: 0px;
    padding-right: 0px;
  }
}
#ipython-main-app {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook_panel {
  margin: 0px;
  padding: 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook {
  font-size: 14px;
  line-height: 20px;
  overflow-y: hidden;
  overflow-x: auto;
  width: 100%;
  /* This spaces the page away from the edge of the notebook area */
  padding-top: 20px;
  margin: 0px;
  outline: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  min-height: 100%;
}
@media not print {
  #notebook-container {
    padding: 15px;
    background-color: #fff;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
@media print {
  #notebook-container {
    width: 100%;
  }
}
div.ui-widget-content {
  border: 1px solid #ababab;
  outline: none;
}
pre.dialog {
  background-color: #f7f7f7;
  border: 1px solid #ddd;
  border-radius: 2px;
  padding: 0.4em;
  padding-left: 2em;
}
p.dialog {
  padding: 0.2em;
}
/* Word-wrap output correctly.  This is the CSS3 spelling, though Firefox seems
   to not honor it correctly.  Webkit browsers (Chrome, rekonq, Safari) do.
 */
pre,
code,
kbd,
samp {
  white-space: pre-wrap;
}
#fonttest {
  font-family: monospace;
}
p {
  margin-bottom: 0;
}
.end_space {
  min-height: 100px;
  transition: height .2s ease;
}
.notebook_app > #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
@media not print {
  .notebook_app {
    background-color: #EEE;
  }
}
kbd {
  border-style: solid;
  border-width: 1px;
  box-shadow: none;
  margin: 2px;
  padding-left: 2px;
  padding-right: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
/* CSS for the cell toolbar */
.celltoolbar {
  border: thin solid #CFCFCF;
  border-bottom: none;
  background: #EEE;
  border-radius: 2px 2px 0px 0px;
  width: 100%;
  height: 29px;
  padding-right: 4px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
  display: -webkit-flex;
}
@media print {
  .celltoolbar {
    display: none;
  }
}
.ctb_hideshow {
  display: none;
  vertical-align: bottom;
}
/* ctb_show is added to the ctb_hideshow div to show the cell toolbar.
   Cell toolbars are only shown when the ctb_global_show class is also set.
*/
.ctb_global_show .ctb_show.ctb_hideshow {
  display: block;
}
.ctb_global_show .ctb_show + .input_area,
.ctb_global_show .ctb_show + div.text_cell_input,
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border-top-right-radius: 0px;
  border-top-left-radius: 0px;
}
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border: 1px solid #cfcfcf;
}
.celltoolbar {
  font-size: 87%;
  padding-top: 3px;
}
.celltoolbar select {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  width: inherit;
  font-size: inherit;
  height: 22px;
  padding: 0px;
  display: inline-block;
}
.celltoolbar select:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.celltoolbar select::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.celltoolbar select:-ms-input-placeholder {
  color: #999;
}
.celltoolbar select::-webkit-input-placeholder {
  color: #999;
}
.celltoolbar select::-ms-expand {
  border: 0;
  background-color: transparent;
}
.celltoolbar select[disabled],
.celltoolbar select[readonly],
fieldset[disabled] .celltoolbar select {
  background-color: #eeeeee;
  opacity: 1;
}
.celltoolbar select[disabled],
fieldset[disabled] .celltoolbar select {
  cursor: not-allowed;
}
textarea.celltoolbar select {
  height: auto;
}
select.celltoolbar select {
  height: 30px;
  line-height: 30px;
}
textarea.celltoolbar select,
select[multiple].celltoolbar select {
  height: auto;
}
.celltoolbar label {
  margin-left: 5px;
  margin-right: 5px;
}
.completions {
  position: absolute;
  z-index: 110;
  overflow: hidden;
  border: 1px solid #ababab;
  border-radius: 2px;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  line-height: 1;
}
.completions select {
  background: white;
  outline: none;
  border: none;
  padding: 0px;
  margin: 0px;
  overflow: auto;
  font-family: monospace;
  font-size: 110%;
  color: #000;
  width: auto;
}
.completions select option.context {
  color: #286090;
}
#kernel_logo_widget {
  float: right !important;
  float: right;
}
#kernel_logo_widget .current_kernel_logo {
  display: none;
  margin-top: -1px;
  margin-bottom: -1px;
  width: 32px;
  height: 32px;
}
#menubar {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  margin-top: 1px;
}
#menubar .navbar {
  border-top: 1px;
  border-radius: 0px 0px 2px 2px;
  margin-bottom: 0px;
}
#menubar .navbar-toggle {
  float: left;
  padding-top: 7px;
  padding-bottom: 7px;
  border: none;
}
#menubar .navbar-collapse {
  clear: left;
}
.nav-wrapper {
  border-bottom: 1px solid #e7e7e7;
}
i.menu-icon {
  padding-top: 4px;
}
ul#help_menu li a {
  overflow: hidden;
  padding-right: 2.2em;
}
ul#help_menu li a i {
  margin-right: -1.2em;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu > .dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
}
.dropdown-submenu:hover > .dropdown-menu {
  display: block;
}
.dropdown-submenu > a:after {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  display: block;
  content: "\f0da";
  float: right;
  color: #333333;
  margin-top: 2px;
  margin-right: -10px;
}
.dropdown-submenu > a:after.pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.pull-right {
  margin-left: .3em;
}
.dropdown-submenu:hover > a:after {
  color: #262626;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left > .dropdown-menu {
  left: -100%;
  margin-left: 10px;
}
#notification_area {
  float: right !important;
  float: right;
  z-index: 10;
}
.indicator_area {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
#kernel_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  border-left: 1px solid;
}
#kernel_indicator .kernel_indicator_name {
  padding-left: 5px;
  padding-right: 5px;
}
#modal_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
#readonly-indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  margin-top: 2px;
  margin-bottom: 0px;
  margin-left: 0px;
  margin-right: 0px;
  display: none;
}
.modal_indicator:before {
  width: 1.28571429em;
  text-align: center;
}
.edit_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f040";
}
.edit_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: ' ';
}
.command_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f10c";
}
.kernel_idle_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f111";
}
.kernel_busy_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f1e2";
}
.kernel_dead_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f127";
}
.kernel_disconnected_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.pull-right {
  margin-left: .3em;
}
.notification_widget {
  color: #777;
  z-index: 10;
  background: rgba(240, 240, 240, 0.5);
  margin-right: 4px;
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget:focus,
.notification_widget.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.notification_widget:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  background-image: none;
}
.notification_widget.disabled:hover,
.notification_widget[disabled]:hover,
fieldset[disabled] .notification_widget:hover,
.notification_widget.disabled:focus,
.notification_widget[disabled]:focus,
fieldset[disabled] .notification_widget:focus,
.notification_widget.disabled.focus,
.notification_widget[disabled].focus,
fieldset[disabled] .notification_widget.focus {
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget .badge {
  color: #fff;
  background-color: #333;
}
.notification_widget.warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning:focus,
.notification_widget.warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.notification_widget.warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active:hover,
.notification_widget.warning.active:hover,
.open > .dropdown-toggle.notification_widget.warning:hover,
.notification_widget.warning:active:focus,
.notification_widget.warning.active:focus,
.open > .dropdown-toggle.notification_widget.warning:focus,
.notification_widget.warning:active.focus,
.notification_widget.warning.active.focus,
.open > .dropdown-toggle.notification_widget.warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  background-image: none;
}
.notification_widget.warning.disabled:hover,
.notification_widget.warning[disabled]:hover,
fieldset[disabled] .notification_widget.warning:hover,
.notification_widget.warning.disabled:focus,
.notification_widget.warning[disabled]:focus,
fieldset[disabled] .notification_widget.warning:focus,
.notification_widget.warning.disabled.focus,
.notification_widget.warning[disabled].focus,
fieldset[disabled] .notification_widget.warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.notification_widget.success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success:focus,
.notification_widget.success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.notification_widget.success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active:hover,
.notification_widget.success.active:hover,
.open > .dropdown-toggle.notification_widget.success:hover,
.notification_widget.success:active:focus,
.notification_widget.success.active:focus,
.open > .dropdown-toggle.notification_widget.success:focus,
.notification_widget.success:active.focus,
.notification_widget.success.active.focus,
.open > .dropdown-toggle.notification_widget.success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  background-image: none;
}
.notification_widget.success.disabled:hover,
.notification_widget.success[disabled]:hover,
fieldset[disabled] .notification_widget.success:hover,
.notification_widget.success.disabled:focus,
.notification_widget.success[disabled]:focus,
fieldset[disabled] .notification_widget.success:focus,
.notification_widget.success.disabled.focus,
.notification_widget.success[disabled].focus,
fieldset[disabled] .notification_widget.success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.notification_widget.info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info:focus,
.notification_widget.info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.notification_widget.info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active:hover,
.notification_widget.info.active:hover,
.open > .dropdown-toggle.notification_widget.info:hover,
.notification_widget.info:active:focus,
.notification_widget.info.active:focus,
.open > .dropdown-toggle.notification_widget.info:focus,
.notification_widget.info:active.focus,
.notification_widget.info.active.focus,
.open > .dropdown-toggle.notification_widget.info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  background-image: none;
}
.notification_widget.info.disabled:hover,
.notification_widget.info[disabled]:hover,
fieldset[disabled] .notification_widget.info:hover,
.notification_widget.info.disabled:focus,
.notification_widget.info[disabled]:focus,
fieldset[disabled] .notification_widget.info:focus,
.notification_widget.info.disabled.focus,
.notification_widget.info[disabled].focus,
fieldset[disabled] .notification_widget.info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.notification_widget.danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger:focus,
.notification_widget.danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.notification_widget.danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active:hover,
.notification_widget.danger.active:hover,
.open > .dropdown-toggle.notification_widget.danger:hover,
.notification_widget.danger:active:focus,
.notification_widget.danger.active:focus,
.open > .dropdown-toggle.notification_widget.danger:focus,
.notification_widget.danger:active.focus,
.notification_widget.danger.active.focus,
.open > .dropdown-toggle.notification_widget.danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  background-image: none;
}
.notification_widget.danger.disabled:hover,
.notification_widget.danger[disabled]:hover,
fieldset[disabled] .notification_widget.danger:hover,
.notification_widget.danger.disabled:focus,
.notification_widget.danger[disabled]:focus,
fieldset[disabled] .notification_widget.danger:focus,
.notification_widget.danger.disabled.focus,
.notification_widget.danger[disabled].focus,
fieldset[disabled] .notification_widget.danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger .badge {
  color: #d9534f;
  background-color: #fff;
}
div#pager {
  background-color: #fff;
  font-size: 14px;
  line-height: 20px;
  overflow: hidden;
  display: none;
  position: fixed;
  bottom: 0px;
  width: 100%;
  max-height: 50%;
  padding-top: 8px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  /* Display over codemirror */
  z-index: 100;
  /* Hack which prevents jquery ui resizable from changing top. */
  top: auto !important;
}
div#pager pre {
  line-height: 1.21429em;
  color: #000;
  background-color: #f7f7f7;
  padding: 0.4em;
}
div#pager #pager-button-area {
  position: absolute;
  top: 8px;
  right: 20px;
}
div#pager #pager-contents {
  position: relative;
  overflow: auto;
  width: 100%;
  height: 100%;
}
div#pager #pager-contents #pager-container {
  position: relative;
  padding: 15px 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
div#pager .ui-resizable-handle {
  top: 0px;
  height: 8px;
  background: #f7f7f7;
  border-top: 1px solid #cfcfcf;
  border-bottom: 1px solid #cfcfcf;
  /* This injects handle bars (a short, wide = symbol) for 
        the resize handle. */
}
div#pager .ui-resizable-handle::after {
  content: '';
  top: 2px;
  left: 50%;
  height: 3px;
  width: 30px;
  margin-left: -15px;
  position: absolute;
  border-top: 1px solid #cfcfcf;
}
.quickhelp {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  line-height: 1.8em;
}
.shortcut_key {
  display: inline-block;
  width: 21ex;
  text-align: right;
  font-family: monospace;
}
.shortcut_descr {
  display: inline-block;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
span.save_widget {
  margin-top: 6px;
}
span.save_widget span.filename {
  height: 1em;
  line-height: 1em;
  padding: 3px;
  margin-left: 16px;
  border: none;
  font-size: 146.5%;
  border-radius: 2px;
}
span.save_widget span.filename:hover {
  background-color: #e6e6e6;
}
span.checkpoint_status,
span.autosave_status {
  font-size: small;
}
@media (max-width: 767px) {
  span.save_widget {
    font-size: small;
  }
  span.checkpoint_status,
  span.autosave_status {
    display: none;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  span.checkpoint_status {
    display: none;
  }
  span.autosave_status {
    font-size: x-small;
  }
}
.toolbar {
  padding: 0px;
  margin-left: -5px;
  margin-top: 2px;
  margin-bottom: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.toolbar select,
.toolbar label {
  width: auto;
  vertical-align: middle;
  margin-right: 2px;
  margin-bottom: 0px;
  display: inline;
  font-size: 92%;
  margin-left: 0.3em;
  margin-right: 0.3em;
  padding: 0px;
  padding-top: 3px;
}
.toolbar .btn {
  padding: 2px 8px;
}
.toolbar .btn-group {
  margin-top: 0px;
  margin-left: 5px;
}
#maintoolbar {
  margin-bottom: -3px;
  margin-top: -8px;
  border: 0px;
  min-height: 27px;
  margin-left: 0px;
  padding-top: 11px;
  padding-bottom: 3px;
}
#maintoolbar .navbar-text {
  float: none;
  vertical-align: middle;
  text-align: right;
  margin-left: 5px;
  margin-right: 0px;
  margin-top: 0px;
}
.select-xs {
  height: 24px;
}
.pulse,
.dropdown-menu > li > a.pulse,
li.pulse > a.dropdown-toggle,
li.pulse.open > a.dropdown-toggle {
  background-color: #F37626;
  color: white;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
/** WARNING IF YOU ARE EDITTING THIS FILE, if this is a .css file, It has a lot
 * of chance of beeing generated from the ../less/[samename].less file, you can
 * try to get back the less file by reverting somme commit in history
 **/
/*
 * We'll try to get something pretty, so we
 * have some strange css to have the scroll bar on
 * the left with fix button on the top right of the tooltip
 */
@-moz-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-webkit-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-moz-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
@-webkit-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
/*properties of tooltip after "expand"*/
.bigtooltip {
  overflow: auto;
  height: 200px;
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
}
/*properties of tooltip before "expand"*/
.smalltooltip {
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
  text-overflow: ellipsis;
  overflow: hidden;
  height: 80px;
}
.tooltipbuttons {
  position: absolute;
  padding-right: 15px;
  top: 0px;
  right: 0px;
}
.tooltiptext {
  /*avoid the button to overlap on some docstring*/
  padding-right: 30px;
}
.ipython_tooltip {
  max-width: 700px;
  /*fade-in animation when inserted*/
  -webkit-animation: fadeOut 400ms;
  -moz-animation: fadeOut 400ms;
  animation: fadeOut 400ms;
  -webkit-animation: fadeIn 400ms;
  -moz-animation: fadeIn 400ms;
  animation: fadeIn 400ms;
  vertical-align: middle;
  background-color: #f7f7f7;
  overflow: visible;
  border: #ababab 1px solid;
  outline: none;
  padding: 3px;
  margin: 0px;
  padding-left: 7px;
  font-family: monospace;
  min-height: 50px;
  -moz-box-shadow: 0px 6px 10px -1px #adadad;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  border-radius: 2px;
  position: absolute;
  z-index: 1000;
}
.ipython_tooltip a {
  float: right;
}
.ipython_tooltip .tooltiptext pre {
  border: 0;
  border-radius: 0;
  font-size: 100%;
  background-color: #f7f7f7;
}
.pretooltiparrow {
  left: 0px;
  margin: 0px;
  top: -16px;
  width: 40px;
  height: 16px;
  overflow: hidden;
  position: absolute;
}
.pretooltiparrow:before {
  background-color: #f7f7f7;
  border: 1px #ababab solid;
  z-index: 11;
  content: "";
  position: absolute;
  left: 15px;
  top: 10px;
  width: 25px;
  height: 25px;
  -webkit-transform: rotate(45deg);
  -moz-transform: rotate(45deg);
  -ms-transform: rotate(45deg);
  -o-transform: rotate(45deg);
}
ul.typeahead-list i {
  margin-left: -10px;
  width: 18px;
}
ul.typeahead-list {
  max-height: 80vh;
  overflow: auto;
}
ul.typeahead-list > li > a {
  /** Firefox bug **/
  /* see https://github.com/jupyter/notebook/issues/559 */
  white-space: normal;
}
.cmd-palette .modal-body {
  padding: 7px;
}
.cmd-palette form {
  background: white;
}
.cmd-palette input {
  outline: none;
}
.no-shortcut {
  display: none;
}
.command-shortcut:before {
  content: "(command)";
  padding-right: 3px;
  color: #777777;
}
.edit-shortcut:before {
  content: "(edit)";
  padding-right: 3px;
  color: #777777;
}
#find-and-replace #replace-preview .match,
#find-and-replace #replace-preview .insert {
  background-color: #BBDEFB;
  border-color: #90CAF9;
  border-style: solid;
  border-width: 1px;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .match {
  background-color: #FFCDD2;
  border-color: #EF9A9A;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .insert {
  background-color: #C8E6C9;
  border-color: #A5D6A7;
  border-radius: 0px;
}
#find-and-replace #replace-preview {
  max-height: 60vh;
  overflow: auto;
}
#find-and-replace #replace-preview pre {
  padding: 5px 10px;
}
.terminal-app {
  background: #EEE;
}
.terminal-app #header {
  background: #fff;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.terminal-app .terminal {
  width: 100%;
  float: left;
  font-family: monospace;
  color: white;
  background: black;
  padding: 0.4em;
  border-radius: 2px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
}
.terminal-app .terminal,
.terminal-app .terminal dummy-screen {
  line-height: 1em;
  font-size: 14px;
}
.terminal-app .terminal .xterm-rows {
  padding: 10px;
}
.terminal-app .terminal-cursor {
  color: black;
  background: white;
}
.terminal-app #terminado-container {
  margin-top: 20px;
}
/*# sourceMappingURL=style.min.css.map */
    </style>
<style type="text/css">
    .highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
    </style>
<style type="text/css">
    
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

    </style>


<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
body {
  overflow: visible;
  padding: 8px;
}

div#notebook {
  overflow: visible;
  border-top: none;
}@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Implement-Deep-Q-Learning-Network:-Train-and-Test">Implement Deep Q-Learning Network: Train and Test<a class="anchor-link" href="#Implement-Deep-Q-Learning-Network:-Train-and-Test">&#182;</a></h1><h2 id="Imports:">Imports:<a class="anchor-link" href="#Imports:">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">import</span> <span class="nn">argparse</span>

<span class="kn">from</span> <span class="nn">PIL</span> <span class="k">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">gym</span>

<span class="kn">from</span> <span class="nn">keras.models</span> <span class="k">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="k">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Activation</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Convolution2D</span><span class="p">,</span> <span class="n">Permute</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="k">import</span> <span class="n">Adam</span>
<span class="kn">import</span> <span class="nn">keras.backend</span> <span class="k">as</span> <span class="nn">K</span>

<span class="kn">from</span> <span class="nn">rl.agents.dqn</span> <span class="k">import</span> <span class="n">DQNAgent</span>
<span class="kn">from</span> <span class="nn">rl.policy</span> <span class="k">import</span> <span class="n">LinearAnnealedPolicy</span><span class="p">,</span> <span class="n">BoltzmannQPolicy</span><span class="p">,</span> <span class="n">EpsGreedyQPolicy</span>
<span class="kn">from</span> <span class="nn">rl.memory</span> <span class="k">import</span> <span class="n">SequentialMemory</span>
<span class="kn">from</span> <span class="nn">rl.core</span> <span class="k">import</span> <span class="n">Processor</span>
<span class="kn">from</span> <span class="nn">rl.callbacks</span> <span class="k">import</span> <span class="n">FileLogger</span><span class="p">,</span> <span class="n">ModelIntervalCheckpoint</span>

<span class="kn">from</span> <span class="nn">gym</span> <span class="k">import</span> <span class="n">Wrapper</span>
<span class="kn">from</span> <span class="nn">gym.wrappers.monitoring</span> <span class="k">import</span> <span class="n">stats_recorder</span><span class="p">,</span> <span class="n">video_recorder</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>C:\Users\Luke\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Train-Model:">Train Model:<a class="anchor-link" href="#Train-Model:">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[24]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;LunarLander-v2&#39;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="c1">#env = gym.wrappers.Monitor(env, </span>
<span class="c1">#                           &#39;recording&#39;, </span>
<span class="c1">#                           resume=True, </span>
<span class="c1">#                           video_callable=lambda count: count % 10 == 0)</span>
<span class="c1"># record_video_every</span>
<span class="n">nb_actions</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>

<span class="n">WINDOW_LENGTH</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">WINDOW_LENGTH</span><span class="p">,)</span> <span class="o">+</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>

<span class="n">memory</span> <span class="o">=</span> <span class="n">SequentialMemory</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">500000</span><span class="p">,</span> <span class="n">window_length</span><span class="o">=</span><span class="n">WINDOW_LENGTH</span><span class="p">)</span>

<span class="n">policy</span> <span class="o">=</span> <span class="n">LinearAnnealedPolicy</span><span class="p">(</span><span class="n">EpsGreedyQPolicy</span><span class="p">(),</span> <span class="n">attr</span><span class="o">=</span><span class="s1">&#39;eps&#39;</span><span class="p">,</span> <span class="n">value_max</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">value_min</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span> <span class="n">value_test</span><span class="o">=.</span><span class="mi">05</span><span class="p">,</span> <span class="n">nb_steps</span><span class="o">=</span><span class="mi">1000000</span><span class="p">)</span>

<span class="n">dqn</span> <span class="o">=</span> <span class="n">DQNAgent</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">nb_actions</span><span class="o">=</span><span class="n">nb_actions</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span> <span class="n">memory</span><span class="o">=</span><span class="n">memory</span><span class="p">,</span> \
               <span class="n">nb_steps_warmup</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=.</span><span class="mi">99</span><span class="p">,</span> <span class="n">target_model_update</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> \
               <span class="n">train_interval</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">delta_clip</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>

<span class="n">dqn</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=.</span><span class="mi">00025</span><span class="p">),</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mae&#39;</span><span class="p">])</span>

<span class="n">fitted</span> <span class="o">=</span> <span class="n">dqn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">nb_steps</span><span class="o">=</span><span class="mi">2000000</span><span class="p">,</span> <span class="n">visualize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">dqn</span><span class="o">.</span><span class="n">save_weights</span><span class="p">(</span><span class="s1">&#39;dqn_model1_weights.h5f&#39;</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre><span class="ansi-yellow-fg">WARN: gym.spaces.Box autodetected dtype as &lt;class &#39;numpy.float32&#39;&gt;. Please provide explicit dtype.</span>
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_10 (Flatten)         (None, 800)               0         
_________________________________________________________________
dense_23 (Dense)             (None, 512)               410112    
_________________________________________________________________
activation_44 (Activation)   (None, 512)               0         
_________________________________________________________________
dense_24 (Dense)             (None, 64)                32832     
_________________________________________________________________
activation_45 (Activation)   (None, 64)                0         
_________________________________________________________________
dense_25 (Dense)             (None, 16)                1040      
_________________________________________________________________
activation_46 (Activation)   (None, 16)                0         
_________________________________________________________________
dense_26 (Dense)             (None, 4)                 68        
_________________________________________________________________
activation_47 (Activation)   (None, 4)                 0         
=================================================================
Total params: 444,052
Trainable params: 444,052
Non-trainable params: 0
_________________________________________________________________
None
Training for 2000000 steps ...
      83/2000000: episode: 1, duration: 1.041s, episode steps: 83, steps per second: 80, episode reward: -141.641, mean reward: -1.707 [-100.000, 6.293], mean action: 1.602 [0.000, 3.000], mean observation: 0.024 [-1.256, 4.420], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
     158/2000000: episode: 2, duration: 0.157s, episode steps: 75, steps per second: 477, episode reward: -129.249, mean reward: -1.723 [-100.000, 8.258], mean action: 1.387 [0.000, 3.000], mean observation: 0.015 [-1.396, 1.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
     263/2000000: episode: 3, duration: 0.253s, episode steps: 105, steps per second: 415, episode reward: -352.492, mean reward: -3.357 [-100.000, 6.612], mean action: 1.505 [0.000, 3.000], mean observation: -0.074 [-6.147, 1.472], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
     351/2000000: episode: 4, duration: 0.183s, episode steps: 88, steps per second: 481, episode reward: -284.525, mean reward: -3.233 [-100.000, 28.347], mean action: 1.580 [0.000, 3.000], mean observation: 0.010 [-1.469, 4.369], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
     417/2000000: episode: 5, duration: 0.166s, episode steps: 66, steps per second: 398, episode reward: -136.113, mean reward: -2.062 [-100.000, 12.346], mean action: 1.576 [0.000, 3.000], mean observation: -0.108 [-2.057, 1.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
     519/2000000: episode: 6, duration: 0.249s, episode steps: 102, steps per second: 409, episode reward: -243.501, mean reward: -2.387 [-100.000, 41.714], mean action: 1.304 [0.000, 3.000], mean observation: 0.085 [-4.675, 1.162], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
     616/2000000: episode: 7, duration: 0.207s, episode steps: 97, steps per second: 470, episode reward: -179.390, mean reward: -1.849 [-100.000, 17.738], mean action: 1.361 [0.000, 3.000], mean observation: 0.090 [-3.288, 1.037], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
     717/2000000: episode: 8, duration: 0.265s, episode steps: 101, steps per second: 381, episode reward: -387.931, mean reward: -3.841 [-100.000, 5.468], mean action: 1.554 [0.000, 3.000], mean observation: 0.055 [-6.182, 1.652], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
     823/2000000: episode: 9, duration: 0.335s, episode steps: 106, steps per second: 317, episode reward: -185.300, mean reward: -1.748 [-100.000, 8.669], mean action: 1.575 [0.000, 3.000], mean observation: 0.049 [-1.190, 4.018], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
     925/2000000: episode: 10, duration: 0.276s, episode steps: 102, steps per second: 370, episode reward: -325.906, mean reward: -3.195 [-100.000, 1.337], mean action: 1.578 [0.000, 3.000], mean observation: -0.005 [-1.431, 1.066], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    1018/2000000: episode: 11, duration: 0.268s, episode steps: 93, steps per second: 346, episode reward: -156.371, mean reward: -1.681 [-100.000, 7.358], mean action: 1.452 [0.000, 3.000], mean observation: -0.041 [-4.921, 1.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    1138/2000000: episode: 12, duration: 0.289s, episode steps: 120, steps per second: 415, episode reward: -274.683, mean reward: -2.289 [-100.000, 80.754], mean action: 1.367 [0.000, 3.000], mean observation: 0.044 [-1.336, 3.112], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    1209/2000000: episode: 13, duration: 0.167s, episode steps: 71, steps per second: 425, episode reward: -127.536, mean reward: -1.796 [-100.000, 62.918], mean action: 1.549 [0.000, 3.000], mean observation: -0.159 [-1.319, 1.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    1283/2000000: episode: 14, duration: 0.255s, episode steps: 74, steps per second: 290, episode reward: -344.795, mean reward: -4.659 [-100.000, 2.055], mean action: 1.405 [0.000, 3.000], mean observation: 0.005 [-1.521, 6.206], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    1407/2000000: episode: 15, duration: 0.286s, episode steps: 124, steps per second: 434, episode reward: -159.348, mean reward: -1.285 [-100.000, 6.921], mean action: 1.444 [0.000, 3.000], mean observation: -0.021 [-4.648, 1.007], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    1516/2000000: episode: 16, duration: 0.221s, episode steps: 109, steps per second: 494, episode reward: -129.890, mean reward: -1.192 [-100.000, 12.197], mean action: 1.560 [0.000, 3.000], mean observation: -0.075 [-3.720, 1.034], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    1634/2000000: episode: 17, duration: 0.267s, episode steps: 118, steps per second: 442, episode reward: -281.469, mean reward: -2.385 [-100.000, 2.004], mean action: 1.458 [0.000, 3.000], mean observation: 0.168 [-0.928, 1.427], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    1745/2000000: episode: 18, duration: 0.225s, episode steps: 111, steps per second: 494, episode reward: -302.702, mean reward: -2.727 [-100.000, 1.106], mean action: 1.559 [0.000, 3.000], mean observation: -0.037 [-1.400, 1.051], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    1842/2000000: episode: 19, duration: 0.210s, episode steps: 97, steps per second: 463, episode reward: -334.936, mean reward: -3.453 [-100.000, 16.361], mean action: 1.680 [0.000, 3.000], mean observation: 0.037 [-4.405, 1.341], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    1954/2000000: episode: 20, duration: 0.255s, episode steps: 112, steps per second: 440, episode reward: -171.911, mean reward: -1.535 [-100.000, 11.144], mean action: 1.509 [0.000, 3.000], mean observation: -0.099 [-1.557, 1.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    2032/2000000: episode: 21, duration: 0.162s, episode steps: 78, steps per second: 482, episode reward: -245.927, mean reward: -3.153 [-100.000, 7.154], mean action: 1.359 [0.000, 3.000], mean observation: -0.040 [-1.517, 1.038], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    2161/2000000: episode: 22, duration: 0.284s, episode steps: 129, steps per second: 454, episode reward: -183.179, mean reward: -1.420 [-100.000, 11.141], mean action: 1.473 [0.000, 3.000], mean observation: 0.178 [-3.217, 1.009], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    2241/2000000: episode: 23, duration: 0.159s, episode steps: 80, steps per second: 504, episode reward: -254.581, mean reward: -3.182 [-100.000, 70.513], mean action: 1.575 [0.000, 3.000], mean observation: -0.026 [-3.013, 1.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    2328/2000000: episode: 24, duration: 0.200s, episode steps: 87, steps per second: 434, episode reward: -358.821, mean reward: -4.124 [-100.000, 21.254], mean action: 1.667 [0.000, 3.000], mean observation: -0.006 [-1.466, 5.484], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    2406/2000000: episode: 25, duration: 0.162s, episode steps: 78, steps per second: 480, episode reward: -312.324, mean reward: -4.004 [-100.000, 8.845], mean action: 1.321 [0.000, 3.000], mean observation: 0.036 [-1.448, 4.469], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    2476/2000000: episode: 26, duration: 0.156s, episode steps: 70, steps per second: 448, episode reward: -79.479, mean reward: -1.135 [-100.000, 61.105], mean action: 1.571 [0.000, 3.000], mean observation: 0.000 [-1.555, 3.920], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    2594/2000000: episode: 27, duration: 0.263s, episode steps: 118, steps per second: 449, episode reward: -412.837, mean reward: -3.499 [-100.000, 5.389], mean action: 1.636 [0.000, 3.000], mean observation: 0.055 [-5.241, 1.523], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    2685/2000000: episode: 28, duration: 0.246s, episode steps: 91, steps per second: 370, episode reward: -129.134, mean reward: -1.419 [-100.000, 10.886], mean action: 1.593 [0.000, 3.000], mean observation: -0.037 [-3.739, 1.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    2776/2000000: episode: 29, duration: 0.471s, episode steps: 91, steps per second: 193, episode reward: -429.567, mean reward: -4.721 [-100.000, 4.147], mean action: 1.505 [0.000, 3.000], mean observation: 0.032 [-1.741, 4.063], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    2880/2000000: episode: 30, duration: 0.274s, episode steps: 104, steps per second: 379, episode reward: -215.498, mean reward: -2.072 [-100.000, 8.369], mean action: 1.413 [0.000, 3.000], mean observation: -0.010 [-5.120, 1.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    2993/2000000: episode: 31, duration: 0.236s, episode steps: 113, steps per second: 479, episode reward: -205.534, mean reward: -1.819 [-100.000, 8.209], mean action: 1.628 [0.000, 3.000], mean observation: 0.050 [-1.451, 4.688], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    3070/2000000: episode: 32, duration: 0.353s, episode steps: 77, steps per second: 218, episode reward: -315.420, mean reward: -4.096 [-100.000, 1.831], mean action: 1.519 [0.000, 3.000], mean observation: 0.019 [-1.659, 6.045], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    3168/2000000: episode: 33, duration: 0.229s, episode steps: 98, steps per second: 428, episode reward: -390.960, mean reward: -3.989 [-100.000, 3.677], mean action: 1.571 [0.000, 3.000], mean observation: 0.032 [-4.558, 1.476], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    3268/2000000: episode: 34, duration: 0.277s, episode steps: 100, steps per second: 361, episode reward: -184.705, mean reward: -1.847 [-100.000, 10.723], mean action: 1.560 [0.000, 3.000], mean observation: -0.042 [-1.515, 4.664], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    3386/2000000: episode: 35, duration: 0.325s, episode steps: 118, steps per second: 363, episode reward: -140.293, mean reward: -1.189 [-100.000, 8.259], mean action: 1.441 [0.000, 3.000], mean observation: 0.015 [-4.913, 1.020], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    3489/2000000: episode: 36, duration: 0.295s, episode steps: 103, steps per second: 349, episode reward: -331.199, mean reward: -3.216 [-100.000, 4.311], mean action: 1.311 [0.000, 3.000], mean observation: 0.009 [-1.534, 1.343], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    3561/2000000: episode: 37, duration: 0.286s, episode steps: 72, steps per second: 252, episode reward: -162.915, mean reward: -2.263 [-100.000, 5.531], mean action: 1.375 [0.000, 3.000], mean observation: 0.060 [-4.389, 1.200], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    3645/2000000: episode: 38, duration: 0.166s, episode steps: 84, steps per second: 506, episode reward: -148.360, mean reward: -1.766 [-100.000, 21.577], mean action: 1.393 [0.000, 3.000], mean observation: 0.122 [-1.308, 4.756], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    3741/2000000: episode: 39, duration: 0.211s, episode steps: 96, steps per second: 456, episode reward: -341.093, mean reward: -3.553 [-100.000, 5.087], mean action: 1.625 [0.000, 3.000], mean observation: -0.100 [-1.939, 1.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    3819/2000000: episode: 40, duration: 0.153s, episode steps: 78, steps per second: 510, episode reward: -270.670, mean reward: -3.470 [-100.000, 24.421], mean action: 1.282 [0.000, 3.000], mean observation: 0.095 [-1.463, 4.475], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    3929/2000000: episode: 41, duration: 0.390s, episode steps: 110, steps per second: 282, episode reward: -406.313, mean reward: -3.694 [-100.000, 3.450], mean action: 1.645 [0.000, 3.000], mean observation: 0.034 [-1.751, 1.371], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    4023/2000000: episode: 42, duration: 0.234s, episode steps: 94, steps per second: 402, episode reward: -145.268, mean reward: -1.545 [-100.000, 7.245], mean action: 1.426 [0.000, 3.000], mean observation: -0.040 [-1.370, 1.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    4126/2000000: episode: 43, duration: 0.219s, episode steps: 103, steps per second: 469, episode reward: -172.475, mean reward: -1.675 [-100.000, 37.544], mean action: 1.408 [0.000, 3.000], mean observation: 0.089 [-1.428, 3.159], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    4222/2000000: episode: 44, duration: 0.222s, episode steps: 96, steps per second: 431, episode reward: -314.072, mean reward: -3.272 [-100.000, 117.583], mean action: 1.542 [0.000, 3.000], mean observation: -0.010 [-2.893, 1.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    4308/2000000: episode: 45, duration: 0.365s, episode steps: 86, steps per second: 235, episode reward: -206.314, mean reward: -2.399 [-100.000, 7.826], mean action: 1.488 [0.000, 3.000], mean observation: -0.023 [-1.224, 3.895], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    4397/2000000: episode: 46, duration: 0.173s, episode steps: 89, steps per second: 514, episode reward: -228.740, mean reward: -2.570 [-100.000, 9.889], mean action: 1.596 [0.000, 3.000], mean observation: -0.129 [-3.777, 1.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    4471/2000000: episode: 47, duration: 0.182s, episode steps: 74, steps per second: 407, episode reward: -308.311, mean reward: -4.166 [-100.000, 5.831], mean action: 1.649 [0.000, 3.000], mean observation: -0.039 [-4.287, 1.270], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    4557/2000000: episode: 48, duration: 0.273s, episode steps: 86, steps per second: 315, episode reward: -485.745, mean reward: -5.648 [-100.000, 4.367], mean action: 1.523 [0.000, 3.000], mean observation: -0.017 [-1.962, 4.303], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    4629/2000000: episode: 49, duration: 0.148s, episode steps: 72, steps per second: 486, episode reward: -161.856, mean reward: -2.248 [-100.000, 7.473], mean action: 1.458 [0.000, 3.000], mean observation: -0.001 [-4.802, 1.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    4704/2000000: episode: 50, duration: 0.173s, episode steps: 75, steps per second: 434, episode reward: -160.259, mean reward: -2.137 [-100.000, 5.726], mean action: 1.427 [0.000, 3.000], mean observation: -0.109 [-1.563, 5.025], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    4776/2000000: episode: 51, duration: 0.146s, episode steps: 72, steps per second: 492, episode reward: -172.784, mean reward: -2.400 [-100.000, 12.574], mean action: 1.583 [0.000, 3.000], mean observation: 0.052 [-1.280, 2.236], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    4882/2000000: episode: 52, duration: 0.249s, episode steps: 106, steps per second: 426, episode reward: -273.808, mean reward: -2.583 [-100.000, 3.716], mean action: 1.557 [0.000, 3.000], mean observation: -0.082 [-1.522, 1.515], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    4935/2000000: episode: 53, duration: 0.114s, episode steps: 53, steps per second: 465, episode reward: -221.544, mean reward: -4.180 [-100.000, 6.063], mean action: 1.377 [0.000, 3.000], mean observation: -0.112 [-1.628, 1.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --
    5031/2000000: episode: 54, duration: 3.033s, episode steps: 96, steps per second: 32, episode reward: -266.325, mean reward: -2.774 [-100.000, 6.969], mean action: 1.844 [0.000, 3.000], mean observation: -0.153 [-5.146, 1.000], loss: 3.351781, mean_absolute_error: 1.042846, mean_q: 0.207218, mean_eps: 0.995486
    5136/2000000: episode: 55, duration: 1.493s, episode steps: 105, steps per second: 70, episode reward: -490.070, mean reward: -4.667 [-100.000, 1.031], mean action: 1.610 [0.000, 3.000], mean observation: -0.012 [-2.142, 1.387], loss: 2.613690, mean_absolute_error: 1.011581, mean_q: 0.022184, mean_eps: 0.995426
    5213/2000000: episode: 56, duration: 1.209s, episode steps: 77, steps per second: 64, episode reward: -175.689, mean reward: -2.282 [-100.000, 7.527], mean action: 1.455 [0.000, 3.000], mean observation: 0.001 [-4.097, 1.000], loss: 2.027688, mean_absolute_error: 1.407927, mean_q: -0.036736, mean_eps: 0.995343
    5286/2000000: episode: 57, duration: 1.196s, episode steps: 73, steps per second: 61, episode reward: -142.003, mean reward: -1.945 [-100.000, 7.919], mean action: 1.315 [0.000, 3.000], mean observation: 0.006 [-1.330, 4.004], loss: 2.474608, mean_absolute_error: 1.908338, mean_q: -0.043101, mean_eps: 0.995275
    5419/2000000: episode: 58, duration: 1.959s, episode steps: 133, steps per second: 68, episode reward: -258.646, mean reward: -1.945 [-100.000, 32.805], mean action: 1.639 [0.000, 3.000], mean observation: -0.027 [-1.410, 4.201], loss: 2.300287, mean_absolute_error: 1.871495, mean_q: -0.230613, mean_eps: 0.995183
    5485/2000000: episode: 59, duration: 0.965s, episode steps: 66, steps per second: 68, episode reward: -162.688, mean reward: -2.465 [-100.000, 12.177], mean action: 1.379 [0.000, 3.000], mean observation: 0.025 [-4.690, 1.000], loss: 1.349740, mean_absolute_error: 1.629346, mean_q: 0.026947, mean_eps: 0.995093
    5557/2000000: episode: 60, duration: 1.018s, episode steps: 72, steps per second: 71, episode reward: -140.376, mean reward: -1.950 [-100.000, 6.353], mean action: 1.597 [0.000, 3.000], mean observation: -0.020 [-1.486, 1.000], loss: 2.187380, mean_absolute_error: 1.887088, mean_q: -0.203316, mean_eps: 0.995030
    5643/2000000: episode: 61, duration: 1.183s, episode steps: 86, steps per second: 73, episode reward: -182.146, mean reward: -2.118 [-100.000, 12.645], mean action: 1.360 [0.000, 3.000], mean observation: 0.008 [-2.830, 1.000], loss: 1.462027, mean_absolute_error: 1.731381, mean_q: -0.254450, mean_eps: 0.994960
    5712/2000000: episode: 62, duration: 1.012s, episode steps: 69, steps per second: 68, episode reward: -156.001, mean reward: -2.261 [-100.000, 7.549], mean action: 1.159 [0.000, 3.000], mean observation: 0.000 [-4.963, 1.000], loss: 2.155747, mean_absolute_error: 1.938206, mean_q: -0.041335, mean_eps: 0.994892
    5819/2000000: episode: 63, duration: 1.513s, episode steps: 107, steps per second: 71, episode reward: -195.164, mean reward: -1.824 [-100.000, 1.344], mean action: 1.598 [0.000, 3.000], mean observation: -0.083 [-1.119, 1.087], loss: 1.539408, mean_absolute_error: 1.838816, mean_q: 0.080736, mean_eps: 0.994812
    5886/2000000: episode: 64, duration: 0.994s, episode steps: 67, steps per second: 67, episode reward: -161.589, mean reward: -2.412 [-100.000, 7.085], mean action: 1.448 [0.000, 3.000], mean observation: -0.052 [-1.490, 4.786], loss: 2.342501, mean_absolute_error: 2.025163, mean_q: 0.222779, mean_eps: 0.994733
    5950/2000000: episode: 65, duration: 0.936s, episode steps: 64, steps per second: 68, episode reward: -120.338, mean reward: -1.880 [-100.000, 62.193], mean action: 1.344 [0.000, 3.000], mean observation: -0.064 [-1.382, 1.000], loss: 1.999442, mean_absolute_error: 1.926507, mean_q: -0.062863, mean_eps: 0.994674
    6042/2000000: episode: 66, duration: 1.265s, episode steps: 92, steps per second: 73, episode reward: -434.262, mean reward: -4.720 [-100.000, 11.811], mean action: 1.424 [0.000, 3.000], mean observation: -0.073 [-1.837, 3.370], loss: 1.442596, mean_absolute_error: 1.807643, mean_q: 0.092636, mean_eps: 0.994604
    6120/2000000: episode: 67, duration: 1.153s, episode steps: 78, steps per second: 68, episode reward: -127.926, mean reward: -1.640 [-100.000, 16.512], mean action: 1.500 [0.000, 3.000], mean observation: 0.001 [-1.520, 1.000], loss: 2.006024, mean_absolute_error: 1.963007, mean_q: -0.060633, mean_eps: 0.994528
    6228/2000000: episode: 68, duration: 1.782s, episode steps: 108, steps per second: 61, episode reward: -172.115, mean reward: -1.594 [-100.000, 11.465], mean action: 1.620 [0.000, 3.000], mean observation: -0.130 [-1.510, 1.000], loss: 1.704870, mean_absolute_error: 1.933141, mean_q: 0.141202, mean_eps: 0.994445
    6345/2000000: episode: 69, duration: 1.684s, episode steps: 117, steps per second: 69, episode reward: -273.489, mean reward: -2.338 [-100.000, 11.601], mean action: 1.556 [0.000, 3.000], mean observation: -0.099 [-4.317, 1.000], loss: 1.922841, mean_absolute_error: 1.975178, mean_q: -0.101836, mean_eps: 0.994343
    6483/2000000: episode: 70, duration: 1.903s, episode steps: 138, steps per second: 73, episode reward: -130.374, mean reward: -0.945 [-100.000, 17.728], mean action: 1.529 [0.000, 3.000], mean observation: 0.116 [-1.340, 1.052], loss: 1.426876, mean_absolute_error: 1.881146, mean_q: 0.002165, mean_eps: 0.994227
    6579/2000000: episode: 71, duration: 1.351s, episode steps: 96, steps per second: 71, episode reward: -191.307, mean reward: -1.993 [-100.000, 68.284], mean action: 1.406 [0.000, 3.000], mean observation: -0.108 [-1.739, 2.146], loss: 1.565681, mean_absolute_error: 1.895657, mean_q: 0.285564, mean_eps: 0.994123
    6653/2000000: episode: 72, duration: 1.071s, episode steps: 74, steps per second: 69, episode reward: -128.579, mean reward: -1.738 [-100.000, 13.142], mean action: 1.541 [0.000, 3.000], mean observation: 0.035 [-1.353, 4.972], loss: 1.645495, mean_absolute_error: 1.936180, mean_q: 0.137480, mean_eps: 0.994046
    6716/2000000: episode: 73, duration: 0.910s, episode steps: 63, steps per second: 69, episode reward: -147.279, mean reward: -2.338 [-100.000, 12.480], mean action: 1.794 [0.000, 3.000], mean observation: -0.037 [-1.421, 2.824], loss: 1.022953, mean_absolute_error: 1.774839, mean_q: 0.034459, mean_eps: 0.993984
    6827/2000000: episode: 74, duration: 1.576s, episode steps: 111, steps per second: 70, episode reward: -188.653, mean reward: -1.700 [-100.000, 13.231], mean action: 1.559 [0.000, 3.000], mean observation: -0.094 [-1.555, 4.593], loss: 1.696522, mean_absolute_error: 1.908707, mean_q: 0.213093, mean_eps: 0.993907
    6897/2000000: episode: 75, duration: 0.998s, episode steps: 70, steps per second: 70, episode reward: -161.433, mean reward: -2.306 [-100.000, 6.012], mean action: 1.429 [0.000, 3.000], mean observation: -0.076 [-5.130, 1.000], loss: 2.418020, mean_absolute_error: 2.141341, mean_q: 0.107013, mean_eps: 0.993824
    6967/2000000: episode: 76, duration: 0.995s, episode steps: 70, steps per second: 70, episode reward: -143.813, mean reward: -2.054 [-100.000, 6.801], mean action: 1.443 [0.000, 3.000], mean observation: 0.079 [-1.488, 5.430], loss: 2.027377, mean_absolute_error: 2.088880, mean_q: 0.133326, mean_eps: 0.993761
    7078/2000000: episode: 77, duration: 1.563s, episode steps: 111, steps per second: 71, episode reward: -206.082, mean reward: -1.857 [-100.000, 5.792], mean action: 1.613 [0.000, 3.000], mean observation: 0.042 [-1.458, 1.035], loss: 1.433294, mean_absolute_error: 1.900618, mean_q: 0.153312, mean_eps: 0.993680
    7176/2000000: episode: 78, duration: 1.424s, episode steps: 98, steps per second: 69, episode reward: -278.717, mean reward: -2.844 [-100.000, 1.199], mean action: 1.490 [0.000, 3.000], mean observation: 0.178 [-1.122, 1.511], loss: 1.693172, mean_absolute_error: 1.916475, mean_q: 0.132845, mean_eps: 0.993587
    7242/2000000: episode: 79, duration: 0.959s, episode steps: 66, steps per second: 69, episode reward: -100.987, mean reward: -1.530 [-100.000, 48.901], mean action: 1.697 [0.000, 3.000], mean observation: 0.007 [-1.328, 1.432], loss: 2.228135, mean_absolute_error: 2.104658, mean_q: 0.050234, mean_eps: 0.993513
    7356/2000000: episode: 80, duration: 1.609s, episode steps: 114, steps per second: 71, episode reward: -287.455, mean reward: -2.522 [-100.000, 1.414], mean action: 1.596 [0.000, 3.000], mean observation: -0.067 [-1.406, 1.011], loss: 1.445395, mean_absolute_error: 1.824757, mean_q: 0.237408, mean_eps: 0.993432
    7451/2000000: episode: 81, duration: 1.376s, episode steps: 95, steps per second: 69, episode reward: -421.244, mean reward: -4.434 [-100.000, 103.724], mean action: 1.568 [0.000, 3.000], mean observation: 0.131 [-1.491, 3.238], loss: 1.896255, mean_absolute_error: 2.054312, mean_q: 0.226252, mean_eps: 0.993338
    7546/2000000: episode: 82, duration: 1.349s, episode steps: 95, steps per second: 70, episode reward: -338.400, mean reward: -3.562 [-100.000, 0.703], mean action: 1.526 [0.000, 3.000], mean observation: 0.169 [-1.087, 1.670], loss: 1.695909, mean_absolute_error: 2.011472, mean_q: 0.254783, mean_eps: 0.993252
    7612/2000000: episode: 83, duration: 0.962s, episode steps: 66, steps per second: 69, episode reward: -118.487, mean reward: -1.795 [-100.000, 7.267], mean action: 1.500 [0.000, 3.000], mean observation: -0.014 [-1.488, 1.000], loss: 1.274934, mean_absolute_error: 1.827933, mean_q: 0.191380, mean_eps: 0.993180
    7802/2000000: episode: 84, duration: 2.771s, episode steps: 190, steps per second: 69, episode reward: -192.721, mean reward: -1.014 [-100.000, 37.836], mean action: 1.547 [0.000, 3.000], mean observation: -0.141 [-1.948, 1.000], loss: 2.236170, mean_absolute_error: 2.159892, mean_q: 0.239692, mean_eps: 0.993065
    7874/2000000: episode: 85, duration: 1.017s, episode steps: 72, steps per second: 71, episode reward: -80.099, mean reward: -1.112 [-100.000, 11.176], mean action: 1.389 [0.000, 3.000], mean observation: 0.004 [-3.675, 1.000], loss: 1.920016, mean_absolute_error: 2.006004, mean_q: 0.310996, mean_eps: 0.992946
    7947/2000000: episode: 86, duration: 1.024s, episode steps: 73, steps per second: 71, episode reward: -359.693, mean reward: -4.927 [-100.000, 5.306], mean action: 1.589 [0.000, 3.000], mean observation: -0.039 [-1.532, 1.621], loss: 1.979941, mean_absolute_error: 2.055746, mean_q: 0.254977, mean_eps: 0.992881
    8012/2000000: episode: 87, duration: 0.955s, episode steps: 65, steps per second: 68, episode reward: -122.341, mean reward: -1.882 [-100.000, 18.503], mean action: 1.615 [0.000, 3.000], mean observation: -0.099 [-4.593, 1.000], loss: 0.935702, mean_absolute_error: 1.828184, mean_q: 0.207735, mean_eps: 0.992820
    8112/2000000: episode: 88, duration: 1.448s, episode steps: 100, steps per second: 69, episode reward: -160.825, mean reward: -1.608 [-100.000, 29.854], mean action: 1.300 [0.000, 3.000], mean observation: -0.036 [-2.489, 1.174], loss: 2.308194, mean_absolute_error: 2.243797, mean_q: 0.285992, mean_eps: 0.992746
    8221/2000000: episode: 89, duration: 1.602s, episode steps: 109, steps per second: 68, episode reward: -216.227, mean reward: -1.984 [-100.000, 6.947], mean action: 1.550 [0.000, 3.000], mean observation: 0.019 [-1.399, 4.635], loss: 1.412232, mean_absolute_error: 1.992654, mean_q: 0.349152, mean_eps: 0.992651
    8324/2000000: episode: 90, duration: 1.477s, episode steps: 103, steps per second: 70, episode reward: -168.175, mean reward: -1.633 [-100.000, 6.605], mean action: 1.524 [0.000, 3.000], mean observation: 0.089 [-1.350, 1.000], loss: 1.563665, mean_absolute_error: 1.982804, mean_q: 0.446104, mean_eps: 0.992555
    8449/2000000: episode: 91, duration: 1.795s, episode steps: 125, steps per second: 70, episode reward: -247.644, mean reward: -1.981 [-100.000, 6.841], mean action: 1.536 [0.000, 3.000], mean observation: 0.071 [-1.486, 4.704], loss: 1.049176, mean_absolute_error: 1.832565, mean_q: 0.299246, mean_eps: 0.992453
    8543/2000000: episode: 92, duration: 1.313s, episode steps: 94, steps per second: 72, episode reward: -120.797, mean reward: -1.285 [-100.000, 12.418], mean action: 1.564 [0.000, 3.000], mean observation: -0.001 [-1.355, 1.000], loss: 1.989339, mean_absolute_error: 2.147150, mean_q: 0.215288, mean_eps: 0.992354
    8612/2000000: episode: 93, duration: 1.021s, episode steps: 69, steps per second: 68, episode reward: -146.147, mean reward: -2.118 [-100.000, 9.309], mean action: 1.507 [0.000, 3.000], mean observation: -0.089 [-4.415, 1.000], loss: 1.388218, mean_absolute_error: 1.941536, mean_q: 0.334472, mean_eps: 0.992282
    8724/2000000: episode: 94, duration: 1.618s, episode steps: 112, steps per second: 69, episode reward: -338.117, mean reward: -3.019 [-100.000, 9.856], mean action: 1.580 [0.000, 3.000], mean observation: 0.019 [-6.264, 1.521], loss: 1.490096, mean_absolute_error: 1.944829, mean_q: 0.286544, mean_eps: 0.992201
    8805/2000000: episode: 95, duration: 1.186s, episode steps: 81, steps per second: 68, episode reward: -129.858, mean reward: -1.603 [-100.000, 12.731], mean action: 1.642 [0.000, 3.000], mean observation: -0.086 [-1.242, 4.121], loss: 1.339269, mean_absolute_error: 1.904647, mean_q: 0.269324, mean_eps: 0.992112
    8929/2000000: episode: 96, duration: 1.783s, episode steps: 124, steps per second: 70, episode reward: -421.087, mean reward: -3.396 [-100.000, 0.757], mean action: 1.347 [0.000, 3.000], mean observation: 0.009 [-1.785, 1.533], loss: 1.656019, mean_absolute_error: 2.034919, mean_q: 0.283605, mean_eps: 0.992019
    9010/2000000: episode: 97, duration: 1.120s, episode steps: 81, steps per second: 72, episode reward: -123.779, mean reward: -1.528 [-100.000, 24.919], mean action: 1.704 [0.000, 3.000], mean observation: -0.062 [-1.056, 4.157], loss: 1.382766, mean_absolute_error: 1.996629, mean_q: 0.212827, mean_eps: 0.991927
    9071/2000000: episode: 98, duration: 0.865s, episode steps: 61, steps per second: 70, episode reward: -144.604, mean reward: -2.371 [-100.000, 16.048], mean action: 1.311 [0.000, 3.000], mean observation: -0.025 [-1.472, 1.013], loss: 1.829385, mean_absolute_error: 2.089841, mean_q: 0.275855, mean_eps: 0.991864
    9156/2000000: episode: 99, duration: 1.225s, episode steps: 85, steps per second: 69, episode reward: -546.728, mean reward: -6.432 [-100.000, 5.007], mean action: 1.565 [0.000, 3.000], mean observation: -0.021 [-1.802, 3.238], loss: 1.933300, mean_absolute_error: 2.116459, mean_q: 0.334779, mean_eps: 0.991799
    9230/2000000: episode: 100, duration: 1.072s, episode steps: 74, steps per second: 69, episode reward: -118.906, mean reward: -1.607 [-100.000, 25.688], mean action: 1.662 [0.000, 3.000], mean observation: -0.120 [-4.855, 1.000], loss: 1.632156, mean_absolute_error: 2.012510, mean_q: 0.272133, mean_eps: 0.991727
    9324/2000000: episode: 101, duration: 1.344s, episode steps: 94, steps per second: 70, episode reward: -214.862, mean reward: -2.286 [-100.000, 87.787], mean action: 1.298 [0.000, 3.000], mean observation: -0.026 [-3.042, 1.000], loss: 1.555046, mean_absolute_error: 2.022199, mean_q: 0.181650, mean_eps: 0.991652
    9431/2000000: episode: 102, duration: 1.523s, episode steps: 107, steps per second: 70, episode reward: -149.372, mean reward: -1.396 [-100.000, 27.265], mean action: 1.598 [0.000, 3.000], mean observation: -0.075 [-1.297, 3.843], loss: 1.920696, mean_absolute_error: 2.124986, mean_q: 0.373528, mean_eps: 0.991562
    9511/2000000: episode: 103, duration: 1.124s, episode steps: 80, steps per second: 71, episode reward: -185.922, mean reward: -2.324 [-100.000, 6.367], mean action: 1.525 [0.000, 3.000], mean observation: -0.011 [-1.434, 4.543], loss: 2.128243, mean_absolute_error: 2.185444, mean_q: 0.521941, mean_eps: 0.991477
    9628/2000000: episode: 104, duration: 1.689s, episode steps: 117, steps per second: 69, episode reward: -270.617, mean reward: -2.313 [-100.000, 30.648], mean action: 1.308 [0.000, 3.000], mean observation: 0.070 [-1.602, 4.625], loss: 1.533198, mean_absolute_error: 1.982285, mean_q: 0.479533, mean_eps: 0.991389
    9725/2000000: episode: 105, duration: 1.411s, episode steps: 97, steps per second: 69, episode reward: -392.704, mean reward: -4.048 [-100.000, 0.673], mean action: 1.423 [0.000, 3.000], mean observation: 0.042 [-5.140, 1.558], loss: 1.499807, mean_absolute_error: 1.982037, mean_q: 0.217349, mean_eps: 0.991292
    9797/2000000: episode: 106, duration: 1.023s, episode steps: 72, steps per second: 70, episode reward: -149.464, mean reward: -2.076 [-100.000, 6.591], mean action: 1.569 [0.000, 3.000], mean observation: -0.039 [-1.321, 4.416], loss: 2.132752, mean_absolute_error: 2.130330, mean_q: 0.622765, mean_eps: 0.991214
    9861/2000000: episode: 107, duration: 0.898s, episode steps: 64, steps per second: 71, episode reward: -143.016, mean reward: -2.235 [-100.000, 11.171], mean action: 1.656 [0.000, 3.000], mean observation: -0.026 [-4.381, 1.000], loss: 1.721578, mean_absolute_error: 2.166567, mean_q: 0.432230, mean_eps: 0.991153
    9929/2000000: episode: 108, duration: 0.964s, episode steps: 68, steps per second: 71, episode reward: -130.297, mean reward: -1.916 [-100.000, 13.074], mean action: 1.529 [0.000, 3.000], mean observation: -0.045 [-1.483, 1.000], loss: 2.002688, mean_absolute_error: 2.126404, mean_q: 0.304628, mean_eps: 0.991094
   10030/2000000: episode: 109, duration: 1.399s, episode steps: 101, steps per second: 72, episode reward: -183.335, mean reward: -1.815 [-100.000, 6.222], mean action: 1.465 [0.000, 3.000], mean observation: -0.088 [-5.133, 1.000], loss: 1.719741, mean_absolute_error: 2.127928, mean_q: 0.286678, mean_eps: 0.991018
   10102/2000000: episode: 110, duration: 1.017s, episode steps: 72, steps per second: 71, episode reward: -121.611, mean reward: -1.689 [-100.000, 22.332], mean action: 1.681 [0.000, 3.000], mean observation: -0.011 [-1.502, 1.000], loss: 2.533284, mean_absolute_error: 2.297787, mean_q: 0.570751, mean_eps: 0.990941
   10231/2000000: episode: 111, duration: 1.813s, episode steps: 129, steps per second: 71, episode reward: -468.451, mean reward: -3.631 [-100.000, 5.537], mean action: 1.349 [0.000, 3.000], mean observation: 0.068 [-1.931, 3.176], loss: 1.736423, mean_absolute_error: 2.357941, mean_q: 0.460537, mean_eps: 0.990851
   10314/2000000: episode: 112, duration: 1.184s, episode steps: 83, steps per second: 70, episode reward: -434.629, mean reward: -5.236 [-100.000, 0.091], mean action: 1.313 [0.000, 3.000], mean observation: 0.007 [-1.600, 1.959], loss: 2.521508, mean_absolute_error: 2.490394, mean_q: 0.606277, mean_eps: 0.990755
   10403/2000000: episode: 113, duration: 1.296s, episode steps: 89, steps per second: 69, episode reward: -178.146, mean reward: -2.002 [-100.000, 5.363], mean action: 1.393 [0.000, 3.000], mean observation: 0.111 [-1.412, 1.907], loss: 1.991290, mean_absolute_error: 2.432054, mean_q: 0.493668, mean_eps: 0.990678
   10475/2000000: episode: 114, duration: 1.082s, episode steps: 72, steps per second: 67, episode reward: -228.282, mean reward: -3.171 [-100.000, 4.875], mean action: 1.639 [0.000, 3.000], mean observation: 0.029 [-1.317, 2.398], loss: 2.290649, mean_absolute_error: 2.689936, mean_q: 0.428665, mean_eps: 0.990606
   10554/2000000: episode: 115, duration: 1.169s, episode steps: 79, steps per second: 68, episode reward: -381.778, mean reward: -4.833 [-100.000, 4.234], mean action: 1.266 [0.000, 3.000], mean observation: -0.002 [-3.174, 1.628], loss: 0.869386, mean_absolute_error: 2.251476, mean_q: 0.134167, mean_eps: 0.990537
   10628/2000000: episode: 116, duration: 1.059s, episode steps: 74, steps per second: 70, episode reward: -160.337, mean reward: -2.167 [-100.000, 11.823], mean action: 1.378 [0.000, 3.000], mean observation: 0.015 [-4.380, 1.000], loss: 1.807515, mean_absolute_error: 2.387125, mean_q: 0.464184, mean_eps: 0.990469
   10706/2000000: episode: 117, duration: 1.129s, episode steps: 78, steps per second: 69, episode reward: -152.567, mean reward: -1.956 [-100.000, 6.124], mean action: 1.205 [0.000, 3.000], mean observation: 0.020 [-4.922, 1.000], loss: 1.232492, mean_absolute_error: 2.292263, mean_q: 0.688970, mean_eps: 0.990401
   10775/2000000: episode: 118, duration: 0.973s, episode steps: 69, steps per second: 71, episode reward: -176.940, mean reward: -2.564 [-100.000, 6.978], mean action: 1.870 [0.000, 3.000], mean observation: -0.134 [-1.513, 3.197], loss: 1.377460, mean_absolute_error: 2.145377, mean_q: 0.793811, mean_eps: 0.990334
   10890/2000000: episode: 119, duration: 1.626s, episode steps: 115, steps per second: 71, episode reward: -171.339, mean reward: -1.490 [-100.000, 6.959], mean action: 1.435 [0.000, 3.000], mean observation: -0.014 [-4.962, 1.033], loss: 1.780370, mean_absolute_error: 2.510343, mean_q: 0.494722, mean_eps: 0.990251
   10955/2000000: episode: 120, duration: 0.914s, episode steps: 65, steps per second: 71, episode reward: -150.411, mean reward: -2.314 [-100.000, 7.341], mean action: 1.477 [0.000, 3.000], mean observation: -0.114 [-5.794, 1.000], loss: 2.081349, mean_absolute_error: 2.527420, mean_q: 0.743756, mean_eps: 0.990170
   11058/2000000: episode: 121, duration: 1.465s, episode steps: 103, steps per second: 70, episode reward: -196.886, mean reward: -1.912 [-100.000, 11.000], mean action: 1.456 [0.000, 3.000], mean observation: 0.037 [-1.499, 4.481], loss: 1.697088, mean_absolute_error: 2.429686, mean_q: 0.955120, mean_eps: 0.990095
   11156/2000000: episode: 122, duration: 1.415s, episode steps: 98, steps per second: 69, episode reward: -157.186, mean reward: -1.604 [-100.000, 8.062], mean action: 1.500 [0.000, 3.000], mean observation: -0.017 [-1.407, 4.840], loss: 0.801362, mean_absolute_error: 2.262017, mean_q: 0.656877, mean_eps: 0.990005
   11228/2000000: episode: 123, duration: 1.064s, episode steps: 72, steps per second: 68, episode reward: -85.196, mean reward: -1.183 [-100.000, 17.316], mean action: 1.472 [0.000, 3.000], mean observation: -0.023 [-1.284, 1.000], loss: 1.117370, mean_absolute_error: 2.151786, mean_q: 0.968461, mean_eps: 0.989929
   11302/2000000: episode: 124, duration: 1.072s, episode steps: 74, steps per second: 69, episode reward: -128.803, mean reward: -1.741 [-100.000, 7.362], mean action: 1.676 [0.000, 3.000], mean observation: -0.098 [-4.046, 1.000], loss: 2.601436, mean_absolute_error: 2.905512, mean_q: 0.874199, mean_eps: 0.989862
   11407/2000000: episode: 125, duration: 1.493s, episode steps: 105, steps per second: 70, episode reward: -159.146, mean reward: -1.516 [-100.000, 11.315], mean action: 1.648 [0.000, 3.000], mean observation: -0.025 [-1.227, 2.925], loss: 1.470163, mean_absolute_error: 2.379541, mean_q: 0.734649, mean_eps: 0.989781
   11517/2000000: episode: 126, duration: 1.591s, episode steps: 110, steps per second: 69, episode reward: -125.349, mean reward: -1.140 [-100.000, 7.469], mean action: 1.491 [0.000, 3.000], mean observation: 0.104 [-1.223, 4.188], loss: 1.670390, mean_absolute_error: 2.558963, mean_q: 0.777170, mean_eps: 0.989684
   11635/2000000: episode: 127, duration: 1.652s, episode steps: 118, steps per second: 71, episode reward: -273.427, mean reward: -2.317 [-100.000, 8.009], mean action: 1.508 [0.000, 3.000], mean observation: 0.085 [-1.463, 4.264], loss: 1.348799, mean_absolute_error: 2.354442, mean_q: 0.877801, mean_eps: 0.989582
   11696/2000000: episode: 128, duration: 0.908s, episode steps: 61, steps per second: 67, episode reward: -107.491, mean reward: -1.762 [-100.000, 19.155], mean action: 1.557 [0.000, 3.000], mean observation: -0.094 [-1.596, 1.000], loss: 2.245975, mean_absolute_error: 2.748002, mean_q: 0.241347, mean_eps: 0.989502
   11771/2000000: episode: 129, duration: 1.091s, episode steps: 75, steps per second: 69, episode reward: -141.580, mean reward: -1.888 [-100.000, 9.117], mean action: 1.560 [0.000, 3.000], mean observation: -0.007 [-4.529, 1.000], loss: 1.754260, mean_absolute_error: 2.499052, mean_q: 0.674228, mean_eps: 0.989441
   11877/2000000: episode: 130, duration: 1.524s, episode steps: 106, steps per second: 70, episode reward: -388.898, mean reward: -3.669 [-100.000, 50.222], mean action: 1.632 [0.000, 3.000], mean observation: 0.000 [-3.391, 1.350], loss: 1.741473, mean_absolute_error: 2.419262, mean_q: 0.766529, mean_eps: 0.989358
   11974/2000000: episode: 131, duration: 1.376s, episode steps: 97, steps per second: 70, episode reward: -96.118, mean reward: -0.991 [-100.000, 77.578], mean action: 1.567 [0.000, 3.000], mean observation: -0.006 [-1.282, 2.519], loss: 1.538322, mean_absolute_error: 2.335435, mean_q: 0.988359, mean_eps: 0.989267
   12033/2000000: episode: 132, duration: 0.846s, episode steps: 59, steps per second: 70, episode reward: -193.156, mean reward: -3.274 [-100.000, 7.723], mean action: 1.441 [0.000, 3.000], mean observation: -0.099 [-1.440, 1.000], loss: 1.508216, mean_absolute_error: 2.442030, mean_q: 0.761400, mean_eps: 0.989196
   12135/2000000: episode: 133, duration: 1.409s, episode steps: 102, steps per second: 72, episode reward: -397.893, mean reward: -3.901 [-100.000, 5.122], mean action: 1.333 [0.000, 3.000], mean observation: 0.032 [-1.638, 1.747], loss: 1.608652, mean_absolute_error: 2.445392, mean_q: 0.676156, mean_eps: 0.989124
   12246/2000000: episode: 134, duration: 1.572s, episode steps: 111, steps per second: 71, episode reward: -187.694, mean reward: -1.691 [-100.000, 5.347], mean action: 1.450 [0.000, 3.000], mean observation: 0.151 [-2.812, 1.000], loss: 1.976181, mean_absolute_error: 2.549322, mean_q: 0.891894, mean_eps: 0.989029
   12313/2000000: episode: 135, duration: 0.977s, episode steps: 67, steps per second: 69, episode reward: -101.707, mean reward: -1.518 [-100.000, 12.910], mean action: 1.806 [0.000, 3.000], mean observation: -0.045 [-4.654, 1.000], loss: 1.392632, mean_absolute_error: 2.487997, mean_q: 0.598703, mean_eps: 0.988948
   12421/2000000: episode: 136, duration: 1.559s, episode steps: 108, steps per second: 69, episode reward: -179.588, mean reward: -1.663 [-100.000, 48.267], mean action: 1.593 [0.000, 3.000], mean observation: 0.058 [-1.773, 1.266], loss: 1.574657, mean_absolute_error: 2.505158, mean_q: 0.983107, mean_eps: 0.988869
   12511/2000000: episode: 137, duration: 1.295s, episode steps: 90, steps per second: 69, episode reward: -260.475, mean reward: -2.894 [-100.000, 4.859], mean action: 1.478 [0.000, 3.000], mean observation: 0.077 [-1.512, 4.143], loss: 1.473309, mean_absolute_error: 2.514058, mean_q: 0.715953, mean_eps: 0.988781
   12596/2000000: episode: 138, duration: 1.228s, episode steps: 85, steps per second: 69, episode reward: -146.556, mean reward: -1.724 [-100.000, 7.555], mean action: 1.318 [0.000, 3.000], mean observation: 0.064 [-1.505, 1.000], loss: 2.332825, mean_absolute_error: 2.841593, mean_q: 0.446841, mean_eps: 0.988703
   12692/2000000: episode: 139, duration: 1.411s, episode steps: 96, steps per second: 68, episode reward: -305.066, mean reward: -3.178 [-100.000, 23.981], mean action: 1.417 [0.000, 3.000], mean observation: 0.008 [-1.307, 4.225], loss: 1.870282, mean_absolute_error: 2.521884, mean_q: 0.620201, mean_eps: 0.988622
   12810/2000000: episode: 140, duration: 1.715s, episode steps: 118, steps per second: 69, episode reward: -324.024, mean reward: -2.746 [-100.000, 1.495], mean action: 1.525 [0.000, 3.000], mean observation: 0.130 [-1.354, 1.377], loss: 1.930515, mean_absolute_error: 2.535109, mean_q: 0.717789, mean_eps: 0.988525
   12913/2000000: episode: 141, duration: 1.540s, episode steps: 103, steps per second: 67, episode reward: -472.977, mean reward: -4.592 [-100.000, 1.078], mean action: 1.379 [0.000, 3.000], mean observation: 0.031 [-2.510, 2.045], loss: 1.534432, mean_absolute_error: 2.506965, mean_q: 0.479976, mean_eps: 0.988424
   13016/2000000: episode: 142, duration: 1.487s, episode steps: 103, steps per second: 69, episode reward: -176.957, mean reward: -1.718 [-100.000, 4.637], mean action: 1.359 [0.000, 3.000], mean observation: 0.140 [-3.817, 1.000], loss: 1.171331, mean_absolute_error: 2.475183, mean_q: 0.719851, mean_eps: 0.988332
   13095/2000000: episode: 143, duration: 1.120s, episode steps: 79, steps per second: 71, episode reward: -308.782, mean reward: -3.909 [-100.000, 111.806], mean action: 1.532 [0.000, 3.000], mean observation: -0.067 [-3.063, 1.020], loss: 0.947707, mean_absolute_error: 2.482952, mean_q: 0.294384, mean_eps: 0.988251
   13158/2000000: episode: 144, duration: 0.909s, episode steps: 63, steps per second: 69, episode reward: -134.953, mean reward: -2.142 [-100.000, 10.974], mean action: 1.571 [0.000, 3.000], mean observation: -0.080 [-5.626, 1.000], loss: 1.425909, mean_absolute_error: 2.639465, mean_q: 0.667957, mean_eps: 0.988187
   13259/2000000: episode: 145, duration: 1.421s, episode steps: 101, steps per second: 71, episode reward: -163.772, mean reward: -1.622 [-100.000, 12.342], mean action: 1.594 [0.000, 3.000], mean observation: 0.178 [-1.217, 4.176], loss: 2.161529, mean_absolute_error: 2.839609, mean_q: 0.632843, mean_eps: 0.988113
   13354/2000000: episode: 146, duration: 1.349s, episode steps: 95, steps per second: 70, episode reward: -14.563, mean reward: -0.153 [-100.000, 100.235], mean action: 1.432 [0.000, 3.000], mean observation: -0.019 [-1.355, 2.159], loss: 1.088589, mean_absolute_error: 2.488711, mean_q: 0.721240, mean_eps: 0.988025
   13441/2000000: episode: 147, duration: 1.229s, episode steps: 87, steps per second: 71, episode reward: -214.842, mean reward: -2.469 [-100.000, 32.612], mean action: 1.483 [0.000, 3.000], mean observation: -0.037 [-1.424, 1.877], loss: 1.615578, mean_absolute_error: 2.461554, mean_q: 0.851935, mean_eps: 0.987942
   13538/2000000: episode: 148, duration: 1.349s, episode steps: 97, steps per second: 72, episode reward: -395.209, mean reward: -4.074 [-100.000, 1.073], mean action: 1.299 [0.000, 3.000], mean observation: -0.007 [-1.586, 1.575], loss: 1.363965, mean_absolute_error: 2.479094, mean_q: 0.685838, mean_eps: 0.987859
   13610/2000000: episode: 149, duration: 1.001s, episode steps: 72, steps per second: 72, episode reward: -131.441, mean reward: -1.826 [-100.000, 47.191], mean action: 1.431 [0.000, 3.000], mean observation: 0.050 [-1.424, 1.879], loss: 1.022333, mean_absolute_error: 2.419465, mean_q: 0.702576, mean_eps: 0.987783
   13692/2000000: episode: 150, duration: 1.169s, episode steps: 82, steps per second: 70, episode reward: -134.426, mean reward: -1.639 [-100.000, 8.250], mean action: 1.634 [0.000, 3.000], mean observation: 0.010 [-4.914, 1.000], loss: 1.374569, mean_absolute_error: 2.412835, mean_q: 0.786577, mean_eps: 0.987715
   13774/2000000: episode: 151, duration: 1.194s, episode steps: 82, steps per second: 69, episode reward: -99.132, mean reward: -1.209 [-100.000, 12.994], mean action: 1.573 [0.000, 3.000], mean observation: -0.004 [-1.267, 1.000], loss: 1.122988, mean_absolute_error: 2.323976, mean_q: 0.670124, mean_eps: 0.987641
   13883/2000000: episode: 152, duration: 1.534s, episode steps: 109, steps per second: 71, episode reward: -164.535, mean reward: -1.509 [-100.000, 3.483], mean action: 1.477 [0.000, 3.000], mean observation: -0.039 [-1.450, 5.306], loss: 1.707132, mean_absolute_error: 2.540838, mean_q: 0.621066, mean_eps: 0.987555
   13979/2000000: episode: 153, duration: 1.359s, episode steps: 96, steps per second: 71, episode reward: -446.472, mean reward: -4.651 [-100.000, 0.644], mean action: 1.698 [0.000, 3.000], mean observation: 0.074 [-1.869, 1.711], loss: 1.099193, mean_absolute_error: 2.408563, mean_q: 0.675552, mean_eps: 0.987463
   14082/2000000: episode: 154, duration: 1.441s, episode steps: 103, steps per second: 71, episode reward: -281.687, mean reward: -2.735 [-100.000, 18.678], mean action: 1.553 [0.000, 3.000], mean observation: 0.006 [-1.416, 2.812], loss: 1.636281, mean_absolute_error: 2.486495, mean_q: 0.684273, mean_eps: 0.987373
   14194/2000000: episode: 155, duration: 1.330s, episode steps: 112, steps per second: 84, episode reward: -185.821, mean reward: -1.659 [-100.000, 9.540], mean action: 1.455 [0.000, 3.000], mean observation: 0.071 [-3.683, 1.000], loss: 1.827651, mean_absolute_error: 2.550803, mean_q: 0.631901, mean_eps: 0.987276
   14263/2000000: episode: 156, duration: 0.832s, episode steps: 69, steps per second: 83, episode reward: -180.292, mean reward: -2.613 [-100.000, 5.347], mean action: 1.594 [0.000, 3.000], mean observation: 0.039 [-1.458, 4.794], loss: 1.329283, mean_absolute_error: 2.562029, mean_q: 0.238950, mean_eps: 0.987195
   14376/2000000: episode: 157, duration: 1.480s, episode steps: 113, steps per second: 76, episode reward: -449.881, mean reward: -3.981 [-100.000, 6.250], mean action: 1.451 [0.000, 3.000], mean observation: 0.044 [-4.469, 1.733], loss: 1.078262, mean_absolute_error: 2.458869, mean_q: 0.547508, mean_eps: 0.987114
   14473/2000000: episode: 158, duration: 1.204s, episode steps: 97, steps per second: 81, episode reward: -250.074, mean reward: -2.578 [-100.000, 80.624], mean action: 1.546 [0.000, 3.000], mean observation: 0.080 [-2.967, 1.108], loss: 1.197383, mean_absolute_error: 2.455212, mean_q: 0.778934, mean_eps: 0.987018
   14583/2000000: episode: 159, duration: 1.385s, episode steps: 110, steps per second: 79, episode reward: -343.851, mean reward: -3.126 [-100.000, 78.476], mean action: 1.509 [0.000, 3.000], mean observation: -0.034 [-1.295, 2.979], loss: 1.722361, mean_absolute_error: 2.575611, mean_q: 0.668266, mean_eps: 0.986925
   14651/2000000: episode: 160, duration: 0.977s, episode steps: 68, steps per second: 70, episode reward: -129.085, mean reward: -1.898 [-100.000, 9.540], mean action: 1.574 [0.000, 3.000], mean observation: -0.055 [-1.432, 5.229], loss: 1.814290, mean_absolute_error: 2.793023, mean_q: 0.522515, mean_eps: 0.986846
   14718/2000000: episode: 161, duration: 0.875s, episode steps: 67, steps per second: 77, episode reward: -226.370, mean reward: -3.379 [-100.000, 47.396], mean action: 1.358 [0.000, 3.000], mean observation: -0.025 [-1.493, 4.779], loss: 1.889446, mean_absolute_error: 2.581877, mean_q: 0.871732, mean_eps: 0.986784
   14833/2000000: episode: 162, duration: 1.390s, episode steps: 115, steps per second: 83, episode reward: -250.700, mean reward: -2.180 [-100.000, 5.681], mean action: 1.539 [0.000, 3.000], mean observation: -0.048 [-4.129, 1.060], loss: 1.421025, mean_absolute_error: 2.578507, mean_q: 0.434508, mean_eps: 0.986702
   14935/2000000: episode: 163, duration: 1.514s, episode steps: 102, steps per second: 67, episode reward: -377.538, mean reward: -3.701 [-100.000, 40.958], mean action: 1.422 [0.000, 3.000], mean observation: -0.047 [-1.614, 2.584], loss: 1.535503, mean_absolute_error: 2.592182, mean_q: 0.448689, mean_eps: 0.986604
   15067/2000000: episode: 164, duration: 1.912s, episode steps: 132, steps per second: 69, episode reward: -166.039, mean reward: -1.258 [-100.000, 7.938], mean action: 1.591 [0.000, 3.000], mean observation: -0.034 [-1.299, 3.864], loss: 1.327202, mean_absolute_error: 2.521162, mean_q: 0.573360, mean_eps: 0.986500
   15179/2000000: episode: 165, duration: 1.595s, episode steps: 112, steps per second: 70, episode reward: -301.810, mean reward: -2.695 [-100.000, 1.226], mean action: 1.625 [0.000, 3.000], mean observation: 0.149 [-1.283, 1.352], loss: 1.123487, mean_absolute_error: 2.475724, mean_q: 0.844885, mean_eps: 0.986390
   15246/2000000: episode: 166, duration: 0.994s, episode steps: 67, steps per second: 67, episode reward: -270.198, mean reward: -4.033 [-100.000, 29.671], mean action: 1.358 [0.000, 3.000], mean observation: -0.029 [-1.365, 3.997], loss: 1.156262, mean_absolute_error: 2.521763, mean_q: 0.536411, mean_eps: 0.986309
   15331/2000000: episode: 167, duration: 1.241s, episode steps: 85, steps per second: 69, episode reward: -232.362, mean reward: -2.734 [-100.000, 30.435], mean action: 1.306 [0.000, 3.000], mean observation: -0.007 [-1.487, 4.402], loss: 1.423896, mean_absolute_error: 2.534930, mean_q: 0.739995, mean_eps: 0.986241
   15442/2000000: episode: 168, duration: 1.635s, episode steps: 111, steps per second: 68, episode reward: -187.562, mean reward: -1.690 [-100.000, 11.325], mean action: 1.559 [0.000, 3.000], mean observation: 0.108 [-3.945, 1.000], loss: 1.438751, mean_absolute_error: 2.600368, mean_q: 0.371502, mean_eps: 0.986153
   15524/2000000: episode: 169, duration: 1.198s, episode steps: 82, steps per second: 68, episode reward: -146.454, mean reward: -1.786 [-100.000, 7.037], mean action: 1.622 [0.000, 3.000], mean observation: 0.070 [-3.472, 1.000], loss: 1.337865, mean_absolute_error: 2.745701, mean_q: 0.066754, mean_eps: 0.986066
   15636/2000000: episode: 170, duration: 1.903s, episode steps: 112, steps per second: 59, episode reward: -143.842, mean reward: -1.284 [-100.000, 11.697], mean action: 1.741 [0.000, 3.000], mean observation: 0.017 [-1.366, 4.589], loss: 1.423301, mean_absolute_error: 2.464096, mean_q: 0.602777, mean_eps: 0.985980
   15700/2000000: episode: 171, duration: 1.013s, episode steps: 64, steps per second: 63, episode reward: -120.730, mean reward: -1.886 [-100.000, 6.551], mean action: 1.641 [0.000, 3.000], mean observation: 0.040 [-1.364, 4.928], loss: 1.207387, mean_absolute_error: 2.224035, mean_q: 1.060497, mean_eps: 0.985901
   15765/2000000: episode: 172, duration: 1.040s, episode steps: 65, steps per second: 63, episode reward: -97.088, mean reward: -1.494 [-100.000, 17.028], mean action: 1.462 [0.000, 3.000], mean observation: -0.137 [-1.447, 1.000], loss: 1.678220, mean_absolute_error: 2.561043, mean_q: 0.523696, mean_eps: 0.985841
   15843/2000000: episode: 173, duration: 1.138s, episode steps: 78, steps per second: 69, episode reward: -124.083, mean reward: -1.591 [-100.000, 7.021], mean action: 1.628 [0.000, 3.000], mean observation: -0.030 [-4.292, 1.000], loss: 0.947368, mean_absolute_error: 2.418599, mean_q: 0.814515, mean_eps: 0.985776
   15936/2000000: episode: 174, duration: 1.389s, episode steps: 93, steps per second: 67, episode reward: -378.921, mean reward: -4.074 [-100.000, 5.191], mean action: 1.538 [0.000, 3.000], mean observation: 0.033 [-3.467, 1.853], loss: 1.519213, mean_absolute_error: 2.454347, mean_q: 0.552213, mean_eps: 0.985701
   16049/2000000: episode: 175, duration: 1.663s, episode steps: 113, steps per second: 68, episode reward: -190.361, mean reward: -1.685 [-100.000, 36.371], mean action: 1.619 [0.000, 3.000], mean observation: -0.084 [-5.352, 1.000], loss: 1.414286, mean_absolute_error: 2.587206, mean_q: 0.654697, mean_eps: 0.985607
   16126/2000000: episode: 176, duration: 1.109s, episode steps: 77, steps per second: 69, episode reward: -168.470, mean reward: -2.188 [-100.000, 7.367], mean action: 1.286 [0.000, 3.000], mean observation: -0.048 [-5.966, 1.000], loss: 2.063353, mean_absolute_error: 2.717140, mean_q: 0.625131, mean_eps: 0.985521
   16202/2000000: episode: 177, duration: 1.086s, episode steps: 76, steps per second: 70, episode reward: -115.237, mean reward: -1.516 [-100.000, 53.915], mean action: 1.368 [0.000, 3.000], mean observation: 0.008 [-4.507, 1.000], loss: 1.449142, mean_absolute_error: 2.586275, mean_q: 0.512563, mean_eps: 0.985452
   16281/2000000: episode: 178, duration: 1.132s, episode steps: 79, steps per second: 70, episode reward: -111.729, mean reward: -1.414 [-100.000, 7.918], mean action: 1.405 [0.000, 3.000], mean observation: 0.018 [-3.344, 1.000], loss: 0.887847, mean_absolute_error: 2.237548, mean_q: 0.877985, mean_eps: 0.985382
   16398/2000000: episode: 179, duration: 1.662s, episode steps: 117, steps per second: 70, episode reward: -255.553, mean reward: -2.184 [-100.000, 36.832], mean action: 1.581 [0.000, 3.000], mean observation: -0.023 [-4.649, 1.061], loss: 1.392275, mean_absolute_error: 2.461696, mean_q: 0.718738, mean_eps: 0.985294
   16471/2000000: episode: 180, duration: 1.024s, episode steps: 73, steps per second: 71, episode reward: -146.389, mean reward: -2.005 [-100.000, 8.735], mean action: 1.712 [0.000, 3.000], mean observation: -0.069 [-1.488, 5.170], loss: 1.100290, mean_absolute_error: 2.333546, mean_q: 0.822335, mean_eps: 0.985209
   16536/2000000: episode: 181, duration: 1.002s, episode steps: 65, steps per second: 65, episode reward: -205.284, mean reward: -3.158 [-100.000, 6.244], mean action: 1.508 [0.000, 3.000], mean observation: 0.009 [-1.363, 1.000], loss: 0.929890, mean_absolute_error: 2.338940, mean_q: 0.564560, mean_eps: 0.985148
   16617/2000000: episode: 182, duration: 1.194s, episode steps: 81, steps per second: 68, episode reward: -257.640, mean reward: -3.181 [-100.000, 6.705], mean action: 1.593 [0.000, 3.000], mean observation: 0.068 [-1.404, 4.105], loss: 0.966126, mean_absolute_error: 2.409232, mean_q: 0.672409, mean_eps: 0.985082
   16686/2000000: episode: 183, duration: 0.987s, episode steps: 69, steps per second: 70, episode reward: -162.512, mean reward: -2.355 [-100.000, 8.548], mean action: 1.522 [0.000, 3.000], mean observation: -0.060 [-1.465, 1.000], loss: 1.219996, mean_absolute_error: 2.314085, mean_q: 0.733154, mean_eps: 0.985013
   16754/2000000: episode: 184, duration: 0.978s, episode steps: 68, steps per second: 70, episode reward: -122.985, mean reward: -1.809 [-100.000, 6.823], mean action: 1.662 [0.000, 3.000], mean observation: -0.109 [-5.208, 1.000], loss: 2.435415, mean_absolute_error: 2.808252, mean_q: 0.714463, mean_eps: 0.984952
   16843/2000000: episode: 185, duration: 1.258s, episode steps: 89, steps per second: 71, episode reward: -303.117, mean reward: -3.406 [-100.000, 6.113], mean action: 1.517 [0.000, 3.000], mean observation: -0.003 [-3.119, 1.093], loss: 1.132238, mean_absolute_error: 2.629776, mean_q: 0.558056, mean_eps: 0.984882
   16977/2000000: episode: 186, duration: 1.925s, episode steps: 134, steps per second: 70, episode reward: -463.440, mean reward: -3.459 [-100.000, 4.864], mean action: 1.560 [0.000, 3.000], mean observation: 0.041 [-2.056, 4.234], loss: 1.591752, mean_absolute_error: 2.598461, mean_q: 0.664814, mean_eps: 0.984781
   17036/2000000: episode: 187, duration: 0.852s, episode steps: 59, steps per second: 69, episode reward: -201.459, mean reward: -3.415 [-100.000, 5.536], mean action: 1.492 [0.000, 3.000], mean observation: -0.097 [-3.923, 1.000], loss: 1.485748, mean_absolute_error: 2.573397, mean_q: 0.761745, mean_eps: 0.984695
   17133/2000000: episode: 188, duration: 1.413s, episode steps: 97, steps per second: 69, episode reward: -302.883, mean reward: -3.123 [-100.000, 0.575], mean action: 1.742 [0.000, 3.000], mean observation: 0.188 [-0.906, 1.585], loss: 1.694463, mean_absolute_error: 2.780096, mean_q: 0.390262, mean_eps: 0.984624
   17219/2000000: episode: 189, duration: 1.207s, episode steps: 86, steps per second: 71, episode reward: -172.491, mean reward: -2.006 [-100.000, 13.061], mean action: 1.523 [0.000, 3.000], mean observation: 0.011 [-1.469, 1.837], loss: 1.314206, mean_absolute_error: 2.629037, mean_q: 0.674058, mean_eps: 0.984542
   17280/2000000: episode: 190, duration: 0.921s, episode steps: 61, steps per second: 66, episode reward: -150.582, mean reward: -2.469 [-100.000, 9.760], mean action: 1.410 [0.000, 3.000], mean observation: -0.066 [-1.493, 4.760], loss: 1.634911, mean_absolute_error: 2.550582, mean_q: 0.611773, mean_eps: 0.984477
   17337/2000000: episode: 191, duration: 0.871s, episode steps: 57, steps per second: 65, episode reward: -104.844, mean reward: -1.839 [-100.000, 14.074], mean action: 1.456 [0.000, 3.000], mean observation: -0.045 [-1.579, 1.000], loss: 1.102229, mean_absolute_error: 2.511038, mean_q: 0.542613, mean_eps: 0.984423
   17442/2000000: episode: 192, duration: 1.498s, episode steps: 105, steps per second: 70, episode reward: -360.787, mean reward: -3.436 [-100.000, 1.268], mean action: 1.495 [0.000, 3.000], mean observation: 0.031 [-1.746, 1.153], loss: 1.567814, mean_absolute_error: 2.558880, mean_q: 0.457288, mean_eps: 0.984349
   17538/2000000: episode: 193, duration: 1.366s, episode steps: 96, steps per second: 70, episode reward: -432.772, mean reward: -4.508 [-100.000, 4.087], mean action: 1.510 [0.000, 3.000], mean observation: -0.044 [-1.768, 4.592], loss: 1.341957, mean_absolute_error: 2.514675, mean_q: 0.681744, mean_eps: 0.984259
   17652/2000000: episode: 194, duration: 1.650s, episode steps: 114, steps per second: 69, episode reward: -160.882, mean reward: -1.411 [-100.000, 16.863], mean action: 1.544 [0.000, 3.000], mean observation: -0.098 [-1.446, 2.598], loss: 1.427133, mean_absolute_error: 2.650598, mean_q: 0.443524, mean_eps: 0.984165
   17759/2000000: episode: 195, duration: 1.551s, episode steps: 107, steps per second: 69, episode reward: -298.499, mean reward: -2.790 [-100.000, 19.842], mean action: 1.458 [0.000, 3.000], mean observation: -0.082 [-4.711, 1.000], loss: 1.503638, mean_absolute_error: 2.588851, mean_q: 0.699415, mean_eps: 0.984066
   17891/2000000: episode: 196, duration: 2.021s, episode steps: 132, steps per second: 65, episode reward: -302.040, mean reward: -2.288 [-100.000, 5.634], mean action: 1.462 [0.000, 3.000], mean observation: -0.013 [-3.518, 1.052], loss: 1.682356, mean_absolute_error: 2.571359, mean_q: 0.578937, mean_eps: 0.983958
   18001/2000000: episode: 197, duration: 1.596s, episode steps: 110, steps per second: 69, episode reward: -142.000, mean reward: -1.291 [-100.000, 8.266], mean action: 1.536 [0.000, 3.000], mean observation: 0.129 [-1.305, 4.332], loss: 1.248228, mean_absolute_error: 2.339276, mean_q: 0.772242, mean_eps: 0.983849
   18067/2000000: episode: 198, duration: 0.917s, episode steps: 66, steps per second: 72, episode reward: -138.348, mean reward: -2.096 [-100.000, 14.433], mean action: 1.606 [0.000, 3.000], mean observation: -0.031 [-3.907, 1.000], loss: 2.368083, mean_absolute_error: 2.719584, mean_q: 0.658597, mean_eps: 0.983769
   18137/2000000: episode: 199, duration: 1.037s, episode steps: 70, steps per second: 67, episode reward: -231.319, mean reward: -3.305 [-100.000, 5.875], mean action: 1.443 [0.000, 3.000], mean observation: -0.008 [-1.529, 1.248], loss: 1.238963, mean_absolute_error: 2.427570, mean_q: 0.586068, mean_eps: 0.983708
   18240/2000000: episode: 200, duration: 1.455s, episode steps: 103, steps per second: 71, episode reward: -377.536, mean reward: -3.665 [-100.000, 13.322], mean action: 1.437 [0.000, 3.000], mean observation: 0.004 [-1.499, 4.009], loss: 1.856967, mean_absolute_error: 2.627138, mean_q: 0.669722, mean_eps: 0.983631
   18325/2000000: episode: 201, duration: 1.246s, episode steps: 85, steps per second: 68, episode reward: -120.137, mean reward: -1.413 [-100.000, 12.610], mean action: 1.400 [0.000, 3.000], mean observation: 0.013 [-4.336, 1.000], loss: 1.256053, mean_absolute_error: 2.279703, mean_q: 0.863351, mean_eps: 0.983546
   18408/2000000: episode: 202, duration: 1.200s, episode steps: 83, steps per second: 69, episode reward: -394.067, mean reward: -4.748 [-100.000, 38.651], mean action: 1.446 [0.000, 3.000], mean observation: 0.018 [-3.949, 1.372], loss: 1.654686, mean_absolute_error: 2.595951, mean_q: 0.702199, mean_eps: 0.983471
   18504/2000000: episode: 203, duration: 1.404s, episode steps: 96, steps per second: 68, episode reward: -163.167, mean reward: -1.700 [-100.000, 7.209], mean action: 1.458 [0.000, 3.000], mean observation: 0.111 [-5.211, 1.000], loss: 1.522185, mean_absolute_error: 2.650433, mean_q: 0.481325, mean_eps: 0.983391
   18609/2000000: episode: 204, duration: 1.531s, episode steps: 105, steps per second: 69, episode reward: -351.188, mean reward: -3.345 [-100.000, 22.709], mean action: 1.590 [0.000, 3.000], mean observation: 0.044 [-4.204, 1.187], loss: 1.473080, mean_absolute_error: 2.467269, mean_q: 0.664212, mean_eps: 0.983300
   18687/2000000: episode: 205, duration: 1.092s, episode steps: 78, steps per second: 71, episode reward: -341.952, mean reward: -4.384 [-100.000, 103.441], mean action: 1.590 [0.000, 3.000], mean observation: 0.063 [-1.444, 3.019], loss: 1.833673, mean_absolute_error: 2.592263, mean_q: 0.633324, mean_eps: 0.983217
   18792/2000000: episode: 206, duration: 1.546s, episode steps: 105, steps per second: 68, episode reward: -171.169, mean reward: -1.630 [-100.000, 22.341], mean action: 1.600 [0.000, 3.000], mean observation: -0.075 [-1.435, 1.000], loss: 1.486653, mean_absolute_error: 2.582766, mean_q: 0.570277, mean_eps: 0.983136
   18893/2000000: episode: 207, duration: 1.504s, episode steps: 101, steps per second: 67, episode reward: -215.437, mean reward: -2.133 [-100.000, 8.559], mean action: 1.356 [0.000, 3.000], mean observation: 0.072 [-1.431, 4.845], loss: 1.886934, mean_absolute_error: 2.738189, mean_q: 0.577379, mean_eps: 0.983042
   18951/2000000: episode: 208, duration: 0.806s, episode steps: 58, steps per second: 72, episode reward: -182.396, mean reward: -3.145 [-100.000, 15.279], mean action: 1.603 [0.000, 3.000], mean observation: 0.036 [-1.406, 4.256], loss: 1.463700, mean_absolute_error: 2.853628, mean_q: 0.384009, mean_eps: 0.982970
   19070/2000000: episode: 209, duration: 1.694s, episode steps: 119, steps per second: 70, episode reward: -163.867, mean reward: -1.377 [-100.000, 7.606], mean action: 1.664 [0.000, 3.000], mean observation: -0.130 [-1.499, 5.276], loss: 1.042766, mean_absolute_error: 2.393956, mean_q: 0.506981, mean_eps: 0.982891
   19185/2000000: episode: 210, duration: 1.663s, episode steps: 115, steps per second: 69, episode reward: -143.273, mean reward: -1.246 [-100.000, 7.805], mean action: 1.583 [0.000, 3.000], mean observation: -0.017 [-1.318, 4.833], loss: 2.149123, mean_absolute_error: 2.932104, mean_q: 0.331099, mean_eps: 0.982785
   19271/2000000: episode: 211, duration: 1.187s, episode steps: 86, steps per second: 72, episode reward: -280.940, mean reward: -3.267 [-100.000, 6.553], mean action: 1.640 [0.000, 3.000], mean observation: -0.038 [-6.109, 1.000], loss: 1.522394, mean_absolute_error: 2.467125, mean_q: 0.712047, mean_eps: 0.982695
   19343/2000000: episode: 212, duration: 1.031s, episode steps: 72, steps per second: 70, episode reward: -132.725, mean reward: -1.843 [-100.000, 13.813], mean action: 1.486 [0.000, 3.000], mean observation: -0.024 [-5.061, 1.000], loss: 1.148494, mean_absolute_error: 2.592151, mean_q: 0.405902, mean_eps: 0.982625
   19428/2000000: episode: 213, duration: 1.230s, episode steps: 85, steps per second: 69, episode reward: -370.469, mean reward: -4.358 [-100.000, 5.239], mean action: 1.600 [0.000, 3.000], mean observation: -0.092 [-2.157, 1.252], loss: 1.606899, mean_absolute_error: 2.651692, mean_q: 0.345089, mean_eps: 0.982554
   19536/2000000: episode: 214, duration: 1.603s, episode steps: 108, steps per second: 67, episode reward: -153.113, mean reward: -1.418 [-100.000, 12.586], mean action: 1.685 [0.000, 3.000], mean observation: 0.016 [-1.485, 1.000], loss: 1.989140, mean_absolute_error: 2.757258, mean_q: 0.430974, mean_eps: 0.982468
   19601/2000000: episode: 215, duration: 0.958s, episode steps: 65, steps per second: 68, episode reward: -166.052, mean reward: -2.555 [-100.000, 7.362], mean action: 1.492 [0.000, 3.000], mean observation: -0.113 [-1.626, 5.177], loss: 1.870605, mean_absolute_error: 2.732727, mean_q: 0.649233, mean_eps: 0.982389
   19693/2000000: episode: 216, duration: 1.313s, episode steps: 92, steps per second: 70, episode reward: -257.140, mean reward: -2.795 [-100.000, 4.908], mean action: 1.630 [0.000, 3.000], mean observation: 0.050 [-5.422, 1.000], loss: 1.646897, mean_absolute_error: 2.505046, mean_q: 0.619510, mean_eps: 0.982317
   19767/2000000: episode: 217, duration: 1.031s, episode steps: 74, steps per second: 72, episode reward: -132.403, mean reward: -1.789 [-100.000, 8.437], mean action: 1.622 [0.000, 3.000], mean observation: -0.021 [-1.253, 4.483], loss: 1.807362, mean_absolute_error: 2.595120, mean_q: 0.581304, mean_eps: 0.982243
   19863/2000000: episode: 218, duration: 1.367s, episode steps: 96, steps per second: 70, episode reward: -277.882, mean reward: -2.895 [-100.000, 5.910], mean action: 1.323 [0.000, 3.000], mean observation: -0.045 [-1.509, 1.000], loss: 0.779142, mean_absolute_error: 2.380675, mean_q: 0.446168, mean_eps: 0.982167
   19943/2000000: episode: 219, duration: 1.154s, episode steps: 80, steps per second: 69, episode reward: -273.077, mean reward: -3.413 [-100.000, 24.855], mean action: 1.375 [0.000, 3.000], mean observation: -0.030 [-1.551, 4.724], loss: 1.348959, mean_absolute_error: 2.348782, mean_q: 1.118240, mean_eps: 0.982088
   20039/2000000: episode: 220, duration: 1.342s, episode steps: 96, steps per second: 72, episode reward: -453.335, mean reward: -4.722 [-100.000, 1.558], mean action: 1.646 [0.000, 3.000], mean observation: 0.146 [-1.605, 1.996], loss: 1.881920, mean_absolute_error: 2.660524, mean_q: 0.652121, mean_eps: 0.982009
   20109/2000000: episode: 221, duration: 1.014s, episode steps: 70, steps per second: 69, episode reward: -102.684, mean reward: -1.467 [-100.000, 12.801], mean action: 1.514 [0.000, 3.000], mean observation: -0.076 [-1.421, 1.000], loss: 1.794717, mean_absolute_error: 3.475171, mean_q: 0.944526, mean_eps: 0.981933
   20208/2000000: episode: 222, duration: 1.430s, episode steps: 99, steps per second: 69, episode reward: -155.319, mean reward: -1.569 [-100.000, 16.622], mean action: 1.535 [0.000, 3.000], mean observation: -0.038 [-1.364, 4.826], loss: 1.777707, mean_absolute_error: 3.429721, mean_q: 1.325722, mean_eps: 0.981858
   20313/2000000: episode: 223, duration: 1.495s, episode steps: 105, steps per second: 70, episode reward: -154.917, mean reward: -1.475 [-100.000, 8.460], mean action: 1.410 [0.000, 3.000], mean observation: -0.012 [-1.531, 5.182], loss: 1.627545, mean_absolute_error: 3.518098, mean_q: 1.543149, mean_eps: 0.981766
   20385/2000000: episode: 224, duration: 1.015s, episode steps: 72, steps per second: 71, episode reward: -156.638, mean reward: -2.176 [-100.000, 6.017], mean action: 1.597 [0.000, 3.000], mean observation: -0.020 [-1.395, 4.434], loss: 1.740562, mean_absolute_error: 3.888433, mean_q: 1.161113, mean_eps: 0.981685
   20530/2000000: episode: 225, duration: 2.046s, episode steps: 145, steps per second: 71, episode reward: -97.383, mean reward: -0.672 [-100.000, 15.218], mean action: 1.545 [0.000, 3.000], mean observation: 0.053 [-1.084, 3.968], loss: 1.437226, mean_absolute_error: 3.634805, mean_q: 1.006710, mean_eps: 0.981588
   20625/2000000: episode: 226, duration: 1.371s, episode steps: 95, steps per second: 69, episode reward: -212.104, mean reward: -2.233 [-100.000, 15.433], mean action: 1.368 [0.000, 3.000], mean observation: 0.018 [-1.603, 4.275], loss: 1.723327, mean_absolute_error: 3.621821, mean_q: 1.039084, mean_eps: 0.981480
   20704/2000000: episode: 227, duration: 1.121s, episode steps: 79, steps per second: 70, episode reward: -154.313, mean reward: -1.953 [-100.000, 6.619], mean action: 1.316 [0.000, 3.000], mean observation: 0.039 [-4.772, 1.000], loss: 2.015473, mean_absolute_error: 3.772731, mean_q: 1.136211, mean_eps: 0.981402
   20802/2000000: episode: 228, duration: 1.422s, episode steps: 98, steps per second: 69, episode reward: -352.144, mean reward: -3.593 [-100.000, 91.727], mean action: 1.561 [0.000, 3.000], mean observation: -0.057 [-3.519, 1.000], loss: 1.657768, mean_absolute_error: 3.578698, mean_q: 0.904147, mean_eps: 0.981323
   20914/2000000: episode: 229, duration: 1.600s, episode steps: 112, steps per second: 70, episode reward: -361.946, mean reward: -3.232 [-100.000, 5.125], mean action: 1.598 [0.000, 3.000], mean observation: 0.057 [-1.499, 1.347], loss: 1.309678, mean_absolute_error: 3.520298, mean_q: 1.216459, mean_eps: 0.981228
   20996/2000000: episode: 230, duration: 1.190s, episode steps: 82, steps per second: 69, episode reward: -243.880, mean reward: -2.974 [-100.000, 5.975], mean action: 1.463 [0.000, 3.000], mean observation: 0.033 [-1.427, 1.000], loss: 1.174792, mean_absolute_error: 3.627298, mean_q: 0.995049, mean_eps: 0.981141
   21105/2000000: episode: 231, duration: 1.550s, episode steps: 109, steps per second: 70, episode reward: -267.396, mean reward: -2.453 [-100.000, 39.006], mean action: 1.376 [0.000, 3.000], mean observation: 0.205 [-1.580, 3.885], loss: 1.776164, mean_absolute_error: 3.513061, mean_q: 1.386135, mean_eps: 0.981055
   21185/2000000: episode: 232, duration: 1.157s, episode steps: 80, steps per second: 69, episode reward: -222.681, mean reward: -2.784 [-100.000, 6.476], mean action: 1.400 [0.000, 3.000], mean observation: 0.064 [-1.470, 4.737], loss: 1.832182, mean_absolute_error: 3.655383, mean_q: 1.162161, mean_eps: 0.980969
   21251/2000000: episode: 233, duration: 1.006s, episode steps: 66, steps per second: 66, episode reward: -135.159, mean reward: -2.048 [-100.000, 8.475], mean action: 1.545 [0.000, 3.000], mean observation: 0.054 [-1.473, 1.000], loss: 2.213017, mean_absolute_error: 3.640582, mean_q: 1.254085, mean_eps: 0.980904
   21333/2000000: episode: 234, duration: 1.213s, episode steps: 82, steps per second: 68, episode reward: -147.252, mean reward: -1.796 [-100.000, 17.238], mean action: 1.573 [0.000, 3.000], mean observation: 0.069 [-1.419, 1.000], loss: 1.873554, mean_absolute_error: 3.877647, mean_q: 0.776135, mean_eps: 0.980837
   21414/2000000: episode: 235, duration: 1.183s, episode steps: 81, steps per second: 68, episode reward: -161.131, mean reward: -1.989 [-100.000, 41.872], mean action: 1.543 [0.000, 3.000], mean observation: -0.030 [-4.223, 1.000], loss: 1.181764, mean_absolute_error: 3.651390, mean_q: 0.915245, mean_eps: 0.980763
   21484/2000000: episode: 236, duration: 1.030s, episode steps: 70, steps per second: 68, episode reward: -172.479, mean reward: -2.464 [-100.000, 18.250], mean action: 1.629 [0.000, 3.000], mean observation: -0.082 [-1.483, 2.685], loss: 1.428002, mean_absolute_error: 3.675023, mean_q: 1.165376, mean_eps: 0.980697
   21616/2000000: episode: 237, duration: 1.945s, episode steps: 132, steps per second: 68, episode reward: -190.511, mean reward: -1.443 [-100.000, 11.050], mean action: 1.477 [0.000, 3.000], mean observation: 0.124 [-4.020, 1.000], loss: 1.235237, mean_absolute_error: 3.526526, mean_q: 1.125702, mean_eps: 0.980607
   21704/2000000: episode: 238, duration: 1.311s, episode steps: 88, steps per second: 67, episode reward: -146.185, mean reward: -1.661 [-100.000, 8.221], mean action: 1.557 [0.000, 3.000], mean observation: -0.038 [-4.808, 1.000], loss: 1.135868, mean_absolute_error: 3.663413, mean_q: 0.895108, mean_eps: 0.980508
   21808/2000000: episode: 239, duration: 1.970s, episode steps: 104, steps per second: 53, episode reward: -386.969, mean reward: -3.721 [-100.000, 38.824], mean action: 1.452 [0.000, 3.000], mean observation: -0.085 [-3.358, 1.242], loss: 1.507472, mean_absolute_error: 3.670858, mean_q: 0.756680, mean_eps: 0.980421
   21948/2000000: episode: 240, duration: 2.173s, episode steps: 140, steps per second: 64, episode reward: -86.955, mean reward: -0.621 [-100.000, 39.530], mean action: 1.600 [0.000, 3.000], mean observation: 0.105 [-1.097, 1.354], loss: 1.322000, mean_absolute_error: 3.727601, mean_q: 1.008584, mean_eps: 0.980312
   22056/2000000: episode: 241, duration: 1.644s, episode steps: 108, steps per second: 66, episode reward: -179.455, mean reward: -1.662 [-100.000, 6.135], mean action: 1.444 [0.000, 3.000], mean observation: 0.091 [-4.700, 1.000], loss: 1.713874, mean_absolute_error: 4.010479, mean_q: 0.525019, mean_eps: 0.980200
   22172/2000000: episode: 242, duration: 1.730s, episode steps: 116, steps per second: 67, episode reward: -126.414, mean reward: -1.090 [-100.000, 55.447], mean action: 1.483 [0.000, 3.000], mean observation: 0.092 [-1.488, 1.208], loss: 1.142986, mean_absolute_error: 3.483915, mean_q: 1.207405, mean_eps: 0.980099
   22243/2000000: episode: 243, duration: 1.093s, episode steps: 71, steps per second: 65, episode reward: -237.033, mean reward: -3.338 [-100.000, 6.660], mean action: 1.775 [0.000, 3.000], mean observation: -0.062 [-1.394, 1.756], loss: 2.178975, mean_absolute_error: 4.179305, mean_q: 0.275870, mean_eps: 0.980015
   22330/2000000: episode: 244, duration: 1.269s, episode steps: 87, steps per second: 69, episode reward: -236.581, mean reward: -2.719 [-100.000, 6.186], mean action: 1.575 [0.000, 3.000], mean observation: 0.101 [-1.283, 4.329], loss: 0.827467, mean_absolute_error: 3.248076, mean_q: 1.910579, mean_eps: 0.979943
   22409/2000000: episode: 245, duration: 1.143s, episode steps: 79, steps per second: 69, episode reward: -153.104, mean reward: -1.938 [-100.000, 14.877], mean action: 1.570 [0.000, 3.000], mean observation: -0.048 [-3.205, 1.000], loss: 1.077138, mean_absolute_error: 3.648435, mean_q: 1.251525, mean_eps: 0.979867
   22497/2000000: episode: 246, duration: 1.299s, episode steps: 88, steps per second: 68, episode reward: -194.928, mean reward: -2.215 [-100.000, 18.538], mean action: 1.352 [0.000, 3.000], mean observation: -0.115 [-4.729, 1.000], loss: 1.686902, mean_absolute_error: 3.763745, mean_q: 0.915629, mean_eps: 0.979791
   22585/2000000: episode: 247, duration: 1.375s, episode steps: 88, steps per second: 64, episode reward: -342.444, mean reward: -3.891 [-100.000, -0.153], mean action: 1.636 [0.000, 3.000], mean observation: -0.079 [-1.935, 0.943], loss: 1.859000, mean_absolute_error: 3.933141, mean_q: 0.874357, mean_eps: 0.979712
   22653/2000000: episode: 248, duration: 1.031s, episode steps: 68, steps per second: 66, episode reward: -265.010, mean reward: -3.897 [-100.000, 28.972], mean action: 1.662 [0.000, 3.000], mean observation: -0.009 [-4.403, 1.147], loss: 1.912319, mean_absolute_error: 3.738493, mean_q: 1.151942, mean_eps: 0.979642
   22726/2000000: episode: 249, duration: 1.098s, episode steps: 73, steps per second: 66, episode reward: -149.799, mean reward: -2.052 [-100.000, 8.345], mean action: 1.534 [0.000, 3.000], mean observation: -0.121 [-4.288, 1.000], loss: 1.742274, mean_absolute_error: 3.691091, mean_q: 0.906252, mean_eps: 0.979579
   22852/2000000: episode: 250, duration: 1.912s, episode steps: 126, steps per second: 66, episode reward: -193.798, mean reward: -1.538 [-100.000, 7.056], mean action: 1.341 [0.000, 3.000], mean observation: 0.102 [-3.516, 1.085], loss: 1.532097, mean_absolute_error: 3.868082, mean_q: 0.538666, mean_eps: 0.979491
   22922/2000000: episode: 251, duration: 1.095s, episode steps: 70, steps per second: 64, episode reward: -156.614, mean reward: -2.237 [-100.000, 6.853], mean action: 1.371 [0.000, 3.000], mean observation: -0.041 [-4.595, 1.000], loss: 2.064771, mean_absolute_error: 4.153091, mean_q: 0.673545, mean_eps: 0.979403
   23008/2000000: episode: 252, duration: 1.320s, episode steps: 86, steps per second: 65, episode reward: -297.249, mean reward: -3.456 [-100.000, 4.557], mean action: 1.430 [0.000, 3.000], mean observation: -0.109 [-3.068, 1.000], loss: 1.971338, mean_absolute_error: 3.737694, mean_q: 1.004960, mean_eps: 0.979332
   23098/2000000: episode: 253, duration: 1.366s, episode steps: 90, steps per second: 66, episode reward: -235.452, mean reward: -2.616 [-100.000, 6.261], mean action: 1.511 [0.000, 3.000], mean observation: 0.031 [-1.372, 3.890], loss: 2.032054, mean_absolute_error: 4.071764, mean_q: 0.507819, mean_eps: 0.979253
   23210/2000000: episode: 254, duration: 1.604s, episode steps: 112, steps per second: 70, episode reward: -429.955, mean reward: -3.839 [-100.000, 5.836], mean action: 1.500 [0.000, 3.000], mean observation: 0.003 [-4.655, 1.739], loss: 1.594515, mean_absolute_error: 4.019291, mean_q: 0.510320, mean_eps: 0.979161
   23278/2000000: episode: 255, duration: 0.993s, episode steps: 68, steps per second: 69, episode reward: -225.286, mean reward: -3.313 [-100.000, 9.340], mean action: 1.426 [0.000, 3.000], mean observation: 0.036 [-1.365, 3.534], loss: 1.982116, mean_absolute_error: 4.007319, mean_q: 0.335566, mean_eps: 0.979080
   23378/2000000: episode: 256, duration: 1.428s, episode steps: 100, steps per second: 70, episode reward: -192.760, mean reward: -1.928 [-100.000, 7.575], mean action: 1.400 [0.000, 3.000], mean observation: 0.120 [-4.455, 1.235], loss: 1.690188, mean_absolute_error: 3.700737, mean_q: 0.866915, mean_eps: 0.979005
   23457/2000000: episode: 257, duration: 1.152s, episode steps: 79, steps per second: 69, episode reward: -159.605, mean reward: -2.020 [-100.000, 5.654], mean action: 1.595 [0.000, 3.000], mean observation: -0.150 [-1.490, 4.584], loss: 2.627994, mean_absolute_error: 3.966848, mean_q: 1.036150, mean_eps: 0.978924
   23533/2000000: episode: 258, duration: 1.089s, episode steps: 76, steps per second: 70, episode reward: -116.191, mean reward: -1.529 [-100.000, 6.699], mean action: 1.250 [0.000, 3.000], mean observation: 0.071 [-1.124, 4.321], loss: 1.563749, mean_absolute_error: 3.767743, mean_q: 1.150748, mean_eps: 0.978854
   23627/2000000: episode: 259, duration: 1.330s, episode steps: 94, steps per second: 71, episode reward: -302.488, mean reward: -3.218 [-100.000, 1.159], mean action: 1.521 [0.000, 3.000], mean observation: 0.220 [-0.795, 1.641], loss: 1.294303, mean_absolute_error: 3.704224, mean_q: 1.141312, mean_eps: 0.978778
   23691/2000000: episode: 260, duration: 0.917s, episode steps: 64, steps per second: 70, episode reward: -165.921, mean reward: -2.593 [-100.000, 6.880], mean action: 1.531 [0.000, 3.000], mean observation: 0.061 [-1.286, 1.000], loss: 1.968051, mean_absolute_error: 3.874611, mean_q: 0.870097, mean_eps: 0.978708
   23784/2000000: episode: 261, duration: 1.366s, episode steps: 93, steps per second: 68, episode reward: -166.061, mean reward: -1.786 [-100.000, 8.229], mean action: 1.419 [0.000, 3.000], mean observation: 0.049 [-1.446, 4.439], loss: 1.448966, mean_absolute_error: 3.529268, mean_q: 1.482976, mean_eps: 0.978638
   23903/2000000: episode: 262, duration: 1.716s, episode steps: 119, steps per second: 69, episode reward: -295.451, mean reward: -2.483 [-100.000, 92.597], mean action: 1.622 [0.000, 3.000], mean observation: -0.032 [-3.067, 1.000], loss: 1.861523, mean_absolute_error: 3.959821, mean_q: 0.806531, mean_eps: 0.978542
   24065/2000000: episode: 263, duration: 2.318s, episode steps: 162, steps per second: 70, episode reward: -170.108, mean reward: -1.050 [-100.000, 16.791], mean action: 1.537 [0.000, 3.000], mean observation: -0.039 [-1.588, 1.094], loss: 1.278491, mean_absolute_error: 3.673299, mean_q: 0.871718, mean_eps: 0.978414
   24148/2000000: episode: 264, duration: 1.209s, episode steps: 83, steps per second: 69, episode reward: -181.760, mean reward: -2.190 [-100.000, 6.263], mean action: 1.542 [0.000, 3.000], mean observation: -0.104 [-1.452, 4.764], loss: 1.965198, mean_absolute_error: 3.787297, mean_q: 0.644526, mean_eps: 0.978305
   24223/2000000: episode: 265, duration: 1.074s, episode steps: 75, steps per second: 70, episode reward: -262.635, mean reward: -3.502 [-100.000, 3.588], mean action: 1.587 [0.000, 3.000], mean observation: -0.067 [-1.322, 1.000], loss: 1.148376, mean_absolute_error: 3.450020, mean_q: 1.286764, mean_eps: 0.978234
   24351/2000000: episode: 266, duration: 1.805s, episode steps: 128, steps per second: 71, episode reward: -411.787, mean reward: -3.217 [-100.000, 1.667], mean action: 1.484 [0.000, 3.000], mean observation: 0.079 [-1.793, 1.510], loss: 1.529879, mean_absolute_error: 3.750738, mean_q: 1.025349, mean_eps: 0.978143
   24473/2000000: episode: 267, duration: 1.743s, episode steps: 122, steps per second: 70, episode reward: -341.475, mean reward: -2.799 [-100.000, 3.784], mean action: 1.607 [0.000, 3.000], mean observation: -0.117 [-1.905, 5.029], loss: 1.576044, mean_absolute_error: 4.030133, mean_q: 0.577282, mean_eps: 0.978029
   24567/2000000: episode: 268, duration: 1.315s, episode steps: 94, steps per second: 71, episode reward: -191.893, mean reward: -2.041 [-100.000, 7.110], mean action: 1.660 [0.000, 3.000], mean observation: -0.023 [-4.144, 1.000], loss: 1.313793, mean_absolute_error: 3.774116, mean_q: 0.904132, mean_eps: 0.977932
   24694/2000000: episode: 269, duration: 1.804s, episode steps: 127, steps per second: 70, episode reward: -180.266, mean reward: -1.419 [-100.000, 3.678], mean action: 1.512 [0.000, 3.000], mean observation: 0.217 [-0.978, 1.067], loss: 1.458117, mean_absolute_error: 3.677663, mean_q: 1.111532, mean_eps: 0.977833
   24766/2000000: episode: 270, duration: 1.014s, episode steps: 72, steps per second: 71, episode reward: -92.498, mean reward: -1.285 [-100.000, 15.750], mean action: 1.583 [0.000, 3.000], mean observation: -0.063 [-4.146, 1.000], loss: 1.700121, mean_absolute_error: 4.114162, mean_q: 0.226497, mean_eps: 0.977743
   24871/2000000: episode: 271, duration: 1.469s, episode steps: 105, steps per second: 71, episode reward: -294.050, mean reward: -2.800 [-100.000, 7.996], mean action: 1.619 [0.000, 3.000], mean observation: -0.020 [-3.250, 1.000], loss: 1.681096, mean_absolute_error: 3.614972, mean_q: 1.073911, mean_eps: 0.977664
   24959/2000000: episode: 272, duration: 1.260s, episode steps: 88, steps per second: 70, episode reward: -152.664, mean reward: -1.735 [-100.000, 12.602], mean action: 1.466 [0.000, 3.000], mean observation: 0.051 [-1.330, 4.918], loss: 1.640706, mean_absolute_error: 3.634560, mean_q: 1.020368, mean_eps: 0.977577
   25040/2000000: episode: 273, duration: 1.201s, episode steps: 81, steps per second: 67, episode reward: -109.720, mean reward: -1.355 [-100.000, 12.208], mean action: 1.444 [0.000, 3.000], mean observation: -0.016 [-3.979, 1.000], loss: 1.301777, mean_absolute_error: 3.536647, mean_q: 1.289164, mean_eps: 0.977502
   25104/2000000: episode: 274, duration: 0.952s, episode steps: 64, steps per second: 67, episode reward: -109.914, mean reward: -1.717 [-100.000, 6.843], mean action: 1.547 [0.000, 3.000], mean observation: -0.052 [-1.399, 1.000], loss: 1.398529, mean_absolute_error: 3.369405, mean_q: 1.151757, mean_eps: 0.977437
   25207/2000000: episode: 275, duration: 1.460s, episode steps: 103, steps per second: 71, episode reward: -134.936, mean reward: -1.310 [-100.000, 22.190], mean action: 1.485 [0.000, 3.000], mean observation: 0.016 [-4.734, 1.000], loss: 1.438178, mean_absolute_error: 3.542222, mean_q: 0.852632, mean_eps: 0.977361
   25326/2000000: episode: 276, duration: 1.694s, episode steps: 119, steps per second: 70, episode reward: -119.156, mean reward: -1.001 [-100.000, 10.765], mean action: 1.664 [0.000, 3.000], mean observation: -0.036 [-3.994, 1.000], loss: 0.846624, mean_absolute_error: 3.285006, mean_q: 1.395709, mean_eps: 0.977261
   25410/2000000: episode: 277, duration: 1.176s, episode steps: 84, steps per second: 71, episode reward: -133.880, mean reward: -1.594 [-100.000, 68.097], mean action: 1.536 [0.000, 3.000], mean observation: -0.140 [-4.222, 1.000], loss: 1.698378, mean_absolute_error: 3.605203, mean_q: 1.065959, mean_eps: 0.977169
   25529/2000000: episode: 278, duration: 1.698s, episode steps: 119, steps per second: 70, episode reward: -437.539, mean reward: -3.677 [-100.000, 1.331], mean action: 1.454 [0.000, 3.000], mean observation: 0.050 [-1.935, 1.319], loss: 1.190543, mean_absolute_error: 3.505754, mean_q: 1.440458, mean_eps: 0.977077
   25610/2000000: episode: 279, duration: 1.144s, episode steps: 81, steps per second: 71, episode reward: -78.454, mean reward: -0.969 [-100.000, 12.982], mean action: 1.691 [0.000, 3.000], mean observation: -0.001 [-1.087, 3.235], loss: 1.262092, mean_absolute_error: 3.622862, mean_q: 0.761936, mean_eps: 0.976987
   25734/2000000: episode: 280, duration: 1.886s, episode steps: 124, steps per second: 66, episode reward: -338.808, mean reward: -2.732 [-100.000, 12.024], mean action: 1.355 [0.000, 3.000], mean observation: 0.084 [-4.981, 1.350], loss: 1.167276, mean_absolute_error: 3.703586, mean_q: 1.140870, mean_eps: 0.976895
   25835/2000000: episode: 281, duration: 1.408s, episode steps: 101, steps per second: 72, episode reward: -260.857, mean reward: -2.583 [-100.000, 7.507], mean action: 1.564 [0.000, 3.000], mean observation: 0.059 [-1.493, 4.109], loss: 1.270966, mean_absolute_error: 3.526398, mean_q: 1.310980, mean_eps: 0.976794
   25905/2000000: episode: 282, duration: 1.038s, episode steps: 70, steps per second: 67, episode reward: -197.575, mean reward: -2.822 [-100.000, 8.000], mean action: 1.686 [0.000, 3.000], mean observation: -0.073 [-1.359, 1.000], loss: 1.461820, mean_absolute_error: 3.620313, mean_q: 1.243382, mean_eps: 0.976717
   26011/2000000: episode: 283, duration: 1.488s, episode steps: 106, steps per second: 71, episode reward: -368.637, mean reward: -3.478 [-100.000, 9.521], mean action: 1.604 [0.000, 3.000], mean observation: -0.000 [-4.258, 1.316], loss: 1.446812, mean_absolute_error: 3.803712, mean_q: 0.635469, mean_eps: 0.976638
   26132/2000000: episode: 284, duration: 1.751s, episode steps: 121, steps per second: 69, episode reward: -69.802, mean reward: -0.577 [-100.000, 97.250], mean action: 1.446 [0.000, 3.000], mean observation: 0.053 [-1.208, 1.326], loss: 1.497783, mean_absolute_error: 3.500657, mean_q: 1.133259, mean_eps: 0.976537
   26231/2000000: episode: 285, duration: 1.453s, episode steps: 99, steps per second: 68, episode reward: -184.136, mean reward: -1.860 [-100.000, 4.550], mean action: 1.444 [0.000, 3.000], mean observation: -0.131 [-1.388, 1.000], loss: 1.839732, mean_absolute_error: 3.850991, mean_q: 0.710652, mean_eps: 0.976438
   26308/2000000: episode: 286, duration: 1.134s, episode steps: 77, steps per second: 68, episode reward: -141.890, mean reward: -1.843 [-100.000, 6.514], mean action: 1.403 [0.000, 3.000], mean observation: -0.094 [-4.901, 1.000], loss: 1.263393, mean_absolute_error: 3.756951, mean_q: 0.944448, mean_eps: 0.976359
   26372/2000000: episode: 287, duration: 0.968s, episode steps: 64, steps per second: 66, episode reward: -116.793, mean reward: -1.825 [-100.000, 12.299], mean action: 1.219 [0.000, 3.000], mean observation: 0.014 [-1.505, 1.000], loss: 1.621553, mean_absolute_error: 3.467008, mean_q: 0.979046, mean_eps: 0.976296
   26436/2000000: episode: 288, duration: 0.958s, episode steps: 64, steps per second: 67, episode reward: -155.712, mean reward: -2.433 [-100.000, 5.761], mean action: 1.562 [0.000, 3.000], mean observation: -0.125 [-1.508, 1.000], loss: 1.194409, mean_absolute_error: 3.462835, mean_q: 1.139254, mean_eps: 0.976238
   26556/2000000: episode: 289, duration: 1.754s, episode steps: 120, steps per second: 68, episode reward: -244.382, mean reward: -2.037 [-100.000, 15.283], mean action: 1.333 [0.000, 3.000], mean observation: 0.141 [-1.350, 3.685], loss: 1.358189, mean_absolute_error: 3.576384, mean_q: 1.058471, mean_eps: 0.976155
   26647/2000000: episode: 290, duration: 1.314s, episode steps: 91, steps per second: 69, episode reward: -99.210, mean reward: -1.090 [-100.000, 49.896], mean action: 1.637 [0.000, 3.000], mean observation: -0.077 [-4.892, 1.000], loss: 0.762495, mean_absolute_error: 3.392921, mean_q: 1.178646, mean_eps: 0.976060
   26726/2000000: episode: 291, duration: 1.138s, episode steps: 79, steps per second: 69, episode reward: -154.752, mean reward: -1.959 [-100.000, 9.482], mean action: 1.342 [0.000, 3.000], mean observation: 0.002 [-3.125, 1.000], loss: 0.767145, mean_absolute_error: 3.480838, mean_q: 1.307783, mean_eps: 0.975983
   26805/2000000: episode: 292, duration: 1.138s, episode steps: 79, steps per second: 69, episode reward: -113.498, mean reward: -1.437 [-100.000, 16.345], mean action: 1.557 [0.000, 3.000], mean observation: -0.156 [-4.230, 1.000], loss: 1.049079, mean_absolute_error: 3.866495, mean_q: 0.915766, mean_eps: 0.975911
   26869/2000000: episode: 293, duration: 0.936s, episode steps: 64, steps per second: 68, episode reward: -77.975, mean reward: -1.218 [-100.000, 17.080], mean action: 1.531 [0.000, 3.000], mean observation: -0.045 [-1.346, 1.000], loss: 1.244824, mean_absolute_error: 3.675506, mean_q: 0.991935, mean_eps: 0.975846
   26977/2000000: episode: 294, duration: 1.872s, episode steps: 108, steps per second: 58, episode reward: -252.148, mean reward: -2.335 [-100.000, 1.088], mean action: 1.556 [0.000, 3.000], mean observation: -0.084 [-1.579, 1.029], loss: 1.023228, mean_absolute_error: 3.478339, mean_q: 1.147097, mean_eps: 0.975768
   27072/2000000: episode: 295, duration: 1.620s, episode steps: 95, steps per second: 59, episode reward: -189.173, mean reward: -1.991 [-100.000, 5.446], mean action: 1.463 [0.000, 3.000], mean observation: -0.105 [-1.330, 4.839], loss: 1.380821, mean_absolute_error: 3.760771, mean_q: 1.103886, mean_eps: 0.975678
   27203/2000000: episode: 296, duration: 1.875s, episode steps: 131, steps per second: 70, episode reward: -151.311, mean reward: -1.155 [-100.000, 12.931], mean action: 1.695 [0.000, 3.000], mean observation: -0.056 [-1.143, 2.878], loss: 1.150838, mean_absolute_error: 3.507153, mean_q: 1.046813, mean_eps: 0.975578
   27285/2000000: episode: 297, duration: 1.203s, episode steps: 82, steps per second: 68, episode reward: -386.574, mean reward: -4.714 [-100.000, 100.017], mean action: 1.646 [0.000, 3.000], mean observation: 0.024 [-1.429, 3.060], loss: 1.098522, mean_absolute_error: 3.326064, mean_q: 1.556275, mean_eps: 0.975480
   27370/2000000: episode: 298, duration: 1.212s, episode steps: 85, steps per second: 70, episode reward: -373.832, mean reward: -4.398 [-100.000, 3.194], mean action: 1.318 [0.000, 3.000], mean observation: 0.004 [-1.647, 4.232], loss: 1.600183, mean_absolute_error: 3.536338, mean_q: 1.198290, mean_eps: 0.975405
   27489/2000000: episode: 299, duration: 1.694s, episode steps: 119, steps per second: 70, episode reward: -243.971, mean reward: -2.050 [-100.000, 1.495], mean action: 1.487 [0.000, 3.000], mean observation: -0.081 [-5.395, 1.041], loss: 1.656113, mean_absolute_error: 3.573301, mean_q: 1.319603, mean_eps: 0.975313
   27596/2000000: episode: 300, duration: 1.531s, episode steps: 107, steps per second: 70, episode reward: -212.132, mean reward: -1.983 [-100.000, 8.938], mean action: 1.402 [0.000, 3.000], mean observation: 0.144 [-1.389, 4.921], loss: 1.197276, mean_absolute_error: 3.365817, mean_q: 1.429419, mean_eps: 0.975212
   27712/2000000: episode: 301, duration: 1.721s, episode steps: 116, steps per second: 67, episode reward: -315.141, mean reward: -2.717 [-100.000, 1.167], mean action: 1.448 [0.000, 3.000], mean observation: -0.082 [-1.386, 0.967], loss: 0.934124, mean_absolute_error: 3.567365, mean_q: 0.812207, mean_eps: 0.975113
   27798/2000000: episode: 302, duration: 1.259s, episode steps: 86, steps per second: 68, episode reward: -313.526, mean reward: -3.646 [-100.000, 0.354], mean action: 1.570 [0.000, 3.000], mean observation: 0.009 [-1.580, 1.086], loss: 1.523634, mean_absolute_error: 3.758695, mean_q: 0.654814, mean_eps: 0.975021
   27893/2000000: episode: 303, duration: 1.366s, episode steps: 95, steps per second: 70, episode reward: -365.039, mean reward: -3.843 [-100.000, 4.966], mean action: 1.811 [0.000, 3.000], mean observation: 0.025 [-4.001, 1.734], loss: 1.547006, mean_absolute_error: 3.522939, mean_q: 1.097915, mean_eps: 0.974939
   28000/2000000: episode: 304, duration: 1.544s, episode steps: 107, steps per second: 69, episode reward: -275.795, mean reward: -2.578 [-100.000, 1.061], mean action: 1.505 [0.000, 3.000], mean observation: -0.086 [-1.411, 0.985], loss: 1.502469, mean_absolute_error: 3.774458, mean_q: 1.138624, mean_eps: 0.974849
   28095/2000000: episode: 305, duration: 1.386s, episode steps: 95, steps per second: 69, episode reward: -630.431, mean reward: -6.636 [-100.000, 44.439], mean action: 1.400 [0.000, 3.000], mean observation: 0.037 [-1.752, 4.205], loss: 1.859258, mean_absolute_error: 3.743074, mean_q: 0.697158, mean_eps: 0.974759
   28220/2000000: episode: 306, duration: 1.843s, episode steps: 125, steps per second: 68, episode reward: -157.315, mean reward: -1.259 [-100.000, 11.324], mean action: 1.576 [0.000, 3.000], mean observation: 0.096 [-5.425, 1.063], loss: 1.231505, mean_absolute_error: 3.419586, mean_q: 0.907702, mean_eps: 0.974660
   28294/2000000: episode: 307, duration: 1.099s, episode steps: 74, steps per second: 67, episode reward: -196.041, mean reward: -2.649 [-100.000, 18.740], mean action: 1.446 [0.000, 3.000], mean observation: 0.038 [-1.427, 4.246], loss: 1.890513, mean_absolute_error: 3.896860, mean_q: 0.955102, mean_eps: 0.974570
   28406/2000000: episode: 308, duration: 1.631s, episode steps: 112, steps per second: 69, episode reward: -483.833, mean reward: -4.320 [-100.000, 4.067], mean action: 1.446 [0.000, 3.000], mean observation: 0.026 [-2.072, 4.591], loss: 1.808967, mean_absolute_error: 3.654759, mean_q: 1.049635, mean_eps: 0.974485
   28505/2000000: episode: 309, duration: 1.438s, episode steps: 99, steps per second: 69, episode reward: -161.505, mean reward: -1.631 [-100.000, 32.643], mean action: 1.333 [0.000, 3.000], mean observation: 0.143 [-1.813, 1.000], loss: 1.769060, mean_absolute_error: 3.607813, mean_q: 0.675452, mean_eps: 0.974390
   28609/2000000: episode: 310, duration: 1.499s, episode steps: 104, steps per second: 69, episode reward: -179.979, mean reward: -1.731 [-100.000, 64.089], mean action: 1.529 [0.000, 3.000], mean observation: -0.052 [-1.175, 1.000], loss: 1.237554, mean_absolute_error: 3.635795, mean_q: 0.863690, mean_eps: 0.974298
   28692/2000000: episode: 311, duration: 1.213s, episode steps: 83, steps per second: 68, episode reward: -282.525, mean reward: -3.404 [-100.000, 67.195], mean action: 1.482 [0.000, 3.000], mean observation: -0.014 [-1.199, 2.856], loss: 1.982042, mean_absolute_error: 3.785585, mean_q: 0.743730, mean_eps: 0.974215
   28753/2000000: episode: 312, duration: 0.903s, episode steps: 61, steps per second: 68, episode reward: -131.133, mean reward: -2.150 [-100.000, 16.081], mean action: 1.623 [0.000, 3.000], mean observation: -0.017 [-1.545, 5.940], loss: 2.000354, mean_absolute_error: 4.085526, mean_q: 0.596904, mean_eps: 0.974150
   28863/2000000: episode: 313, duration: 1.561s, episode steps: 110, steps per second: 70, episode reward: -139.152, mean reward: -1.265 [-100.000, 14.123], mean action: 1.464 [0.000, 3.000], mean observation: -0.013 [-1.286, 4.515], loss: 1.603713, mean_absolute_error: 3.646566, mean_q: 1.023208, mean_eps: 0.974073
   28958/2000000: episode: 314, duration: 1.378s, episode steps: 95, steps per second: 69, episode reward: -15.302, mean reward: -0.161 [-100.000, 133.256], mean action: 1.432 [0.000, 3.000], mean observation: 0.082 [-2.142, 1.125], loss: 1.769071, mean_absolute_error: 3.792725, mean_q: 0.524007, mean_eps: 0.973981
   29057/2000000: episode: 315, duration: 1.435s, episode steps: 99, steps per second: 69, episode reward: -177.437, mean reward: -1.792 [-100.000, 9.906], mean action: 1.515 [0.000, 3.000], mean observation: 0.020 [-3.776, 1.000], loss: 1.542455, mean_absolute_error: 3.420021, mean_q: 1.329709, mean_eps: 0.973893
   29155/2000000: episode: 316, duration: 1.372s, episode steps: 98, steps per second: 71, episode reward: -74.824, mean reward: -0.764 [-100.000, 115.924], mean action: 1.612 [0.000, 3.000], mean observation: -0.082 [-1.664, 1.000], loss: 1.271370, mean_absolute_error: 3.450506, mean_q: 1.110353, mean_eps: 0.973805
   29246/2000000: episode: 317, duration: 1.300s, episode steps: 91, steps per second: 70, episode reward: -311.881, mean reward: -3.427 [-100.000, 20.481], mean action: 1.484 [0.000, 3.000], mean observation: 0.033 [-1.303, 4.093], loss: 1.331870, mean_absolute_error: 3.577883, mean_q: 1.042401, mean_eps: 0.973720
   29308/2000000: episode: 318, duration: 0.922s, episode steps: 62, steps per second: 67, episode reward: -230.179, mean reward: -3.713 [-100.000, 3.870], mean action: 1.242 [0.000, 3.000], mean observation: -0.064 [-1.389, 2.266], loss: 1.464890, mean_absolute_error: 3.829336, mean_q: 0.823995, mean_eps: 0.973652
   29376/2000000: episode: 319, duration: 1.109s, episode steps: 68, steps per second: 61, episode reward: -186.602, mean reward: -2.744 [-100.000, 20.328], mean action: 1.485 [0.000, 3.000], mean observation: 0.043 [-1.379, 4.349], loss: 1.595041, mean_absolute_error: 3.470914, mean_q: 1.233584, mean_eps: 0.973594
   29482/2000000: episode: 320, duration: 1.544s, episode steps: 106, steps per second: 69, episode reward: -279.375, mean reward: -2.636 [-100.000, 5.655], mean action: 1.509 [0.000, 3.000], mean observation: 0.046 [-1.534, 3.591], loss: 1.143252, mean_absolute_error: 3.592294, mean_q: 1.071876, mean_eps: 0.973515
   29588/2000000: episode: 321, duration: 1.577s, episode steps: 106, steps per second: 67, episode reward: -216.318, mean reward: -2.041 [-100.000, 8.220], mean action: 1.519 [0.000, 3.000], mean observation: -0.090 [-2.180, 1.000], loss: 1.955992, mean_absolute_error: 3.860621, mean_q: 1.063689, mean_eps: 0.973419
   29689/2000000: episode: 322, duration: 1.498s, episode steps: 101, steps per second: 67, episode reward: -345.106, mean reward: -3.417 [-100.000, 0.692], mean action: 1.604 [0.000, 3.000], mean observation: 0.131 [-1.410, 1.482], loss: 1.325925, mean_absolute_error: 3.508348, mean_q: 0.775923, mean_eps: 0.973326
   29785/2000000: episode: 323, duration: 1.366s, episode steps: 96, steps per second: 70, episode reward: -223.468, mean reward: -2.328 [-100.000, 10.512], mean action: 1.490 [0.000, 3.000], mean observation: 0.108 [-1.421, 5.000], loss: 1.380180, mean_absolute_error: 3.734807, mean_q: 0.634291, mean_eps: 0.973236
   29846/2000000: episode: 324, duration: 0.872s, episode steps: 61, steps per second: 70, episode reward: -160.357, mean reward: -2.629 [-100.000, 11.364], mean action: 1.197 [0.000, 3.000], mean observation: -0.027 [-4.651, 1.000], loss: 1.385460, mean_absolute_error: 3.658588, mean_q: 0.863289, mean_eps: 0.973166
   29927/2000000: episode: 325, duration: 1.127s, episode steps: 81, steps per second: 72, episode reward: -98.520, mean reward: -1.216 [-100.000, 83.531], mean action: 1.568 [0.000, 3.000], mean observation: -0.016 [-1.480, 1.856], loss: 1.105883, mean_absolute_error: 3.792687, mean_q: 0.615487, mean_eps: 0.973103
   29996/2000000: episode: 326, duration: 1.026s, episode steps: 69, steps per second: 67, episode reward: -122.889, mean reward: -1.781 [-100.000, 9.878], mean action: 1.522 [0.000, 3.000], mean observation: -0.071 [-4.273, 1.000], loss: 1.579366, mean_absolute_error: 3.725335, mean_q: 1.139672, mean_eps: 0.973036
   30061/2000000: episode: 327, duration: 0.971s, episode steps: 65, steps per second: 67, episode reward: -166.588, mean reward: -2.563 [-100.000, 17.475], mean action: 1.415 [0.000, 3.000], mean observation: 0.012 [-1.503, 1.000], loss: 1.545973, mean_absolute_error: 3.833261, mean_q: 1.155271, mean_eps: 0.972975
   30170/2000000: episode: 328, duration: 1.537s, episode steps: 109, steps per second: 71, episode reward: -301.863, mean reward: -2.769 [-100.000, 6.142], mean action: 1.514 [0.000, 3.000], mean observation: 0.013 [-5.877, 1.083], loss: 1.515289, mean_absolute_error: 4.491390, mean_q: 1.629251, mean_eps: 0.972896
   30279/2000000: episode: 329, duration: 1.560s, episode steps: 109, steps per second: 70, episode reward: -429.792, mean reward: -3.943 [-100.000, 4.362], mean action: 1.532 [0.000, 3.000], mean observation: 0.041 [-5.684, 1.961], loss: 1.816954, mean_absolute_error: 4.743772, mean_q: 0.722602, mean_eps: 0.972798
   30349/2000000: episode: 330, duration: 1.018s, episode steps: 70, steps per second: 69, episode reward: -153.461, mean reward: -2.192 [-100.000, 16.114], mean action: 1.529 [0.000, 3.000], mean observation: -0.107 [-1.384, 3.363], loss: 1.831259, mean_absolute_error: 5.412232, mean_q: 0.610761, mean_eps: 0.972717
   30441/2000000: episode: 331, duration: 1.328s, episode steps: 92, steps per second: 69, episode reward: -182.186, mean reward: -1.980 [-100.000, 8.810], mean action: 1.511 [0.000, 3.000], mean observation: -0.128 [-1.599, 6.315], loss: 0.949307, mean_absolute_error: 4.599781, mean_q: 0.829661, mean_eps: 0.972644
   30502/2000000: episode: 332, duration: 0.852s, episode steps: 61, steps per second: 72, episode reward: -113.411, mean reward: -1.859 [-100.000, 18.426], mean action: 1.492 [0.000, 3.000], mean observation: 0.042 [-1.267, 3.162], loss: 1.033786, mean_absolute_error: 4.549493, mean_q: 0.831128, mean_eps: 0.972575
   30565/2000000: episode: 333, duration: 0.925s, episode steps: 63, steps per second: 68, episode reward: -123.482, mean reward: -1.960 [-100.000, 7.699], mean action: 1.619 [0.000, 3.000], mean observation: -0.019 [-4.971, 1.000], loss: 1.853282, mean_absolute_error: 4.934466, mean_q: 1.085298, mean_eps: 0.972519
   30676/2000000: episode: 334, duration: 1.594s, episode steps: 111, steps per second: 70, episode reward: -378.823, mean reward: -3.413 [-100.000, 30.589], mean action: 1.360 [0.000, 3.000], mean observation: 0.077 [-1.605, 2.937], loss: 2.211750, mean_absolute_error: 5.167651, mean_q: 0.420222, mean_eps: 0.972442
   30818/2000000: episode: 335, duration: 2.053s, episode steps: 142, steps per second: 69, episode reward: -185.116, mean reward: -1.304 [-100.000, 68.665], mean action: 1.683 [0.000, 3.000], mean observation: -0.091 [-1.728, 1.741], loss: 1.061692, mean_absolute_error: 4.746257, mean_q: 1.060211, mean_eps: 0.972329
   30910/2000000: episode: 336, duration: 1.316s, episode steps: 92, steps per second: 70, episode reward: -235.192, mean reward: -2.556 [-100.000, 6.774], mean action: 1.446 [0.000, 3.000], mean observation: 0.051 [-1.474, 4.505], loss: 2.044389, mean_absolute_error: 4.915127, mean_q: 1.199915, mean_eps: 0.972222
   31039/2000000: episode: 337, duration: 1.848s, episode steps: 129, steps per second: 70, episode reward: -245.174, mean reward: -1.901 [-100.000, 0.600], mean action: 1.450 [0.000, 3.000], mean observation: 0.144 [-1.399, 1.097], loss: 1.366924, mean_absolute_error: 4.360106, mean_q: 1.450374, mean_eps: 0.972123
   31162/2000000: episode: 338, duration: 1.800s, episode steps: 123, steps per second: 68, episode reward: -386.669, mean reward: -3.144 [-100.000, 117.710], mean action: 1.366 [0.000, 3.000], mean observation: 0.043 [-1.275, 2.913], loss: 1.180601, mean_absolute_error: 4.565010, mean_q: 1.413451, mean_eps: 0.972010
   31270/2000000: episode: 339, duration: 1.536s, episode steps: 108, steps per second: 70, episode reward: -293.321, mean reward: -2.716 [-100.000, 4.501], mean action: 1.324 [0.000, 3.000], mean observation: 0.071 [-1.616, 1.575], loss: 1.586727, mean_absolute_error: 4.618437, mean_q: 1.602184, mean_eps: 0.971906
   31365/2000000: episode: 340, duration: 1.378s, episode steps: 95, steps per second: 69, episode reward: -285.218, mean reward: -3.002 [-100.000, 7.092], mean action: 1.368 [0.000, 3.000], mean observation: -0.077 [-3.415, 1.000], loss: 1.325306, mean_absolute_error: 4.851704, mean_q: 0.592574, mean_eps: 0.971814
   31473/2000000: episode: 341, duration: 1.705s, episode steps: 108, steps per second: 63, episode reward: -213.342, mean reward: -1.975 [-100.000, 4.143], mean action: 1.583 [0.000, 3.000], mean observation: -0.131 [-1.351, 4.681], loss: 1.142286, mean_absolute_error: 4.759007, mean_q: 0.810200, mean_eps: 0.971722
   31589/2000000: episode: 342, duration: 1.876s, episode steps: 116, steps per second: 62, episode reward: -207.133, mean reward: -1.786 [-100.000, 5.015], mean action: 1.500 [0.000, 3.000], mean observation: 0.081 [-3.425, 1.000], loss: 1.076629, mean_absolute_error: 4.798203, mean_q: 1.077472, mean_eps: 0.971621
   31692/2000000: episode: 343, duration: 1.599s, episode steps: 103, steps per second: 64, episode reward: -479.327, mean reward: -4.654 [-100.000, 2.482], mean action: 1.233 [0.000, 3.000], mean observation: -0.011 [-2.176, 3.514], loss: 1.345684, mean_absolute_error: 4.699840, mean_q: 1.762053, mean_eps: 0.971524
   31783/2000000: episode: 344, duration: 1.326s, episode steps: 91, steps per second: 69, episode reward: -214.513, mean reward: -2.357 [-100.000, 35.490], mean action: 1.516 [0.000, 3.000], mean observation: -0.110 [-2.363, 1.000], loss: 1.395707, mean_absolute_error: 4.721120, mean_q: 1.408209, mean_eps: 0.971438
   31857/2000000: episode: 345, duration: 1.095s, episode steps: 74, steps per second: 68, episode reward: -136.347, mean reward: -1.843 [-100.000, 58.723], mean action: 1.514 [0.000, 3.000], mean observation: -0.046 [-2.134, 1.000], loss: 1.182524, mean_absolute_error: 5.062654, mean_q: 0.615198, mean_eps: 0.971362
   32857/2000000: episode: 346, duration: 15.033s, episode steps: 1000, steps per second: 67, episode reward: 50.213, mean reward: 0.050 [-23.555, 127.188], mean action: 1.545 [0.000, 3.000], mean observation: 0.082 [-1.420, 1.846], loss: 1.386338, mean_absolute_error: 4.895433, mean_q: 0.800425, mean_eps: 0.970878
   32953/2000000: episode: 347, duration: 1.402s, episode steps: 96, steps per second: 68, episode reward: -433.470, mean reward: -4.515 [-100.000, 89.183], mean action: 1.312 [0.000, 3.000], mean observation: -0.055 [-3.208, 1.180], loss: 1.225817, mean_absolute_error: 4.886091, mean_q: 0.532957, mean_eps: 0.970385
   33044/2000000: episode: 348, duration: 1.335s, episode steps: 91, steps per second: 68, episode reward: -61.880, mean reward: -0.680 [-100.000, 95.728], mean action: 1.385 [0.000, 3.000], mean observation: -0.102 [-1.740, 1.863], loss: 2.086493, mean_absolute_error: 4.999679, mean_q: 0.617103, mean_eps: 0.970302
   33145/2000000: episode: 349, duration: 1.507s, episode steps: 101, steps per second: 67, episode reward: -232.037, mean reward: -2.297 [-100.000, 20.155], mean action: 1.396 [0.000, 3.000], mean observation: 0.140 [-1.475, 5.482], loss: 1.189941, mean_absolute_error: 4.935663, mean_q: 0.530619, mean_eps: 0.970215
   33244/2000000: episode: 350, duration: 1.446s, episode steps: 99, steps per second: 68, episode reward: -308.347, mean reward: -3.115 [-100.000, 5.741], mean action: 1.556 [0.000, 3.000], mean observation: 0.068 [-4.175, 1.145], loss: 1.174905, mean_absolute_error: 4.472041, mean_q: 1.074426, mean_eps: 0.970125
   33355/2000000: episode: 351, duration: 1.616s, episode steps: 111, steps per second: 69, episode reward: -142.655, mean reward: -1.285 [-100.000, 10.569], mean action: 1.505 [0.000, 3.000], mean observation: 0.055 [-1.533, 1.000], loss: 1.849558, mean_absolute_error: 5.203149, mean_q: 0.283492, mean_eps: 0.970032
   33431/2000000: episode: 352, duration: 1.106s, episode steps: 76, steps per second: 69, episode reward: -109.170, mean reward: -1.436 [-100.000, 12.743], mean action: 1.526 [0.000, 3.000], mean observation: -0.041 [-1.342, 1.000], loss: 1.365442, mean_absolute_error: 4.966558, mean_q: 0.523023, mean_eps: 0.969947
   33557/2000000: episode: 353, duration: 1.819s, episode steps: 126, steps per second: 69, episode reward: -313.146, mean reward: -2.485 [-100.000, 6.740], mean action: 1.627 [0.000, 3.000], mean observation: -0.039 [-1.675, 1.022], loss: 1.373870, mean_absolute_error: 5.019332, mean_q: 0.410453, mean_eps: 0.969855
   33631/2000000: episode: 354, duration: 1.044s, episode steps: 74, steps per second: 71, episode reward: -172.304, mean reward: -2.328 [-100.000, 10.408], mean action: 1.595 [0.000, 3.000], mean observation: -0.138 [-7.184, 1.000], loss: 1.088281, mean_absolute_error: 4.967594, mean_q: 0.727577, mean_eps: 0.969765
   33730/2000000: episode: 355, duration: 1.425s, episode steps: 99, steps per second: 69, episode reward: -436.903, mean reward: -4.413 [-100.000, 0.573], mean action: 1.475 [0.000, 3.000], mean observation: 0.093 [-1.904, 1.622], loss: 1.044035, mean_absolute_error: 4.773955, mean_q: 0.625207, mean_eps: 0.969688
   33809/2000000: episode: 356, duration: 1.177s, episode steps: 79, steps per second: 67, episode reward: -301.458, mean reward: -3.816 [-100.000, 102.775], mean action: 1.570 [0.000, 3.000], mean observation: -0.040 [-3.136, 1.000], loss: 1.235113, mean_absolute_error: 5.108689, mean_q: 1.174860, mean_eps: 0.969607
   33898/2000000: episode: 357, duration: 1.284s, episode steps: 89, steps per second: 69, episode reward: -165.688, mean reward: -1.862 [-100.000, 8.134], mean action: 1.303 [0.000, 3.000], mean observation: 0.160 [-1.116, 1.041], loss: 1.331969, mean_absolute_error: 4.947594, mean_q: 0.374224, mean_eps: 0.969531
   33996/2000000: episode: 358, duration: 1.406s, episode steps: 98, steps per second: 70, episode reward: -169.842, mean reward: -1.733 [-100.000, 15.896], mean action: 1.480 [0.000, 3.000], mean observation: 0.052 [-1.398, 3.289], loss: 1.290990, mean_absolute_error: 4.904424, mean_q: 0.697047, mean_eps: 0.969449
   34068/2000000: episode: 359, duration: 1.062s, episode steps: 72, steps per second: 68, episode reward: -176.520, mean reward: -2.452 [-100.000, 8.000], mean action: 1.389 [0.000, 3.000], mean observation: -0.013 [-4.080, 1.000], loss: 1.107052, mean_absolute_error: 4.510376, mean_q: 1.178117, mean_eps: 0.969373
   34165/2000000: episode: 360, duration: 1.435s, episode steps: 97, steps per second: 68, episode reward: -171.094, mean reward: -1.764 [-100.000, 6.940], mean action: 1.567 [0.000, 3.000], mean observation: 0.064 [-1.375, 4.754], loss: 1.926301, mean_absolute_error: 5.134934, mean_q: 0.561109, mean_eps: 0.969296
   34254/2000000: episode: 361, duration: 1.250s, episode steps: 89, steps per second: 71, episode reward: -319.937, mean reward: -3.595 [-100.000, 4.701], mean action: 1.652 [0.000, 3.000], mean observation: -0.076 [-1.650, 6.452], loss: 1.354963, mean_absolute_error: 4.399994, mean_q: 1.391278, mean_eps: 0.969211
   34343/2000000: episode: 362, duration: 1.303s, episode steps: 89, steps per second: 68, episode reward: -272.520, mean reward: -3.062 [-100.000, 19.937], mean action: 1.742 [0.000, 3.000], mean observation: -0.045 [-5.617, 1.000], loss: 1.334413, mean_absolute_error: 4.821862, mean_q: 1.198409, mean_eps: 0.969132
   34427/2000000: episode: 363, duration: 1.209s, episode steps: 84, steps per second: 69, episode reward: -148.735, mean reward: -1.771 [-100.000, 5.378], mean action: 1.429 [0.000, 3.000], mean observation: -0.055 [-2.949, 1.000], loss: 1.109030, mean_absolute_error: 4.783035, mean_q: 0.343864, mean_eps: 0.969054
   34531/2000000: episode: 364, duration: 1.490s, episode steps: 104, steps per second: 70, episode reward: -149.286, mean reward: -1.435 [-100.000, 12.175], mean action: 1.481 [0.000, 3.000], mean observation: -0.058 [-1.392, 1.000], loss: 1.470048, mean_absolute_error: 4.994433, mean_q: 0.740057, mean_eps: 0.968970
   34638/2000000: episode: 365, duration: 1.522s, episode steps: 107, steps per second: 70, episode reward: -229.455, mean reward: -2.144 [-100.000, 1.382], mean action: 1.645 [0.000, 3.000], mean observation: -0.079 [-1.270, 1.020], loss: 1.212116, mean_absolute_error: 4.885875, mean_q: 0.779808, mean_eps: 0.968874
   34697/2000000: episode: 366, duration: 0.854s, episode steps: 59, steps per second: 69, episode reward: -202.984, mean reward: -3.440 [-100.000, 5.321], mean action: 1.576 [0.000, 3.000], mean observation: 0.021 [-1.606, 4.443], loss: 1.139339, mean_absolute_error: 4.717423, mean_q: 0.880237, mean_eps: 0.968799
   34841/2000000: episode: 367, duration: 2.019s, episode steps: 144, steps per second: 71, episode reward: -376.266, mean reward: -2.613 [-100.000, 87.879], mean action: 1.486 [0.000, 3.000], mean observation: -0.084 [-3.312, 1.048], loss: 1.592211, mean_absolute_error: 4.953028, mean_q: 0.815968, mean_eps: 0.968707
   34933/2000000: episode: 368, duration: 1.308s, episode steps: 92, steps per second: 70, episode reward: -342.884, mean reward: -3.727 [-100.000, -0.354], mean action: 1.446 [0.000, 3.000], mean observation: -0.071 [-1.670, 0.983], loss: 1.378526, mean_absolute_error: 4.992083, mean_q: 1.030282, mean_eps: 0.968601
   35007/2000000: episode: 369, duration: 1.046s, episode steps: 74, steps per second: 71, episode reward: -273.207, mean reward: -3.692 [-100.000, 4.976], mean action: 1.446 [0.000, 3.000], mean observation: 0.009 [-5.554, 1.256], loss: 1.731536, mean_absolute_error: 5.163185, mean_q: 0.233667, mean_eps: 0.968527
   35094/2000000: episode: 370, duration: 1.252s, episode steps: 87, steps per second: 70, episode reward: -168.478, mean reward: -1.937 [-100.000, 10.785], mean action: 1.690 [0.000, 3.000], mean observation: -0.102 [-1.337, 4.528], loss: 1.133878, mean_absolute_error: 4.938673, mean_q: 0.415091, mean_eps: 0.968455
   35179/2000000: episode: 371, duration: 1.244s, episode steps: 85, steps per second: 68, episode reward: -275.283, mean reward: -3.239 [-100.000, 5.479], mean action: 1.682 [0.000, 3.000], mean observation: 0.023 [-1.399, 1.144], loss: 1.294473, mean_absolute_error: 5.084335, mean_q: 0.202278, mean_eps: 0.968378
   35264/2000000: episode: 372, duration: 1.260s, episode steps: 85, steps per second: 67, episode reward: -173.045, mean reward: -2.036 [-100.000, 11.425], mean action: 1.376 [0.000, 3.000], mean observation: 0.036 [-3.799, 1.000], loss: 1.111714, mean_absolute_error: 4.722255, mean_q: 1.144605, mean_eps: 0.968302
   35340/2000000: episode: 373, duration: 1.146s, episode steps: 76, steps per second: 66, episode reward: -157.598, mean reward: -2.074 [-100.000, 6.940], mean action: 1.711 [0.000, 3.000], mean observation: -0.133 [-4.170, 1.000], loss: 1.847074, mean_absolute_error: 4.637822, mean_q: 1.257380, mean_eps: 0.968230
   35425/2000000: episode: 374, duration: 1.242s, episode steps: 85, steps per second: 68, episode reward: -391.372, mean reward: -4.604 [-100.000, 4.051], mean action: 1.412 [0.000, 3.000], mean observation: -0.053 [-1.731, 3.935], loss: 0.771672, mean_absolute_error: 4.545237, mean_q: 1.106208, mean_eps: 0.968156
   35488/2000000: episode: 375, duration: 0.906s, episode steps: 63, steps per second: 70, episode reward: -204.001, mean reward: -3.238 [-100.000, 30.242], mean action: 1.381 [0.000, 3.000], mean observation: -0.034 [-1.372, 5.037], loss: 1.411756, mean_absolute_error: 4.720013, mean_q: 0.835998, mean_eps: 0.968090
   35585/2000000: episode: 376, duration: 1.405s, episode steps: 97, steps per second: 69, episode reward: -205.423, mean reward: -2.118 [-100.000, 14.928], mean action: 1.619 [0.000, 3.000], mean observation: -0.178 [-3.610, 1.000], loss: 1.846904, mean_absolute_error: 5.016798, mean_q: 0.521354, mean_eps: 0.968018
   35688/2000000: episode: 377, duration: 1.474s, episode steps: 103, steps per second: 70, episode reward: -166.250, mean reward: -1.614 [-100.000, 22.179], mean action: 1.699 [0.000, 3.000], mean observation: -0.061 [-4.785, 1.000], loss: 1.653858, mean_absolute_error: 5.477564, mean_q: -0.233247, mean_eps: 0.967928
   35787/2000000: episode: 378, duration: 1.440s, episode steps: 99, steps per second: 69, episode reward: -131.460, mean reward: -1.328 [-100.000, 13.469], mean action: 1.576 [0.000, 3.000], mean observation: 0.101 [-1.139, 3.925], loss: 2.020916, mean_absolute_error: 5.291640, mean_q: 0.340897, mean_eps: 0.967838
   35879/2000000: episode: 379, duration: 1.307s, episode steps: 92, steps per second: 70, episode reward: -161.415, mean reward: -1.755 [-100.000, 6.113], mean action: 1.413 [0.000, 3.000], mean observation: 0.135 [-1.265, 1.340], loss: 1.252741, mean_absolute_error: 4.877894, mean_q: 0.561515, mean_eps: 0.967751
   35951/2000000: episode: 380, duration: 1.037s, episode steps: 72, steps per second: 69, episode reward: -170.504, mean reward: -2.368 [-100.000, 25.715], mean action: 1.389 [0.000, 3.000], mean observation: -0.030 [-1.558, 2.700], loss: 1.115139, mean_absolute_error: 4.408784, mean_q: 1.449735, mean_eps: 0.967677
   36065/2000000: episode: 381, duration: 1.663s, episode steps: 114, steps per second: 69, episode reward: -253.578, mean reward: -2.224 [-100.000, 79.822], mean action: 1.588 [0.000, 3.000], mean observation: 0.029 [-1.245, 3.108], loss: 1.325299, mean_absolute_error: 5.321406, mean_q: -0.086947, mean_eps: 0.967593
   36150/2000000: episode: 382, duration: 1.216s, episode steps: 85, steps per second: 70, episode reward: -149.661, mean reward: -1.761 [-100.000, 3.526], mean action: 1.518 [0.000, 3.000], mean observation: -0.088 [-1.288, 2.361], loss: 1.703278, mean_absolute_error: 4.749697, mean_q: 0.937516, mean_eps: 0.967503
   36218/2000000: episode: 383, duration: 0.984s, episode steps: 68, steps per second: 69, episode reward: -211.155, mean reward: -3.105 [-100.000, 5.078], mean action: 1.426 [0.000, 3.000], mean observation: -0.150 [-3.315, 1.000], loss: 1.045579, mean_absolute_error: 4.267877, mean_q: 1.645507, mean_eps: 0.967434
   36333/2000000: episode: 384, duration: 1.665s, episode steps: 115, steps per second: 69, episode reward: -192.280, mean reward: -1.672 [-100.000, 25.075], mean action: 1.443 [0.000, 3.000], mean observation: 0.023 [-2.532, 1.000], loss: 1.206442, mean_absolute_error: 4.657732, mean_q: 0.464602, mean_eps: 0.967352
   36435/2000000: episode: 385, duration: 1.435s, episode steps: 102, steps per second: 71, episode reward: -227.758, mean reward: -2.233 [-100.000, 1.111], mean action: 1.363 [0.000, 3.000], mean observation: 0.186 [-1.058, 1.331], loss: 0.993382, mean_absolute_error: 4.754048, mean_q: 0.547649, mean_eps: 0.967254
   36564/2000000: episode: 386, duration: 1.912s, episode steps: 129, steps per second: 67, episode reward: -225.995, mean reward: -1.752 [-100.000, 6.218], mean action: 1.605 [0.000, 3.000], mean observation: 0.079 [-1.385, 1.006], loss: 1.737116, mean_absolute_error: 5.206154, mean_q: 0.124173, mean_eps: 0.967152
   36682/2000000: episode: 387, duration: 1.721s, episode steps: 118, steps per second: 69, episode reward: -399.990, mean reward: -3.390 [-100.000, 1.573], mean action: 1.398 [0.000, 3.000], mean observation: 0.042 [-3.436, 1.978], loss: 1.193391, mean_absolute_error: 4.767355, mean_q: 0.843329, mean_eps: 0.967040
   36773/2000000: episode: 388, duration: 1.325s, episode steps: 91, steps per second: 69, episode reward: -251.590, mean reward: -2.765 [-100.000, 38.360], mean action: 1.330 [0.000, 3.000], mean observation: 0.035 [-3.840, 1.000], loss: 1.483136, mean_absolute_error: 4.816250, mean_q: 0.837418, mean_eps: 0.966945
   36845/2000000: episode: 389, duration: 1.039s, episode steps: 72, steps per second: 69, episode reward: -256.166, mean reward: -3.558 [-100.000, 21.975], mean action: 1.542 [0.000, 3.000], mean observation: -0.060 [-1.339, 4.225], loss: 1.566868, mean_absolute_error: 5.633951, mean_q: 0.180847, mean_eps: 0.966871
   36916/2000000: episode: 390, duration: 1.040s, episode steps: 71, steps per second: 68, episode reward: -99.796, mean reward: -1.406 [-100.000, 21.588], mean action: 1.408 [0.000, 3.000], mean observation: -0.066 [-1.429, 1.000], loss: 1.441685, mean_absolute_error: 4.388110, mean_q: 1.701857, mean_eps: 0.966808
   36991/2000000: episode: 391, duration: 1.099s, episode steps: 75, steps per second: 68, episode reward: -185.822, mean reward: -2.478 [-100.000, 40.573], mean action: 1.280 [0.000, 3.000], mean observation: 0.019 [-5.202, 1.000], loss: 1.156997, mean_absolute_error: 5.409907, mean_q: -0.438580, mean_eps: 0.966743
   37076/2000000: episode: 392, duration: 1.253s, episode steps: 85, steps per second: 68, episode reward: -335.712, mean reward: -3.950 [-100.000, 5.247], mean action: 1.671 [0.000, 3.000], mean observation: 0.027 [-2.281, 1.668], loss: 1.710678, mean_absolute_error: 4.705305, mean_q: 0.663107, mean_eps: 0.966671
   37157/2000000: episode: 393, duration: 1.182s, episode steps: 81, steps per second: 69, episode reward: -271.899, mean reward: -3.357 [-100.000, 4.411], mean action: 1.494 [0.000, 3.000], mean observation: -0.114 [-1.368, 1.000], loss: 1.976283, mean_absolute_error: 5.527017, mean_q: -0.024354, mean_eps: 0.966596
   37228/2000000: episode: 394, duration: 1.046s, episode steps: 71, steps per second: 68, episode reward: -146.301, mean reward: -2.061 [-100.000, 7.111], mean action: 1.465 [0.000, 3.000], mean observation: -0.112 [-5.115, 1.000], loss: 1.261511, mean_absolute_error: 4.905450, mean_q: 0.334946, mean_eps: 0.966527
   37310/2000000: episode: 395, duration: 1.203s, episode steps: 82, steps per second: 68, episode reward: -420.389, mean reward: -5.127 [-100.000, 106.146], mean action: 1.463 [0.000, 3.000], mean observation: 0.059 [-1.436, 3.141], loss: 1.006572, mean_absolute_error: 5.027988, mean_q: 0.526836, mean_eps: 0.966459
   37445/2000000: episode: 396, duration: 2.075s, episode steps: 135, steps per second: 65, episode reward: -295.796, mean reward: -2.191 [-100.000, 12.418], mean action: 1.385 [0.000, 3.000], mean observation: -0.006 [-3.899, 1.061], loss: 1.250112, mean_absolute_error: 4.918844, mean_q: 0.150522, mean_eps: 0.966360
   37532/2000000: episode: 397, duration: 1.358s, episode steps: 87, steps per second: 64, episode reward: -322.307, mean reward: -3.705 [-100.000, 23.850], mean action: 1.644 [0.000, 3.000], mean observation: 0.069 [-4.232, 1.265], loss: 1.381919, mean_absolute_error: 5.219482, mean_q: 0.143872, mean_eps: 0.966261
   37608/2000000: episode: 398, duration: 1.176s, episode steps: 76, steps per second: 65, episode reward: -162.862, mean reward: -2.143 [-100.000, 6.227], mean action: 1.395 [0.000, 3.000], mean observation: 0.019 [-5.168, 1.000], loss: 1.755378, mean_absolute_error: 5.178097, mean_q: 0.239909, mean_eps: 0.966189
   37673/2000000: episode: 399, duration: 0.999s, episode steps: 65, steps per second: 65, episode reward: -106.039, mean reward: -1.631 [-100.000, 23.426], mean action: 1.508 [0.000, 3.000], mean observation: -0.048 [-1.452, 1.000], loss: 1.436712, mean_absolute_error: 4.979160, mean_q: 0.167614, mean_eps: 0.966124
   37774/2000000: episode: 400, duration: 1.440s, episode steps: 101, steps per second: 70, episode reward: -280.011, mean reward: -2.772 [-100.000, 5.295], mean action: 1.693 [0.000, 3.000], mean observation: 0.081 [-1.426, 2.150], loss: 1.156069, mean_absolute_error: 4.930336, mean_q: 0.674535, mean_eps: 0.966048
   37873/2000000: episode: 401, duration: 1.447s, episode steps: 99, steps per second: 68, episode reward: -303.698, mean reward: -3.068 [-100.000, 6.226], mean action: 1.444 [0.000, 3.000], mean observation: 0.142 [-1.458, 4.244], loss: 1.323975, mean_absolute_error: 4.845372, mean_q: 0.862580, mean_eps: 0.965958
   37965/2000000: episode: 402, duration: 1.329s, episode steps: 92, steps per second: 69, episode reward: -124.099, mean reward: -1.349 [-100.000, 16.648], mean action: 1.663 [0.000, 3.000], mean observation: -0.002 [-1.375, 1.000], loss: 1.048564, mean_absolute_error: 4.405553, mean_q: 0.676775, mean_eps: 0.965872
   38024/2000000: episode: 403, duration: 0.853s, episode steps: 59, steps per second: 69, episode reward: -95.343, mean reward: -1.616 [-100.000, 91.735], mean action: 1.593 [0.000, 3.000], mean observation: -0.002 [-1.579, 1.000], loss: 1.614879, mean_absolute_error: 5.354627, mean_q: 0.271457, mean_eps: 0.965805
   38109/2000000: episode: 404, duration: 1.240s, episode steps: 85, steps per second: 69, episode reward: -241.538, mean reward: -2.842 [-100.000, 6.703], mean action: 1.341 [0.000, 3.000], mean observation: 0.025 [-1.532, 1.000], loss: 1.590057, mean_absolute_error: 4.978205, mean_q: 0.667399, mean_eps: 0.965741
   38168/2000000: episode: 405, duration: 0.856s, episode steps: 59, steps per second: 69, episode reward: -156.351, mean reward: -2.650 [-100.000, 9.457], mean action: 1.339 [0.000, 3.000], mean observation: -0.052 [-4.002, 1.000], loss: 0.979862, mean_absolute_error: 4.566477, mean_q: 0.958724, mean_eps: 0.965676
   38253/2000000: episode: 406, duration: 1.252s, episode steps: 85, steps per second: 68, episode reward: -142.025, mean reward: -1.671 [-100.000, 8.691], mean action: 1.471 [0.000, 3.000], mean observation: 0.028 [-1.776, 1.000], loss: 1.248141, mean_absolute_error: 4.872767, mean_q: 0.432775, mean_eps: 0.965611
   38367/2000000: episode: 407, duration: 1.602s, episode steps: 114, steps per second: 71, episode reward: -190.573, mean reward: -1.672 [-100.000, 17.033], mean action: 1.596 [0.000, 3.000], mean observation: -0.088 [-1.286, 1.000], loss: 1.332725, mean_absolute_error: 5.125982, mean_q: 0.201276, mean_eps: 0.965521
   38456/2000000: episode: 408, duration: 1.300s, episode steps: 89, steps per second: 68, episode reward: -151.279, mean reward: -1.700 [-100.000, 16.896], mean action: 1.404 [0.000, 3.000], mean observation: 0.083 [-4.487, 1.000], loss: 1.489365, mean_absolute_error: 4.758837, mean_q: 1.149542, mean_eps: 0.965431
   38532/2000000: episode: 409, duration: 1.131s, episode steps: 76, steps per second: 67, episode reward: -40.830, mean reward: -0.537 [-100.000, 83.439], mean action: 1.421 [0.000, 3.000], mean observation: 0.011 [-1.362, 1.628], loss: 1.636730, mean_absolute_error: 5.186495, mean_q: 0.380132, mean_eps: 0.965357
   38624/2000000: episode: 410, duration: 1.381s, episode steps: 92, steps per second: 67, episode reward: -375.577, mean reward: -4.082 [-100.000, 7.018], mean action: 1.511 [0.000, 3.000], mean observation: 0.083 [-1.342, 2.070], loss: 0.945771, mean_absolute_error: 5.138274, mean_q: 0.270666, mean_eps: 0.965282
   38724/2000000: episode: 411, duration: 1.469s, episode steps: 100, steps per second: 68, episode reward: -140.587, mean reward: -1.406 [-100.000, 10.513], mean action: 1.340 [0.000, 3.000], mean observation: 0.032 [-4.106, 1.000], loss: 0.976636, mean_absolute_error: 4.972861, mean_q: 0.388595, mean_eps: 0.965195
   38848/2000000: episode: 412, duration: 1.823s, episode steps: 124, steps per second: 68, episode reward: -351.139, mean reward: -2.832 [-100.000, 106.589], mean action: 1.444 [0.000, 3.000], mean observation: -0.032 [-2.763, 1.484], loss: 1.265879, mean_absolute_error: 4.834924, mean_q: 0.762585, mean_eps: 0.965094
   38950/2000000: episode: 413, duration: 1.495s, episode steps: 102, steps per second: 68, episode reward: -256.113, mean reward: -2.511 [-100.000, 5.378], mean action: 1.676 [0.000, 3.000], mean observation: 0.044 [-1.442, 3.858], loss: 1.635460, mean_absolute_error: 5.268364, mean_q: -0.082963, mean_eps: 0.964992
   39031/2000000: episode: 414, duration: 1.172s, episode steps: 81, steps per second: 69, episode reward: -269.626, mean reward: -3.329 [-100.000, 3.115], mean action: 1.728 [0.000, 3.000], mean observation: -0.038 [-1.316, 1.430], loss: 1.490034, mean_absolute_error: 5.126671, mean_q: 0.239235, mean_eps: 0.964909
   39167/2000000: episode: 415, duration: 1.949s, episode steps: 136, steps per second: 70, episode reward: -468.469, mean reward: -3.445 [-100.000, 3.230], mean action: 1.522 [0.000, 3.000], mean observation: 0.097 [-1.939, 3.372], loss: 0.917278, mean_absolute_error: 4.470727, mean_q: 0.952527, mean_eps: 0.964812
   39267/2000000: episode: 416, duration: 1.463s, episode steps: 100, steps per second: 68, episode reward: -180.440, mean reward: -1.804 [-100.000, 30.925], mean action: 1.580 [0.000, 3.000], mean observation: -0.038 [-1.698, 1.000], loss: 1.250482, mean_absolute_error: 4.681498, mean_q: 0.750494, mean_eps: 0.964706
   39399/2000000: episode: 417, duration: 1.895s, episode steps: 132, steps per second: 70, episode reward: -157.195, mean reward: -1.191 [-100.000, 19.442], mean action: 1.591 [0.000, 3.000], mean observation: -0.047 [-1.341, 1.739], loss: 1.795858, mean_absolute_error: 5.018977, mean_q: 0.829345, mean_eps: 0.964601
   39467/2000000: episode: 418, duration: 0.973s, episode steps: 68, steps per second: 70, episode reward: -146.574, mean reward: -2.156 [-100.000, 7.210], mean action: 1.559 [0.000, 3.000], mean observation: -0.086 [-1.504, 5.427], loss: 0.935134, mean_absolute_error: 4.976381, mean_q: 0.524992, mean_eps: 0.964511
   39574/2000000: episode: 419, duration: 1.536s, episode steps: 107, steps per second: 70, episode reward: -70.706, mean reward: -0.661 [-100.000, 115.628], mean action: 1.486 [0.000, 3.000], mean observation: 0.080 [-1.370, 1.144], loss: 1.455791, mean_absolute_error: 4.721539, mean_q: 0.979767, mean_eps: 0.964432
   39671/2000000: episode: 420, duration: 1.385s, episode steps: 97, steps per second: 70, episode reward: -161.677, mean reward: -1.667 [-100.000, 16.774], mean action: 1.505 [0.000, 3.000], mean observation: 0.016 [-2.698, 1.000], loss: 1.189186, mean_absolute_error: 4.491556, mean_q: 1.049089, mean_eps: 0.964340
   39752/2000000: episode: 421, duration: 1.200s, episode steps: 81, steps per second: 68, episode reward: -145.524, mean reward: -1.797 [-100.000, 17.071], mean action: 1.506 [0.000, 3.000], mean observation: 0.024 [-2.279, 1.000], loss: 1.220833, mean_absolute_error: 4.660939, mean_q: 0.626570, mean_eps: 0.964261
   39817/2000000: episode: 422, duration: 0.981s, episode steps: 65, steps per second: 66, episode reward: -160.670, mean reward: -2.472 [-100.000, 10.993], mean action: 1.492 [0.000, 3.000], mean observation: -0.050 [-1.470, 4.643], loss: 1.338201, mean_absolute_error: 4.847461, mean_q: 0.478835, mean_eps: 0.964194
   39937/2000000: episode: 423, duration: 1.732s, episode steps: 120, steps per second: 69, episode reward: -188.027, mean reward: -1.567 [-100.000, 13.895], mean action: 1.550 [0.000, 3.000], mean observation: -0.041 [-3.101, 1.000], loss: 1.508050, mean_absolute_error: 5.183457, mean_q: 0.304275, mean_eps: 0.964110
   40038/2000000: episode: 424, duration: 1.452s, episode steps: 101, steps per second: 70, episode reward: -152.760, mean reward: -1.512 [-100.000, 6.807], mean action: 1.475 [0.000, 3.000], mean observation: 0.115 [-4.793, 1.000], loss: 1.717567, mean_absolute_error: 4.940470, mean_q: 0.067458, mean_eps: 0.964011
   40147/2000000: episode: 425, duration: 1.564s, episode steps: 109, steps per second: 70, episode reward: -190.161, mean reward: -1.745 [-100.000, 23.425], mean action: 1.495 [0.000, 3.000], mean observation: -0.112 [-3.665, 1.000], loss: 1.425624, mean_absolute_error: 5.507047, mean_q: 1.196672, mean_eps: 0.963917
   40261/2000000: episode: 426, duration: 1.658s, episode steps: 114, steps per second: 69, episode reward: -355.447, mean reward: -3.118 [-100.000, 118.341], mean action: 1.614 [0.000, 3.000], mean observation: -0.046 [-1.425, 2.554], loss: 1.286807, mean_absolute_error: 5.839268, mean_q: 1.032907, mean_eps: 0.963816
   40398/2000000: episode: 427, duration: 1.951s, episode steps: 137, steps per second: 70, episode reward: -314.300, mean reward: -2.294 [-100.000, 5.005], mean action: 1.496 [0.000, 3.000], mean observation: 0.165 [-1.862, 1.832], loss: 1.849050, mean_absolute_error: 6.083934, mean_q: 0.584869, mean_eps: 0.963703
   40510/2000000: episode: 428, duration: 1.619s, episode steps: 112, steps per second: 69, episode reward: -168.358, mean reward: -1.503 [-100.000, 15.888], mean action: 1.652 [0.000, 3.000], mean observation: 0.046 [-1.510, 1.000], loss: 1.689498, mean_absolute_error: 5.827133, mean_q: 0.908524, mean_eps: 0.963591
   40576/2000000: episode: 429, duration: 0.985s, episode steps: 66, steps per second: 67, episode reward: -145.279, mean reward: -2.201 [-100.000, 54.467], mean action: 1.682 [0.000, 3.000], mean observation: 0.015 [-1.298, 1.000], loss: 1.296642, mean_absolute_error: 5.889586, mean_q: 0.632463, mean_eps: 0.963512
   40649/2000000: episode: 430, duration: 1.087s, episode steps: 73, steps per second: 67, episode reward: -178.840, mean reward: -2.450 [-100.000, 6.090], mean action: 1.493 [0.000, 3.000], mean observation: -0.144 [-5.078, 1.000], loss: 1.219058, mean_absolute_error: 6.062393, mean_q: 0.498752, mean_eps: 0.963449
   40772/2000000: episode: 431, duration: 1.783s, episode steps: 123, steps per second: 69, episode reward: -177.810, mean reward: -1.446 [-100.000, 6.749], mean action: 1.577 [0.000, 3.000], mean observation: 0.178 [-4.586, 1.000], loss: 1.369434, mean_absolute_error: 5.815203, mean_q: 1.017527, mean_eps: 0.963361
   40880/2000000: episode: 432, duration: 1.586s, episode steps: 108, steps per second: 68, episode reward: -138.469, mean reward: -1.282 [-100.000, 36.327], mean action: 1.565 [0.000, 3.000], mean observation: -0.068 [-2.936, 1.000], loss: 1.308060, mean_absolute_error: 6.289549, mean_q: 0.005429, mean_eps: 0.963258
   40987/2000000: episode: 433, duration: 1.556s, episode steps: 107, steps per second: 69, episode reward: -157.429, mean reward: -1.471 [-100.000, 7.672], mean action: 1.542 [0.000, 3.000], mean observation: -0.096 [-4.881, 1.000], loss: 2.198255, mean_absolute_error: 6.426596, mean_q: 0.317371, mean_eps: 0.963161
   41055/2000000: episode: 434, duration: 0.992s, episode steps: 68, steps per second: 69, episode reward: -196.855, mean reward: -2.895 [-100.000, 5.402], mean action: 1.632 [0.000, 3.000], mean observation: 0.014 [-1.374, 3.749], loss: 1.102660, mean_absolute_error: 5.853613, mean_q: 1.116438, mean_eps: 0.963082
   41206/2000000: episode: 435, duration: 2.167s, episode steps: 151, steps per second: 70, episode reward: -308.456, mean reward: -2.043 [-100.000, 4.946], mean action: 1.543 [0.000, 3.000], mean observation: 0.088 [-1.535, 3.803], loss: 1.227736, mean_absolute_error: 5.798477, mean_q: 0.971515, mean_eps: 0.962983
   41273/2000000: episode: 436, duration: 0.990s, episode steps: 67, steps per second: 68, episode reward: -211.125, mean reward: -3.151 [-100.000, 5.289], mean action: 1.582 [0.000, 3.000], mean observation: 0.051 [-1.546, 4.492], loss: 1.702598, mean_absolute_error: 5.841569, mean_q: 0.790713, mean_eps: 0.962884
   41334/2000000: episode: 437, duration: 0.863s, episode steps: 61, steps per second: 71, episode reward: -178.389, mean reward: -2.924 [-100.000, 5.988], mean action: 1.377 [0.000, 3.000], mean observation: -0.097 [-4.776, 1.000], loss: 1.703567, mean_absolute_error: 6.132522, mean_q: -0.265653, mean_eps: 0.962826
   41437/2000000: episode: 438, duration: 1.494s, episode steps: 103, steps per second: 69, episode reward: -356.824, mean reward: -3.464 [-100.000, 3.730], mean action: 1.505 [0.000, 3.000], mean observation: 0.075 [-1.627, 2.904], loss: 1.547947, mean_absolute_error: 5.981097, mean_q: 1.011163, mean_eps: 0.962753
   41545/2000000: episode: 439, duration: 1.557s, episode steps: 108, steps per second: 69, episode reward: -165.787, mean reward: -1.535 [-100.000, 15.749], mean action: 1.583 [0.000, 3.000], mean observation: 0.004 [-1.160, 4.195], loss: 1.460872, mean_absolute_error: 6.141319, mean_q: 0.169488, mean_eps: 0.962657
   41656/2000000: episode: 440, duration: 1.617s, episode steps: 111, steps per second: 69, episode reward: -239.405, mean reward: -2.157 [-100.000, 0.906], mean action: 1.595 [0.000, 3.000], mean observation: 0.178 [-1.143, 1.296], loss: 1.476144, mean_absolute_error: 6.328762, mean_q: 0.082613, mean_eps: 0.962560
   41774/2000000: episode: 441, duration: 1.719s, episode steps: 118, steps per second: 69, episode reward: -226.743, mean reward: -1.922 [-100.000, 16.587], mean action: 1.525 [0.000, 3.000], mean observation: 0.156 [-1.223, 4.280], loss: 1.332286, mean_absolute_error: 5.926420, mean_q: 1.046186, mean_eps: 0.962457
   41847/2000000: episode: 442, duration: 1.042s, episode steps: 73, steps per second: 70, episode reward: -109.995, mean reward: -1.507 [-100.000, 17.244], mean action: 1.589 [0.000, 3.000], mean observation: -0.089 [-1.323, 4.685], loss: 1.542067, mean_absolute_error: 6.244746, mean_q: 0.454386, mean_eps: 0.962371
   41966/2000000: episode: 443, duration: 1.711s, episode steps: 119, steps per second: 70, episode reward: -140.678, mean reward: -1.182 [-100.000, 13.817], mean action: 1.403 [0.000, 3.000], mean observation: -0.026 [-1.477, 1.009], loss: 2.163176, mean_absolute_error: 6.397500, mean_q: 0.620678, mean_eps: 0.962285
   42044/2000000: episode: 444, duration: 1.153s, episode steps: 78, steps per second: 68, episode reward: -165.365, mean reward: -2.120 [-100.000, 10.600], mean action: 1.615 [0.000, 3.000], mean observation: -0.152 [-1.500, 1.000], loss: 0.948778, mean_absolute_error: 6.063533, mean_q: 1.036149, mean_eps: 0.962196
   42127/2000000: episode: 445, duration: 1.208s, episode steps: 83, steps per second: 69, episode reward: -252.537, mean reward: -3.043 [-100.000, 26.954], mean action: 1.494 [0.000, 3.000], mean observation: -0.017 [-1.410, 4.329], loss: 1.098352, mean_absolute_error: 6.120577, mean_q: 0.454527, mean_eps: 0.962124
   42235/2000000: episode: 446, duration: 1.556s, episode steps: 108, steps per second: 69, episode reward: -269.302, mean reward: -2.494 [-100.000, 101.389], mean action: 1.602 [0.000, 3.000], mean observation: -0.144 [-2.299, 1.000], loss: 1.077406, mean_absolute_error: 5.732091, mean_q: 1.331149, mean_eps: 0.962038
   42343/2000000: episode: 447, duration: 1.550s, episode steps: 108, steps per second: 70, episode reward: -316.162, mean reward: -2.927 [-100.000, 35.641], mean action: 1.463 [0.000, 3.000], mean observation: -0.017 [-1.523, 4.541], loss: 1.199841, mean_absolute_error: 6.150270, mean_q: 0.228316, mean_eps: 0.961941
   42484/2000000: episode: 448, duration: 2.058s, episode steps: 141, steps per second: 69, episode reward: -153.862, mean reward: -1.091 [-100.000, 11.465], mean action: 1.482 [0.000, 3.000], mean observation: -0.064 [-1.270, 3.328], loss: 0.967248, mean_absolute_error: 6.101869, mean_q: 0.102480, mean_eps: 0.961829
   42575/2000000: episode: 449, duration: 1.335s, episode steps: 91, steps per second: 68, episode reward: -381.479, mean reward: -4.192 [-100.000, 0.032], mean action: 1.473 [0.000, 3.000], mean observation: 0.125 [-1.389, 1.610], loss: 1.823332, mean_absolute_error: 5.825799, mean_q: 1.064473, mean_eps: 0.961725
   42672/2000000: episode: 450, duration: 1.427s, episode steps: 97, steps per second: 68, episode reward: -230.109, mean reward: -2.372 [-100.000, 13.287], mean action: 1.567 [0.000, 3.000], mean observation: 0.082 [-1.283, 3.749], loss: 1.097517, mean_absolute_error: 5.769464, mean_q: 1.726564, mean_eps: 0.961640
   42790/2000000: episode: 451, duration: 1.724s, episode steps: 118, steps per second: 68, episode reward: -160.279, mean reward: -1.358 [-100.000, 5.894], mean action: 1.610 [0.000, 3.000], mean observation: -0.062 [-1.105, 3.599], loss: 1.427564, mean_absolute_error: 6.145531, mean_q: 0.763791, mean_eps: 0.961543
   42918/2000000: episode: 452, duration: 1.837s, episode steps: 128, steps per second: 70, episode reward: -388.342, mean reward: -3.034 [-100.000, 58.242], mean action: 1.602 [0.000, 3.000], mean observation: -0.086 [-3.563, 1.126], loss: 1.021284, mean_absolute_error: 6.041359, mean_q: 0.575131, mean_eps: 0.961431
   43000/2000000: episode: 453, duration: 1.219s, episode steps: 82, steps per second: 67, episode reward: -146.442, mean reward: -1.786 [-100.000, 6.968], mean action: 1.561 [0.000, 3.000], mean observation: -0.102 [-5.464, 1.000], loss: 1.315319, mean_absolute_error: 6.428168, mean_q: -0.328113, mean_eps: 0.961338
   43066/2000000: episode: 454, duration: 0.984s, episode steps: 66, steps per second: 67, episode reward: -102.847, mean reward: -1.558 [-100.000, 24.428], mean action: 1.333 [0.000, 3.000], mean observation: -0.047 [-1.438, 1.000], loss: 1.776064, mean_absolute_error: 6.510492, mean_q: -0.483358, mean_eps: 0.961271
   43169/2000000: episode: 455, duration: 1.493s, episode steps: 103, steps per second: 69, episode reward: -190.286, mean reward: -1.847 [-100.000, 6.547], mean action: 1.495 [0.000, 3.000], mean observation: 0.139 [-1.285, 4.234], loss: 0.769646, mean_absolute_error: 5.185881, mean_q: 1.683426, mean_eps: 0.961194
   43269/2000000: episode: 456, duration: 1.435s, episode steps: 100, steps per second: 70, episode reward: -294.469, mean reward: -2.945 [-100.000, 65.668], mean action: 1.390 [0.000, 3.000], mean observation: 0.030 [-1.302, 3.064], loss: 1.180011, mean_absolute_error: 6.127869, mean_q: 0.628780, mean_eps: 0.961102
   43367/2000000: episode: 457, duration: 1.425s, episode steps: 98, steps per second: 69, episode reward: -112.422, mean reward: -1.147 [-100.000, 95.272], mean action: 1.602 [0.000, 3.000], mean observation: 0.056 [-1.319, 2.426], loss: 1.408567, mean_absolute_error: 5.711400, mean_q: 1.160561, mean_eps: 0.961014
   43522/2000000: episode: 458, duration: 2.240s, episode steps: 155, steps per second: 69, episode reward: -225.687, mean reward: -1.456 [-100.000, 6.583], mean action: 1.677 [0.000, 3.000], mean observation: -0.078 [-1.291, 3.887], loss: 1.114247, mean_absolute_error: 6.114769, mean_q: 0.233956, mean_eps: 0.960900
   43618/2000000: episode: 459, duration: 1.373s, episode steps: 96, steps per second: 70, episode reward: -195.488, mean reward: -2.036 [-100.000, 6.383], mean action: 1.458 [0.000, 3.000], mean observation: -0.001 [-5.247, 1.000], loss: 1.442855, mean_absolute_error: 5.706644, mean_q: 1.426379, mean_eps: 0.960787
   43737/2000000: episode: 460, duration: 1.720s, episode steps: 119, steps per second: 69, episode reward: -210.125, mean reward: -1.766 [-100.000, 3.778], mean action: 1.395 [0.000, 3.000], mean observation: 0.194 [-1.051, 1.000], loss: 0.983043, mean_absolute_error: 6.062695, mean_q: 0.215774, mean_eps: 0.960690
   43851/2000000: episode: 461, duration: 1.610s, episode steps: 114, steps per second: 71, episode reward: -142.353, mean reward: -1.249 [-100.000, 7.446], mean action: 1.544 [0.000, 3.000], mean observation: -0.080 [-4.656, 1.022], loss: 1.323935, mean_absolute_error: 6.102914, mean_q: 0.403609, mean_eps: 0.960585
   43958/2000000: episode: 462, duration: 1.572s, episode steps: 107, steps per second: 68, episode reward: -231.180, mean reward: -2.161 [-100.000, 7.235], mean action: 1.523 [0.000, 3.000], mean observation: -0.125 [-1.522, 1.177], loss: 1.228458, mean_absolute_error: 6.364715, mean_q: 0.379517, mean_eps: 0.960486
   44064/2000000: episode: 463, duration: 1.576s, episode steps: 106, steps per second: 67, episode reward: -248.497, mean reward: -2.344 [-100.000, 6.007], mean action: 1.566 [0.000, 3.000], mean observation: -0.113 [-3.356, 1.000], loss: 1.414258, mean_absolute_error: 6.224891, mean_q: 0.060030, mean_eps: 0.960391
   44139/2000000: episode: 464, duration: 1.101s, episode steps: 75, steps per second: 68, episode reward: -149.692, mean reward: -1.996 [-100.000, 11.280], mean action: 1.427 [0.000, 3.000], mean observation: -0.016 [-4.434, 1.000], loss: 1.371828, mean_absolute_error: 5.768248, mean_q: 0.692377, mean_eps: 0.960310
   44206/2000000: episode: 465, duration: 0.984s, episode steps: 67, steps per second: 68, episode reward: -202.441, mean reward: -3.022 [-100.000, 9.414], mean action: 1.478 [0.000, 3.000], mean observation: 0.048 [-1.511, 5.077], loss: 1.377878, mean_absolute_error: 6.630209, mean_q: 0.520476, mean_eps: 0.960245
   44311/2000000: episode: 466, duration: 1.489s, episode steps: 105, steps per second: 71, episode reward: -268.743, mean reward: -2.559 [-100.000, 1.056], mean action: 1.610 [0.000, 3.000], mean observation: -0.061 [-1.310, 1.024], loss: 0.965067, mean_absolute_error: 6.007682, mean_q: 0.672247, mean_eps: 0.960168
   44432/2000000: episode: 467, duration: 1.781s, episode steps: 121, steps per second: 68, episode reward: -276.764, mean reward: -2.287 [-100.000, 17.187], mean action: 1.479 [0.000, 3.000], mean observation: -0.023 [-4.129, 1.000], loss: 1.151354, mean_absolute_error: 5.750697, mean_q: 0.835895, mean_eps: 0.960067
   44505/2000000: episode: 468, duration: 1.065s, episode steps: 73, steps per second: 69, episode reward: -235.173, mean reward: -3.222 [-100.000, 7.791], mean action: 1.438 [0.000, 3.000], mean observation: -0.136 [-3.725, 1.000], loss: 0.932209, mean_absolute_error: 5.381717, mean_q: 1.294287, mean_eps: 0.959979
   44577/2000000: episode: 469, duration: 1.043s, episode steps: 72, steps per second: 69, episode reward: -196.563, mean reward: -2.730 [-100.000, 6.974], mean action: 1.472 [0.000, 3.000], mean observation: -0.018 [-1.548, 5.128], loss: 1.092652, mean_absolute_error: 5.892276, mean_q: 1.015181, mean_eps: 0.959912
   44676/2000000: episode: 470, duration: 1.421s, episode steps: 99, steps per second: 70, episode reward: -369.521, mean reward: -3.733 [-100.000, 2.309], mean action: 1.404 [0.000, 3.000], mean observation: 0.038 [-1.784, 7.254], loss: 1.401418, mean_absolute_error: 6.230975, mean_q: 0.248209, mean_eps: 0.959837
   44742/2000000: episode: 471, duration: 0.987s, episode steps: 66, steps per second: 67, episode reward: -126.760, mean reward: -1.921 [-100.000, 9.422], mean action: 1.515 [0.000, 3.000], mean observation: -0.023 [-4.572, 1.000], loss: 1.020869, mean_absolute_error: 5.887250, mean_q: 0.389670, mean_eps: 0.959763
   44872/2000000: episode: 472, duration: 1.890s, episode steps: 130, steps per second: 69, episode reward: -211.997, mean reward: -1.631 [-100.000, 6.561], mean action: 1.600 [0.000, 3.000], mean observation: 0.053 [-1.308, 3.353], loss: 1.200746, mean_absolute_error: 6.075087, mean_q: 0.872449, mean_eps: 0.959675
   44976/2000000: episode: 473, duration: 1.546s, episode steps: 104, steps per second: 67, episode reward: -167.693, mean reward: -1.612 [-100.000, 5.594], mean action: 1.327 [0.000, 3.000], mean observation: 0.039 [-4.090, 1.000], loss: 1.125668, mean_absolute_error: 5.631040, mean_q: 0.799263, mean_eps: 0.959570
   45077/2000000: episode: 474, duration: 1.479s, episode steps: 101, steps per second: 68, episode reward: -261.420, mean reward: -2.588 [-100.000, 8.244], mean action: 1.604 [0.000, 3.000], mean observation: 0.070 [-1.412, 3.812], loss: 0.892926, mean_absolute_error: 5.447956, mean_q: 1.472641, mean_eps: 0.959477
   45156/2000000: episode: 475, duration: 1.158s, episode steps: 79, steps per second: 68, episode reward: -208.903, mean reward: -2.644 [-100.000, 7.339], mean action: 1.658 [0.000, 3.000], mean observation: -0.103 [-3.936, 1.000], loss: 1.265556, mean_absolute_error: 6.002471, mean_q: 0.385929, mean_eps: 0.959396
   45266/2000000: episode: 476, duration: 1.598s, episode steps: 110, steps per second: 69, episode reward: -173.333, mean reward: -1.576 [-100.000, 56.744], mean action: 1.345 [0.000, 3.000], mean observation: 0.167 [-5.739, 1.000], loss: 1.363036, mean_absolute_error: 6.209462, mean_q: 0.779085, mean_eps: 0.959311
   45373/2000000: episode: 477, duration: 1.582s, episode steps: 107, steps per second: 68, episode reward: -342.302, mean reward: -3.199 [-100.000, 29.436], mean action: 1.542 [0.000, 3.000], mean observation: 0.113 [-1.251, 3.048], loss: 1.227525, mean_absolute_error: 6.403039, mean_q: 0.124393, mean_eps: 0.959212
   45455/2000000: episode: 478, duration: 1.171s, episode steps: 82, steps per second: 70, episode reward: -243.580, mean reward: -2.970 [-100.000, 22.816], mean action: 1.561 [0.000, 3.000], mean observation: 0.175 [-1.502, 4.253], loss: 0.946113, mean_absolute_error: 5.810096, mean_q: 0.566916, mean_eps: 0.959127
   45538/2000000: episode: 479, duration: 1.201s, episode steps: 83, steps per second: 69, episode reward: -133.652, mean reward: -1.610 [-100.000, 7.066], mean action: 1.554 [0.000, 3.000], mean observation: -0.101 [-1.265, 1.000], loss: 1.362215, mean_absolute_error: 6.175448, mean_q: 0.082250, mean_eps: 0.959054
   45625/2000000: episode: 480, duration: 1.264s, episode steps: 87, steps per second: 69, episode reward: -151.544, mean reward: -1.742 [-100.000, 7.623], mean action: 1.471 [0.000, 3.000], mean observation: 0.033 [-4.229, 1.000], loss: 1.172027, mean_absolute_error: 6.244930, mean_q: -0.019565, mean_eps: 0.958976
   45742/2000000: episode: 481, duration: 1.669s, episode steps: 117, steps per second: 70, episode reward: -69.464, mean reward: -0.594 [-100.000, 75.898], mean action: 1.547 [0.000, 3.000], mean observation: 0.025 [-1.307, 1.637], loss: 1.008532, mean_absolute_error: 5.859049, mean_q: 0.697808, mean_eps: 0.958884
   45879/2000000: episode: 482, duration: 2.056s, episode steps: 137, steps per second: 67, episode reward: -187.238, mean reward: -1.367 [-100.000, 16.208], mean action: 1.766 [0.000, 3.000], mean observation: -0.063 [-1.225, 3.224], loss: 1.539221, mean_absolute_error: 5.903226, mean_q: 0.451358, mean_eps: 0.958771
   45939/2000000: episode: 483, duration: 0.893s, episode steps: 60, steps per second: 67, episode reward: -134.865, mean reward: -2.248 [-100.000, 12.692], mean action: 1.350 [0.000, 3.000], mean observation: -0.017 [-5.665, 1.000], loss: 2.038498, mean_absolute_error: 6.514105, mean_q: 0.213658, mean_eps: 0.958683
   46053/2000000: episode: 484, duration: 1.674s, episode steps: 114, steps per second: 68, episode reward: -433.822, mean reward: -3.805 [-100.000, 4.904], mean action: 1.605 [0.000, 3.000], mean observation: -0.039 [-5.963, 1.706], loss: 1.487199, mean_absolute_error: 6.090161, mean_q: 0.788313, mean_eps: 0.958604
   46111/2000000: episode: 485, duration: 0.810s, episode steps: 58, steps per second: 72, episode reward: -109.939, mean reward: -1.896 [-100.000, 25.260], mean action: 1.345 [0.000, 3.000], mean observation: -0.122 [-5.281, 1.000], loss: 1.439997, mean_absolute_error: 6.428185, mean_q: 0.379706, mean_eps: 0.958526
   46186/2000000: episode: 486, duration: 1.123s, episode steps: 75, steps per second: 67, episode reward: -116.225, mean reward: -1.550 [-100.000, 64.308], mean action: 1.693 [0.000, 3.000], mean observation: -0.037 [-1.322, 3.207], loss: 1.083398, mean_absolute_error: 5.652033, mean_q: 1.303389, mean_eps: 0.958467
   46269/2000000: episode: 487, duration: 1.201s, episode steps: 83, steps per second: 69, episode reward: -342.342, mean reward: -4.125 [-100.000, 10.284], mean action: 1.687 [0.000, 3.000], mean observation: -0.066 [-2.912, 1.261], loss: 0.945108, mean_absolute_error: 5.563125, mean_q: 0.286600, mean_eps: 0.958395
   46355/2000000: episode: 488, duration: 1.203s, episode steps: 86, steps per second: 71, episode reward: -150.482, mean reward: -1.750 [-100.000, 13.905], mean action: 1.523 [0.000, 3.000], mean observation: -0.039 [-1.500, 1.000], loss: 1.435803, mean_absolute_error: 6.109654, mean_q: 0.786975, mean_eps: 0.958319
   46436/2000000: episode: 489, duration: 1.208s, episode steps: 81, steps per second: 67, episode reward: -179.265, mean reward: -2.213 [-100.000, 5.695], mean action: 1.716 [0.000, 3.000], mean observation: -0.091 [-1.448, 3.907], loss: 1.248579, mean_absolute_error: 6.454478, mean_q: -0.290910, mean_eps: 0.958245
   46511/2000000: episode: 490, duration: 1.089s, episode steps: 75, steps per second: 69, episode reward: -148.247, mean reward: -1.977 [-100.000, 9.916], mean action: 1.493 [0.000, 3.000], mean observation: 0.051 [-1.439, 4.690], loss: 0.968224, mean_absolute_error: 5.183684, mean_q: 1.624265, mean_eps: 0.958175
   46607/2000000: episode: 491, duration: 1.391s, episode steps: 96, steps per second: 69, episode reward: -350.493, mean reward: -3.651 [-100.000, 7.080], mean action: 1.562 [0.000, 3.000], mean observation: -0.010 [-1.691, 1.235], loss: 1.659445, mean_absolute_error: 6.242647, mean_q: -0.049626, mean_eps: 0.958098
   46699/2000000: episode: 492, duration: 1.526s, episode steps: 92, steps per second: 60, episode reward: -197.642, mean reward: -2.148 [-100.000, 8.917], mean action: 1.413 [0.000, 3.000], mean observation: -0.001 [-1.377, 1.031], loss: 1.111415, mean_absolute_error: 6.192984, mean_q: 0.895918, mean_eps: 0.958013
   46773/2000000: episode: 493, duration: 1.224s, episode steps: 74, steps per second: 60, episode reward: -229.876, mean reward: -3.106 [-100.000, 10.568], mean action: 1.486 [0.000, 3.000], mean observation: 0.049 [-1.522, 3.782], loss: 1.167179, mean_absolute_error: 6.408613, mean_q: -0.071925, mean_eps: 0.957938
   46854/2000000: episode: 494, duration: 1.176s, episode steps: 81, steps per second: 69, episode reward: -163.849, mean reward: -2.023 [-100.000, 6.355], mean action: 1.654 [0.000, 3.000], mean observation: -0.110 [-1.328, 4.688], loss: 1.394625, mean_absolute_error: 6.211626, mean_q: 0.357881, mean_eps: 0.957867
   46952/2000000: episode: 495, duration: 1.420s, episode steps: 98, steps per second: 69, episode reward: -161.464, mean reward: -1.648 [-100.000, 8.889], mean action: 1.459 [0.000, 3.000], mean observation: 0.116 [-1.374, 1.000], loss: 1.419699, mean_absolute_error: 6.363098, mean_q: 0.282421, mean_eps: 0.957788
   47030/2000000: episode: 496, duration: 1.132s, episode steps: 78, steps per second: 69, episode reward: -142.795, mean reward: -1.831 [-100.000, 6.679], mean action: 1.346 [0.000, 3.000], mean observation: 0.093 [-4.494, 1.000], loss: 1.265626, mean_absolute_error: 6.015263, mean_q: 0.791080, mean_eps: 0.957709
   47090/2000000: episode: 497, duration: 0.868s, episode steps: 60, steps per second: 69, episode reward: -101.888, mean reward: -1.698 [-100.000, 16.460], mean action: 1.567 [0.000, 3.000], mean observation: -0.071 [-1.490, 1.000], loss: 1.498352, mean_absolute_error: 6.433436, mean_q: 0.502217, mean_eps: 0.957646
   47259/2000000: episode: 498, duration: 2.395s, episode steps: 169, steps per second: 71, episode reward: -229.893, mean reward: -1.360 [-100.000, 18.291], mean action: 1.586 [0.000, 3.000], mean observation: 0.208 [-1.560, 4.814], loss: 1.523999, mean_absolute_error: 6.721496, mean_q: -0.217638, mean_eps: 0.957543
   47361/2000000: episode: 499, duration: 1.506s, episode steps: 102, steps per second: 68, episode reward: -200.194, mean reward: -1.963 [-100.000, 7.979], mean action: 1.529 [0.000, 3.000], mean observation: 0.010 [-1.464, 3.383], loss: 1.416767, mean_absolute_error: 6.239176, mean_q: 0.090557, mean_eps: 0.957421
   47432/2000000: episode: 500, duration: 1.045s, episode steps: 71, steps per second: 68, episode reward: -160.725, mean reward: -2.264 [-100.000, 13.509], mean action: 1.437 [0.000, 3.000], mean observation: 0.086 [-1.265, 3.904], loss: 0.942934, mean_absolute_error: 5.902207, mean_q: 0.728215, mean_eps: 0.957344
   47513/2000000: episode: 501, duration: 1.220s, episode steps: 81, steps per second: 66, episode reward: -113.360, mean reward: -1.400 [-100.000, 10.567], mean action: 1.531 [0.000, 3.000], mean observation: -0.068 [-4.454, 1.000], loss: 1.303825, mean_absolute_error: 6.171742, mean_q: 0.356969, mean_eps: 0.957275
   47592/2000000: episode: 502, duration: 1.147s, episode steps: 79, steps per second: 69, episode reward: -168.276, mean reward: -2.130 [-100.000, 7.677], mean action: 1.671 [0.000, 3.000], mean observation: -0.131 [-1.368, 4.030], loss: 0.781183, mean_absolute_error: 5.595456, mean_q: 1.004349, mean_eps: 0.957203
   47663/2000000: episode: 503, duration: 1.033s, episode steps: 71, steps per second: 69, episode reward: -116.450, mean reward: -1.640 [-100.000, 12.223], mean action: 1.803 [0.000, 3.000], mean observation: -0.095 [-4.622, 1.000], loss: 1.213274, mean_absolute_error: 6.016695, mean_q: 0.873943, mean_eps: 0.957137
   47725/2000000: episode: 504, duration: 0.904s, episode steps: 62, steps per second: 69, episode reward: -159.603, mean reward: -2.574 [-100.000, 30.680], mean action: 1.177 [0.000, 3.000], mean observation: 0.029 [-1.671, 1.000], loss: 1.293403, mean_absolute_error: 6.280272, mean_q: 0.230117, mean_eps: 0.957075
   47797/2000000: episode: 505, duration: 1.038s, episode steps: 72, steps per second: 69, episode reward: -227.683, mean reward: -3.162 [-100.000, 12.499], mean action: 1.389 [0.000, 3.000], mean observation: -0.018 [-2.859, 1.000], loss: 1.103569, mean_absolute_error: 5.897210, mean_q: 0.606645, mean_eps: 0.957014
   47920/2000000: episode: 506, duration: 1.773s, episode steps: 123, steps per second: 69, episode reward: -224.414, mean reward: -1.825 [-100.000, 1.983], mean action: 1.480 [0.000, 3.000], mean observation: 0.154 [-1.328, 1.084], loss: 1.008894, mean_absolute_error: 5.789213, mean_q: 0.954619, mean_eps: 0.956928
   48066/2000000: episode: 507, duration: 2.101s, episode steps: 146, steps per second: 69, episode reward: -39.451, mean reward: -0.270 [-100.000, 83.511], mean action: 1.418 [0.000, 3.000], mean observation: 0.088 [-1.282, 1.439], loss: 1.061757, mean_absolute_error: 5.931072, mean_q: 0.980399, mean_eps: 0.956807
   48161/2000000: episode: 508, duration: 1.417s, episode steps: 95, steps per second: 67, episode reward: -346.653, mean reward: -3.649 [-100.000, 0.768], mean action: 1.568 [0.000, 3.000], mean observation: 0.121 [-1.367, 1.635], loss: 1.058103, mean_absolute_error: 5.836815, mean_q: 0.464261, mean_eps: 0.956697
   48235/2000000: episode: 509, duration: 1.046s, episode steps: 74, steps per second: 71, episode reward: -189.111, mean reward: -2.556 [-100.000, 12.194], mean action: 1.703 [0.000, 3.000], mean observation: -0.109 [-4.147, 1.000], loss: 1.189029, mean_absolute_error: 6.390640, mean_q: 0.072171, mean_eps: 0.956622
   48321/2000000: episode: 510, duration: 1.251s, episode steps: 86, steps per second: 69, episode reward: -289.226, mean reward: -3.363 [-100.000, 6.175], mean action: 1.605 [0.000, 3.000], mean observation: -0.077 [-1.510, 6.058], loss: 0.927506, mean_absolute_error: 5.462153, mean_q: 1.679535, mean_eps: 0.956550
   48390/2000000: episode: 511, duration: 0.969s, episode steps: 69, steps per second: 71, episode reward: -126.385, mean reward: -1.832 [-100.000, 17.152], mean action: 1.261 [0.000, 3.000], mean observation: 0.027 [-1.446, 1.000], loss: 1.541232, mean_absolute_error: 6.294375, mean_q: 0.499599, mean_eps: 0.956480
   48459/2000000: episode: 512, duration: 0.974s, episode steps: 69, steps per second: 71, episode reward: -155.225, mean reward: -2.250 [-100.000, 10.451], mean action: 1.478 [0.000, 3.000], mean observation: 0.042 [-1.453, 3.780], loss: 1.323903, mean_absolute_error: 5.538430, mean_q: 1.717601, mean_eps: 0.956418
   48517/2000000: episode: 513, duration: 0.880s, episode steps: 58, steps per second: 66, episode reward: -212.916, mean reward: -3.671 [-100.000, 67.224], mean action: 1.621 [0.000, 3.000], mean observation: -0.036 [-3.541, 1.000], loss: 1.055224, mean_absolute_error: 5.807005, mean_q: 1.090328, mean_eps: 0.956361
   48626/2000000: episode: 514, duration: 1.552s, episode steps: 109, steps per second: 70, episode reward: -344.196, mean reward: -3.158 [-100.000, 125.527], mean action: 1.468 [0.000, 3.000], mean observation: -0.027 [-1.374, 2.645], loss: 0.968772, mean_absolute_error: 5.593931, mean_q: 1.411687, mean_eps: 0.956285
   48695/2000000: episode: 515, duration: 0.982s, episode steps: 69, steps per second: 70, episode reward: -109.534, mean reward: -1.587 [-100.000, 9.916], mean action: 1.667 [0.000, 3.000], mean observation: -0.080 [-1.398, 3.138], loss: 1.107854, mean_absolute_error: 6.016230, mean_q: 0.429648, mean_eps: 0.956206
   48788/2000000: episode: 516, duration: 1.390s, episode steps: 93, steps per second: 67, episode reward: -314.855, mean reward: -3.386 [-100.000, 109.596], mean action: 1.484 [0.000, 3.000], mean observation: -0.066 [-2.604, 1.002], loss: 1.773611, mean_absolute_error: 6.279254, mean_q: 0.940639, mean_eps: 0.956134
   48856/2000000: episode: 517, duration: 1.016s, episode steps: 68, steps per second: 67, episode reward: -197.579, mean reward: -2.906 [-100.000, 12.222], mean action: 1.456 [0.000, 3.000], mean observation: 0.026 [-1.549, 3.557], loss: 1.029666, mean_absolute_error: 6.713166, mean_q: -0.821096, mean_eps: 0.956062
   48955/2000000: episode: 518, duration: 1.429s, episode steps: 99, steps per second: 69, episode reward: -118.680, mean reward: -1.199 [-100.000, 10.992], mean action: 1.646 [0.000, 3.000], mean observation: -0.005 [-1.297, 1.000], loss: 1.208807, mean_absolute_error: 5.703780, mean_q: 1.231720, mean_eps: 0.955986
   49029/2000000: episode: 519, duration: 1.078s, episode steps: 74, steps per second: 69, episode reward: -142.405, mean reward: -1.924 [-100.000, 8.770], mean action: 1.446 [0.000, 3.000], mean observation: 0.019 [-5.082, 1.000], loss: 0.817567, mean_absolute_error: 5.496839, mean_q: 1.406118, mean_eps: 0.955907
   49100/2000000: episode: 520, duration: 1.017s, episode steps: 71, steps per second: 70, episode reward: -98.972, mean reward: -1.394 [-100.000, 10.084], mean action: 1.662 [0.000, 3.000], mean observation: -0.075 [-4.259, 1.000], loss: 1.282288, mean_absolute_error: 6.050171, mean_q: 0.874154, mean_eps: 0.955842
   49204/2000000: episode: 521, duration: 1.539s, episode steps: 104, steps per second: 68, episode reward: -183.232, mean reward: -1.762 [-100.000, 12.378], mean action: 1.490 [0.000, 3.000], mean observation: 0.164 [-1.471, 1.000], loss: 1.533304, mean_absolute_error: 6.373371, mean_q: 0.026658, mean_eps: 0.955765
   49276/2000000: episode: 522, duration: 1.076s, episode steps: 72, steps per second: 67, episode reward: -110.474, mean reward: -1.534 [-100.000, 13.912], mean action: 1.486 [0.000, 3.000], mean observation: 0.065 [-1.220, 3.802], loss: 1.774088, mean_absolute_error: 6.511869, mean_q: 0.120180, mean_eps: 0.955686
   49335/2000000: episode: 523, duration: 0.851s, episode steps: 59, steps per second: 69, episode reward: -110.556, mean reward: -1.874 [-100.000, 12.364], mean action: 1.441 [0.000, 3.000], mean observation: -0.063 [-1.596, 1.000], loss: 1.470897, mean_absolute_error: 6.153641, mean_q: 0.080569, mean_eps: 0.955626
   49426/2000000: episode: 524, duration: 1.286s, episode steps: 91, steps per second: 71, episode reward: -299.890, mean reward: -3.295 [-100.000, 4.357], mean action: 1.484 [0.000, 3.000], mean observation: -0.004 [-1.504, 3.196], loss: 1.578485, mean_absolute_error: 6.310033, mean_q: 0.751356, mean_eps: 0.955558
   49499/2000000: episode: 525, duration: 1.042s, episode steps: 73, steps per second: 70, episode reward: -122.738, mean reward: -1.681 [-100.000, 21.472], mean action: 1.356 [0.000, 3.000], mean observation: -0.125 [-1.311, 1.000], loss: 1.258471, mean_absolute_error: 5.742248, mean_q: 1.447451, mean_eps: 0.955484
   49629/2000000: episode: 526, duration: 1.985s, episode steps: 130, steps per second: 66, episode reward: -112.804, mean reward: -0.868 [-100.000, 6.894], mean action: 1.523 [0.000, 3.000], mean observation: 0.042 [-0.902, 3.279], loss: 0.920451, mean_absolute_error: 6.054525, mean_q: 0.821683, mean_eps: 0.955392
   49766/2000000: episode: 527, duration: 1.938s, episode steps: 137, steps per second: 71, episode reward: -200.273, mean reward: -1.462 [-100.000, 5.671], mean action: 1.474 [0.000, 3.000], mean observation: -0.012 [-1.373, 1.039], loss: 1.436882, mean_absolute_error: 6.179083, mean_q: 0.194870, mean_eps: 0.955272
   49884/2000000: episode: 528, duration: 1.716s, episode steps: 118, steps per second: 69, episode reward: -357.589, mean reward: -3.030 [-100.000, 1.680], mean action: 1.483 [0.000, 3.000], mean observation: -0.026 [-1.481, 1.028], loss: 1.349567, mean_absolute_error: 6.572708, mean_q: 0.394625, mean_eps: 0.955158
   49975/2000000: episode: 529, duration: 1.302s, episode steps: 91, steps per second: 70, episode reward: -395.549, mean reward: -4.347 [-100.000, 99.162], mean action: 1.681 [0.000, 3.000], mean observation: 0.009 [-1.419, 3.084], loss: 1.640030, mean_absolute_error: 6.015580, mean_q: 0.878495, mean_eps: 0.955065
   50096/2000000: episode: 530, duration: 1.764s, episode steps: 121, steps per second: 69, episode reward: -301.396, mean reward: -2.491 [-100.000, 10.950], mean action: 1.620 [0.000, 3.000], mean observation: -0.066 [-4.734, 1.123], loss: 1.642759, mean_absolute_error: 6.687703, mean_q: 0.938799, mean_eps: 0.954969
   50197/2000000: episode: 531, duration: 1.493s, episode steps: 101, steps per second: 68, episode reward: -385.214, mean reward: -3.814 [-100.000, 5.370], mean action: 1.525 [0.000, 3.000], mean observation: 0.070 [-5.064, 1.537], loss: 1.559608, mean_absolute_error: 7.130185, mean_q: 0.360712, mean_eps: 0.954869
   50294/2000000: episode: 532, duration: 1.403s, episode steps: 97, steps per second: 69, episode reward: -173.698, mean reward: -1.791 [-100.000, 8.280], mean action: 1.557 [0.000, 3.000], mean observation: 0.104 [-1.234, 4.676], loss: 1.097748, mean_absolute_error: 7.223963, mean_q: 0.507075, mean_eps: 0.954779
   50364/2000000: episode: 533, duration: 1.032s, episode steps: 70, steps per second: 68, episode reward: -135.179, mean reward: -1.931 [-100.000, 33.696], mean action: 1.729 [0.000, 3.000], mean observation: -0.058 [-1.334, 2.687], loss: 1.041818, mean_absolute_error: 6.542264, mean_q: 1.931718, mean_eps: 0.954705
   50449/2000000: episode: 534, duration: 1.258s, episode steps: 85, steps per second: 68, episode reward: -200.395, mean reward: -2.358 [-100.000, 18.890], mean action: 1.553 [0.000, 3.000], mean observation: -0.075 [-4.225, 1.000], loss: 1.338611, mean_absolute_error: 7.071617, mean_q: 0.978764, mean_eps: 0.954635
   50550/2000000: episode: 535, duration: 1.429s, episode steps: 101, steps per second: 71, episode reward: -167.577, mean reward: -1.659 [-100.000, 7.869], mean action: 1.644 [0.000, 3.000], mean observation: -0.046 [-3.465, 1.000], loss: 1.544544, mean_absolute_error: 7.117346, mean_q: 0.824328, mean_eps: 0.954550
   50680/2000000: episode: 536, duration: 1.875s, episode steps: 130, steps per second: 69, episode reward: -72.746, mean reward: -0.560 [-100.000, 70.498], mean action: 1.562 [0.000, 3.000], mean observation: 0.053 [-1.552, 1.747], loss: 1.435409, mean_absolute_error: 6.995233, mean_q: 1.100254, mean_eps: 0.954447
   50781/2000000: episode: 537, duration: 1.488s, episode steps: 101, steps per second: 68, episode reward: -132.178, mean reward: -1.309 [-100.000, 14.195], mean action: 1.545 [0.000, 3.000], mean observation: 0.092 [-1.285, 1.000], loss: 1.313875, mean_absolute_error: 6.883660, mean_q: 1.870432, mean_eps: 0.954343
   50860/2000000: episode: 538, duration: 1.141s, episode steps: 79, steps per second: 69, episode reward: -136.380, mean reward: -1.726 [-100.000, 11.877], mean action: 1.468 [0.000, 3.000], mean observation: -0.054 [-1.384, 4.385], loss: 1.373959, mean_absolute_error: 6.779587, mean_q: 0.382295, mean_eps: 0.954262
   50935/2000000: episode: 539, duration: 1.092s, episode steps: 75, steps per second: 69, episode reward: -216.458, mean reward: -2.886 [-100.000, 6.911], mean action: 1.427 [0.000, 3.000], mean observation: 0.011 [-1.380, 4.352], loss: 1.108740, mean_absolute_error: 6.711712, mean_q: 1.188719, mean_eps: 0.954194
   51039/2000000: episode: 540, duration: 1.485s, episode steps: 104, steps per second: 70, episode reward: -183.361, mean reward: -1.763 [-100.000, 10.674], mean action: 1.788 [0.000, 3.000], mean observation: -0.083 [-1.260, 4.174], loss: 1.291424, mean_absolute_error: 7.207602, mean_q: 0.298948, mean_eps: 0.954113
   51129/2000000: episode: 541, duration: 1.324s, episode steps: 90, steps per second: 68, episode reward: -119.247, mean reward: -1.325 [-100.000, 11.396], mean action: 1.467 [0.000, 3.000], mean observation: 0.000 [-1.329, 1.000], loss: 1.373643, mean_absolute_error: 6.903193, mean_q: 1.238153, mean_eps: 0.954024
   51204/2000000: episode: 542, duration: 1.084s, episode steps: 75, steps per second: 69, episode reward: -170.427, mean reward: -2.272 [-100.000, 5.779], mean action: 1.307 [0.000, 3.000], mean observation: 0.033 [-4.323, 1.000], loss: 1.188977, mean_absolute_error: 6.832657, mean_q: 0.394287, mean_eps: 0.953951
   51277/2000000: episode: 543, duration: 1.089s, episode steps: 73, steps per second: 67, episode reward: -152.302, mean reward: -2.086 [-100.000, 6.682], mean action: 1.699 [0.000, 3.000], mean observation: -0.111 [-1.352, 4.462], loss: 1.329971, mean_absolute_error: 7.036473, mean_q: 1.095626, mean_eps: 0.953884
   51394/2000000: episode: 544, duration: 1.641s, episode steps: 117, steps per second: 71, episode reward: -179.936, mean reward: -1.538 [-100.000, 9.426], mean action: 1.470 [0.000, 3.000], mean observation: 0.042 [-4.099, 1.000], loss: 0.918862, mean_absolute_error: 6.804770, mean_q: 1.246985, mean_eps: 0.953798
   51490/2000000: episode: 545, duration: 1.372s, episode steps: 96, steps per second: 70, episode reward: -368.226, mean reward: -3.836 [-100.000, 5.691], mean action: 1.562 [0.000, 3.000], mean observation: 0.075 [-5.740, 1.661], loss: 0.887715, mean_absolute_error: 7.211966, mean_q: 0.660315, mean_eps: 0.953702
   51596/2000000: episode: 546, duration: 1.567s, episode steps: 106, steps per second: 68, episode reward: -174.175, mean reward: -1.643 [-100.000, 15.414], mean action: 1.500 [0.000, 3.000], mean observation: 0.142 [-1.243, 1.000], loss: 1.345642, mean_absolute_error: 7.048703, mean_q: 1.323842, mean_eps: 0.953612
   51666/2000000: episode: 547, duration: 1.090s, episode steps: 70, steps per second: 64, episode reward: -156.054, mean reward: -2.229 [-100.000, 8.959], mean action: 1.314 [0.000, 3.000], mean observation: 0.049 [-1.458, 1.000], loss: 0.961333, mean_absolute_error: 7.119725, mean_q: 0.791470, mean_eps: 0.953533
   51753/2000000: episode: 548, duration: 1.263s, episode steps: 87, steps per second: 69, episode reward: -149.209, mean reward: -1.715 [-100.000, 10.563], mean action: 1.460 [0.000, 3.000], mean observation: 0.077 [-4.447, 1.000], loss: 1.240905, mean_absolute_error: 6.156433, mean_q: 1.631449, mean_eps: 0.953461
   51826/2000000: episode: 549, duration: 1.026s, episode steps: 73, steps per second: 71, episode reward: -253.285, mean reward: -3.470 [-100.000, 8.797], mean action: 1.562 [0.000, 3.000], mean observation: -0.037 [-1.680, 4.061], loss: 1.601728, mean_absolute_error: 6.911628, mean_q: 1.401389, mean_eps: 0.953389
   51934/2000000: episode: 550, duration: 1.517s, episode steps: 108, steps per second: 71, episode reward: -275.704, mean reward: -2.553 [-100.000, 5.771], mean action: 1.685 [0.000, 3.000], mean observation: 0.002 [-1.579, 1.000], loss: 1.287704, mean_absolute_error: 7.640112, mean_q: -0.248867, mean_eps: 0.953308
   52075/2000000: episode: 551, duration: 2.001s, episode steps: 141, steps per second: 70, episode reward: -521.685, mean reward: -3.700 [-100.000, 67.505], mean action: 1.468 [0.000, 3.000], mean observation: 0.087 [-1.363, 3.359], loss: 1.444861, mean_absolute_error: 7.316814, mean_q: 0.337755, mean_eps: 0.953196
   52149/2000000: episode: 552, duration: 1.094s, episode steps: 74, steps per second: 68, episode reward: -80.020, mean reward: -1.081 [-100.000, 13.193], mean action: 1.554 [0.000, 3.000], mean observation: -0.062 [-1.232, 1.000], loss: 1.226396, mean_absolute_error: 7.394609, mean_q: 0.615760, mean_eps: 0.953099
   52238/2000000: episode: 553, duration: 1.268s, episode steps: 89, steps per second: 70, episode reward: -261.612, mean reward: -2.939 [-100.000, 7.878], mean action: 1.236 [0.000, 3.000], mean observation: 0.096 [-1.683, 1.211], loss: 1.062521, mean_absolute_error: 7.633412, mean_q: 0.188569, mean_eps: 0.953025
   52337/2000000: episode: 554, duration: 1.449s, episode steps: 99, steps per second: 68, episode reward: -217.208, mean reward: -2.194 [-100.000, 4.802], mean action: 1.535 [0.000, 3.000], mean observation: -0.056 [-3.990, 1.000], loss: 1.438339, mean_absolute_error: 7.146544, mean_q: 0.291943, mean_eps: 0.952941
   52447/2000000: episode: 555, duration: 1.534s, episode steps: 110, steps per second: 72, episode reward: -281.662, mean reward: -2.561 [-100.000, 6.964], mean action: 1.618 [0.000, 3.000], mean observation: 0.002 [-1.562, 4.732], loss: 1.704946, mean_absolute_error: 7.242771, mean_q: 0.878937, mean_eps: 0.952847
   52536/2000000: episode: 556, duration: 1.312s, episode steps: 89, steps per second: 68, episode reward: -157.185, mean reward: -1.766 [-100.000, 6.265], mean action: 1.315 [0.000, 3.000], mean observation: 0.080 [-1.408, 1.000], loss: 1.199712, mean_absolute_error: 7.132378, mean_q: 0.255885, mean_eps: 0.952759
   52643/2000000: episode: 557, duration: 1.534s, episode steps: 107, steps per second: 70, episode reward: -331.222, mean reward: -3.096 [-100.000, 8.055], mean action: 1.514 [0.000, 3.000], mean observation: -0.018 [-4.474, 1.000], loss: 1.473391, mean_absolute_error: 7.528049, mean_q: 0.761012, mean_eps: 0.952671
   52739/2000000: episode: 558, duration: 1.370s, episode steps: 96, steps per second: 70, episode reward: -340.057, mean reward: -3.542 [-100.000, 3.644], mean action: 1.542 [0.000, 3.000], mean observation: -0.043 [-1.538, 2.424], loss: 1.034925, mean_absolute_error: 6.845801, mean_q: 1.338802, mean_eps: 0.952579
   52892/2000000: episode: 559, duration: 2.356s, episode steps: 153, steps per second: 65, episode reward: -136.126, mean reward: -0.890 [-100.000, 44.357], mean action: 1.647 [0.000, 3.000], mean observation: -0.088 [-1.463, 1.878], loss: 1.170303, mean_absolute_error: 6.930568, mean_q: 0.997604, mean_eps: 0.952467
   53018/2000000: episode: 560, duration: 1.850s, episode steps: 126, steps per second: 68, episode reward: -457.021, mean reward: -3.627 [-100.000, 5.662], mean action: 1.500 [0.000, 3.000], mean observation: 0.063 [-4.211, 2.026], loss: 1.687863, mean_absolute_error: 6.987681, mean_q: 1.047962, mean_eps: 0.952341
   53134/2000000: episode: 561, duration: 1.806s, episode steps: 116, steps per second: 64, episode reward: -239.383, mean reward: -2.064 [-100.000, 2.036], mean action: 1.388 [0.000, 3.000], mean observation: 0.178 [-1.606, 1.068], loss: 1.252816, mean_absolute_error: 7.630124, mean_q: 0.065722, mean_eps: 0.952232
   53240/2000000: episode: 562, duration: 1.564s, episode steps: 106, steps per second: 68, episode reward: -214.867, mean reward: -2.027 [-100.000, 5.347], mean action: 1.575 [0.000, 3.000], mean observation: -0.112 [-1.365, 4.448], loss: 1.412171, mean_absolute_error: 7.503533, mean_q: 0.291802, mean_eps: 0.952133
   53325/2000000: episode: 563, duration: 1.257s, episode steps: 85, steps per second: 68, episode reward: -426.063, mean reward: -5.013 [-100.000, 0.173], mean action: 1.588 [0.000, 3.000], mean observation: -0.024 [-2.067, 5.514], loss: 1.612719, mean_absolute_error: 7.623801, mean_q: -0.473321, mean_eps: 0.952046
   53388/2000000: episode: 564, duration: 0.929s, episode steps: 63, steps per second: 68, episode reward: -235.063, mean reward: -3.731 [-100.000, 5.494], mean action: 1.444 [0.000, 3.000], mean observation: -0.098 [-1.529, 1.000], loss: 1.242435, mean_absolute_error: 7.219401, mean_q: 1.546114, mean_eps: 0.951980
   53466/2000000: episode: 565, duration: 1.145s, episode steps: 78, steps per second: 68, episode reward: -154.408, mean reward: -1.980 [-100.000, 5.747], mean action: 1.577 [0.000, 3.000], mean observation: 0.046 [-1.315, 4.463], loss: 0.744606, mean_absolute_error: 6.792000, mean_q: 1.232642, mean_eps: 0.951917
   53575/2000000: episode: 566, duration: 1.555s, episode steps: 109, steps per second: 70, episode reward: -478.734, mean reward: -4.392 [-100.000, 43.136], mean action: 1.514 [0.000, 3.000], mean observation: 0.069 [-1.372, 3.698], loss: 1.632188, mean_absolute_error: 7.439711, mean_q: 0.394961, mean_eps: 0.951832
   53703/2000000: episode: 567, duration: 1.838s, episode steps: 128, steps per second: 70, episode reward: -184.509, mean reward: -1.441 [-100.000, 48.755], mean action: 1.586 [0.000, 3.000], mean observation: 0.144 [-1.536, 3.485], loss: 1.171909, mean_absolute_error: 7.120529, mean_q: 0.331430, mean_eps: 0.951726
   53819/2000000: episode: 568, duration: 1.647s, episode steps: 116, steps per second: 70, episode reward: -157.003, mean reward: -1.353 [-100.000, 22.954], mean action: 1.466 [0.000, 3.000], mean observation: -0.019 [-1.361, 4.013], loss: 1.323270, mean_absolute_error: 7.146710, mean_q: 0.360617, mean_eps: 0.951616
   53907/2000000: episode: 569, duration: 1.257s, episode steps: 88, steps per second: 70, episode reward: -270.894, mean reward: -3.078 [-100.000, 59.176], mean action: 1.489 [0.000, 3.000], mean observation: 0.015 [-3.588, 1.263], loss: 1.323743, mean_absolute_error: 7.200463, mean_q: -0.048426, mean_eps: 0.951524
   54022/2000000: episode: 570, duration: 1.730s, episode steps: 115, steps per second: 66, episode reward: -93.974, mean reward: -0.817 [-100.000, 73.630], mean action: 1.652 [0.000, 3.000], mean observation: -0.043 [-1.156, 1.608], loss: 1.197789, mean_absolute_error: 7.249834, mean_q: 0.087888, mean_eps: 0.951432
   54150/2000000: episode: 571, duration: 1.864s, episode steps: 128, steps per second: 69, episode reward: -163.243, mean reward: -1.275 [-100.000, 10.591], mean action: 1.680 [0.000, 3.000], mean observation: -0.069 [-1.250, 3.863], loss: 1.181962, mean_absolute_error: 7.409463, mean_q: 0.456427, mean_eps: 0.951323
   54224/2000000: episode: 572, duration: 1.083s, episode steps: 74, steps per second: 68, episode reward: -135.989, mean reward: -1.838 [-100.000, 10.877], mean action: 1.419 [0.000, 3.000], mean observation: 0.060 [-4.152, 1.000], loss: 0.945056, mean_absolute_error: 6.981837, mean_q: 0.328776, mean_eps: 0.951233
   54298/2000000: episode: 573, duration: 1.068s, episode steps: 74, steps per second: 69, episode reward: -119.463, mean reward: -1.614 [-100.000, 6.945], mean action: 1.568 [0.000, 3.000], mean observation: -0.092 [-4.303, 1.000], loss: 1.229258, mean_absolute_error: 6.706056, mean_q: 1.154874, mean_eps: 0.951166
   54432/2000000: episode: 574, duration: 1.943s, episode steps: 134, steps per second: 69, episode reward: -251.032, mean reward: -1.873 [-100.000, 7.683], mean action: 1.433 [0.000, 3.000], mean observation: 0.010 [-1.610, 3.109], loss: 1.104348, mean_absolute_error: 6.995334, mean_q: 0.492708, mean_eps: 0.951072
   54517/2000000: episode: 575, duration: 1.251s, episode steps: 85, steps per second: 68, episode reward: -389.253, mean reward: -4.579 [-100.000, 0.859], mean action: 1.624 [0.000, 3.000], mean observation: -0.005 [-7.029, 1.589], loss: 1.210482, mean_absolute_error: 7.315805, mean_q: 0.565464, mean_eps: 0.950973
   54605/2000000: episode: 576, duration: 1.246s, episode steps: 88, steps per second: 71, episode reward: -195.697, mean reward: -2.224 [-100.000, 9.145], mean action: 1.489 [0.000, 3.000], mean observation: 0.066 [-1.352, 4.330], loss: 1.129804, mean_absolute_error: 7.234836, mean_q: 0.934753, mean_eps: 0.950894
   54670/2000000: episode: 577, duration: 0.914s, episode steps: 65, steps per second: 71, episode reward: -116.915, mean reward: -1.799 [-100.000, 11.117], mean action: 1.431 [0.000, 3.000], mean observation: -0.047 [-1.409, 5.109], loss: 1.170066, mean_absolute_error: 6.849604, mean_q: 0.509800, mean_eps: 0.950826
   54802/2000000: episode: 578, duration: 1.883s, episode steps: 132, steps per second: 70, episode reward: -387.135, mean reward: -2.933 [-100.000, 83.981], mean action: 1.606 [0.000, 3.000], mean observation: -0.116 [-3.165, 1.001], loss: 1.373329, mean_absolute_error: 7.639984, mean_q: -0.087047, mean_eps: 0.950738
   54879/2000000: episode: 579, duration: 1.102s, episode steps: 77, steps per second: 70, episode reward: -155.405, mean reward: -2.018 [-100.000, 4.147], mean action: 1.377 [0.000, 3.000], mean observation: -0.097 [-4.101, 1.000], loss: 1.285976, mean_absolute_error: 7.599210, mean_q: -0.078015, mean_eps: 0.950644
   54963/2000000: episode: 580, duration: 1.205s, episode steps: 84, steps per second: 70, episode reward: -225.407, mean reward: -2.683 [-100.000, 14.141], mean action: 1.536 [0.000, 3.000], mean observation: -0.035 [-1.403, 3.448], loss: 0.676200, mean_absolute_error: 6.700839, mean_q: 0.660471, mean_eps: 0.950572
   55081/2000000: episode: 581, duration: 1.716s, episode steps: 118, steps per second: 69, episode reward: -155.983, mean reward: -1.322 [-100.000, 12.366], mean action: 1.424 [0.000, 3.000], mean observation: 0.109 [-1.277, 1.000], loss: 1.252078, mean_absolute_error: 6.932849, mean_q: 0.673080, mean_eps: 0.950480
   55167/2000000: episode: 582, duration: 1.196s, episode steps: 86, steps per second: 72, episode reward: -160.622, mean reward: -1.868 [-100.000, 8.437], mean action: 1.488 [0.000, 3.000], mean observation: 0.058 [-4.915, 1.000], loss: 0.827622, mean_absolute_error: 7.141204, mean_q: 1.183756, mean_eps: 0.950388
   55271/2000000: episode: 583, duration: 1.486s, episode steps: 104, steps per second: 70, episode reward: -373.449, mean reward: -3.591 [-100.000, 2.980], mean action: 1.577 [0.000, 3.000], mean observation: -0.033 [-1.641, 1.000], loss: 1.615816, mean_absolute_error: 7.419843, mean_q: 0.952736, mean_eps: 0.950304
   55386/2000000: episode: 584, duration: 1.767s, episode steps: 115, steps per second: 65, episode reward: -290.197, mean reward: -2.523 [-100.000, 1.591], mean action: 1.643 [0.000, 3.000], mean observation: 0.191 [-1.236, 1.296], loss: 1.505613, mean_absolute_error: 7.497599, mean_q: -0.606808, mean_eps: 0.950205
   55484/2000000: episode: 585, duration: 1.500s, episode steps: 98, steps per second: 65, episode reward: -130.052, mean reward: -1.327 [-100.000, 18.009], mean action: 1.571 [0.000, 3.000], mean observation: -0.074 [-1.252, 1.000], loss: 1.254764, mean_absolute_error: 6.924885, mean_q: 0.940088, mean_eps: 0.950109
   55551/2000000: episode: 586, duration: 0.981s, episode steps: 67, steps per second: 68, episode reward: -309.899, mean reward: -4.625 [-100.000, 5.355], mean action: 1.507 [0.000, 3.000], mean observation: -0.022 [-1.320, 1.495], loss: 1.617607, mean_absolute_error: 7.579517, mean_q: 0.031915, mean_eps: 0.950036
   55631/2000000: episode: 587, duration: 1.165s, episode steps: 80, steps per second: 69, episode reward: -205.653, mean reward: -2.571 [-100.000, 9.001], mean action: 1.375 [0.000, 3.000], mean observation: -0.046 [-1.403, 3.595], loss: 1.185450, mean_absolute_error: 7.377377, mean_q: -0.410086, mean_eps: 0.949969
   55725/2000000: episode: 588, duration: 1.385s, episode steps: 94, steps per second: 68, episode reward: -375.407, mean reward: -3.994 [-100.000, 15.980], mean action: 1.638 [0.000, 3.000], mean observation: -0.100 [-4.358, 1.022], loss: 1.333256, mean_absolute_error: 7.114855, mean_q: 0.379280, mean_eps: 0.949890
   55800/2000000: episode: 589, duration: 1.110s, episode steps: 75, steps per second: 68, episode reward: -115.124, mean reward: -1.535 [-100.000, 16.170], mean action: 1.307 [0.000, 3.000], mean observation: 0.004 [-1.410, 1.000], loss: 0.892563, mean_absolute_error: 7.162107, mean_q: -0.212068, mean_eps: 0.949814
   55894/2000000: episode: 590, duration: 1.372s, episode steps: 94, steps per second: 69, episode reward: -155.768, mean reward: -1.657 [-100.000, 6.793], mean action: 1.660 [0.000, 3.000], mean observation: 0.014 [-1.316, 4.559], loss: 1.080640, mean_absolute_error: 7.020385, mean_q: 1.134404, mean_eps: 0.949739
   55992/2000000: episode: 591, duration: 1.426s, episode steps: 98, steps per second: 69, episode reward: -236.298, mean reward: -2.411 [-100.000, 30.369], mean action: 1.582 [0.000, 3.000], mean observation: -0.039 [-5.487, 1.000], loss: 1.157977, mean_absolute_error: 7.376281, mean_q: 0.505246, mean_eps: 0.949652
   56142/2000000: episode: 592, duration: 2.160s, episode steps: 150, steps per second: 69, episode reward: -260.696, mean reward: -1.738 [-100.000, 21.222], mean action: 1.560 [0.000, 3.000], mean observation: -0.081 [-4.852, 1.100], loss: 1.212013, mean_absolute_error: 7.285969, mean_q: 0.835299, mean_eps: 0.949541
   56239/2000000: episode: 593, duration: 1.384s, episode steps: 97, steps per second: 70, episode reward: -188.556, mean reward: -1.944 [-100.000, 27.101], mean action: 1.567 [0.000, 3.000], mean observation: -0.006 [-1.274, 3.083], loss: 0.987617, mean_absolute_error: 7.092207, mean_q: 0.954113, mean_eps: 0.949429
   56342/2000000: episode: 594, duration: 1.513s, episode steps: 103, steps per second: 68, episode reward: -235.358, mean reward: -2.285 [-100.000, 7.394], mean action: 1.631 [0.000, 3.000], mean observation: -0.157 [-1.390, 1.000], loss: 1.410007, mean_absolute_error: 6.996480, mean_q: 1.250091, mean_eps: 0.949339
   56414/2000000: episode: 595, duration: 1.049s, episode steps: 72, steps per second: 69, episode reward: -359.334, mean reward: -4.991 [-100.000, 4.953], mean action: 1.681 [0.000, 3.000], mean observation: -0.060 [-1.703, 5.601], loss: 1.103411, mean_absolute_error: 7.268744, mean_q: 0.503539, mean_eps: 0.949260
   56512/2000000: episode: 596, duration: 1.446s, episode steps: 98, steps per second: 68, episode reward: -334.591, mean reward: -3.414 [-100.000, 7.427], mean action: 1.388 [0.000, 3.000], mean observation: -0.022 [-1.441, 4.191], loss: 1.411410, mean_absolute_error: 7.629320, mean_q: 0.031238, mean_eps: 0.949184
   56587/2000000: episode: 597, duration: 1.114s, episode steps: 75, steps per second: 67, episode reward: -128.024, mean reward: -1.707 [-100.000, 9.856], mean action: 1.480 [0.000, 3.000], mean observation: -0.019 [-4.009, 1.000], loss: 1.111798, mean_absolute_error: 7.638481, mean_q: 0.253929, mean_eps: 0.949107
   56688/2000000: episode: 598, duration: 1.493s, episode steps: 101, steps per second: 68, episode reward: -349.316, mean reward: -3.459 [-100.000, 6.330], mean action: 1.475 [0.000, 3.000], mean observation: 0.030 [-6.043, 1.718], loss: 1.176833, mean_absolute_error: 7.575155, mean_q: -0.631451, mean_eps: 0.949028
   56795/2000000: episode: 599, duration: 1.551s, episode steps: 107, steps per second: 69, episode reward: -260.482, mean reward: -2.434 [-100.000, 1.170], mean action: 1.430 [0.000, 3.000], mean observation: 0.170 [-1.151, 1.391], loss: 0.993291, mean_absolute_error: 6.677051, mean_q: 1.081148, mean_eps: 0.948934
   56878/2000000: episode: 600, duration: 1.227s, episode steps: 83, steps per second: 68, episode reward: -176.560, mean reward: -2.127 [-100.000, 8.452], mean action: 1.494 [0.000, 3.000], mean observation: 0.074 [-1.455, 4.645], loss: 1.092450, mean_absolute_error: 7.054055, mean_q: 0.236490, mean_eps: 0.948848
   56997/2000000: episode: 601, duration: 1.724s, episode steps: 119, steps per second: 69, episode reward: -178.718, mean reward: -1.502 [-100.000, 6.230], mean action: 1.563 [0.000, 3.000], mean observation: 0.147 [-4.704, 1.041], loss: 0.989722, mean_absolute_error: 6.895108, mean_q: 0.875400, mean_eps: 0.948756
   57100/2000000: episode: 602, duration: 1.496s, episode steps: 103, steps per second: 69, episode reward: -185.996, mean reward: -1.806 [-100.000, 35.834], mean action: 1.515 [0.000, 3.000], mean observation: -0.098 [-1.424, 2.285], loss: 1.198233, mean_absolute_error: 6.998580, mean_q: 0.957670, mean_eps: 0.948657
   57193/2000000: episode: 603, duration: 1.400s, episode steps: 93, steps per second: 66, episode reward: -343.424, mean reward: -3.693 [-100.000, 5.654], mean action: 1.366 [0.000, 3.000], mean observation: -0.082 [-1.666, 1.080], loss: 1.608025, mean_absolute_error: 7.939665, mean_q: -0.489036, mean_eps: 0.948569
   57276/2000000: episode: 604, duration: 1.198s, episode steps: 83, steps per second: 69, episode reward: -364.114, mean reward: -4.387 [-100.000, 4.605], mean action: 1.578 [0.000, 3.000], mean observation: 0.027 [-6.654, 1.762], loss: 1.210515, mean_absolute_error: 7.753453, mean_q: 0.095592, mean_eps: 0.948489
   57376/2000000: episode: 605, duration: 1.468s, episode steps: 100, steps per second: 68, episode reward: -194.198, mean reward: -1.942 [-100.000, 15.675], mean action: 1.640 [0.000, 3.000], mean observation: -0.070 [-1.289, 1.000], loss: 1.121473, mean_absolute_error: 6.863143, mean_q: 0.989577, mean_eps: 0.948408
   57445/2000000: episode: 606, duration: 1.047s, episode steps: 69, steps per second: 66, episode reward: -83.808, mean reward: -1.215 [-100.000, 16.724], mean action: 1.420 [0.000, 3.000], mean observation: -0.020 [-1.284, 1.000], loss: 1.091451, mean_absolute_error: 7.070677, mean_q: 0.895529, mean_eps: 0.948331
   57548/2000000: episode: 607, duration: 1.487s, episode steps: 103, steps per second: 69, episode reward: -327.539, mean reward: -3.180 [-100.000, 0.823], mean action: 1.563 [0.000, 3.000], mean observation: -0.046 [-1.472, 1.039], loss: 1.550344, mean_absolute_error: 8.137539, mean_q: -1.084833, mean_eps: 0.948254
   57662/2000000: episode: 608, duration: 1.648s, episode steps: 114, steps per second: 69, episode reward: -195.657, mean reward: -1.716 [-100.000, 11.328], mean action: 1.474 [0.000, 3.000], mean observation: -0.113 [-1.358, 1.000], loss: 1.583577, mean_absolute_error: 7.737573, mean_q: -0.214933, mean_eps: 0.948156
   57814/2000000: episode: 609, duration: 2.205s, episode steps: 152, steps per second: 69, episode reward: -39.517, mean reward: -0.260 [-100.000, 56.759], mean action: 1.559 [0.000, 3.000], mean observation: 0.086 [-1.132, 1.109], loss: 1.499411, mean_absolute_error: 7.633481, mean_q: -0.224746, mean_eps: 0.948036
   57926/2000000: episode: 610, duration: 1.614s, episode steps: 112, steps per second: 69, episode reward: -176.414, mean reward: -1.575 [-100.000, 6.774], mean action: 1.536 [0.000, 3.000], mean observation: 0.041 [-4.195, 1.000], loss: 0.899389, mean_absolute_error: 6.921775, mean_q: 0.773458, mean_eps: 0.947917
   58005/2000000: episode: 611, duration: 1.145s, episode steps: 79, steps per second: 69, episode reward: -198.424, mean reward: -2.512 [-100.000, 19.017], mean action: 1.215 [0.000, 3.000], mean observation: -0.002 [-2.784, 1.000], loss: 1.156564, mean_absolute_error: 6.569613, mean_q: 0.897603, mean_eps: 0.947831
   58078/2000000: episode: 612, duration: 1.033s, episode steps: 73, steps per second: 71, episode reward: -189.324, mean reward: -2.593 [-100.000, 7.498], mean action: 1.507 [0.000, 3.000], mean observation: 0.073 [-1.458, 4.881], loss: 1.130692, mean_absolute_error: 7.034368, mean_q: 0.848108, mean_eps: 0.947762
   58145/2000000: episode: 613, duration: 0.976s, episode steps: 67, steps per second: 69, episode reward: -126.503, mean reward: -1.888 [-100.000, 7.591], mean action: 1.537 [0.000, 3.000], mean observation: 0.010 [-1.465, 4.332], loss: 1.398823, mean_absolute_error: 7.763905, mean_q: 0.169741, mean_eps: 0.947699
   58254/2000000: episode: 614, duration: 1.538s, episode steps: 109, steps per second: 71, episode reward: -208.614, mean reward: -1.914 [-100.000, 9.236], mean action: 1.495 [0.000, 3.000], mean observation: 0.137 [-1.388, 4.533], loss: 1.071068, mean_absolute_error: 7.142475, mean_q: 0.555354, mean_eps: 0.947620
   58347/2000000: episode: 615, duration: 1.339s, episode steps: 93, steps per second: 69, episode reward: -188.517, mean reward: -2.027 [-100.000, 5.180], mean action: 1.495 [0.000, 3.000], mean observation: -0.131 [-1.448, 4.797], loss: 1.194976, mean_absolute_error: 7.079311, mean_q: 0.645490, mean_eps: 0.947530
   58412/2000000: episode: 616, duration: 0.961s, episode steps: 65, steps per second: 68, episode reward: -106.844, mean reward: -1.644 [-100.000, 15.462], mean action: 1.631 [0.000, 3.000], mean observation: -0.045 [-4.489, 1.000], loss: 1.331745, mean_absolute_error: 7.571837, mean_q: 0.238606, mean_eps: 0.947460
   58486/2000000: episode: 617, duration: 1.088s, episode steps: 74, steps per second: 68, episode reward: -134.439, mean reward: -1.817 [-100.000, 16.835], mean action: 1.554 [0.000, 3.000], mean observation: -0.119 [-1.405, 1.000], loss: 1.191368, mean_absolute_error: 6.883727, mean_q: 0.571412, mean_eps: 0.947397
   58589/2000000: episode: 618, duration: 1.459s, episode steps: 103, steps per second: 71, episode reward: -140.651, mean reward: -1.366 [-100.000, 12.921], mean action: 1.573 [0.000, 3.000], mean observation: -0.069 [-1.284, 1.000], loss: 1.164040, mean_absolute_error: 7.243916, mean_q: 0.372413, mean_eps: 0.947316
   58683/2000000: episode: 619, duration: 1.320s, episode steps: 94, steps per second: 71, episode reward: -101.807, mean reward: -1.083 [-100.000, 80.974], mean action: 1.351 [0.000, 3.000], mean observation: 0.028 [-1.400, 1.937], loss: 1.323676, mean_absolute_error: 7.410067, mean_q: -0.054243, mean_eps: 0.947228
   58772/2000000: episode: 620, duration: 1.304s, episode steps: 89, steps per second: 68, episode reward: -346.559, mean reward: -3.894 [-100.000, 45.251], mean action: 1.438 [0.000, 3.000], mean observation: 0.022 [-2.335, 1.451], loss: 1.255619, mean_absolute_error: 7.363998, mean_q: 0.999733, mean_eps: 0.947147
   58858/2000000: episode: 621, duration: 1.239s, episode steps: 86, steps per second: 69, episode reward: -140.143, mean reward: -1.630 [-100.000, 6.537], mean action: 1.547 [0.000, 3.000], mean observation: -0.098 [-1.296, 3.701], loss: 1.228014, mean_absolute_error: 7.431865, mean_q: 0.362703, mean_eps: 0.947067
   58927/2000000: episode: 622, duration: 0.977s, episode steps: 69, steps per second: 71, episode reward: -141.900, mean reward: -2.057 [-100.000, 8.709], mean action: 1.797 [0.000, 3.000], mean observation: -0.055 [-5.592, 1.000], loss: 0.982644, mean_absolute_error: 7.447529, mean_q: 0.247000, mean_eps: 0.946997
   59009/2000000: episode: 623, duration: 1.194s, episode steps: 82, steps per second: 69, episode reward: -144.584, mean reward: -1.763 [-100.000, 12.684], mean action: 1.756 [0.000, 3.000], mean observation: -0.048 [-4.485, 1.000], loss: 0.931684, mean_absolute_error: 6.728092, mean_q: 1.410298, mean_eps: 0.946929
   59086/2000000: episode: 624, duration: 1.092s, episode steps: 77, steps per second: 70, episode reward: -144.064, mean reward: -1.871 [-100.000, 16.154], mean action: 1.390 [0.000, 3.000], mean observation: 0.001 [-1.525, 1.000], loss: 0.964084, mean_absolute_error: 6.584172, mean_q: 0.986230, mean_eps: 0.946857
   59164/2000000: episode: 625, duration: 1.147s, episode steps: 78, steps per second: 68, episode reward: -307.920, mean reward: -3.948 [-100.000, 50.950], mean action: 1.474 [0.000, 3.000], mean observation: 0.081 [-1.237, 3.069], loss: 1.564476, mean_absolute_error: 7.237851, mean_q: -0.276892, mean_eps: 0.946788
   59256/2000000: episode: 626, duration: 1.350s, episode steps: 92, steps per second: 68, episode reward: -417.229, mean reward: -4.535 [-100.000, 4.641], mean action: 1.630 [0.000, 3.000], mean observation: 0.014 [-4.770, 1.740], loss: 0.808312, mean_absolute_error: 6.970388, mean_q: 1.098586, mean_eps: 0.946713
   59364/2000000: episode: 627, duration: 1.581s, episode steps: 108, steps per second: 68, episode reward: -324.476, mean reward: -3.004 [-100.000, 1.147], mean action: 1.481 [0.000, 3.000], mean observation: 0.131 [-1.362, 1.596], loss: 0.800096, mean_absolute_error: 6.948696, mean_q: 1.201010, mean_eps: 0.946623
   59469/2000000: episode: 628, duration: 1.533s, episode steps: 105, steps per second: 68, episode reward: -195.353, mean reward: -1.861 [-100.000, 10.042], mean action: 1.476 [0.000, 3.000], mean observation: 0.011 [-1.469, 5.027], loss: 1.256835, mean_absolute_error: 6.673686, mean_q: 1.454606, mean_eps: 0.946526
   59593/2000000: episode: 629, duration: 1.782s, episode steps: 124, steps per second: 70, episode reward: -58.187, mean reward: -0.469 [-100.000, 82.321], mean action: 1.427 [0.000, 3.000], mean observation: 0.088 [-1.601, 1.986], loss: 1.067499, mean_absolute_error: 7.175712, mean_q: 0.018174, mean_eps: 0.946421
   59681/2000000: episode: 630, duration: 1.257s, episode steps: 88, steps per second: 70, episode reward: -168.902, mean reward: -1.919 [-100.000, 9.487], mean action: 1.625 [0.000, 3.000], mean observation: -0.109 [-1.125, 2.802], loss: 0.825241, mean_absolute_error: 7.011134, mean_q: 0.538879, mean_eps: 0.946326
   59766/2000000: episode: 631, duration: 1.203s, episode steps: 85, steps per second: 71, episode reward: -152.309, mean reward: -1.792 [-100.000, 23.944], mean action: 1.659 [0.000, 3.000], mean observation: 0.056 [-1.397, 5.089], loss: 1.146911, mean_absolute_error: 6.889717, mean_q: 0.948953, mean_eps: 0.946248
   59865/2000000: episode: 632, duration: 1.574s, episode steps: 99, steps per second: 63, episode reward: -491.591, mean reward: -4.966 [-100.000, 0.090], mean action: 1.545 [0.000, 3.000], mean observation: 0.018 [-1.919, 1.761], loss: 1.008696, mean_absolute_error: 6.985486, mean_q: 0.852491, mean_eps: 0.946166
   59963/2000000: episode: 633, duration: 1.510s, episode steps: 98, steps per second: 65, episode reward: -152.522, mean reward: -1.556 [-100.000, 9.126], mean action: 1.735 [0.000, 3.000], mean observation: 0.072 [-1.367, 1.114], loss: 0.883295, mean_absolute_error: 6.714927, mean_q: 0.962958, mean_eps: 0.946077
   60055/2000000: episode: 634, duration: 1.332s, episode steps: 92, steps per second: 69, episode reward: -174.911, mean reward: -1.901 [-100.000, 5.420], mean action: 1.435 [0.000, 3.000], mean observation: 0.100 [-4.572, 1.129], loss: 1.617884, mean_absolute_error: 7.827753, mean_q: 0.122997, mean_eps: 0.945993
   60171/2000000: episode: 635, duration: 1.633s, episode steps: 116, steps per second: 71, episode reward: -186.849, mean reward: -1.611 [-100.000, 13.648], mean action: 1.569 [0.000, 3.000], mean observation: -0.043 [-1.533, 1.024], loss: 1.330387, mean_absolute_error: 8.503460, mean_q: 0.247833, mean_eps: 0.945899
   60260/2000000: episode: 636, duration: 1.315s, episode steps: 89, steps per second: 68, episode reward: -157.267, mean reward: -1.767 [-100.000, 8.184], mean action: 1.506 [0.000, 3.000], mean observation: -0.046 [-1.443, 4.609], loss: 1.645280, mean_absolute_error: 8.728469, mean_q: 0.313585, mean_eps: 0.945807
   60376/2000000: episode: 637, duration: 1.696s, episode steps: 116, steps per second: 68, episode reward: -228.176, mean reward: -1.967 [-100.000, 30.930], mean action: 1.500 [0.000, 3.000], mean observation: -0.069 [-1.157, 3.684], loss: 1.332157, mean_absolute_error: 8.038610, mean_q: 0.687832, mean_eps: 0.945716
   60478/2000000: episode: 638, duration: 1.463s, episode steps: 102, steps per second: 70, episode reward: -156.795, mean reward: -1.537 [-100.000, 6.414], mean action: 1.480 [0.000, 3.000], mean observation: 0.037 [-3.856, 1.000], loss: 1.471499, mean_absolute_error: 8.077985, mean_q: 0.855990, mean_eps: 0.945617
   60560/2000000: episode: 639, duration: 1.204s, episode steps: 82, steps per second: 68, episode reward: -220.423, mean reward: -2.688 [-100.000, 7.400], mean action: 1.427 [0.000, 3.000], mean observation: -0.015 [-4.878, 1.000], loss: 1.672691, mean_absolute_error: 9.244021, mean_q: -1.042650, mean_eps: 0.945534
   60657/2000000: episode: 640, duration: 1.417s, episode steps: 97, steps per second: 68, episode reward: -78.853, mean reward: -0.813 [-100.000, 45.952], mean action: 1.577 [0.000, 3.000], mean observation: 0.078 [-1.245, 3.844], loss: 1.139031, mean_absolute_error: 7.598089, mean_q: 1.862926, mean_eps: 0.945453
   60750/2000000: episode: 641, duration: 1.308s, episode steps: 93, steps per second: 71, episode reward: -140.949, mean reward: -1.516 [-100.000, 16.623], mean action: 1.634 [0.000, 3.000], mean observation: -0.092 [-1.167, 1.000], loss: 1.024577, mean_absolute_error: 7.341884, mean_q: 1.589670, mean_eps: 0.945366
   60850/2000000: episode: 642, duration: 1.414s, episode steps: 100, steps per second: 71, episode reward: -172.679, mean reward: -1.727 [-100.000, 5.900], mean action: 1.510 [0.000, 3.000], mean observation: -0.131 [-5.159, 1.000], loss: 1.310648, mean_absolute_error: 8.619306, mean_q: -0.253769, mean_eps: 0.945280
   60956/2000000: episode: 643, duration: 1.536s, episode steps: 106, steps per second: 69, episode reward: -148.145, mean reward: -1.398 [-100.000, 30.030], mean action: 1.585 [0.000, 3.000], mean observation: -0.138 [-1.202, 1.762], loss: 1.521342, mean_absolute_error: 8.603897, mean_q: 0.369663, mean_eps: 0.945188
   61022/2000000: episode: 644, duration: 0.982s, episode steps: 66, steps per second: 67, episode reward: -117.730, mean reward: -1.784 [-100.000, 10.691], mean action: 1.470 [0.000, 3.000], mean observation: 0.052 [-1.374, 5.060], loss: 1.290309, mean_absolute_error: 7.864827, mean_q: 1.842202, mean_eps: 0.945111
   61142/2000000: episode: 645, duration: 1.702s, episode steps: 120, steps per second: 70, episode reward: -235.269, mean reward: -1.961 [-100.000, 25.479], mean action: 1.367 [0.000, 3.000], mean observation: 0.147 [-1.601, 4.364], loss: 1.741534, mean_absolute_error: 8.283941, mean_q: 0.642099, mean_eps: 0.945026
   61248/2000000: episode: 646, duration: 1.568s, episode steps: 106, steps per second: 68, episode reward: -192.077, mean reward: -1.812 [-100.000, 8.394], mean action: 1.651 [0.000, 3.000], mean observation: 0.014 [-4.532, 1.000], loss: 1.492181, mean_absolute_error: 8.872016, mean_q: -0.167282, mean_eps: 0.944925
   61337/2000000: episode: 647, duration: 1.295s, episode steps: 89, steps per second: 69, episode reward: -212.104, mean reward: -2.383 [-100.000, 6.807], mean action: 1.326 [0.000, 3.000], mean observation: 0.027 [-1.476, 1.204], loss: 1.614725, mean_absolute_error: 7.852176, mean_q: 1.176210, mean_eps: 0.944837
   61418/2000000: episode: 648, duration: 1.131s, episode steps: 81, steps per second: 72, episode reward: -255.983, mean reward: -3.160 [-100.000, 6.389], mean action: 1.420 [0.000, 3.000], mean observation: -0.111 [-1.500, 1.000], loss: 1.763765, mean_absolute_error: 9.216939, mean_q: -0.874087, mean_eps: 0.944760
   61502/2000000: episode: 649, duration: 1.189s, episode steps: 84, steps per second: 71, episode reward: -140.907, mean reward: -1.677 [-100.000, 13.830], mean action: 1.512 [0.000, 3.000], mean observation: 0.008 [-1.190, 4.318], loss: 1.244195, mean_absolute_error: 8.210544, mean_q: 0.087954, mean_eps: 0.944686
   61585/2000000: episode: 650, duration: 1.227s, episode steps: 83, steps per second: 68, episode reward: -115.150, mean reward: -1.387 [-100.000, 17.063], mean action: 1.398 [0.000, 3.000], mean observation: -0.033 [-1.370, 1.000], loss: 1.413466, mean_absolute_error: 8.430657, mean_q: -0.294172, mean_eps: 0.944610
   61673/2000000: episode: 651, duration: 1.274s, episode steps: 88, steps per second: 69, episode reward: -156.664, mean reward: -1.780 [-100.000, 7.069], mean action: 1.545 [0.000, 3.000], mean observation: -0.111 [-1.381, 4.419], loss: 1.400580, mean_absolute_error: 8.613257, mean_q: 1.125324, mean_eps: 0.944533
   61770/2000000: episode: 652, duration: 1.358s, episode steps: 97, steps per second: 71, episode reward: -139.067, mean reward: -1.434 [-100.000, 59.687], mean action: 1.433 [0.000, 3.000], mean observation: 0.025 [-1.584, 4.805], loss: 1.526863, mean_absolute_error: 8.816022, mean_q: -0.820010, mean_eps: 0.944450
   61855/2000000: episode: 653, duration: 1.205s, episode steps: 85, steps per second: 71, episode reward: -143.315, mean reward: -1.686 [-100.000, 11.913], mean action: 1.565 [0.000, 3.000], mean observation: -0.065 [-1.235, 3.874], loss: 1.824313, mean_absolute_error: 8.349422, mean_q: 0.860858, mean_eps: 0.944369
   61959/2000000: episode: 654, duration: 1.511s, episode steps: 104, steps per second: 69, episode reward: -150.429, mean reward: -1.446 [-100.000, 7.182], mean action: 1.433 [0.000, 3.000], mean observation: 0.093 [-1.492, 5.221], loss: 1.338739, mean_absolute_error: 8.788250, mean_q: -0.497386, mean_eps: 0.944285
   62081/2000000: episode: 655, duration: 1.760s, episode steps: 122, steps per second: 69, episode reward: -268.783, mean reward: -2.203 [-100.000, 0.801], mean action: 1.484 [0.000, 3.000], mean observation: 0.114 [-1.505, 1.237], loss: 1.443121, mean_absolute_error: 8.230262, mean_q: 0.635699, mean_eps: 0.944182
   62208/2000000: episode: 656, duration: 1.961s, episode steps: 127, steps per second: 65, episode reward: -290.314, mean reward: -2.286 [-100.000, 7.119], mean action: 1.496 [0.000, 3.000], mean observation: 0.048 [-1.491, 5.982], loss: 1.446405, mean_absolute_error: 9.005864, mean_q: -0.506232, mean_eps: 0.944070
   62320/2000000: episode: 657, duration: 1.765s, episode steps: 112, steps per second: 63, episode reward: -345.154, mean reward: -3.082 [-100.000, 5.052], mean action: 1.420 [0.000, 3.000], mean observation: -0.039 [-1.613, 3.315], loss: 1.359454, mean_absolute_error: 8.184994, mean_q: 0.057594, mean_eps: 0.943964
   62413/2000000: episode: 658, duration: 1.394s, episode steps: 93, steps per second: 67, episode reward: -111.383, mean reward: -1.198 [-100.000, 6.333], mean action: 1.677 [0.000, 3.000], mean observation: -0.064 [-1.076, 3.692], loss: 1.825458, mean_absolute_error: 8.765858, mean_q: 0.238868, mean_eps: 0.943871
   62513/2000000: episode: 659, duration: 1.460s, episode steps: 100, steps per second: 68, episode reward: -231.438, mean reward: -2.314 [-100.000, 8.721], mean action: 1.470 [0.000, 3.000], mean observation: 0.080 [-1.603, 4.725], loss: 1.421092, mean_absolute_error: 8.717171, mean_q: 0.016688, mean_eps: 0.943782
   62644/2000000: episode: 660, duration: 1.885s, episode steps: 131, steps per second: 70, episode reward: -147.508, mean reward: -1.126 [-100.000, 62.294], mean action: 1.565 [0.000, 3.000], mean observation: 0.033 [-1.425, 1.665], loss: 1.696005, mean_absolute_error: 8.942134, mean_q: 0.037044, mean_eps: 0.943680
   62766/2000000: episode: 661, duration: 1.772s, episode steps: 122, steps per second: 69, episode reward: -220.078, mean reward: -1.804 [-100.000, 5.494], mean action: 1.582 [0.000, 3.000], mean observation: -0.119 [-1.334, 4.042], loss: 1.728194, mean_absolute_error: 9.381204, mean_q: -0.089310, mean_eps: 0.943566
   62866/2000000: episode: 662, duration: 1.402s, episode steps: 100, steps per second: 71, episode reward: -140.680, mean reward: -1.407 [-100.000, 12.096], mean action: 1.380 [0.000, 3.000], mean observation: 0.134 [-3.458, 1.135], loss: 1.240919, mean_absolute_error: 8.483363, mean_q: 0.761648, mean_eps: 0.943466
   62958/2000000: episode: 663, duration: 1.337s, episode steps: 92, steps per second: 69, episode reward: -246.676, mean reward: -2.681 [-100.000, 30.272], mean action: 1.478 [0.000, 3.000], mean observation: -0.045 [-5.670, 1.000], loss: 1.407769, mean_absolute_error: 8.193830, mean_q: 0.562614, mean_eps: 0.943379
   63053/2000000: episode: 664, duration: 1.351s, episode steps: 95, steps per second: 70, episode reward: -181.436, mean reward: -1.910 [-100.000, 14.663], mean action: 1.432 [0.000, 3.000], mean observation: -0.029 [-1.339, 1.000], loss: 1.384488, mean_absolute_error: 8.322952, mean_q: 0.630577, mean_eps: 0.943295
   63126/2000000: episode: 665, duration: 1.020s, episode steps: 73, steps per second: 72, episode reward: -116.624, mean reward: -1.598 [-100.000, 6.268], mean action: 1.507 [0.000, 3.000], mean observation: -0.051 [-4.368, 1.000], loss: 0.945746, mean_absolute_error: 7.951632, mean_q: 1.537615, mean_eps: 0.943219
   63226/2000000: episode: 666, duration: 1.427s, episode steps: 100, steps per second: 70, episode reward: -169.591, mean reward: -1.696 [-100.000, 17.042], mean action: 1.450 [0.000, 3.000], mean observation: 0.112 [-1.368, 1.000], loss: 1.799862, mean_absolute_error: 8.833086, mean_q: 0.270462, mean_eps: 0.943142
   63314/2000000: episode: 667, duration: 1.299s, episode steps: 88, steps per second: 68, episode reward: -237.173, mean reward: -2.695 [-100.000, 13.226], mean action: 1.466 [0.000, 3.000], mean observation: -0.070 [-4.173, 1.000], loss: 1.518222, mean_absolute_error: 8.122242, mean_q: 0.603052, mean_eps: 0.943057
   63402/2000000: episode: 668, duration: 1.245s, episode steps: 88, steps per second: 71, episode reward: -133.429, mean reward: -1.516 [-100.000, 26.161], mean action: 1.523 [0.000, 3.000], mean observation: -0.071 [-2.342, 1.000], loss: 1.547311, mean_absolute_error: 9.059828, mean_q: -0.199512, mean_eps: 0.942978
   63505/2000000: episode: 669, duration: 1.486s, episode steps: 103, steps per second: 69, episode reward: -157.674, mean reward: -1.531 [-100.000, 17.273], mean action: 1.680 [0.000, 3.000], mean observation: -0.120 [-1.252, 1.000], loss: 1.523650, mean_absolute_error: 8.643780, mean_q: -0.549487, mean_eps: 0.942891
   63610/2000000: episode: 670, duration: 1.518s, episode steps: 105, steps per second: 69, episode reward: -219.412, mean reward: -2.090 [-100.000, 6.469], mean action: 1.419 [0.000, 3.000], mean observation: 0.168 [-1.678, 1.000], loss: 1.434077, mean_absolute_error: 8.892023, mean_q: -0.701022, mean_eps: 0.942798
   63669/2000000: episode: 671, duration: 1.093s, episode steps: 59, steps per second: 54, episode reward: -146.322, mean reward: -2.480 [-100.000, 7.760], mean action: 1.373 [0.000, 3.000], mean observation: -0.016 [-5.944, 1.000], loss: 1.316694, mean_absolute_error: 8.650579, mean_q: 0.579534, mean_eps: 0.942724
   63792/2000000: episode: 672, duration: 1.906s, episode steps: 123, steps per second: 65, episode reward: -220.979, mean reward: -1.797 [-100.000, 7.809], mean action: 1.553 [0.000, 3.000], mean observation: 0.087 [-4.004, 1.129], loss: 1.737824, mean_absolute_error: 7.756163, mean_q: 1.372420, mean_eps: 0.942643
   63932/2000000: episode: 673, duration: 2.145s, episode steps: 140, steps per second: 65, episode reward: -127.025, mean reward: -0.907 [-100.000, 36.580], mean action: 1.300 [0.000, 3.000], mean observation: 0.158 [-3.215, 1.043], loss: 1.324701, mean_absolute_error: 8.269153, mean_q: 1.228194, mean_eps: 0.942526
   64055/2000000: episode: 674, duration: 1.753s, episode steps: 123, steps per second: 70, episode reward: -117.235, mean reward: -0.953 [-100.000, 74.512], mean action: 1.488 [0.000, 3.000], mean observation: 0.097 [-1.471, 1.023], loss: 0.975718, mean_absolute_error: 8.626508, mean_q: -0.161582, mean_eps: 0.942407
   64175/2000000: episode: 675, duration: 1.705s, episode steps: 120, steps per second: 70, episode reward: -438.247, mean reward: -3.652 [-100.000, 46.804], mean action: 1.608 [0.000, 3.000], mean observation: -0.022 [-3.896, 1.469], loss: 1.378403, mean_absolute_error: 8.468856, mean_q: 0.280875, mean_eps: 0.942297
   64247/2000000: episode: 676, duration: 1.041s, episode steps: 72, steps per second: 69, episode reward: -217.981, mean reward: -3.028 [-100.000, 8.023], mean action: 1.431 [0.000, 3.000], mean observation: -0.076 [-5.303, 1.000], loss: 1.323585, mean_absolute_error: 8.500085, mean_q: 0.513781, mean_eps: 0.942211
   64329/2000000: episode: 677, duration: 1.187s, episode steps: 82, steps per second: 69, episode reward: -156.088, mean reward: -1.904 [-100.000, 14.626], mean action: 1.537 [0.000, 3.000], mean observation: 0.073 [-1.213, 3.939], loss: 1.412672, mean_absolute_error: 7.919324, mean_q: 1.771497, mean_eps: 0.942141
   64405/2000000: episode: 678, duration: 1.079s, episode steps: 76, steps per second: 70, episode reward: -115.191, mean reward: -1.516 [-100.000, 7.216], mean action: 1.684 [0.000, 3.000], mean observation: -0.059 [-4.012, 1.000], loss: 1.128924, mean_absolute_error: 8.450824, mean_q: 0.396792, mean_eps: 0.942069
   64512/2000000: episode: 679, duration: 1.533s, episode steps: 107, steps per second: 70, episode reward: -343.556, mean reward: -3.211 [-100.000, 95.076], mean action: 1.570 [0.000, 3.000], mean observation: 0.091 [-1.414, 3.880], loss: 1.495720, mean_absolute_error: 8.384940, mean_q: 0.419036, mean_eps: 0.941988
   64600/2000000: episode: 680, duration: 1.304s, episode steps: 88, steps per second: 67, episode reward: -259.424, mean reward: -2.948 [-100.000, 86.855], mean action: 1.580 [0.000, 3.000], mean observation: 0.026 [-1.284, 2.841], loss: 1.369516, mean_absolute_error: 8.559126, mean_q: 0.575905, mean_eps: 0.941901
   64663/2000000: episode: 681, duration: 0.962s, episode steps: 63, steps per second: 65, episode reward: -214.457, mean reward: -3.404 [-100.000, 73.394], mean action: 1.603 [0.000, 3.000], mean observation: 0.014 [-1.538, 3.589], loss: 1.162733, mean_absolute_error: 7.820063, mean_q: 1.220531, mean_eps: 0.941833
   64830/2000000: episode: 682, duration: 2.515s, episode steps: 167, steps per second: 66, episode reward: -249.296, mean reward: -1.493 [-100.000, 118.311], mean action: 1.461 [0.000, 3.000], mean observation: 0.043 [-2.539, 1.000], loss: 1.175249, mean_absolute_error: 8.217018, mean_q: 0.748425, mean_eps: 0.941729
   64902/2000000: episode: 683, duration: 1.036s, episode steps: 72, steps per second: 70, episode reward: -180.419, mean reward: -2.506 [-100.000, 9.919], mean action: 1.736 [0.000, 3.000], mean observation: -0.153 [-1.468, 4.583], loss: 1.313464, mean_absolute_error: 8.240228, mean_q: 0.722768, mean_eps: 0.941621
   64996/2000000: episode: 684, duration: 1.368s, episode steps: 94, steps per second: 69, episode reward: -309.678, mean reward: -3.294 [-100.000, 4.810], mean action: 1.489 [0.000, 3.000], mean observation: -0.052 [-1.457, 1.771], loss: 1.438116, mean_absolute_error: 8.925512, mean_q: -1.038141, mean_eps: 0.941547
   65089/2000000: episode: 685, duration: 1.371s, episode steps: 93, steps per second: 68, episode reward: -208.244, mean reward: -2.239 [-100.000, 2.337], mean action: 1.323 [0.000, 3.000], mean observation: 0.131 [-3.660, 1.020], loss: 1.369682, mean_absolute_error: 8.497404, mean_q: 0.425106, mean_eps: 0.941462
   65217/2000000: episode: 686, duration: 1.831s, episode steps: 128, steps per second: 70, episode reward: -151.461, mean reward: -1.183 [-100.000, 45.540], mean action: 1.711 [0.000, 3.000], mean observation: -0.081 [-1.403, 2.460], loss: 1.343751, mean_absolute_error: 8.336838, mean_q: 0.248354, mean_eps: 0.941361
   65303/2000000: episode: 687, duration: 1.209s, episode steps: 86, steps per second: 71, episode reward: -189.695, mean reward: -2.206 [-100.000, 9.592], mean action: 1.698 [0.000, 3.000], mean observation: 0.057 [-1.255, 3.955], loss: 1.646042, mean_absolute_error: 8.715274, mean_q: 1.012869, mean_eps: 0.941266
   65393/2000000: episode: 688, duration: 1.311s, episode steps: 90, steps per second: 69, episode reward: -277.017, mean reward: -3.078 [-100.000, 5.573], mean action: 1.433 [0.000, 3.000], mean observation: -0.040 [-1.374, 1.000], loss: 1.638506, mean_absolute_error: 9.106940, mean_q: -0.551432, mean_eps: 0.941187
   65469/2000000: episode: 689, duration: 1.091s, episode steps: 76, steps per second: 70, episode reward: -178.098, mean reward: -2.343 [-100.000, 19.540], mean action: 1.605 [0.000, 3.000], mean observation: 0.048 [-1.463, 4.549], loss: 1.544313, mean_absolute_error: 8.431444, mean_q: -0.703511, mean_eps: 0.941111
   65556/2000000: episode: 690, duration: 1.257s, episode steps: 87, steps per second: 69, episode reward: -198.574, mean reward: -2.282 [-100.000, 13.816], mean action: 1.575 [0.000, 3.000], mean observation: -0.174 [-3.610, 1.000], loss: 1.539633, mean_absolute_error: 7.904220, mean_q: 1.004316, mean_eps: 0.941039
   65672/2000000: episode: 691, duration: 1.701s, episode steps: 116, steps per second: 68, episode reward: -196.895, mean reward: -1.697 [-100.000, 5.907], mean action: 1.629 [0.000, 3.000], mean observation: -0.134 [-1.325, 1.000], loss: 1.563469, mean_absolute_error: 8.669068, mean_q: 0.323247, mean_eps: 0.940949
   65790/2000000: episode: 692, duration: 1.726s, episode steps: 118, steps per second: 68, episode reward: -203.910, mean reward: -1.728 [-100.000, 2.688], mean action: 1.602 [0.000, 3.000], mean observation: 0.173 [-1.205, 1.122], loss: 1.144597, mean_absolute_error: 8.144352, mean_q: 0.697436, mean_eps: 0.940843
   65892/2000000: episode: 693, duration: 1.508s, episode steps: 102, steps per second: 68, episode reward: -170.122, mean reward: -1.668 [-100.000, 9.366], mean action: 1.480 [0.000, 3.000], mean observation: 0.110 [-3.821, 1.000], loss: 1.356872, mean_absolute_error: 8.586931, mean_q: 0.599750, mean_eps: 0.940744
   65971/2000000: episode: 694, duration: 1.174s, episode steps: 79, steps per second: 67, episode reward: -149.279, mean reward: -1.890 [-100.000, 5.839], mean action: 1.620 [0.000, 3.000], mean observation: -0.021 [-1.338, 4.417], loss: 1.313447, mean_absolute_error: 9.120020, mean_q: -0.699837, mean_eps: 0.940663
   66111/2000000: episode: 695, duration: 2.013s, episode steps: 140, steps per second: 70, episode reward: -385.564, mean reward: -2.754 [-100.000, 94.297], mean action: 1.600 [0.000, 3.000], mean observation: -0.074 [-3.192, 1.026], loss: 1.291346, mean_absolute_error: 8.257122, mean_q: 0.669571, mean_eps: 0.940564
   66176/2000000: episode: 696, duration: 0.977s, episode steps: 65, steps per second: 67, episode reward: -217.794, mean reward: -3.351 [-100.000, 33.828], mean action: 1.569 [0.000, 3.000], mean observation: -0.027 [-4.754, 1.000], loss: 1.035739, mean_absolute_error: 8.724036, mean_q: 0.202932, mean_eps: 0.940472
   66272/2000000: episode: 697, duration: 1.408s, episode steps: 96, steps per second: 68, episode reward: -216.604, mean reward: -2.256 [-100.000, 36.707], mean action: 1.562 [0.000, 3.000], mean observation: 0.027 [-1.952, 1.000], loss: 1.395889, mean_absolute_error: 8.257919, mean_q: 0.741709, mean_eps: 0.940400
   66405/2000000: episode: 698, duration: 1.955s, episode steps: 133, steps per second: 68, episode reward: -217.580, mean reward: -1.636 [-100.000, 6.071], mean action: 1.609 [0.000, 3.000], mean observation: 0.009 [-1.366, 1.045], loss: 1.435540, mean_absolute_error: 8.246461, mean_q: 0.356320, mean_eps: 0.940296
   66503/2000000: episode: 699, duration: 1.407s, episode steps: 98, steps per second: 70, episode reward: -305.058, mean reward: -3.113 [-100.000, 72.540], mean action: 1.398 [0.000, 3.000], mean observation: 0.073 [-1.503, 2.912], loss: 1.487078, mean_absolute_error: 9.436566, mean_q: -0.211659, mean_eps: 0.940191
   66623/2000000: episode: 700, duration: 1.679s, episode steps: 120, steps per second: 71, episode reward: -193.112, mean reward: -1.609 [-100.000, 16.114], mean action: 1.508 [0.000, 3.000], mean observation: 0.136 [-1.469, 5.400], loss: 1.186552, mean_absolute_error: 8.294329, mean_q: 0.738116, mean_eps: 0.940094
   66730/2000000: episode: 701, duration: 1.542s, episode steps: 107, steps per second: 69, episode reward: -106.984, mean reward: -1.000 [-100.000, 41.331], mean action: 1.785 [0.000, 3.000], mean observation: -0.144 [-1.309, 2.517], loss: 1.634859, mean_absolute_error: 8.048655, mean_q: 1.402940, mean_eps: 0.939992
   66848/2000000: episode: 702, duration: 1.699s, episode steps: 118, steps per second: 69, episode reward: -367.889, mean reward: -3.118 [-100.000, 95.898], mean action: 1.483 [0.000, 3.000], mean observation: 0.193 [-1.296, 3.156], loss: 1.330371, mean_absolute_error: 8.094552, mean_q: 0.530815, mean_eps: 0.939891
   66916/2000000: episode: 703, duration: 1.084s, episode steps: 68, steps per second: 63, episode reward: -152.126, mean reward: -2.237 [-100.000, 17.167], mean action: 1.382 [0.000, 3.000], mean observation: 0.015 [-1.304, 5.283], loss: 1.168984, mean_absolute_error: 7.513810, mean_q: 1.307916, mean_eps: 0.939808
   67017/2000000: episode: 704, duration: 1.539s, episode steps: 101, steps per second: 66, episode reward: -166.231, mean reward: -1.646 [-100.000, 12.803], mean action: 1.673 [0.000, 3.000], mean observation: -0.043 [-1.244, 2.896], loss: 1.275520, mean_absolute_error: 8.137165, mean_q: 1.442764, mean_eps: 0.939731
   67079/2000000: episode: 705, duration: 0.873s, episode steps: 62, steps per second: 71, episode reward: -151.840, mean reward: -2.449 [-100.000, 19.128], mean action: 1.290 [0.000, 3.000], mean observation: -0.004 [-5.422, 1.000], loss: 1.612628, mean_absolute_error: 8.817182, mean_q: 0.192785, mean_eps: 0.939657
   67198/2000000: episode: 706, duration: 1.710s, episode steps: 119, steps per second: 70, episode reward: -216.445, mean reward: -1.819 [-100.000, 6.371], mean action: 1.361 [0.000, 3.000], mean observation: 0.153 [-1.559, 4.961], loss: 1.387114, mean_absolute_error: 8.509614, mean_q: 0.197481, mean_eps: 0.939576
   67292/2000000: episode: 707, duration: 1.350s, episode steps: 94, steps per second: 70, episode reward: -113.687, mean reward: -1.209 [-100.000, 11.670], mean action: 1.436 [0.000, 3.000], mean observation: -0.077 [-3.723, 1.000], loss: 1.302635, mean_absolute_error: 8.576832, mean_q: 1.030851, mean_eps: 0.939480
   67367/2000000: episode: 708, duration: 1.091s, episode steps: 75, steps per second: 69, episode reward: -93.429, mean reward: -1.246 [-100.000, 12.036], mean action: 1.587 [0.000, 3.000], mean observation: -0.084 [-4.054, 1.000], loss: 1.167799, mean_absolute_error: 8.534015, mean_q: 0.348554, mean_eps: 0.939405
   67489/2000000: episode: 709, duration: 1.757s, episode steps: 122, steps per second: 69, episode reward: -33.748, mean reward: -0.277 [-100.000, 104.413], mean action: 1.582 [0.000, 3.000], mean observation: 0.045 [-1.364, 1.645], loss: 1.314682, mean_absolute_error: 8.642242, mean_q: 0.093500, mean_eps: 0.939315
   67590/2000000: episode: 710, duration: 1.432s, episode steps: 101, steps per second: 71, episode reward: -334.096, mean reward: -3.308 [-100.000, 0.589], mean action: 1.604 [0.000, 3.000], mean observation: -0.072 [-1.633, 0.987], loss: 1.116470, mean_absolute_error: 8.143075, mean_q: 1.121277, mean_eps: 0.939214
   67671/2000000: episode: 711, duration: 1.145s, episode steps: 81, steps per second: 71, episode reward: -110.234, mean reward: -1.361 [-100.000, 11.508], mean action: 1.457 [0.000, 3.000], mean observation: -0.012 [-1.329, 1.000], loss: 1.060459, mean_absolute_error: 8.918347, mean_q: -0.349195, mean_eps: 0.939133
   67753/2000000: episode: 712, duration: 1.181s, episode steps: 82, steps per second: 69, episode reward: -12.912, mean reward: -0.157 [-100.000, 82.905], mean action: 1.537 [0.000, 3.000], mean observation: -0.029 [-1.668, 1.000], loss: 1.232852, mean_absolute_error: 8.486191, mean_q: 0.572181, mean_eps: 0.939059
   67817/2000000: episode: 713, duration: 0.911s, episode steps: 64, steps per second: 70, episode reward: -153.582, mean reward: -2.400 [-100.000, 21.505], mean action: 1.297 [0.000, 3.000], mean observation: 0.062 [-1.431, 5.349], loss: 1.252789, mean_absolute_error: 8.499433, mean_q: -0.179881, mean_eps: 0.938993
   67933/2000000: episode: 714, duration: 1.657s, episode steps: 116, steps per second: 70, episode reward: -246.465, mean reward: -2.125 [-100.000, 13.610], mean action: 1.517 [0.000, 3.000], mean observation: 0.185 [-1.575, 3.903], loss: 0.981779, mean_absolute_error: 7.585663, mean_q: 1.664350, mean_eps: 0.938912
   68027/2000000: episode: 715, duration: 1.350s, episode steps: 94, steps per second: 70, episode reward: -173.393, mean reward: -1.845 [-100.000, 6.080], mean action: 1.489 [0.000, 3.000], mean observation: 0.079 [-1.241, 4.359], loss: 0.948264, mean_absolute_error: 8.416446, mean_q: 0.337269, mean_eps: 0.938818
   68100/2000000: episode: 716, duration: 1.136s, episode steps: 73, steps per second: 64, episode reward: -158.338, mean reward: -2.169 [-100.000, 6.246], mean action: 1.534 [0.000, 3.000], mean observation: 0.070 [-1.161, 4.266], loss: 1.749465, mean_absolute_error: 8.599249, mean_q: -0.011231, mean_eps: 0.938744
   68240/2000000: episode: 717, duration: 2.053s, episode steps: 140, steps per second: 68, episode reward: -256.953, mean reward: -1.835 [-100.000, 15.327], mean action: 1.636 [0.000, 3.000], mean observation: 0.033 [-1.517, 5.012], loss: 1.356460, mean_absolute_error: 8.008638, mean_q: 0.793680, mean_eps: 0.938649
   68378/2000000: episode: 718, duration: 1.992s, episode steps: 138, steps per second: 69, episode reward: -274.818, mean reward: -1.991 [-100.000, 6.080], mean action: 1.406 [0.000, 3.000], mean observation: 0.143 [-1.426, 2.754], loss: 1.062658, mean_absolute_error: 7.641249, mean_q: 1.607521, mean_eps: 0.938523
   68451/2000000: episode: 719, duration: 1.046s, episode steps: 73, steps per second: 70, episode reward: -175.355, mean reward: -2.402 [-100.000, 7.167], mean action: 1.644 [0.000, 3.000], mean observation: -0.094 [-2.277, 1.000], loss: 1.662947, mean_absolute_error: 8.773224, mean_q: 0.494122, mean_eps: 0.938427
   68560/2000000: episode: 720, duration: 1.582s, episode steps: 109, steps per second: 69, episode reward: -242.089, mean reward: -2.221 [-100.000, 9.139], mean action: 1.716 [0.000, 3.000], mean observation: -0.022 [-2.973, 1.000], loss: 1.456386, mean_absolute_error: 8.574196, mean_q: -0.100919, mean_eps: 0.938346
   68682/2000000: episode: 721, duration: 1.778s, episode steps: 122, steps per second: 69, episode reward: -135.699, mean reward: -1.112 [-100.000, 7.861], mean action: 1.516 [0.000, 3.000], mean observation: -0.009 [-1.501, 5.267], loss: 1.227078, mean_absolute_error: 9.049581, mean_q: -0.609867, mean_eps: 0.938242
   68798/2000000: episode: 722, duration: 1.668s, episode steps: 116, steps per second: 70, episode reward: -274.534, mean reward: -2.367 [-100.000, 4.212], mean action: 1.569 [0.000, 3.000], mean observation: 0.042 [-1.559, 2.878], loss: 1.286816, mean_absolute_error: 8.317969, mean_q: 0.292992, mean_eps: 0.938134
   68909/2000000: episode: 723, duration: 1.581s, episode steps: 111, steps per second: 70, episode reward: -209.847, mean reward: -1.891 [-100.000, 7.954], mean action: 1.468 [0.000, 3.000], mean observation: -0.028 [-1.418, 1.287], loss: 1.106943, mean_absolute_error: 8.180881, mean_q: 0.725326, mean_eps: 0.938031
   69003/2000000: episode: 724, duration: 1.310s, episode steps: 94, steps per second: 72, episode reward: -127.749, mean reward: -1.359 [-100.000, 10.935], mean action: 1.426 [0.000, 3.000], mean observation: 0.015 [-4.007, 1.000], loss: 1.400281, mean_absolute_error: 8.082077, mean_q: 1.120921, mean_eps: 0.937940
   69109/2000000: episode: 725, duration: 1.528s, episode steps: 106, steps per second: 69, episode reward: -149.575, mean reward: -1.411 [-100.000, 4.618], mean action: 1.387 [0.000, 3.000], mean observation: -0.051 [-1.238, 2.267], loss: 1.229671, mean_absolute_error: 8.204441, mean_q: 0.833555, mean_eps: 0.937850
   69188/2000000: episode: 726, duration: 1.142s, episode steps: 79, steps per second: 69, episode reward: -103.889, mean reward: -1.315 [-100.000, 10.446], mean action: 1.443 [0.000, 3.000], mean observation: 0.070 [-1.202, 4.131], loss: 1.270450, mean_absolute_error: 8.714344, mean_q: 0.936808, mean_eps: 0.937767
   69253/2000000: episode: 727, duration: 0.968s, episode steps: 65, steps per second: 67, episode reward: -186.459, mean reward: -2.869 [-100.000, 7.032], mean action: 1.385 [0.000, 3.000], mean observation: -0.093 [-4.308, 1.000], loss: 1.432604, mean_absolute_error: 7.992520, mean_q: 1.274987, mean_eps: 0.937702
   69362/2000000: episode: 728, duration: 1.561s, episode steps: 109, steps per second: 70, episode reward: -316.532, mean reward: -2.904 [-100.000, 1.069], mean action: 1.569 [0.000, 3.000], mean observation: 0.151 [-1.370, 1.310], loss: 1.479767, mean_absolute_error: 8.717110, mean_q: 0.516002, mean_eps: 0.937623
   69475/2000000: episode: 729, duration: 1.626s, episode steps: 113, steps per second: 69, episode reward: -346.912, mean reward: -3.070 [-100.000, 114.197], mean action: 1.540 [0.000, 3.000], mean observation: 0.097 [-1.493, 2.900], loss: 1.128205, mean_absolute_error: 8.452868, mean_q: -0.252002, mean_eps: 0.937524
   69558/2000000: episode: 730, duration: 1.215s, episode steps: 83, steps per second: 68, episode reward: -198.179, mean reward: -2.388 [-100.000, 5.365], mean action: 1.446 [0.000, 3.000], mean observation: -0.065 [-4.625, 1.000], loss: 1.091060, mean_absolute_error: 8.621738, mean_q: 0.216167, mean_eps: 0.937436
   69662/2000000: episode: 731, duration: 1.467s, episode steps: 104, steps per second: 71, episode reward: -216.621, mean reward: -2.083 [-100.000, 5.384], mean action: 1.683 [0.000, 3.000], mean observation: -0.158 [-1.461, 1.000], loss: 1.660140, mean_absolute_error: 8.591529, mean_q: 0.755822, mean_eps: 0.937351
   69784/2000000: episode: 732, duration: 1.767s, episode steps: 122, steps per second: 69, episode reward: -375.319, mean reward: -3.076 [-100.000, 10.320], mean action: 1.475 [0.000, 3.000], mean observation: 0.073 [-4.813, 1.499], loss: 1.426767, mean_absolute_error: 9.017742, mean_q: 0.154927, mean_eps: 0.937250
   69859/2000000: episode: 733, duration: 1.087s, episode steps: 75, steps per second: 69, episode reward: -147.648, mean reward: -1.969 [-100.000, 10.639], mean action: 1.587 [0.000, 3.000], mean observation: -0.123 [-1.282, 1.000], loss: 1.180926, mean_absolute_error: 7.425027, mean_q: 2.077716, mean_eps: 0.937162
   69964/2000000: episode: 734, duration: 1.530s, episode steps: 105, steps per second: 69, episode reward: -240.048, mean reward: -2.286 [-100.000, 7.096], mean action: 1.590 [0.000, 3.000], mean observation: -0.031 [-4.281, 1.000], loss: 1.079754, mean_absolute_error: 8.306795, mean_q: 0.498632, mean_eps: 0.937081
   70074/2000000: episode: 735, duration: 1.613s, episode steps: 110, steps per second: 68, episode reward: -287.571, mean reward: -2.614 [-100.000, 6.280], mean action: 1.600 [0.000, 3.000], mean observation: -0.079 [-1.367, 1.000], loss: 1.513532, mean_absolute_error: 8.699253, mean_q: 1.080642, mean_eps: 0.936984
   70193/2000000: episode: 736, duration: 1.722s, episode steps: 119, steps per second: 69, episode reward: -301.174, mean reward: -2.531 [-100.000, 4.352], mean action: 1.361 [0.000, 3.000], mean observation: 0.211 [-1.459, 3.158], loss: 1.318764, mean_absolute_error: 9.638820, mean_q: 0.811014, mean_eps: 0.936879
   70282/2000000: episode: 737, duration: 1.297s, episode steps: 89, steps per second: 69, episode reward: -141.260, mean reward: -1.587 [-100.000, 11.180], mean action: 1.562 [0.000, 3.000], mean observation: -0.139 [-1.377, 1.000], loss: 1.727070, mean_absolute_error: 8.945787, mean_q: 1.423805, mean_eps: 0.936786
   70369/2000000: episode: 738, duration: 1.414s, episode steps: 87, steps per second: 62, episode reward: -100.106, mean reward: -1.151 [-100.000, 21.114], mean action: 1.483 [0.000, 3.000], mean observation: -0.075 [-1.157, 1.000], loss: 1.476805, mean_absolute_error: 9.341272, mean_q: 0.761413, mean_eps: 0.936707
   70434/2000000: episode: 739, duration: 0.991s, episode steps: 65, steps per second: 66, episode reward: -347.993, mean reward: -5.354 [-100.000, 23.967], mean action: 1.400 [0.000, 3.000], mean observation: -0.022 [-1.536, 5.403], loss: 1.805792, mean_absolute_error: 9.023174, mean_q: 1.137608, mean_eps: 0.936638
   70509/2000000: episode: 740, duration: 1.134s, episode steps: 75, steps per second: 66, episode reward: -214.624, mean reward: -2.862 [-100.000, 6.824], mean action: 1.347 [0.000, 3.000], mean observation: -0.071 [-1.552, 1.000], loss: 1.656253, mean_absolute_error: 9.379384, mean_q: 0.722158, mean_eps: 0.936575
   70573/2000000: episode: 741, duration: 0.925s, episode steps: 64, steps per second: 69, episode reward: -203.993, mean reward: -3.187 [-100.000, 26.176], mean action: 1.578 [0.000, 3.000], mean observation: -0.058 [-5.565, 1.000], loss: 1.411277, mean_absolute_error: 9.066992, mean_q: 1.943181, mean_eps: 0.936512
   70640/2000000: episode: 742, duration: 0.995s, episode steps: 67, steps per second: 67, episode reward: -269.693, mean reward: -4.025 [-100.000, 4.488], mean action: 1.567 [0.000, 3.000], mean observation: -0.057 [-1.581, 1.241], loss: 1.730576, mean_absolute_error: 10.266413, mean_q: 0.855806, mean_eps: 0.936455
   70743/2000000: episode: 743, duration: 1.498s, episode steps: 103, steps per second: 69, episode reward: -212.195, mean reward: -2.060 [-100.000, 6.489], mean action: 1.573 [0.000, 3.000], mean observation: 0.109 [-1.423, 4.890], loss: 1.718995, mean_absolute_error: 10.415792, mean_q: -0.502484, mean_eps: 0.936379
   70848/2000000: episode: 744, duration: 1.558s, episode steps: 105, steps per second: 67, episode reward: -183.984, mean reward: -1.752 [-100.000, 12.469], mean action: 1.552 [0.000, 3.000], mean observation: 0.027 [-3.407, 1.000], loss: 1.804896, mean_absolute_error: 9.838420, mean_q: 0.154792, mean_eps: 0.936285
   70954/2000000: episode: 745, duration: 1.528s, episode steps: 106, steps per second: 69, episode reward: -54.043, mean reward: -0.510 [-100.000, 125.368], mean action: 1.538 [0.000, 3.000], mean observation: -0.049 [-1.446, 1.913], loss: 1.477405, mean_absolute_error: 9.591578, mean_q: 0.094585, mean_eps: 0.936190
   71047/2000000: episode: 746, duration: 1.330s, episode steps: 93, steps per second: 70, episode reward: -129.227, mean reward: -1.390 [-100.000, 22.737], mean action: 1.505 [0.000, 3.000], mean observation: 0.066 [-2.748, 1.000], loss: 1.728100, mean_absolute_error: 9.966616, mean_q: 1.094329, mean_eps: 0.936100
   71154/2000000: episode: 747, duration: 1.530s, episode steps: 107, steps per second: 70, episode reward: -279.107, mean reward: -2.608 [-100.000, 6.743], mean action: 1.458 [0.000, 3.000], mean observation: 0.046 [-1.584, 1.000], loss: 1.495624, mean_absolute_error: 9.539230, mean_q: 0.678802, mean_eps: 0.936010
   71225/2000000: episode: 748, duration: 1.031s, episode steps: 71, steps per second: 69, episode reward: -127.126, mean reward: -1.791 [-100.000, 16.659], mean action: 1.493 [0.000, 3.000], mean observation: -0.087 [-1.461, 1.000], loss: 1.643616, mean_absolute_error: 10.177983, mean_q: -1.159702, mean_eps: 0.935929
   71310/2000000: episode: 749, duration: 1.211s, episode steps: 85, steps per second: 70, episode reward: -166.327, mean reward: -1.957 [-100.000, 16.594], mean action: 1.706 [0.000, 3.000], mean observation: -0.040 [-1.324, 2.967], loss: 1.738696, mean_absolute_error: 9.547351, mean_q: 0.555249, mean_eps: 0.935859
   71436/2000000: episode: 750, duration: 1.851s, episode steps: 126, steps per second: 68, episode reward: -144.453, mean reward: -1.146 [-100.000, 10.378], mean action: 1.492 [0.000, 3.000], mean observation: 0.044 [-4.438, 1.035], loss: 1.644616, mean_absolute_error: 9.688899, mean_q: 0.209655, mean_eps: 0.935765
   71522/2000000: episode: 751, duration: 1.280s, episode steps: 86, steps per second: 67, episode reward: -102.122, mean reward: -1.187 [-100.000, 6.900], mean action: 1.547 [0.000, 3.000], mean observation: 0.094 [-1.135, 4.067], loss: 1.839587, mean_absolute_error: 10.257444, mean_q: -0.588513, mean_eps: 0.935670
   71610/2000000: episode: 752, duration: 1.275s, episode steps: 88, steps per second: 69, episode reward: -148.206, mean reward: -1.684 [-100.000, 16.420], mean action: 1.568 [0.000, 3.000], mean observation: -0.001 [-1.273, 1.000], loss: 1.292132, mean_absolute_error: 8.395436, mean_q: 1.802906, mean_eps: 0.935591
   71679/2000000: episode: 753, duration: 0.977s, episode steps: 69, steps per second: 71, episode reward: -109.227, mean reward: -1.583 [-100.000, 8.863], mean action: 1.478 [0.000, 3.000], mean observation: 0.069 [-1.253, 4.517], loss: 1.480348, mean_absolute_error: 9.910154, mean_q: -0.967628, mean_eps: 0.935520
   71750/2000000: episode: 754, duration: 1.023s, episode steps: 71, steps per second: 69, episode reward: -149.494, mean reward: -2.106 [-100.000, 26.922], mean action: 1.423 [0.000, 3.000], mean observation: -0.062 [-2.274, 1.000], loss: 1.263307, mean_absolute_error: 8.347065, mean_q: 1.784404, mean_eps: 0.935457
   71840/2000000: episode: 755, duration: 1.318s, episode steps: 90, steps per second: 68, episode reward: -159.460, mean reward: -1.772 [-100.000, 12.349], mean action: 1.511 [0.000, 3.000], mean observation: -0.105 [-1.310, 1.000], loss: 1.502127, mean_absolute_error: 9.513724, mean_q: 0.427038, mean_eps: 0.935385
   71949/2000000: episode: 756, duration: 1.607s, episode steps: 109, steps per second: 68, episode reward: -340.687, mean reward: -3.126 [-100.000, 5.609], mean action: 1.459 [0.000, 3.000], mean observation: 0.045 [-1.586, 2.973], loss: 1.211965, mean_absolute_error: 9.254746, mean_q: 0.778002, mean_eps: 0.935295
   72053/2000000: episode: 757, duration: 1.507s, episode steps: 104, steps per second: 69, episode reward: -180.309, mean reward: -1.734 [-100.000, 5.055], mean action: 1.615 [0.000, 3.000], mean observation: -0.115 [-1.294, 4.313], loss: 1.994213, mean_absolute_error: 10.770898, mean_q: -0.214315, mean_eps: 0.935198
   72167/2000000: episode: 758, duration: 1.657s, episode steps: 114, steps per second: 69, episode reward: -163.434, mean reward: -1.434 [-100.000, 43.801], mean action: 1.465 [0.000, 3.000], mean observation: 0.073 [-1.312, 2.249], loss: 1.594609, mean_absolute_error: 9.473622, mean_q: 0.431001, mean_eps: 0.935101
   72249/2000000: episode: 759, duration: 1.297s, episode steps: 82, steps per second: 63, episode reward: -166.951, mean reward: -2.036 [-100.000, 10.820], mean action: 1.488 [0.000, 3.000], mean observation: 0.105 [-3.535, 1.173], loss: 1.535905, mean_absolute_error: 10.189777, mean_q: -0.409050, mean_eps: 0.935013
   72329/2000000: episode: 760, duration: 1.135s, episode steps: 80, steps per second: 70, episode reward: -119.472, mean reward: -1.493 [-100.000, 12.029], mean action: 1.488 [0.000, 3.000], mean observation: -0.157 [-1.201, 1.000], loss: 1.404984, mean_absolute_error: 9.701349, mean_q: 0.682849, mean_eps: 0.934939
   72401/2000000: episode: 761, duration: 1.018s, episode steps: 72, steps per second: 71, episode reward: -197.739, mean reward: -2.746 [-100.000, 7.101], mean action: 1.556 [0.000, 3.000], mean observation: -0.084 [-4.070, 1.000], loss: 1.688482, mean_absolute_error: 9.501319, mean_q: 0.188603, mean_eps: 0.934871
   72523/2000000: episode: 762, duration: 1.692s, episode steps: 122, steps per second: 72, episode reward: -266.705, mean reward: -2.186 [-100.000, 6.763], mean action: 1.607 [0.000, 3.000], mean observation: 0.022 [-3.703, 1.000], loss: 1.273607, mean_absolute_error: 9.532909, mean_q: 0.654099, mean_eps: 0.934784
   72644/2000000: episode: 763, duration: 1.737s, episode steps: 121, steps per second: 70, episode reward: -180.195, mean reward: -1.489 [-100.000, 6.541], mean action: 1.430 [0.000, 3.000], mean observation: 0.109 [-4.860, 1.044], loss: 1.463673, mean_absolute_error: 9.081252, mean_q: 0.611191, mean_eps: 0.934676
   72730/2000000: episode: 764, duration: 1.219s, episode steps: 86, steps per second: 71, episode reward: -264.986, mean reward: -3.081 [-100.000, 5.111], mean action: 1.570 [0.000, 3.000], mean observation: 0.020 [-1.487, 1.000], loss: 1.816679, mean_absolute_error: 10.007398, mean_q: -0.155019, mean_eps: 0.934583
   72837/2000000: episode: 765, duration: 1.595s, episode steps: 107, steps per second: 67, episode reward: -158.392, mean reward: -1.480 [-100.000, 12.204], mean action: 1.561 [0.000, 3.000], mean observation: -0.067 [-3.897, 1.000], loss: 1.625461, mean_absolute_error: 8.967719, mean_q: 1.279611, mean_eps: 0.934494
   72912/2000000: episode: 766, duration: 1.110s, episode steps: 75, steps per second: 68, episode reward: -152.520, mean reward: -2.034 [-100.000, 62.929], mean action: 1.400 [0.000, 3.000], mean observation: -0.058 [-2.591, 1.000], loss: 1.619824, mean_absolute_error: 8.837252, mean_q: 1.507158, mean_eps: 0.934413
   73022/2000000: episode: 767, duration: 1.574s, episode steps: 110, steps per second: 70, episode reward: -316.339, mean reward: -2.876 [-100.000, 1.553], mean action: 1.527 [0.000, 3.000], mean observation: 0.142 [-1.234, 1.456], loss: 1.567817, mean_absolute_error: 9.915459, mean_q: 0.195888, mean_eps: 0.934331
   73114/2000000: episode: 768, duration: 1.307s, episode steps: 92, steps per second: 70, episode reward: -132.372, mean reward: -1.439 [-100.000, 7.194], mean action: 1.435 [0.000, 3.000], mean observation: 0.012 [-4.410, 1.000], loss: 1.547338, mean_absolute_error: 10.448605, mean_q: -0.936181, mean_eps: 0.934239
   73213/2000000: episode: 769, duration: 1.415s, episode steps: 99, steps per second: 70, episode reward: -172.532, mean reward: -1.743 [-100.000, 10.748], mean action: 1.515 [0.000, 3.000], mean observation: 0.145 [-3.166, 1.000], loss: 1.838019, mean_absolute_error: 9.696492, mean_q: 0.014717, mean_eps: 0.934152
   73316/2000000: episode: 770, duration: 1.473s, episode steps: 103, steps per second: 70, episode reward: -328.607, mean reward: -3.190 [-100.000, 77.642], mean action: 1.602 [0.000, 3.000], mean observation: 0.074 [-2.063, 1.273], loss: 1.428039, mean_absolute_error: 9.140824, mean_q: -0.056819, mean_eps: 0.934062
   73410/2000000: episode: 771, duration: 1.346s, episode steps: 94, steps per second: 70, episode reward: -366.650, mean reward: -3.901 [-100.000, 2.845], mean action: 1.606 [0.000, 3.000], mean observation: -0.040 [-1.477, 2.576], loss: 1.885670, mean_absolute_error: 9.465739, mean_q: 0.662352, mean_eps: 0.933974
   73539/2000000: episode: 772, duration: 1.812s, episode steps: 129, steps per second: 71, episode reward: -322.186, mean reward: -2.498 [-100.000, 23.253], mean action: 1.643 [0.000, 3.000], mean observation: -0.112 [-2.821, 1.026], loss: 1.307932, mean_absolute_error: 10.328510, mean_q: -0.042347, mean_eps: 0.933873
   73597/2000000: episode: 773, duration: 0.855s, episode steps: 58, steps per second: 68, episode reward: -180.718, mean reward: -3.116 [-100.000, 15.386], mean action: 1.466 [0.000, 3.000], mean observation: -0.032 [-1.476, 3.612], loss: 1.849712, mean_absolute_error: 9.181574, mean_q: 0.830616, mean_eps: 0.933789
   73695/2000000: episode: 774, duration: 1.371s, episode steps: 98, steps per second: 71, episode reward: -192.569, mean reward: -1.965 [-100.000, 8.659], mean action: 1.653 [0.000, 3.000], mean observation: -0.102 [-1.326, 1.000], loss: 1.692298, mean_absolute_error: 9.228132, mean_q: 1.185373, mean_eps: 0.933719
   73764/2000000: episode: 775, duration: 1.022s, episode steps: 69, steps per second: 68, episode reward: -217.111, mean reward: -3.147 [-100.000, 31.551], mean action: 1.594 [0.000, 3.000], mean observation: -0.073 [-4.239, 1.000], loss: 1.226036, mean_absolute_error: 9.899007, mean_q: -0.542513, mean_eps: 0.933645
   73894/2000000: episode: 776, duration: 1.853s, episode steps: 130, steps per second: 70, episode reward: -296.544, mean reward: -2.281 [-100.000, 8.862], mean action: 1.569 [0.000, 3.000], mean observation: 0.112 [-1.428, 1.463], loss: 1.592184, mean_absolute_error: 9.358251, mean_q: 0.611521, mean_eps: 0.933555
   73975/2000000: episode: 777, duration: 1.132s, episode steps: 81, steps per second: 72, episode reward: -122.947, mean reward: -1.518 [-100.000, 15.980], mean action: 1.556 [0.000, 3.000], mean observation: -0.049 [-1.445, 1.000], loss: 1.504567, mean_absolute_error: 9.309659, mean_q: -0.388409, mean_eps: 0.933459
   74068/2000000: episode: 778, duration: 1.347s, episode steps: 93, steps per second: 69, episode reward: -284.916, mean reward: -3.064 [-100.000, 3.522], mean action: 1.398 [0.000, 3.000], mean observation: 0.158 [-3.131, 1.513], loss: 1.000687, mean_absolute_error: 8.995503, mean_q: 1.735727, mean_eps: 0.933382
   74160/2000000: episode: 779, duration: 1.373s, episode steps: 92, steps per second: 67, episode reward: -190.470, mean reward: -2.070 [-100.000, 64.165], mean action: 1.467 [0.000, 3.000], mean observation: 0.011 [-1.456, 1.000], loss: 1.662299, mean_absolute_error: 9.118076, mean_q: 0.097306, mean_eps: 0.933299
   74275/2000000: episode: 780, duration: 1.638s, episode steps: 115, steps per second: 70, episode reward: -183.278, mean reward: -1.594 [-100.000, 8.600], mean action: 1.687 [0.000, 3.000], mean observation: -0.073 [-1.252, 3.534], loss: 1.549820, mean_absolute_error: 9.711893, mean_q: 0.048320, mean_eps: 0.933206
   74378/2000000: episode: 781, duration: 1.474s, episode steps: 103, steps per second: 70, episode reward: -270.767, mean reward: -2.629 [-100.000, 47.215], mean action: 1.456 [0.000, 3.000], mean observation: 0.074 [-1.621, 4.610], loss: 1.587518, mean_absolute_error: 10.018076, mean_q: -0.138062, mean_eps: 0.933107
   74459/2000000: episode: 782, duration: 1.142s, episode steps: 81, steps per second: 71, episode reward: -145.044, mean reward: -1.791 [-100.000, 16.031], mean action: 1.691 [0.000, 3.000], mean observation: -0.124 [-1.402, 1.000], loss: 1.351962, mean_absolute_error: 8.881279, mean_q: 1.935246, mean_eps: 0.933024
   74525/2000000: episode: 783, duration: 0.959s, episode steps: 66, steps per second: 69, episode reward: -232.408, mean reward: -3.521 [-100.000, 4.574], mean action: 1.727 [0.000, 3.000], mean observation: -0.006 [-5.733, 1.200], loss: 1.685462, mean_absolute_error: 9.035965, mean_q: 1.612102, mean_eps: 0.932957
   74641/2000000: episode: 784, duration: 1.617s, episode steps: 116, steps per second: 72, episode reward: -151.156, mean reward: -1.303 [-100.000, 63.682], mean action: 1.543 [0.000, 3.000], mean observation: 0.185 [-2.896, 1.330], loss: 1.135884, mean_absolute_error: 9.047988, mean_q: 0.868288, mean_eps: 0.932874
   74763/2000000: episode: 785, duration: 1.709s, episode steps: 122, steps per second: 71, episode reward: -214.440, mean reward: -1.758 [-100.000, 63.117], mean action: 1.615 [0.000, 3.000], mean observation: -0.083 [-3.454, 1.060], loss: 1.161042, mean_absolute_error: 9.112619, mean_q: 1.466652, mean_eps: 0.932768
   74870/2000000: episode: 786, duration: 1.530s, episode steps: 107, steps per second: 70, episode reward: -111.964, mean reward: -1.046 [-100.000, 58.016], mean action: 1.579 [0.000, 3.000], mean observation: 0.120 [-4.209, 1.000], loss: 1.854370, mean_absolute_error: 10.122371, mean_q: 0.465948, mean_eps: 0.932666
   74959/2000000: episode: 787, duration: 1.255s, episode steps: 89, steps per second: 71, episode reward: -176.937, mean reward: -1.988 [-100.000, 5.845], mean action: 1.472 [0.000, 3.000], mean observation: -0.004 [-1.280, 3.651], loss: 1.416728, mean_absolute_error: 9.264960, mean_q: 0.684160, mean_eps: 0.932577
   75077/2000000: episode: 788, duration: 1.708s, episode steps: 118, steps per second: 69, episode reward: -230.802, mean reward: -1.956 [-100.000, 10.346], mean action: 1.534 [0.000, 3.000], mean observation: 0.157 [-6.041, 1.013], loss: 1.016460, mean_absolute_error: 9.840649, mean_q: 0.435913, mean_eps: 0.932484
   75175/2000000: episode: 789, duration: 1.379s, episode steps: 98, steps per second: 71, episode reward: -152.845, mean reward: -1.560 [-100.000, 7.395], mean action: 1.378 [0.000, 3.000], mean observation: 0.157 [-4.499, 1.000], loss: 1.508838, mean_absolute_error: 8.587831, mean_q: 0.947619, mean_eps: 0.932387
   75266/2000000: episode: 790, duration: 1.303s, episode steps: 91, steps per second: 70, episode reward: -205.430, mean reward: -2.257 [-100.000, 20.486], mean action: 1.220 [0.000, 3.000], mean observation: -0.075 [-3.489, 1.000], loss: 1.536821, mean_absolute_error: 10.016074, mean_q: -0.865217, mean_eps: 0.932302
   75359/2000000: episode: 791, duration: 1.313s, episode steps: 93, steps per second: 71, episode reward: -299.496, mean reward: -3.220 [-100.000, 25.975], mean action: 1.667 [0.000, 3.000], mean observation: -0.061 [-4.510, 1.000], loss: 1.402529, mean_absolute_error: 8.805001, mean_q: 0.991259, mean_eps: 0.932219
   75454/2000000: episode: 792, duration: 1.349s, episode steps: 95, steps per second: 70, episode reward: -109.444, mean reward: -1.152 [-100.000, 104.269], mean action: 1.505 [0.000, 3.000], mean observation: -0.001 [-1.819, 1.000], loss: 1.312023, mean_absolute_error: 9.000555, mean_q: 0.285545, mean_eps: 0.932135
   75529/2000000: episode: 793, duration: 1.079s, episode steps: 75, steps per second: 70, episode reward: -202.860, mean reward: -2.705 [-100.000, 8.653], mean action: 1.373 [0.000, 3.000], mean observation: 0.066 [-1.569, 4.460], loss: 1.427489, mean_absolute_error: 9.251678, mean_q: 0.755912, mean_eps: 0.932057
   75616/2000000: episode: 794, duration: 1.262s, episode steps: 87, steps per second: 69, episode reward: -119.812, mean reward: -1.377 [-100.000, 9.702], mean action: 1.506 [0.000, 3.000], mean observation: -0.017 [-1.161, 3.687], loss: 1.630159, mean_absolute_error: 9.495056, mean_q: 0.120763, mean_eps: 0.931985
   75687/2000000: episode: 795, duration: 1.011s, episode steps: 71, steps per second: 70, episode reward: -247.495, mean reward: -3.486 [-100.000, 16.654], mean action: 1.732 [0.000, 3.000], mean observation: 0.026 [-4.251, 1.087], loss: 1.035853, mean_absolute_error: 9.627898, mean_q: 0.538856, mean_eps: 0.931915
   75780/2000000: episode: 796, duration: 1.350s, episode steps: 93, steps per second: 69, episode reward: -140.172, mean reward: -1.507 [-100.000, 11.608], mean action: 1.538 [0.000, 3.000], mean observation: -0.083 [-1.297, 4.155], loss: 1.474054, mean_absolute_error: 9.424642, mean_q: 0.012447, mean_eps: 0.931841
   75887/2000000: episode: 797, duration: 1.510s, episode steps: 107, steps per second: 71, episode reward: -219.892, mean reward: -2.055 [-100.000, 5.740], mean action: 1.570 [0.000, 3.000], mean observation: -0.118 [-1.601, 1.000], loss: 1.579909, mean_absolute_error: 9.887961, mean_q: -0.386570, mean_eps: 0.931751
   76011/2000000: episode: 798, duration: 1.749s, episode steps: 124, steps per second: 71, episode reward: -154.798, mean reward: -1.248 [-100.000, 9.093], mean action: 1.605 [0.000, 3.000], mean observation: 0.122 [-3.902, 1.000], loss: 1.635514, mean_absolute_error: 9.923328, mean_q: -0.299549, mean_eps: 0.931647
   76077/2000000: episode: 799, duration: 0.963s, episode steps: 66, steps per second: 69, episode reward: -169.591, mean reward: -2.570 [-100.000, 8.078], mean action: 1.530 [0.000, 3.000], mean observation: -0.048 [-1.546, 4.704], loss: 1.116039, mean_absolute_error: 8.881099, mean_q: 0.847229, mean_eps: 0.931560
   76192/2000000: episode: 800, duration: 1.649s, episode steps: 115, steps per second: 70, episode reward: -157.950, mean reward: -1.373 [-100.000, 12.411], mean action: 1.417 [0.000, 3.000], mean observation: 0.089 [-3.878, 1.000], loss: 1.405693, mean_absolute_error: 8.999263, mean_q: 1.097325, mean_eps: 0.931479
   76264/2000000: episode: 801, duration: 1.078s, episode steps: 72, steps per second: 67, episode reward: -135.812, mean reward: -1.886 [-100.000, 11.365], mean action: 1.556 [0.000, 3.000], mean observation: -0.122 [-1.400, 4.529], loss: 1.751407, mean_absolute_error: 9.265138, mean_q: 1.104948, mean_eps: 0.931397
   76337/2000000: episode: 802, duration: 1.111s, episode steps: 73, steps per second: 66, episode reward: -143.130, mean reward: -1.961 [-100.000, 7.359], mean action: 1.411 [0.000, 3.000], mean observation: -0.026 [-4.938, 1.000], loss: 1.409332, mean_absolute_error: 9.888435, mean_q: -0.189141, mean_eps: 0.931330
   76462/2000000: episode: 803, duration: 2.031s, episode steps: 125, steps per second: 62, episode reward: -188.319, mean reward: -1.507 [-100.000, 6.488], mean action: 1.680 [0.000, 3.000], mean observation: -0.057 [-1.529, 5.097], loss: 1.622915, mean_absolute_error: 9.040382, mean_q: 1.308495, mean_eps: 0.931240
   76588/2000000: episode: 804, duration: 1.806s, episode steps: 126, steps per second: 70, episode reward: -180.960, mean reward: -1.436 [-100.000, 3.085], mean action: 1.667 [0.000, 3.000], mean observation: -0.080 [-1.004, 1.069], loss: 1.330511, mean_absolute_error: 8.993053, mean_q: 1.279858, mean_eps: 0.931128
   76680/2000000: episode: 805, duration: 1.354s, episode steps: 92, steps per second: 68, episode reward: -129.606, mean reward: -1.409 [-100.000, 14.692], mean action: 1.533 [0.000, 3.000], mean observation: 0.028 [-1.372, 1.000], loss: 1.110948, mean_absolute_error: 9.059359, mean_q: 1.227685, mean_eps: 0.931031
   76757/2000000: episode: 806, duration: 1.138s, episode steps: 77, steps per second: 68, episode reward: -136.701, mean reward: -1.775 [-100.000, 17.020], mean action: 1.649 [0.000, 3.000], mean observation: -0.007 [-1.408, 4.868], loss: 1.844203, mean_absolute_error: 9.241652, mean_q: 0.949117, mean_eps: 0.930954
   76839/2000000: episode: 807, duration: 1.119s, episode steps: 82, steps per second: 73, episode reward: -269.818, mean reward: -3.290 [-100.000, 4.241], mean action: 1.585 [0.000, 3.000], mean observation: 0.076 [-1.514, 5.174], loss: 0.954522, mean_absolute_error: 8.628648, mean_q: 1.680009, mean_eps: 0.930882
   76971/2000000: episode: 808, duration: 1.876s, episode steps: 132, steps per second: 70, episode reward: -497.950, mean reward: -3.772 [-100.000, 2.155], mean action: 1.545 [0.000, 3.000], mean observation: 0.052 [-2.035, 1.973], loss: 1.390621, mean_absolute_error: 9.258824, mean_q: 1.421659, mean_eps: 0.930786
   77064/2000000: episode: 809, duration: 1.357s, episode steps: 93, steps per second: 69, episode reward: -33.010, mean reward: -0.355 [-100.000, 104.457], mean action: 1.559 [0.000, 3.000], mean observation: -0.104 [-2.245, 1.501], loss: 1.664834, mean_absolute_error: 8.716526, mean_q: 0.646979, mean_eps: 0.930686
   77147/2000000: episode: 810, duration: 1.199s, episode steps: 83, steps per second: 69, episode reward: -193.682, mean reward: -2.334 [-100.000, 29.013], mean action: 1.699 [0.000, 3.000], mean observation: -0.035 [-5.605, 1.000], loss: 1.668754, mean_absolute_error: 10.405584, mean_q: -1.107587, mean_eps: 0.930606
   77267/2000000: episode: 811, duration: 1.654s, episode steps: 120, steps per second: 73, episode reward: -152.287, mean reward: -1.269 [-100.000, 6.685], mean action: 1.558 [0.000, 3.000], mean observation: -0.007 [-1.376, 4.112], loss: 1.312002, mean_absolute_error: 9.110204, mean_q: 0.943802, mean_eps: 0.930515
   77360/2000000: episode: 812, duration: 1.373s, episode steps: 93, steps per second: 68, episode reward: -345.463, mean reward: -3.715 [-100.000, 2.149], mean action: 1.602 [0.000, 3.000], mean observation: -0.053 [-4.452, 1.272], loss: 1.513020, mean_absolute_error: 9.271631, mean_q: 0.961489, mean_eps: 0.930419
   77445/2000000: episode: 813, duration: 1.233s, episode steps: 85, steps per second: 69, episode reward: -92.292, mean reward: -1.086 [-100.000, 17.446], mean action: 1.541 [0.000, 3.000], mean observation: 0.017 [-1.137, 1.000], loss: 1.334566, mean_absolute_error: 9.973496, mean_q: 0.171064, mean_eps: 0.930338
   77549/2000000: episode: 814, duration: 1.477s, episode steps: 104, steps per second: 70, episode reward: -163.783, mean reward: -1.575 [-100.000, 9.391], mean action: 1.596 [0.000, 3.000], mean observation: 0.108 [-1.154, 1.013], loss: 1.489494, mean_absolute_error: 9.368324, mean_q: 0.472214, mean_eps: 0.930252
   77657/2000000: episode: 815, duration: 1.537s, episode steps: 108, steps per second: 70, episode reward: -216.491, mean reward: -2.005 [-100.000, 6.515], mean action: 1.556 [0.000, 3.000], mean observation: -0.141 [-1.360, 3.215], loss: 1.912286, mean_absolute_error: 9.467170, mean_q: 1.131582, mean_eps: 0.930156
   77775/2000000: episode: 816, duration: 1.647s, episode steps: 118, steps per second: 72, episode reward: -163.375, mean reward: -1.385 [-100.000, 6.560], mean action: 1.517 [0.000, 3.000], mean observation: 0.010 [-1.330, 4.200], loss: 1.564857, mean_absolute_error: 9.146903, mean_q: 1.754318, mean_eps: 0.930056
   77877/2000000: episode: 817, duration: 1.464s, episode steps: 102, steps per second: 70, episode reward: -150.114, mean reward: -1.472 [-100.000, 5.708], mean action: 1.549 [0.000, 3.000], mean observation: 0.057 [-3.499, 1.000], loss: 1.211751, mean_absolute_error: 9.064887, mean_q: 1.217276, mean_eps: 0.929957
   77938/2000000: episode: 818, duration: 0.869s, episode steps: 61, steps per second: 70, episode reward: -113.087, mean reward: -1.854 [-100.000, 20.358], mean action: 1.508 [0.000, 3.000], mean observation: -0.059 [-1.536, 1.000], loss: 2.369210, mean_absolute_error: 12.240133, mean_q: -2.728730, mean_eps: 0.929883
   78051/2000000: episode: 819, duration: 1.573s, episode steps: 113, steps per second: 72, episode reward: -203.489, mean reward: -1.801 [-100.000, 11.050], mean action: 1.451 [0.000, 3.000], mean observation: 0.169 [-1.452, 1.000], loss: 1.615745, mean_absolute_error: 9.562281, mean_q: 0.157099, mean_eps: 0.929805
   78127/2000000: episode: 820, duration: 1.075s, episode steps: 76, steps per second: 71, episode reward: -127.888, mean reward: -1.683 [-100.000, 80.565], mean action: 1.461 [0.000, 3.000], mean observation: 0.014 [-1.527, 3.194], loss: 1.380722, mean_absolute_error: 9.678586, mean_q: 0.464705, mean_eps: 0.929721
   78214/2000000: episode: 821, duration: 1.243s, episode steps: 87, steps per second: 70, episode reward: -132.680, mean reward: -1.525 [-100.000, 8.682], mean action: 1.517 [0.000, 3.000], mean observation: 0.043 [-1.133, 4.050], loss: 1.054093, mean_absolute_error: 8.806815, mean_q: 2.503379, mean_eps: 0.929647
   78313/2000000: episode: 822, duration: 1.413s, episode steps: 99, steps per second: 70, episode reward: -49.412, mean reward: -0.499 [-100.000, 96.290], mean action: 1.414 [0.000, 3.000], mean observation: 0.048 [-1.368, 2.155], loss: 2.251788, mean_absolute_error: 10.589143, mean_q: -0.442740, mean_eps: 0.929562
   78422/2000000: episode: 823, duration: 1.533s, episode steps: 109, steps per second: 71, episode reward: -307.274, mean reward: -2.819 [-100.000, 1.105], mean action: 1.596 [0.000, 3.000], mean observation: 0.124 [-1.508, 1.292], loss: 1.369613, mean_absolute_error: 8.621096, mean_q: 1.027368, mean_eps: 0.929469
   78497/2000000: episode: 824, duration: 1.077s, episode steps: 75, steps per second: 70, episode reward: -96.903, mean reward: -1.292 [-100.000, 17.190], mean action: 1.400 [0.000, 3.000], mean observation: -0.018 [-1.248, 1.000], loss: 1.560345, mean_absolute_error: 9.425075, mean_q: 0.464536, mean_eps: 0.929386
   78582/2000000: episode: 825, duration: 1.202s, episode steps: 85, steps per second: 71, episode reward: -296.004, mean reward: -3.482 [-100.000, 19.479], mean action: 1.682 [0.000, 3.000], mean observation: 0.004 [-6.095, 1.000], loss: 1.322019, mean_absolute_error: 9.144467, mean_q: 1.344587, mean_eps: 0.929314
   78661/2000000: episode: 826, duration: 1.122s, episode steps: 79, steps per second: 70, episode reward: -161.213, mean reward: -2.041 [-100.000, 8.529], mean action: 1.544 [0.000, 3.000], mean observation: 0.021 [-4.415, 1.000], loss: 1.198296, mean_absolute_error: 9.190528, mean_q: 1.148234, mean_eps: 0.929240
   78767/2000000: episode: 827, duration: 1.501s, episode steps: 106, steps per second: 71, episode reward: -252.138, mean reward: -2.379 [-100.000, 5.363], mean action: 1.406 [0.000, 3.000], mean observation: -0.004 [-4.381, 1.000], loss: 1.638465, mean_absolute_error: 9.126147, mean_q: 0.896038, mean_eps: 0.929157
   78905/2000000: episode: 828, duration: 2.020s, episode steps: 138, steps per second: 68, episode reward: -99.662, mean reward: -0.722 [-100.000, 72.205], mean action: 1.696 [0.000, 3.000], mean observation: -0.003 [-1.229, 2.039], loss: 1.370576, mean_absolute_error: 8.744574, mean_q: 1.036024, mean_eps: 0.929048
   78983/2000000: episode: 829, duration: 1.113s, episode steps: 78, steps per second: 70, episode reward: -118.612, mean reward: -1.521 [-100.000, 16.822], mean action: 1.397 [0.000, 3.000], mean observation: -0.001 [-1.385, 1.000], loss: 1.159148, mean_absolute_error: 9.444910, mean_q: 1.082065, mean_eps: 0.928950
   79077/2000000: episode: 830, duration: 1.359s, episode steps: 94, steps per second: 69, episode reward: -146.740, mean reward: -1.561 [-100.000, 66.151], mean action: 1.447 [0.000, 3.000], mean observation: -0.154 [-5.204, 1.000], loss: 1.982337, mean_absolute_error: 9.479649, mean_q: 1.179463, mean_eps: 0.928873
   79192/2000000: episode: 831, duration: 1.675s, episode steps: 115, steps per second: 69, episode reward: -171.946, mean reward: -1.495 [-100.000, 8.235], mean action: 1.530 [0.000, 3.000], mean observation: 0.027 [-4.798, 1.000], loss: 1.257214, mean_absolute_error: 9.138083, mean_q: 0.124754, mean_eps: 0.928779
   79304/2000000: episode: 832, duration: 1.630s, episode steps: 112, steps per second: 69, episode reward: -138.879, mean reward: -1.240 [-100.000, 13.019], mean action: 1.464 [0.000, 3.000], mean observation: -0.019 [-4.043, 1.000], loss: 1.460194, mean_absolute_error: 9.166208, mean_q: 0.709073, mean_eps: 0.928679
   79387/2000000: episode: 833, duration: 1.192s, episode steps: 83, steps per second: 70, episode reward: -115.961, mean reward: -1.397 [-100.000, 13.821], mean action: 1.614 [0.000, 3.000], mean observation: -0.058 [-1.287, 4.465], loss: 1.212235, mean_absolute_error: 8.925977, mean_q: 1.254537, mean_eps: 0.928590
   79471/2000000: episode: 834, duration: 1.194s, episode steps: 84, steps per second: 70, episode reward: -200.189, mean reward: -2.383 [-100.000, 9.069], mean action: 1.536 [0.000, 3.000], mean observation: 0.069 [-5.119, 1.000], loss: 1.148342, mean_absolute_error: 8.619812, mean_q: 2.428313, mean_eps: 0.928515
   79561/2000000: episode: 835, duration: 1.308s, episode steps: 90, steps per second: 69, episode reward: -206.691, mean reward: -2.297 [-100.000, 5.451], mean action: 1.744 [0.000, 3.000], mean observation: -0.126 [-1.435, 1.000], loss: 1.228829, mean_absolute_error: 8.868392, mean_q: 1.273494, mean_eps: 0.928436
   79660/2000000: episode: 836, duration: 1.407s, episode steps: 99, steps per second: 70, episode reward: -188.710, mean reward: -1.906 [-100.000, 5.813], mean action: 1.535 [0.000, 3.000], mean observation: 0.033 [-1.361, 4.254], loss: 1.458601, mean_absolute_error: 8.451521, mean_q: 1.755062, mean_eps: 0.928351
   79741/2000000: episode: 837, duration: 1.194s, episode steps: 81, steps per second: 68, episode reward: -285.358, mean reward: -3.523 [-100.000, 41.029], mean action: 1.568 [0.000, 3.000], mean observation: -0.001 [-1.548, 3.604], loss: 1.298796, mean_absolute_error: 9.239265, mean_q: 0.791158, mean_eps: 0.928270
   79815/2000000: episode: 838, duration: 1.028s, episode steps: 74, steps per second: 72, episode reward: -189.530, mean reward: -2.561 [-100.000, 6.514], mean action: 1.554 [0.000, 3.000], mean observation: -0.000 [-1.485, 4.998], loss: 1.413488, mean_absolute_error: 8.427624, mean_q: 2.167258, mean_eps: 0.928200
   79878/2000000: episode: 839, duration: 0.903s, episode steps: 63, steps per second: 70, episode reward: -172.032, mean reward: -2.731 [-100.000, 25.909], mean action: 1.222 [0.000, 3.000], mean observation: -0.054 [-2.993, 1.000], loss: 1.248138, mean_absolute_error: 8.937352, mean_q: 0.967347, mean_eps: 0.928139
   79943/2000000: episode: 840, duration: 0.914s, episode steps: 65, steps per second: 71, episode reward: -103.014, mean reward: -1.585 [-100.000, 13.046], mean action: 1.708 [0.000, 3.000], mean observation: -0.154 [-4.201, 1.000], loss: 1.380927, mean_absolute_error: 9.400872, mean_q: 0.020917, mean_eps: 0.928081
   80006/2000000: episode: 841, duration: 0.924s, episode steps: 63, steps per second: 68, episode reward: -220.080, mean reward: -3.493 [-100.000, 17.723], mean action: 1.492 [0.000, 3.000], mean observation: -0.024 [-5.893, 1.071], loss: 1.438350, mean_absolute_error: 10.067867, mean_q: 0.250123, mean_eps: 0.928023
   80112/2000000: episode: 842, duration: 1.508s, episode steps: 106, steps per second: 70, episode reward: -132.106, mean reward: -1.246 [-100.000, 11.100], mean action: 1.594 [0.000, 3.000], mean observation: -0.101 [-1.297, 1.000], loss: 1.497267, mean_absolute_error: 9.331478, mean_q: 2.501618, mean_eps: 0.927948
   80185/2000000: episode: 843, duration: 1.099s, episode steps: 73, steps per second: 66, episode reward: -301.601, mean reward: -4.132 [-100.000, 7.034], mean action: 1.603 [0.000, 3.000], mean observation: -0.066 [-1.963, 1.339], loss: 1.218159, mean_absolute_error: 9.255702, mean_q: 0.557437, mean_eps: 0.927867
   80261/2000000: episode: 844, duration: 1.086s, episode steps: 76, steps per second: 70, episode reward: -267.936, mean reward: -3.525 [-100.000, 30.871], mean action: 1.579 [0.000, 3.000], mean observation: -0.055 [-4.239, 1.000], loss: 1.515220, mean_absolute_error: 9.055139, mean_q: 1.034999, mean_eps: 0.927798
   80323/2000000: episode: 845, duration: 0.858s, episode steps: 62, steps per second: 72, episode reward: -123.599, mean reward: -1.994 [-100.000, 6.348], mean action: 1.419 [0.000, 3.000], mean observation: 0.016 [-4.151, 1.000], loss: 0.956410, mean_absolute_error: 8.071350, mean_q: 2.571894, mean_eps: 0.927737
   80453/2000000: episode: 846, duration: 1.880s, episode steps: 130, steps per second: 69, episode reward: -246.418, mean reward: -1.896 [-100.000, 7.576], mean action: 1.631 [0.000, 3.000], mean observation: 0.039 [-4.440, 1.029], loss: 1.314545, mean_absolute_error: 9.641971, mean_q: 1.693324, mean_eps: 0.927651
   80534/2000000: episode: 847, duration: 1.152s, episode steps: 81, steps per second: 70, episode reward: -157.980, mean reward: -1.950 [-100.000, 28.536], mean action: 1.827 [0.000, 3.000], mean observation: -0.102 [-1.329, 1.238], loss: 1.824902, mean_absolute_error: 10.838034, mean_q: 0.646378, mean_eps: 0.927555
   80603/2000000: episode: 848, duration: 0.996s, episode steps: 69, steps per second: 69, episode reward: -181.054, mean reward: -2.624 [-100.000, 6.062], mean action: 1.623 [0.000, 3.000], mean observation: -0.074 [-3.895, 1.000], loss: 1.615094, mean_absolute_error: 10.211931, mean_q: -0.068061, mean_eps: 0.927489
   80666/2000000: episode: 849, duration: 0.914s, episode steps: 63, steps per second: 69, episode reward: -147.248, mean reward: -2.337 [-100.000, 7.933], mean action: 1.444 [0.000, 3.000], mean observation: -0.059 [-1.396, 4.526], loss: 1.668290, mean_absolute_error: 10.406349, mean_q: -0.103046, mean_eps: 0.927429
   80752/2000000: episode: 850, duration: 1.230s, episode steps: 86, steps per second: 70, episode reward: -164.882, mean reward: -1.917 [-100.000, 5.779], mean action: 1.419 [0.000, 3.000], mean observation: -0.116 [-1.393, 4.789], loss: 1.318216, mean_absolute_error: 9.905518, mean_q: 1.395330, mean_eps: 0.927363
   80879/2000000: episode: 851, duration: 1.836s, episode steps: 127, steps per second: 69, episode reward: -114.472, mean reward: -0.901 [-100.000, 18.801], mean action: 1.598 [0.000, 3.000], mean observation: 0.045 [-1.248, 3.124], loss: 1.911591, mean_absolute_error: 10.144386, mean_q: 1.167879, mean_eps: 0.927267
   80950/2000000: episode: 852, duration: 1.024s, episode steps: 71, steps per second: 69, episode reward: -120.442, mean reward: -1.696 [-100.000, 15.682], mean action: 1.296 [0.000, 3.000], mean observation: 0.058 [-4.343, 1.000], loss: 1.619333, mean_absolute_error: 10.123334, mean_q: 0.685129, mean_eps: 0.927177
   81063/2000000: episode: 853, duration: 1.595s, episode steps: 113, steps per second: 71, episode reward: -166.319, mean reward: -1.472 [-100.000, 19.620], mean action: 1.531 [0.000, 3.000], mean observation: -0.138 [-1.245, 3.108], loss: 1.584152, mean_absolute_error: 9.753568, mean_q: 1.290311, mean_eps: 0.927095
   81176/2000000: episode: 854, duration: 1.657s, episode steps: 113, steps per second: 68, episode reward: -214.918, mean reward: -1.902 [-100.000, 6.971], mean action: 1.619 [0.000, 3.000], mean observation: -0.067 [-5.254, 1.018], loss: 1.399617, mean_absolute_error: 9.966078, mean_q: 1.229857, mean_eps: 0.926994
   81255/2000000: episode: 855, duration: 1.156s, episode steps: 79, steps per second: 68, episode reward: -118.413, mean reward: -1.499 [-100.000, 7.412], mean action: 1.608 [0.000, 3.000], mean observation: -0.095 [-1.238, 4.534], loss: 1.213212, mean_absolute_error: 10.462391, mean_q: -0.043391, mean_eps: 0.926907
   81347/2000000: episode: 856, duration: 1.329s, episode steps: 92, steps per second: 69, episode reward: -213.155, mean reward: -2.317 [-100.000, 12.906], mean action: 1.685 [0.000, 3.000], mean observation: -0.103 [-4.649, 1.000], loss: 1.354680, mean_absolute_error: 9.982941, mean_q: 1.818073, mean_eps: 0.926830
   81422/2000000: episode: 857, duration: 1.095s, episode steps: 75, steps per second: 68, episode reward: -364.899, mean reward: -4.865 [-100.000, 14.272], mean action: 1.467 [0.000, 3.000], mean observation: -0.051 [-1.707, 4.351], loss: 1.825044, mean_absolute_error: 10.358247, mean_q: 0.071261, mean_eps: 0.926754
   81498/2000000: episode: 858, duration: 1.080s, episode steps: 76, steps per second: 70, episode reward: -177.193, mean reward: -2.331 [-100.000, 8.205], mean action: 1.461 [0.000, 3.000], mean observation: 0.087 [-1.407, 1.000], loss: 1.824509, mean_absolute_error: 10.681300, mean_q: 0.737633, mean_eps: 0.926686
   81593/2000000: episode: 859, duration: 1.368s, episode steps: 95, steps per second: 69, episode reward: -169.471, mean reward: -1.784 [-100.000, 11.222], mean action: 1.537 [0.000, 3.000], mean observation: -0.054 [-3.039, 1.000], loss: 1.451545, mean_absolute_error: 9.433431, mean_q: 1.509321, mean_eps: 0.926609
   81669/2000000: episode: 860, duration: 1.087s, episode steps: 76, steps per second: 70, episode reward: -159.620, mean reward: -2.100 [-100.000, 9.809], mean action: 1.500 [0.000, 3.000], mean observation: -0.120 [-1.355, 3.730], loss: 1.871347, mean_absolute_error: 10.551141, mean_q: 0.834160, mean_eps: 0.926531
   81783/2000000: episode: 861, duration: 1.598s, episode steps: 114, steps per second: 71, episode reward: -160.083, mean reward: -1.404 [-100.000, 12.236], mean action: 1.421 [0.000, 3.000], mean observation: -0.110 [-1.574, 4.712], loss: 1.637828, mean_absolute_error: 9.981965, mean_q: 1.203097, mean_eps: 0.926447
   81856/2000000: episode: 862, duration: 1.106s, episode steps: 73, steps per second: 66, episode reward: -194.022, mean reward: -2.658 [-100.000, 37.790], mean action: 1.507 [0.000, 3.000], mean observation: -0.036 [-1.374, 5.172], loss: 1.400817, mean_absolute_error: 10.097326, mean_q: 1.355533, mean_eps: 0.926364
   81973/2000000: episode: 863, duration: 1.706s, episode steps: 117, steps per second: 69, episode reward: -196.358, mean reward: -1.678 [-100.000, 8.276], mean action: 1.692 [0.000, 3.000], mean observation: -0.030 [-1.379, 1.026], loss: 1.446915, mean_absolute_error: 10.117435, mean_q: 0.921445, mean_eps: 0.926277
   82065/2000000: episode: 864, duration: 1.320s, episode steps: 92, steps per second: 70, episode reward: -128.208, mean reward: -1.394 [-100.000, 10.646], mean action: 1.500 [0.000, 3.000], mean observation: 0.009 [-4.255, 1.000], loss: 1.475792, mean_absolute_error: 10.092752, mean_q: 0.760701, mean_eps: 0.926182
   82153/2000000: episode: 865, duration: 1.277s, episode steps: 88, steps per second: 69, episode reward: -170.227, mean reward: -1.934 [-100.000, 9.716], mean action: 1.420 [0.000, 3.000], mean observation: -0.033 [-3.739, 1.000], loss: 1.281064, mean_absolute_error: 9.300387, mean_q: 1.343017, mean_eps: 0.926101
   82234/2000000: episode: 866, duration: 1.146s, episode steps: 81, steps per second: 71, episode reward: -161.466, mean reward: -1.993 [-100.000, 38.256], mean action: 1.593 [0.000, 3.000], mean observation: -0.062 [-2.075, 1.000], loss: 1.737586, mean_absolute_error: 10.075049, mean_q: 0.731689, mean_eps: 0.926025
   82300/2000000: episode: 867, duration: 0.972s, episode steps: 66, steps per second: 68, episode reward: -114.485, mean reward: -1.735 [-100.000, 33.643], mean action: 1.667 [0.000, 3.000], mean observation: -0.092 [-4.633, 1.000], loss: 1.168107, mean_absolute_error: 9.802694, mean_q: 0.943726, mean_eps: 0.925961
   82361/2000000: episode: 868, duration: 0.906s, episode steps: 61, steps per second: 67, episode reward: -179.912, mean reward: -2.949 [-100.000, 7.347], mean action: 1.525 [0.000, 3.000], mean observation: -0.088 [-1.440, 1.000], loss: 2.084366, mean_absolute_error: 10.810042, mean_q: 0.747774, mean_eps: 0.925903
   82470/2000000: episode: 869, duration: 1.546s, episode steps: 109, steps per second: 71, episode reward: -345.420, mean reward: -3.169 [-100.000, 1.088], mean action: 1.587 [0.000, 3.000], mean observation: 0.131 [-1.139, 1.630], loss: 1.675853, mean_absolute_error: 9.870857, mean_q: 0.628531, mean_eps: 0.925826
   82538/2000000: episode: 870, duration: 0.989s, episode steps: 68, steps per second: 69, episode reward: -127.815, mean reward: -1.880 [-100.000, 9.501], mean action: 1.471 [0.000, 3.000], mean observation: 0.011 [-4.253, 1.000], loss: 1.630717, mean_absolute_error: 9.923877, mean_q: 0.611805, mean_eps: 0.925746
   82638/2000000: episode: 871, duration: 1.451s, episode steps: 100, steps per second: 69, episode reward: -215.789, mean reward: -2.158 [-100.000, 8.427], mean action: 1.460 [0.000, 3.000], mean observation: -0.034 [-1.200, 3.306], loss: 2.323483, mean_absolute_error: 11.045342, mean_q: 0.394410, mean_eps: 0.925671
   82723/2000000: episode: 872, duration: 1.224s, episode steps: 85, steps per second: 69, episode reward: -53.172, mean reward: -0.626 [-100.000, 103.061], mean action: 1.471 [0.000, 3.000], mean observation: 0.009 [-1.510, 1.702], loss: 1.888191, mean_absolute_error: 10.075539, mean_q: 0.172080, mean_eps: 0.925588
   82835/2000000: episode: 873, duration: 1.579s, episode steps: 112, steps per second: 71, episode reward: -317.892, mean reward: -2.838 [-100.000, 1.013], mean action: 1.455 [0.000, 3.000], mean observation: -0.040 [-1.469, 1.030], loss: 1.833685, mean_absolute_error: 10.094406, mean_q: 0.675841, mean_eps: 0.925500
   82909/2000000: episode: 874, duration: 1.066s, episode steps: 74, steps per second: 69, episode reward: -153.960, mean reward: -2.081 [-100.000, 7.563], mean action: 1.514 [0.000, 3.000], mean observation: -0.046 [-1.288, 4.162], loss: 1.697060, mean_absolute_error: 9.519766, mean_q: 2.276305, mean_eps: 0.925415
   82982/2000000: episode: 875, duration: 1.035s, episode steps: 73, steps per second: 71, episode reward: -180.635, mean reward: -2.474 [-100.000, 11.785], mean action: 1.685 [0.000, 3.000], mean observation: -0.119 [-1.712, 1.000], loss: 2.232445, mean_absolute_error: 10.556376, mean_q: 0.951296, mean_eps: 0.925349
   83115/2000000: episode: 876, duration: 1.883s, episode steps: 133, steps per second: 71, episode reward: -193.755, mean reward: -1.457 [-100.000, 8.080], mean action: 1.526 [0.000, 3.000], mean observation: 0.037 [-1.502, 5.458], loss: 1.851884, mean_absolute_error: 10.293067, mean_q: 0.521635, mean_eps: 0.925257
   83225/2000000: episode: 877, duration: 1.611s, episode steps: 110, steps per second: 68, episode reward: -144.900, mean reward: -1.317 [-100.000, 20.139], mean action: 1.655 [0.000, 3.000], mean observation: 0.185 [-3.235, 1.000], loss: 1.408233, mean_absolute_error: 9.774114, mean_q: 1.095030, mean_eps: 0.925147
   83331/2000000: episode: 878, duration: 1.481s, episode steps: 106, steps per second: 72, episode reward: -202.724, mean reward: -1.912 [-100.000, 7.045], mean action: 1.623 [0.000, 3.000], mean observation: -0.034 [-3.946, 1.000], loss: 1.496913, mean_absolute_error: 9.580827, mean_q: 1.021577, mean_eps: 0.925050
   83438/2000000: episode: 879, duration: 1.558s, episode steps: 107, steps per second: 69, episode reward: -157.320, mean reward: -1.470 [-100.000, 8.439], mean action: 1.505 [0.000, 3.000], mean observation: 0.027 [-1.358, 5.388], loss: 1.382510, mean_absolute_error: 10.452697, mean_q: 1.226053, mean_eps: 0.924954
   83560/2000000: episode: 880, duration: 1.763s, episode steps: 122, steps per second: 69, episode reward: -420.504, mean reward: -3.447 [-100.000, 111.096], mean action: 1.500 [0.000, 3.000], mean observation: 0.188 [-1.437, 3.701], loss: 2.084385, mean_absolute_error: 10.563000, mean_q: 0.308138, mean_eps: 0.924852
   83670/2000000: episode: 881, duration: 1.588s, episode steps: 110, steps per second: 69, episode reward: -147.919, mean reward: -1.345 [-100.000, 13.505], mean action: 1.555 [0.000, 3.000], mean observation: 0.034 [-3.944, 1.000], loss: 1.254880, mean_absolute_error: 9.351858, mean_q: 1.068590, mean_eps: 0.924747
   83743/2000000: episode: 882, duration: 1.021s, episode steps: 73, steps per second: 71, episode reward: -237.137, mean reward: -3.248 [-100.000, 6.928], mean action: 1.438 [0.000, 3.000], mean observation: -0.056 [-1.563, 4.071], loss: 1.443085, mean_absolute_error: 9.805481, mean_q: 0.401650, mean_eps: 0.924665
   83863/2000000: episode: 883, duration: 1.714s, episode steps: 120, steps per second: 70, episode reward: -429.994, mean reward: -3.583 [-100.000, 8.777], mean action: 1.325 [0.000, 3.000], mean observation: 0.076 [-1.468, 4.770], loss: 1.847336, mean_absolute_error: 10.555119, mean_q: 0.353007, mean_eps: 0.924578
   83945/2000000: episode: 884, duration: 1.200s, episode steps: 82, steps per second: 68, episode reward: -191.908, mean reward: -2.340 [-100.000, 14.430], mean action: 1.646 [0.000, 3.000], mean observation: 0.028 [-1.375, 4.487], loss: 1.657963, mean_absolute_error: 10.630648, mean_q: -0.014619, mean_eps: 0.924486
   84028/2000000: episode: 885, duration: 1.201s, episode steps: 83, steps per second: 69, episode reward: -62.231, mean reward: -0.750 [-100.000, 112.994], mean action: 1.759 [0.000, 3.000], mean observation: 0.049 [-1.667, 1.752], loss: 1.660755, mean_absolute_error: 10.191954, mean_q: 1.199111, mean_eps: 0.924413
   84105/2000000: episode: 886, duration: 1.143s, episode steps: 77, steps per second: 67, episode reward: -160.367, mean reward: -2.083 [-100.000, 44.626], mean action: 1.584 [0.000, 3.000], mean observation: -0.052 [-1.756, 1.000], loss: 1.999921, mean_absolute_error: 11.445553, mean_q: -1.182185, mean_eps: 0.924341
   84179/2000000: episode: 887, duration: 1.024s, episode steps: 74, steps per second: 72, episode reward: -158.120, mean reward: -2.137 [-100.000, 10.105], mean action: 1.284 [0.000, 3.000], mean observation: 0.123 [-1.333, 1.000], loss: 2.011049, mean_absolute_error: 11.709688, mean_q: -0.175623, mean_eps: 0.924272
   84279/2000000: episode: 888, duration: 1.426s, episode steps: 100, steps per second: 70, episode reward: -144.030, mean reward: -1.440 [-100.000, 18.571], mean action: 1.540 [0.000, 3.000], mean observation: -0.054 [-1.180, 2.928], loss: 2.066090, mean_absolute_error: 10.190691, mean_q: 0.963435, mean_eps: 0.924195
   84431/2000000: episode: 889, duration: 2.164s, episode steps: 152, steps per second: 70, episode reward: -296.646, mean reward: -1.952 [-100.000, 78.190], mean action: 1.658 [0.000, 3.000], mean observation: -0.182 [-2.246, 1.010], loss: 1.515066, mean_absolute_error: 10.349792, mean_q: 0.694344, mean_eps: 0.924081
   84582/2000000: episode: 890, duration: 2.161s, episode steps: 151, steps per second: 70, episode reward: -64.056, mean reward: -0.424 [-100.000, 116.082], mean action: 1.656 [0.000, 3.000], mean observation: 0.083 [-1.720, 1.000], loss: 1.218562, mean_absolute_error: 10.398222, mean_q: 0.558297, mean_eps: 0.923945
   84653/2000000: episode: 891, duration: 1.062s, episode steps: 71, steps per second: 67, episode reward: -172.070, mean reward: -2.424 [-100.000, 8.130], mean action: 1.549 [0.000, 3.000], mean observation: 0.060 [-1.404, 4.837], loss: 1.801995, mean_absolute_error: 9.817979, mean_q: 1.627491, mean_eps: 0.923844
   84763/2000000: episode: 892, duration: 1.571s, episode steps: 110, steps per second: 70, episode reward: -144.390, mean reward: -1.313 [-100.000, 7.171], mean action: 1.591 [0.000, 3.000], mean observation: -0.077 [-1.214, 1.000], loss: 1.537649, mean_absolute_error: 10.711735, mean_q: 0.263184, mean_eps: 0.923763
   84859/2000000: episode: 893, duration: 1.362s, episode steps: 96, steps per second: 70, episode reward: -516.373, mean reward: -5.379 [-100.000, 0.843], mean action: 1.479 [0.000, 3.000], mean observation: 0.029 [-2.251, 2.340], loss: 1.316395, mean_absolute_error: 10.619329, mean_q: 0.285428, mean_eps: 0.923671
   84961/2000000: episode: 894, duration: 1.484s, episode steps: 102, steps per second: 69, episode reward: -210.559, mean reward: -2.064 [-100.000, 8.378], mean action: 1.716 [0.000, 3.000], mean observation: -0.020 [-1.487, 4.511], loss: 2.065145, mean_absolute_error: 10.515537, mean_q: -0.371675, mean_eps: 0.923581
   85099/2000000: episode: 895, duration: 1.946s, episode steps: 138, steps per second: 71, episode reward: -115.599, mean reward: -0.838 [-100.000, 54.082], mean action: 1.674 [0.000, 3.000], mean observation: -0.045 [-4.380, 1.016], loss: 1.569729, mean_absolute_error: 11.049452, mean_q: -0.912654, mean_eps: 0.923473
   85181/2000000: episode: 896, duration: 1.194s, episode steps: 82, steps per second: 69, episode reward: -124.110, mean reward: -1.514 [-100.000, 13.105], mean action: 1.573 [0.000, 3.000], mean observation: -0.039 [-1.448, 1.000], loss: 1.496762, mean_absolute_error: 10.630572, mean_q: 0.325612, mean_eps: 0.923374
   85329/2000000: episode: 897, duration: 2.121s, episode steps: 148, steps per second: 70, episode reward: -177.315, mean reward: -1.198 [-100.000, 7.482], mean action: 1.480 [0.000, 3.000], mean observation: 0.128 [-3.951, 1.122], loss: 1.389160, mean_absolute_error: 9.342239, mean_q: 1.888993, mean_eps: 0.923270
   85387/2000000: episode: 898, duration: 0.830s, episode steps: 58, steps per second: 70, episode reward: -119.599, mean reward: -2.062 [-100.000, 7.857], mean action: 1.397 [0.000, 3.000], mean observation: -0.137 [-5.963, 1.000], loss: 1.268240, mean_absolute_error: 9.884638, mean_q: 2.893168, mean_eps: 0.923178
   85498/2000000: episode: 899, duration: 1.584s, episode steps: 111, steps per second: 70, episode reward: -91.387, mean reward: -0.823 [-100.000, 17.974], mean action: 1.414 [0.000, 3.000], mean observation: 0.028 [-1.382, 1.036], loss: 1.535602, mean_absolute_error: 10.114853, mean_q: 0.764051, mean_eps: 0.923102
   85584/2000000: episode: 900, duration: 1.246s, episode steps: 86, steps per second: 69, episode reward: -98.398, mean reward: -1.144 [-100.000, 8.907], mean action: 1.419 [0.000, 3.000], mean observation: -0.066 [-1.122, 1.000], loss: 1.742125, mean_absolute_error: 9.955576, mean_q: 1.404778, mean_eps: 0.923014
   85688/2000000: episode: 901, duration: 1.527s, episode steps: 104, steps per second: 68, episode reward: -159.566, mean reward: -1.534 [-100.000, 12.637], mean action: 1.529 [0.000, 3.000], mean observation: 0.092 [-5.283, 1.000], loss: 1.498240, mean_absolute_error: 9.563232, mean_q: 2.098139, mean_eps: 0.922929
   85788/2000000: episode: 902, duration: 1.481s, episode steps: 100, steps per second: 68, episode reward: -219.155, mean reward: -2.192 [-100.000, 5.893], mean action: 1.570 [0.000, 3.000], mean observation: 0.107 [-1.518, 4.751], loss: 1.303816, mean_absolute_error: 9.362527, mean_q: 1.531247, mean_eps: 0.922838
   85879/2000000: episode: 903, duration: 1.317s, episode steps: 91, steps per second: 69, episode reward: -139.406, mean reward: -1.532 [-100.000, 13.615], mean action: 1.538 [0.000, 3.000], mean observation: 0.104 [-3.143, 1.000], loss: 1.520099, mean_absolute_error: 9.521781, mean_q: 1.503240, mean_eps: 0.922751
   85946/2000000: episode: 904, duration: 0.973s, episode steps: 67, steps per second: 69, episode reward: -137.533, mean reward: -2.053 [-100.000, 9.671], mean action: 1.373 [0.000, 3.000], mean observation: -0.055 [-5.392, 1.000], loss: 1.449671, mean_absolute_error: 10.386890, mean_q: 0.575959, mean_eps: 0.922679
   86038/2000000: episode: 905, duration: 1.299s, episode steps: 92, steps per second: 71, episode reward: -121.499, mean reward: -1.321 [-100.000, 7.347], mean action: 1.620 [0.000, 3.000], mean observation: -0.055 [-3.900, 1.000], loss: 1.975867, mean_absolute_error: 10.262176, mean_q: 0.733199, mean_eps: 0.922607
   86103/2000000: episode: 906, duration: 0.925s, episode steps: 65, steps per second: 70, episode reward: -178.527, mean reward: -2.747 [-100.000, 7.132], mean action: 1.646 [0.000, 3.000], mean observation: -0.102 [-1.352, 1.000], loss: 1.636068, mean_absolute_error: 11.022812, mean_q: -0.235580, mean_eps: 0.922537
   86184/2000000: episode: 907, duration: 1.191s, episode steps: 81, steps per second: 68, episode reward: -116.742, mean reward: -1.441 [-100.000, 7.479], mean action: 1.630 [0.000, 3.000], mean observation: -0.016 [-1.278, 4.353], loss: 1.464443, mean_absolute_error: 9.453395, mean_q: 1.108017, mean_eps: 0.922472
   86287/2000000: episode: 908, duration: 1.495s, episode steps: 103, steps per second: 69, episode reward: -183.579, mean reward: -1.782 [-100.000, 16.362], mean action: 1.573 [0.000, 3.000], mean observation: -0.055 [-1.442, 3.823], loss: 1.458189, mean_absolute_error: 9.867285, mean_q: 0.917233, mean_eps: 0.922389
   86418/2000000: episode: 909, duration: 1.893s, episode steps: 131, steps per second: 69, episode reward: -266.498, mean reward: -2.034 [-100.000, 43.712], mean action: 1.504 [0.000, 3.000], mean observation: 0.113 [-1.705, 4.586], loss: 1.425160, mean_absolute_error: 10.144833, mean_q: 0.526853, mean_eps: 0.922283
   86537/2000000: episode: 910, duration: 1.716s, episode steps: 119, steps per second: 69, episode reward: -318.814, mean reward: -2.679 [-100.000, 4.859], mean action: 1.580 [0.000, 3.000], mean observation: -0.069 [-4.335, 1.006], loss: 1.377295, mean_absolute_error: 8.891802, mean_q: 2.844880, mean_eps: 0.922170
   86623/2000000: episode: 911, duration: 1.208s, episode steps: 86, steps per second: 71, episode reward: -128.668, mean reward: -1.496 [-100.000, 12.334], mean action: 1.558 [0.000, 3.000], mean observation: -0.089 [-4.513, 1.000], loss: 1.087890, mean_absolute_error: 8.925103, mean_q: 1.647290, mean_eps: 0.922078
   86711/2000000: episode: 912, duration: 1.245s, episode steps: 88, steps per second: 71, episode reward: -169.564, mean reward: -1.927 [-100.000, 20.006], mean action: 1.659 [0.000, 3.000], mean observation: -0.140 [-4.574, 1.000], loss: 1.822328, mean_absolute_error: 9.682639, mean_q: 0.791430, mean_eps: 0.922001
   86787/2000000: episode: 913, duration: 1.098s, episode steps: 76, steps per second: 69, episode reward: -128.101, mean reward: -1.686 [-100.000, 12.851], mean action: 1.671 [0.000, 3.000], mean observation: -0.125 [-1.243, 1.932], loss: 1.349051, mean_absolute_error: 10.337574, mean_q: 0.813240, mean_eps: 0.921927
   86857/2000000: episode: 914, duration: 1.032s, episode steps: 70, steps per second: 68, episode reward: -127.104, mean reward: -1.816 [-100.000, 6.767], mean action: 1.657 [0.000, 3.000], mean observation: -0.069 [-1.341, 4.800], loss: 1.469541, mean_absolute_error: 9.800524, mean_q: 1.307627, mean_eps: 0.921860
   86925/2000000: episode: 915, duration: 0.979s, episode steps: 68, steps per second: 69, episode reward: -191.990, mean reward: -2.823 [-100.000, 6.884], mean action: 1.676 [0.000, 3.000], mean observation: -0.149 [-2.677, 1.000], loss: 1.148556, mean_absolute_error: 9.394653, mean_q: 0.972111, mean_eps: 0.921797
   87046/2000000: episode: 916, duration: 1.723s, episode steps: 121, steps per second: 70, episode reward: -180.048, mean reward: -1.488 [-100.000, 11.647], mean action: 1.496 [0.000, 3.000], mean observation: 0.150 [-4.543, 1.097], loss: 1.390541, mean_absolute_error: 10.276867, mean_q: 0.578739, mean_eps: 0.921713
   87155/2000000: episode: 917, duration: 1.632s, episode steps: 109, steps per second: 67, episode reward: -148.300, mean reward: -1.361 [-100.000, 14.901], mean action: 1.651 [0.000, 3.000], mean observation: -0.050 [-4.131, 1.000], loss: 1.749517, mean_absolute_error: 9.923925, mean_q: 1.036008, mean_eps: 0.921610
   87271/2000000: episode: 918, duration: 1.676s, episode steps: 116, steps per second: 69, episode reward: -104.216, mean reward: -0.898 [-100.000, 12.810], mean action: 1.552 [0.000, 3.000], mean observation: -0.002 [-1.017, 3.509], loss: 1.428752, mean_absolute_error: 10.133712, mean_q: 0.594430, mean_eps: 0.921509
   87394/2000000: episode: 919, duration: 1.760s, episode steps: 123, steps per second: 70, episode reward: -190.162, mean reward: -1.546 [-100.000, 6.252], mean action: 1.618 [0.000, 3.000], mean observation: -0.090 [-1.379, 3.866], loss: 1.310424, mean_absolute_error: 9.646495, mean_q: 0.691864, mean_eps: 0.921401
   87515/2000000: episode: 920, duration: 1.748s, episode steps: 121, steps per second: 69, episode reward: -140.746, mean reward: -1.163 [-100.000, 13.493], mean action: 1.545 [0.000, 3.000], mean observation: 0.126 [-2.116, 1.000], loss: 1.447540, mean_absolute_error: 9.565188, mean_q: 1.375842, mean_eps: 0.921291
   87597/2000000: episode: 921, duration: 1.253s, episode steps: 82, steps per second: 65, episode reward: -128.045, mean reward: -1.562 [-100.000, 8.489], mean action: 1.659 [0.000, 3.000], mean observation: -0.087 [-4.412, 1.000], loss: 1.623543, mean_absolute_error: 10.369865, mean_q: 0.813577, mean_eps: 0.921200
   87673/2000000: episode: 922, duration: 1.070s, episode steps: 76, steps per second: 71, episode reward: -252.345, mean reward: -3.320 [-100.000, 5.799], mean action: 1.592 [0.000, 3.000], mean observation: -0.038 [-1.557, 5.507], loss: 1.538798, mean_absolute_error: 9.998703, mean_q: 1.687455, mean_eps: 0.921128
   87768/2000000: episode: 923, duration: 1.380s, episode steps: 95, steps per second: 69, episode reward: -225.204, mean reward: -2.371 [-100.000, 57.512], mean action: 1.547 [0.000, 3.000], mean observation: 0.039 [-1.531, 1.875], loss: 1.463106, mean_absolute_error: 9.238232, mean_q: 2.313903, mean_eps: 0.921052
   87851/2000000: episode: 924, duration: 1.184s, episode steps: 83, steps per second: 70, episode reward: -150.468, mean reward: -1.813 [-100.000, 10.899], mean action: 1.482 [0.000, 3.000], mean observation: -0.022 [-1.264, 4.415], loss: 1.078953, mean_absolute_error: 8.936218, mean_q: 2.455568, mean_eps: 0.920973
   87964/2000000: episode: 925, duration: 1.648s, episode steps: 113, steps per second: 69, episode reward: -201.827, mean reward: -1.786 [-100.000, 3.274], mean action: 1.504 [0.000, 3.000], mean observation: -0.097 [-1.208, 1.022], loss: 1.662322, mean_absolute_error: 9.795460, mean_q: 1.114257, mean_eps: 0.920885
   88029/2000000: episode: 926, duration: 0.981s, episode steps: 65, steps per second: 66, episode reward: -89.570, mean reward: -1.378 [-100.000, 16.275], mean action: 1.554 [0.000, 3.000], mean observation: -0.046 [-1.484, 1.000], loss: 1.916076, mean_absolute_error: 11.693928, mean_q: -1.956630, mean_eps: 0.920804
   88108/2000000: episode: 927, duration: 1.135s, episode steps: 79, steps per second: 70, episode reward: -139.468, mean reward: -1.765 [-100.000, 7.896], mean action: 1.620 [0.000, 3.000], mean observation: 0.005 [-1.302, 4.374], loss: 1.653730, mean_absolute_error: 11.152552, mean_q: -0.397980, mean_eps: 0.920739
   88195/2000000: episode: 928, duration: 1.485s, episode steps: 87, steps per second: 59, episode reward: -300.979, mean reward: -3.460 [-100.000, 0.866], mean action: 1.471 [0.000, 3.000], mean observation: 0.209 [-0.767, 1.687], loss: 2.118357, mean_absolute_error: 10.148746, mean_q: 0.830597, mean_eps: 0.920665
   88265/2000000: episode: 929, duration: 1.030s, episode steps: 70, steps per second: 68, episode reward: -159.525, mean reward: -2.279 [-100.000, 12.455], mean action: 1.514 [0.000, 3.000], mean observation: 0.037 [-1.345, 1.903], loss: 1.283733, mean_absolute_error: 11.010248, mean_q: -0.868643, mean_eps: 0.920593
   88335/2000000: episode: 930, duration: 0.967s, episode steps: 70, steps per second: 72, episode reward: -107.991, mean reward: -1.543 [-100.000, 15.792], mean action: 1.429 [0.000, 3.000], mean observation: 0.092 [-1.250, 4.370], loss: 1.200619, mean_absolute_error: 9.184425, mean_q: 1.363158, mean_eps: 0.920530
   88432/2000000: episode: 931, duration: 1.418s, episode steps: 97, steps per second: 68, episode reward: -253.377, mean reward: -2.612 [-100.000, 21.547], mean action: 1.608 [0.000, 3.000], mean observation: 0.056 [-1.566, 3.195], loss: 1.527544, mean_absolute_error: 10.264712, mean_q: 0.641502, mean_eps: 0.920456
   88537/2000000: episode: 932, duration: 1.514s, episode steps: 105, steps per second: 69, episode reward: -129.820, mean reward: -1.236 [-100.000, 7.202], mean action: 1.448 [0.000, 3.000], mean observation: 0.071 [-1.013, 3.575], loss: 1.298160, mean_absolute_error: 9.928733, mean_q: 0.615390, mean_eps: 0.920364
   88622/2000000: episode: 933, duration: 1.194s, episode steps: 85, steps per second: 71, episode reward: -110.228, mean reward: -1.297 [-100.000, 43.351], mean action: 1.529 [0.000, 3.000], mean observation: 0.035 [-3.968, 1.000], loss: 1.799849, mean_absolute_error: 10.632261, mean_q: 0.388642, mean_eps: 0.920278
   88719/2000000: episode: 934, duration: 1.380s, episode steps: 97, steps per second: 70, episode reward: -205.112, mean reward: -2.115 [-100.000, 4.978], mean action: 1.546 [0.000, 3.000], mean observation: -0.163 [-1.584, 5.598], loss: 1.351361, mean_absolute_error: 10.449577, mean_q: -0.390504, mean_eps: 0.920197
   88781/2000000: episode: 935, duration: 0.929s, episode steps: 62, steps per second: 67, episode reward: -181.831, mean reward: -2.933 [-100.000, 8.668], mean action: 1.500 [0.000, 3.000], mean observation: -0.019 [-1.579, 5.005], loss: 1.943064, mean_absolute_error: 10.249963, mean_q: 0.465485, mean_eps: 0.920125
   88910/2000000: episode: 936, duration: 1.833s, episode steps: 129, steps per second: 70, episode reward: -132.182, mean reward: -1.025 [-100.000, 7.752], mean action: 1.651 [0.000, 3.000], mean observation: 0.013 [-1.347, 4.525], loss: 1.863273, mean_absolute_error: 10.718404, mean_q: 0.744106, mean_eps: 0.920039
   89014/2000000: episode: 937, duration: 1.521s, episode steps: 104, steps per second: 68, episode reward: -173.667, mean reward: -1.670 [-100.000, 6.657], mean action: 1.577 [0.000, 3.000], mean observation: -0.051 [-1.301, 4.023], loss: 1.496344, mean_absolute_error: 10.755370, mean_q: -0.627907, mean_eps: 0.919934
   89078/2000000: episode: 938, duration: 0.899s, episode steps: 64, steps per second: 71, episode reward: -124.421, mean reward: -1.944 [-100.000, 19.502], mean action: 1.562 [0.000, 3.000], mean observation: 0.014 [-1.454, 1.000], loss: 1.279564, mean_absolute_error: 9.993953, mean_q: 0.324296, mean_eps: 0.919859
   89195/2000000: episode: 939, duration: 1.657s, episode steps: 117, steps per second: 71, episode reward: -113.184, mean reward: -0.967 [-100.000, 7.092], mean action: 1.538 [0.000, 3.000], mean observation: 0.048 [-4.204, 1.000], loss: 2.078396, mean_absolute_error: 11.209026, mean_q: -1.319260, mean_eps: 0.919778
   89280/2000000: episode: 940, duration: 1.257s, episode steps: 85, steps per second: 68, episode reward: -97.453, mean reward: -1.147 [-100.000, 15.111], mean action: 1.541 [0.000, 3.000], mean observation: -0.040 [-1.216, 3.702], loss: 1.811493, mean_absolute_error: 11.195516, mean_q: -0.839022, mean_eps: 0.919688
   89366/2000000: episode: 941, duration: 1.261s, episode steps: 86, steps per second: 68, episode reward: -186.065, mean reward: -2.164 [-100.000, 22.764], mean action: 1.686 [0.000, 3.000], mean observation: 0.047 [-2.916, 1.000], loss: 2.133303, mean_absolute_error: 10.707582, mean_q: 0.878726, mean_eps: 0.919610
   89434/2000000: episode: 942, duration: 0.982s, episode steps: 68, steps per second: 69, episode reward: -119.278, mean reward: -1.754 [-100.000, 16.214], mean action: 1.603 [0.000, 3.000], mean observation: -0.008 [-1.439, 5.088], loss: 1.495007, mean_absolute_error: 10.611804, mean_q: 1.091182, mean_eps: 0.919540
   89538/2000000: episode: 943, duration: 1.485s, episode steps: 104, steps per second: 70, episode reward: -105.988, mean reward: -1.019 [-100.000, 16.760], mean action: 1.596 [0.000, 3.000], mean observation: 0.017 [-1.317, 1.000], loss: 1.926006, mean_absolute_error: 11.355182, mean_q: -0.781805, mean_eps: 0.919463
   89630/2000000: episode: 944, duration: 1.313s, episode steps: 92, steps per second: 70, episode reward: -169.446, mean reward: -1.842 [-100.000, 5.966], mean action: 1.565 [0.000, 3.000], mean observation: 0.050 [-1.333, 4.958], loss: 1.713135, mean_absolute_error: 10.110977, mean_q: 1.099004, mean_eps: 0.919374
   89788/2000000: episode: 945, duration: 2.275s, episode steps: 158, steps per second: 69, episode reward: -279.775, mean reward: -1.771 [-100.000, 7.841], mean action: 1.620 [0.000, 3.000], mean observation: 0.072 [-3.566, 1.050], loss: 1.546766, mean_absolute_error: 9.854398, mean_q: 1.252044, mean_eps: 0.919263
   89893/2000000: episode: 946, duration: 1.561s, episode steps: 105, steps per second: 67, episode reward: -265.753, mean reward: -2.531 [-100.000, 43.947], mean action: 1.733 [0.000, 3.000], mean observation: -0.020 [-4.098, 1.000], loss: 1.502210, mean_absolute_error: 9.482426, mean_q: 1.781072, mean_eps: 0.919144
   90018/2000000: episode: 947, duration: 1.776s, episode steps: 125, steps per second: 70, episode reward: -156.965, mean reward: -1.256 [-100.000, 6.554], mean action: 1.752 [0.000, 3.000], mean observation: -0.057 [-1.099, 1.707], loss: 1.284653, mean_absolute_error: 9.740060, mean_q: 1.713782, mean_eps: 0.919040
   90122/2000000: episode: 948, duration: 1.503s, episode steps: 104, steps per second: 69, episode reward: -308.224, mean reward: -2.964 [-100.000, 0.666], mean action: 1.510 [0.000, 3.000], mean observation: -0.049 [-1.418, 0.964], loss: 2.300789, mean_absolute_error: 10.463956, mean_q: 2.010013, mean_eps: 0.918937
   90216/2000000: episode: 949, duration: 1.345s, episode steps: 94, steps per second: 70, episode reward: -167.281, mean reward: -1.780 [-100.000, 11.125], mean action: 1.340 [0.000, 3.000], mean observation: 0.068 [-4.346, 1.000], loss: 1.667884, mean_absolute_error: 10.238623, mean_q: 2.146439, mean_eps: 0.918849
   90321/2000000: episode: 950, duration: 1.521s, episode steps: 105, steps per second: 69, episode reward: -211.389, mean reward: -2.013 [-100.000, 4.819], mean action: 1.400 [0.000, 3.000], mean observation: 0.030 [-1.148, 4.838], loss: 1.377380, mean_absolute_error: 11.123454, mean_q: 1.378438, mean_eps: 0.918759
   90397/2000000: episode: 951, duration: 1.091s, episode steps: 76, steps per second: 70, episode reward: -167.475, mean reward: -2.204 [-100.000, 5.073], mean action: 1.737 [0.000, 3.000], mean observation: -0.116 [-1.429, 4.399], loss: 1.405703, mean_absolute_error: 11.339311, mean_q: 0.654691, mean_eps: 0.918676
   90476/2000000: episode: 952, duration: 1.134s, episode steps: 79, steps per second: 70, episode reward: -114.313, mean reward: -1.447 [-100.000, 13.303], mean action: 1.494 [0.000, 3.000], mean observation: -0.004 [-1.445, 1.000], loss: 1.819737, mean_absolute_error: 10.646195, mean_q: 1.625603, mean_eps: 0.918608
   90572/2000000: episode: 953, duration: 1.392s, episode steps: 96, steps per second: 69, episode reward: -244.682, mean reward: -2.549 [-100.000, 15.713], mean action: 1.521 [0.000, 3.000], mean observation: -0.128 [-4.038, 1.000], loss: 1.699931, mean_absolute_error: 10.672948, mean_q: 2.196341, mean_eps: 0.918530
   90654/2000000: episode: 954, duration: 1.197s, episode steps: 82, steps per second: 69, episode reward: -189.563, mean reward: -2.312 [-100.000, 8.985], mean action: 1.707 [0.000, 3.000], mean observation: -0.143 [-1.281, 3.314], loss: 2.003719, mean_absolute_error: 11.818273, mean_q: 0.500233, mean_eps: 0.918449
   90749/2000000: episode: 955, duration: 1.375s, episode steps: 95, steps per second: 69, episode reward: -279.716, mean reward: -2.944 [-100.000, 12.022], mean action: 1.537 [0.000, 3.000], mean observation: 0.024 [-1.483, 3.261], loss: 1.310927, mean_absolute_error: 10.983292, mean_q: 1.513860, mean_eps: 0.918368
   90829/2000000: episode: 956, duration: 1.144s, episode steps: 80, steps per second: 70, episode reward: -86.683, mean reward: -1.084 [-100.000, 9.867], mean action: 1.650 [0.000, 3.000], mean observation: -0.013 [-1.227, 1.000], loss: 1.777624, mean_absolute_error: 11.734409, mean_q: 0.612861, mean_eps: 0.918289
   90914/2000000: episode: 957, duration: 1.196s, episode steps: 85, steps per second: 71, episode reward: -134.334, mean reward: -1.580 [-100.000, 8.639], mean action: 1.659 [0.000, 3.000], mean observation: 0.035 [-4.527, 1.000], loss: 2.121233, mean_absolute_error: 11.251288, mean_q: 1.919790, mean_eps: 0.918215
   90995/2000000: episode: 958, duration: 1.142s, episode steps: 81, steps per second: 71, episode reward: -152.916, mean reward: -1.888 [-100.000, 8.597], mean action: 1.679 [0.000, 3.000], mean observation: -0.016 [-1.380, 4.586], loss: 1.339903, mean_absolute_error: 11.187516, mean_q: 0.295983, mean_eps: 0.918141
   91092/2000000: episode: 959, duration: 1.410s, episode steps: 97, steps per second: 69, episode reward: -199.507, mean reward: -2.057 [-100.000, 6.496], mean action: 1.588 [0.000, 3.000], mean observation: -0.151 [-1.448, 1.000], loss: 1.325229, mean_absolute_error: 11.122448, mean_q: 1.585340, mean_eps: 0.918062
   91189/2000000: episode: 960, duration: 1.426s, episode steps: 97, steps per second: 68, episode reward: -149.689, mean reward: -1.543 [-100.000, 7.207], mean action: 1.505 [0.000, 3.000], mean observation: -0.115 [-1.183, 3.584], loss: 1.522938, mean_absolute_error: 11.177309, mean_q: 0.777627, mean_eps: 0.917974
   91283/2000000: episode: 961, duration: 1.291s, episode steps: 94, steps per second: 73, episode reward: -192.601, mean reward: -2.049 [-100.000, 20.252], mean action: 1.457 [0.000, 3.000], mean observation: 0.152 [-1.261, 3.909], loss: 2.362987, mean_absolute_error: 11.342985, mean_q: 0.917044, mean_eps: 0.917888
   91356/2000000: episode: 962, duration: 1.105s, episode steps: 73, steps per second: 66, episode reward: -118.869, mean reward: -1.628 [-100.000, 7.194], mean action: 1.616 [0.000, 3.000], mean observation: -0.082 [-4.420, 1.000], loss: 2.139907, mean_absolute_error: 12.774178, mean_q: -1.369240, mean_eps: 0.917814
   91471/2000000: episode: 963, duration: 1.645s, episode steps: 115, steps per second: 70, episode reward: -233.068, mean reward: -2.027 [-100.000, 20.510], mean action: 1.478 [0.000, 3.000], mean observation: -0.152 [-4.570, 1.000], loss: 1.586269, mean_absolute_error: 11.390435, mean_q: 0.583667, mean_eps: 0.917729
   91547/2000000: episode: 964, duration: 1.090s, episode steps: 76, steps per second: 70, episode reward: -124.762, mean reward: -1.642 [-100.000, 17.587], mean action: 1.526 [0.000, 3.000], mean observation: 0.065 [-1.308, 1.000], loss: 1.164899, mean_absolute_error: 11.120632, mean_q: 0.146730, mean_eps: 0.917643
   91621/2000000: episode: 965, duration: 1.074s, episode steps: 74, steps per second: 69, episode reward: -122.508, mean reward: -1.656 [-100.000, 11.067], mean action: 1.378 [0.000, 3.000], mean observation: 0.040 [-4.819, 1.000], loss: 1.688901, mean_absolute_error: 11.774907, mean_q: 0.115909, mean_eps: 0.917574
   91702/2000000: episode: 966, duration: 1.158s, episode steps: 81, steps per second: 70, episode reward: -143.999, mean reward: -1.778 [-100.000, 10.563], mean action: 1.630 [0.000, 3.000], mean observation: 0.090 [-1.285, 2.095], loss: 1.230381, mean_absolute_error: 10.693017, mean_q: 2.243705, mean_eps: 0.917504
   91794/2000000: episode: 967, duration: 1.310s, episode steps: 92, steps per second: 70, episode reward: -172.388, mean reward: -1.874 [-100.000, 10.921], mean action: 1.641 [0.000, 3.000], mean observation: -0.043 [-5.373, 1.000], loss: 1.767555, mean_absolute_error: 10.801827, mean_q: 0.644484, mean_eps: 0.917427
   91884/2000000: episode: 968, duration: 1.305s, episode steps: 90, steps per second: 69, episode reward: -21.800, mean reward: -0.242 [-100.000, 124.295], mean action: 1.667 [0.000, 3.000], mean observation: -0.117 [-1.502, 2.158], loss: 1.800013, mean_absolute_error: 10.856525, mean_q: 0.870075, mean_eps: 0.917346
   92017/2000000: episode: 969, duration: 1.924s, episode steps: 133, steps per second: 69, episode reward: -142.540, mean reward: -1.072 [-100.000, 7.236], mean action: 1.586 [0.000, 3.000], mean observation: 0.038 [-4.259, 1.000], loss: 1.552937, mean_absolute_error: 9.991809, mean_q: 2.906874, mean_eps: 0.917245
   92089/2000000: episode: 970, duration: 1.035s, episode steps: 72, steps per second: 70, episode reward: -165.179, mean reward: -2.294 [-100.000, 8.686], mean action: 1.569 [0.000, 3.000], mean observation: -0.040 [-1.292, 4.039], loss: 1.694002, mean_absolute_error: 13.164855, mean_q: -1.418860, mean_eps: 0.917151
   92160/2000000: episode: 971, duration: 1.024s, episode steps: 71, steps per second: 69, episode reward: -124.913, mean reward: -1.759 [-100.000, 10.586], mean action: 1.563 [0.000, 3.000], mean observation: -0.118 [-1.045, 1.000], loss: 2.251232, mean_absolute_error: 12.323923, mean_q: -0.688991, mean_eps: 0.917088
   92265/2000000: episode: 972, duration: 1.525s, episode steps: 105, steps per second: 69, episode reward: -230.196, mean reward: -2.192 [-100.000, 7.929], mean action: 1.505 [0.000, 3.000], mean observation: -0.010 [-2.130, 1.000], loss: 1.709073, mean_absolute_error: 12.275966, mean_q: 0.042114, mean_eps: 0.917009
   92359/2000000: episode: 973, duration: 1.333s, episode steps: 94, steps per second: 71, episode reward: -273.755, mean reward: -2.912 [-100.000, 40.449], mean action: 1.596 [0.000, 3.000], mean observation: -0.099 [-1.595, 4.783], loss: 1.772870, mean_absolute_error: 10.936346, mean_q: 1.253560, mean_eps: 0.916919
   92480/2000000: episode: 974, duration: 1.740s, episode steps: 121, steps per second: 70, episode reward: -176.366, mean reward: -1.458 [-100.000, 11.136], mean action: 1.455 [0.000, 3.000], mean observation: 0.161 [-1.496, 5.209], loss: 1.684613, mean_absolute_error: 11.224272, mean_q: 0.702959, mean_eps: 0.916824
   92578/2000000: episode: 975, duration: 1.429s, episode steps: 98, steps per second: 69, episode reward: -193.813, mean reward: -1.978 [-100.000, 5.370], mean action: 1.724 [0.000, 3.000], mean observation: -0.099 [-1.514, 5.054], loss: 2.201660, mean_absolute_error: 10.635649, mean_q: 2.607170, mean_eps: 0.916725
   92645/2000000: episode: 976, duration: 0.959s, episode steps: 67, steps per second: 70, episode reward: -144.937, mean reward: -2.163 [-100.000, 16.801], mean action: 1.612 [0.000, 3.000], mean observation: 0.016 [-1.550, 1.000], loss: 2.051593, mean_absolute_error: 11.506958, mean_q: 0.082249, mean_eps: 0.916649
   92749/2000000: episode: 977, duration: 1.465s, episode steps: 104, steps per second: 71, episode reward: -500.328, mean reward: -4.811 [-100.000, 0.627], mean action: 1.375 [0.000, 3.000], mean observation: 0.055 [-1.704, 2.242], loss: 1.862069, mean_absolute_error: 11.186946, mean_q: 1.924540, mean_eps: 0.916572
   92819/2000000: episode: 978, duration: 0.972s, episode steps: 70, steps per second: 72, episode reward: -121.102, mean reward: -1.730 [-100.000, 17.489], mean action: 1.486 [0.000, 3.000], mean observation: -0.022 [-1.546, 1.000], loss: 2.050337, mean_absolute_error: 10.948395, mean_q: 1.745543, mean_eps: 0.916494
   92902/2000000: episode: 979, duration: 1.195s, episode steps: 83, steps per second: 69, episode reward: -203.963, mean reward: -2.457 [-100.000, 10.129], mean action: 1.602 [0.000, 3.000], mean observation: -0.006 [-1.333, 4.145], loss: 1.617708, mean_absolute_error: 10.979485, mean_q: 1.339928, mean_eps: 0.916426
   92992/2000000: episode: 980, duration: 1.326s, episode steps: 90, steps per second: 68, episode reward: -323.808, mean reward: -3.598 [-100.000, 79.275], mean action: 1.600 [0.000, 3.000], mean observation: 0.022 [-1.252, 3.082], loss: 2.005418, mean_absolute_error: 11.358838, mean_q: 1.113443, mean_eps: 0.916349
   93092/2000000: episode: 981, duration: 1.458s, episode steps: 100, steps per second: 69, episode reward: -155.310, mean reward: -1.553 [-100.000, 7.392], mean action: 1.540 [0.000, 3.000], mean observation: -0.137 [-1.321, 3.598], loss: 1.430693, mean_absolute_error: 10.829247, mean_q: 2.123069, mean_eps: 0.916264
   93187/2000000: episode: 982, duration: 1.354s, episode steps: 95, steps per second: 70, episode reward: -189.100, mean reward: -1.991 [-100.000, 18.199], mean action: 1.484 [0.000, 3.000], mean observation: 0.076 [-2.832, 1.092], loss: 1.920342, mean_absolute_error: 11.169380, mean_q: 1.508379, mean_eps: 0.916176
   93274/2000000: episode: 983, duration: 1.246s, episode steps: 87, steps per second: 70, episode reward: -173.170, mean reward: -1.990 [-100.000, 10.384], mean action: 1.713 [0.000, 3.000], mean observation: -0.032 [-1.253, 3.406], loss: 1.647273, mean_absolute_error: 11.127109, mean_q: 1.599522, mean_eps: 0.916093
   93378/2000000: episode: 984, duration: 1.485s, episode steps: 104, steps per second: 70, episode reward: -233.212, mean reward: -2.242 [-100.000, 9.957], mean action: 1.327 [0.000, 3.000], mean observation: 0.050 [-1.543, 4.103], loss: 1.563271, mean_absolute_error: 10.788402, mean_q: 1.967098, mean_eps: 0.916007
   93488/2000000: episode: 985, duration: 1.567s, episode steps: 110, steps per second: 70, episode reward: -442.536, mean reward: -4.023 [-100.000, 4.537], mean action: 1.518 [0.000, 3.000], mean observation: 0.004 [-2.014, 1.612], loss: 1.992420, mean_absolute_error: 11.908428, mean_q: 0.461847, mean_eps: 0.915911
   93610/2000000: episode: 986, duration: 1.784s, episode steps: 122, steps per second: 68, episode reward: -470.751, mean reward: -3.859 [-100.000, 118.171], mean action: 1.549 [0.000, 3.000], mean observation: -0.007 [-3.424, 1.031], loss: 1.575402, mean_absolute_error: 10.913489, mean_q: 1.887246, mean_eps: 0.915807
   93711/2000000: episode: 987, duration: 1.429s, episode steps: 101, steps per second: 71, episode reward: -323.348, mean reward: -3.201 [-100.000, 1.655], mean action: 1.446 [0.000, 3.000], mean observation: -0.053 [-1.475, 1.027], loss: 1.470501, mean_absolute_error: 11.639757, mean_q: 0.348965, mean_eps: 0.915706
   93814/2000000: episode: 988, duration: 1.495s, episode steps: 103, steps per second: 69, episode reward: -189.504, mean reward: -1.840 [-100.000, 7.540], mean action: 1.650 [0.000, 3.000], mean observation: 0.018 [-4.420, 1.000], loss: 1.480248, mean_absolute_error: 10.776868, mean_q: 2.051505, mean_eps: 0.915614
   93915/2000000: episode: 989, duration: 1.404s, episode steps: 101, steps per second: 72, episode reward: -151.911, mean reward: -1.504 [-100.000, 8.803], mean action: 1.505 [0.000, 3.000], mean observation: -0.074 [-1.315, 4.176], loss: 1.832905, mean_absolute_error: 11.233343, mean_q: 1.825501, mean_eps: 0.915522
   93988/2000000: episode: 990, duration: 1.070s, episode steps: 73, steps per second: 68, episode reward: -129.162, mean reward: -1.769 [-100.000, 27.594], mean action: 1.616 [0.000, 3.000], mean observation: -0.051 [-2.042, 1.000], loss: 1.440374, mean_absolute_error: 10.822217, mean_q: 1.379417, mean_eps: 0.915445
   94114/2000000: episode: 991, duration: 1.804s, episode steps: 126, steps per second: 70, episode reward: -369.826, mean reward: -2.935 [-100.000, 49.211], mean action: 1.532 [0.000, 3.000], mean observation: 0.093 [-1.413, 3.076], loss: 1.533314, mean_absolute_error: 11.455616, mean_q: 1.046418, mean_eps: 0.915355
   94207/2000000: episode: 992, duration: 1.303s, episode steps: 93, steps per second: 71, episode reward: -165.197, mean reward: -1.776 [-100.000, 6.328], mean action: 1.677 [0.000, 3.000], mean observation: -0.168 [-1.328, 1.000], loss: 1.692431, mean_absolute_error: 11.664278, mean_q: 0.086155, mean_eps: 0.915256
   94283/2000000: episode: 993, duration: 1.078s, episode steps: 76, steps per second: 70, episode reward: -153.079, mean reward: -2.014 [-100.000, 8.390], mean action: 1.421 [0.000, 3.000], mean observation: 0.034 [-1.274, 4.470], loss: 1.795368, mean_absolute_error: 11.121221, mean_q: 0.806321, mean_eps: 0.915180
   94400/2000000: episode: 994, duration: 1.703s, episode steps: 117, steps per second: 69, episode reward: -303.453, mean reward: -2.594 [-100.000, 17.299], mean action: 1.547 [0.000, 3.000], mean observation: 0.129 [-1.613, 4.546], loss: 1.539202, mean_absolute_error: 10.851187, mean_q: 0.996528, mean_eps: 0.915094
   94490/2000000: episode: 995, duration: 1.294s, episode steps: 90, steps per second: 70, episode reward: -188.453, mean reward: -2.094 [-100.000, 8.097], mean action: 1.433 [0.000, 3.000], mean observation: -0.007 [-1.437, 4.814], loss: 1.459980, mean_absolute_error: 11.441797, mean_q: 0.314319, mean_eps: 0.915000
   94626/2000000: episode: 996, duration: 1.915s, episode steps: 136, steps per second: 71, episode reward: -191.532, mean reward: -1.408 [-100.000, 5.984], mean action: 1.485 [0.000, 3.000], mean observation: 0.178 [-4.413, 1.033], loss: 1.114244, mean_absolute_error: 10.449174, mean_q: 1.635099, mean_eps: 0.914898
   94725/2000000: episode: 997, duration: 1.412s, episode steps: 99, steps per second: 70, episode reward: -291.329, mean reward: -2.943 [-100.000, 5.945], mean action: 1.535 [0.000, 3.000], mean observation: -0.085 [-6.201, 1.000], loss: 1.581849, mean_absolute_error: 10.136076, mean_q: 2.047834, mean_eps: 0.914792
   94811/2000000: episode: 998, duration: 1.192s, episode steps: 86, steps per second: 72, episode reward: -243.929, mean reward: -2.836 [-100.000, 100.167], mean action: 1.512 [0.000, 3.000], mean observation: -0.095 [-2.174, 1.000], loss: 1.329236, mean_absolute_error: 11.065972, mean_q: 1.341371, mean_eps: 0.914709
   94887/2000000: episode: 999, duration: 1.061s, episode steps: 76, steps per second: 72, episode reward: -174.542, mean reward: -2.297 [-100.000, 26.725], mean action: 1.737 [0.000, 3.000], mean observation: -0.109 [-3.365, 1.000], loss: 1.704813, mean_absolute_error: 10.709199, mean_q: 1.200838, mean_eps: 0.914637
   95003/2000000: episode: 1000, duration: 1.652s, episode steps: 116, steps per second: 70, episode reward: -154.348, mean reward: -1.331 [-100.000, 4.725], mean action: 1.414 [0.000, 3.000], mean observation: 0.129 [-4.026, 1.008], loss: 1.565201, mean_absolute_error: 10.625489, mean_q: 1.289968, mean_eps: 0.914550
   95088/2000000: episode: 1001, duration: 1.262s, episode steps: 85, steps per second: 67, episode reward: -231.596, mean reward: -2.725 [-100.000, 8.697], mean action: 1.565 [0.000, 3.000], mean observation: 0.067 [-1.511, 1.336], loss: 1.669889, mean_absolute_error: 10.374909, mean_q: 2.383832, mean_eps: 0.914460
   95171/2000000: episode: 1002, duration: 1.198s, episode steps: 83, steps per second: 69, episode reward: -278.807, mean reward: -3.359 [-100.000, 3.868], mean action: 1.771 [0.000, 3.000], mean observation: -0.038 [-1.375, 1.334], loss: 0.990177, mean_absolute_error: 10.305556, mean_q: 2.799764, mean_eps: 0.914385
   95270/2000000: episode: 1003, duration: 1.395s, episode steps: 99, steps per second: 71, episode reward: -168.126, mean reward: -1.698 [-100.000, 21.277], mean action: 1.414 [0.000, 3.000], mean observation: 0.030 [-4.703, 1.000], loss: 1.178874, mean_absolute_error: 10.197244, mean_q: 2.030122, mean_eps: 0.914302
   95357/2000000: episode: 1004, duration: 1.237s, episode steps: 87, steps per second: 70, episode reward: -130.670, mean reward: -1.502 [-100.000, 10.680], mean action: 1.322 [0.000, 3.000], mean observation: 0.033 [-1.429, 1.000], loss: 1.149309, mean_absolute_error: 9.787529, mean_q: 1.949372, mean_eps: 0.914217
   95450/2000000: episode: 1005, duration: 1.385s, episode steps: 93, steps per second: 67, episode reward: -405.894, mean reward: -4.364 [-100.000, 1.028], mean action: 1.710 [0.000, 3.000], mean observation: 0.141 [-1.739, 1.600], loss: 1.692118, mean_absolute_error: 10.749235, mean_q: 2.729759, mean_eps: 0.914136
   95558/2000000: episode: 1006, duration: 1.535s, episode steps: 108, steps per second: 70, episode reward: -157.200, mean reward: -1.456 [-100.000, 8.837], mean action: 1.491 [0.000, 3.000], mean observation: -0.031 [-1.324, 4.592], loss: 2.280802, mean_absolute_error: 11.089516, mean_q: 1.207743, mean_eps: 0.914046
   95708/2000000: episode: 1007, duration: 2.206s, episode steps: 150, steps per second: 68, episode reward: -135.040, mean reward: -0.900 [-100.000, 29.402], mean action: 1.573 [0.000, 3.000], mean observation: -0.077 [-1.451, 1.267], loss: 1.874651, mean_absolute_error: 11.243191, mean_q: 1.690292, mean_eps: 0.913931
   95790/2000000: episode: 1008, duration: 1.213s, episode steps: 82, steps per second: 68, episode reward: -193.891, mean reward: -2.365 [-100.000, 19.473], mean action: 1.659 [0.000, 3.000], mean observation: -0.106 [-3.983, 1.000], loss: 1.808548, mean_absolute_error: 10.531653, mean_q: 1.060489, mean_eps: 0.913827
   95908/2000000: episode: 1009, duration: 1.718s, episode steps: 118, steps per second: 69, episode reward: -274.999, mean reward: -2.331 [-100.000, 6.062], mean action: 1.568 [0.000, 3.000], mean observation: 0.229 [-1.688, 1.765], loss: 1.694524, mean_absolute_error: 11.966025, mean_q: 1.588140, mean_eps: 0.913737
   96054/2000000: episode: 1010, duration: 2.099s, episode steps: 146, steps per second: 70, episode reward: -196.572, mean reward: -1.346 [-100.000, 14.375], mean action: 1.479 [0.000, 3.000], mean observation: 0.173 [-1.393, 1.028], loss: 1.511469, mean_absolute_error: 10.202705, mean_q: 2.468716, mean_eps: 0.913618
   96186/2000000: episode: 1011, duration: 1.872s, episode steps: 132, steps per second: 70, episode reward: -178.577, mean reward: -1.353 [-100.000, 8.588], mean action: 1.614 [0.000, 3.000], mean observation: -0.078 [-1.226, 1.000], loss: 1.448958, mean_absolute_error: 10.871189, mean_q: 1.425385, mean_eps: 0.913492
   96309/2000000: episode: 1012, duration: 1.740s, episode steps: 123, steps per second: 71, episode reward: -364.429, mean reward: -2.963 [-100.000, 69.476], mean action: 1.463 [0.000, 3.000], mean observation: 0.137 [-1.442, 3.747], loss: 1.208579, mean_absolute_error: 9.954385, mean_q: 2.532015, mean_eps: 0.913377
   96422/2000000: episode: 1013, duration: 1.615s, episode steps: 113, steps per second: 70, episode reward: -353.781, mean reward: -3.131 [-100.000, 0.728], mean action: 1.531 [0.000, 3.000], mean observation: -0.006 [-1.558, 1.108], loss: 1.731821, mean_absolute_error: 11.172179, mean_q: 1.037690, mean_eps: 0.913271
   96521/2000000: episode: 1014, duration: 1.435s, episode steps: 99, steps per second: 69, episode reward: -182.564, mean reward: -1.844 [-100.000, 4.815], mean action: 1.768 [0.000, 3.000], mean observation: -0.128 [-5.447, 1.000], loss: 1.343747, mean_absolute_error: 10.633983, mean_q: 0.649687, mean_eps: 0.913175
   96589/2000000: episode: 1015, duration: 0.979s, episode steps: 68, steps per second: 69, episode reward: -119.674, mean reward: -1.760 [-100.000, 7.584], mean action: 1.838 [0.000, 3.000], mean observation: -0.151 [-4.938, 1.000], loss: 0.990775, mean_absolute_error: 9.593158, mean_q: 2.611990, mean_eps: 0.913100
   96717/2000000: episode: 1016, duration: 1.824s, episode steps: 128, steps per second: 70, episode reward: -293.424, mean reward: -2.292 [-100.000, 39.162], mean action: 1.500 [0.000, 3.000], mean observation: 0.166 [-1.427, 3.628], loss: 1.215198, mean_absolute_error: 10.783506, mean_q: 1.443620, mean_eps: 0.913011
   96783/2000000: episode: 1017, duration: 0.924s, episode steps: 66, steps per second: 71, episode reward: -148.484, mean reward: -2.250 [-100.000, 5.327], mean action: 1.636 [0.000, 3.000], mean observation: -0.130 [-1.512, 4.775], loss: 1.720680, mean_absolute_error: 11.303205, mean_q: 1.134859, mean_eps: 0.912925
   96847/2000000: episode: 1018, duration: 0.911s, episode steps: 64, steps per second: 70, episode reward: -134.784, mean reward: -2.106 [-100.000, 6.038], mean action: 1.531 [0.000, 3.000], mean observation: -0.183 [-1.571, 5.302], loss: 1.862836, mean_absolute_error: 10.932983, mean_q: 2.133892, mean_eps: 0.912867
   96971/2000000: episode: 1019, duration: 1.772s, episode steps: 124, steps per second: 70, episode reward: -137.247, mean reward: -1.107 [-100.000, 10.685], mean action: 1.605 [0.000, 3.000], mean observation: -0.029 [-1.234, 4.523], loss: 1.642267, mean_absolute_error: 11.216551, mean_q: 0.984129, mean_eps: 0.912783
   97059/2000000: episode: 1020, duration: 1.257s, episode steps: 88, steps per second: 70, episode reward: -183.926, mean reward: -2.090 [-100.000, 6.286], mean action: 1.455 [0.000, 3.000], mean observation: -0.123 [-4.440, 1.000], loss: 1.430232, mean_absolute_error: 10.383012, mean_q: 1.617281, mean_eps: 0.912687
   97165/2000000: episode: 1021, duration: 1.554s, episode steps: 106, steps per second: 68, episode reward: -199.550, mean reward: -1.883 [-100.000, 7.903], mean action: 1.462 [0.000, 3.000], mean observation: -0.054 [-1.528, 1.017], loss: 2.254147, mean_absolute_error: 10.525044, mean_q: 2.472342, mean_eps: 0.912599
   97279/2000000: episode: 1022, duration: 1.587s, episode steps: 114, steps per second: 72, episode reward: -250.697, mean reward: -2.199 [-100.000, 95.752], mean action: 1.614 [0.000, 3.000], mean observation: 0.107 [-2.125, 1.374], loss: 1.382680, mean_absolute_error: 10.601089, mean_q: 2.983829, mean_eps: 0.912500
   97399/2000000: episode: 1023, duration: 1.717s, episode steps: 120, steps per second: 70, episode reward: -239.690, mean reward: -1.997 [-100.000, 15.844], mean action: 1.550 [0.000, 3.000], mean observation: 0.109 [-1.356, 1.498], loss: 1.730088, mean_absolute_error: 11.368822, mean_q: 0.842338, mean_eps: 0.912396
   97460/2000000: episode: 1024, duration: 0.900s, episode steps: 61, steps per second: 68, episode reward: -117.167, mean reward: -1.921 [-100.000, 7.440], mean action: 1.049 [0.000, 3.000], mean observation: 0.030 [-1.420, 5.038], loss: 1.469069, mean_absolute_error: 11.014600, mean_q: 1.562086, mean_eps: 0.912315
   97597/2000000: episode: 1025, duration: 1.999s, episode steps: 137, steps per second: 69, episode reward: -190.469, mean reward: -1.390 [-100.000, 12.444], mean action: 1.679 [0.000, 3.000], mean observation: 0.039 [-1.304, 3.446], loss: 1.711058, mean_absolute_error: 10.667405, mean_q: 2.027923, mean_eps: 0.912225
   97707/2000000: episode: 1026, duration: 1.539s, episode steps: 110, steps per second: 71, episode reward: -174.139, mean reward: -1.583 [-100.000, 7.901], mean action: 1.573 [0.000, 3.000], mean observation: 0.129 [-1.408, 5.330], loss: 1.915955, mean_absolute_error: 11.054857, mean_q: 1.297166, mean_eps: 0.912113
   97808/2000000: episode: 1027, duration: 1.502s, episode steps: 101, steps per second: 67, episode reward: -156.977, mean reward: -1.554 [-100.000, 16.934], mean action: 1.594 [0.000, 3.000], mean observation: -0.124 [-1.208, 1.000], loss: 2.638412, mean_absolute_error: 11.626762, mean_q: 0.244662, mean_eps: 0.912020
   97907/2000000: episode: 1028, duration: 1.451s, episode steps: 99, steps per second: 68, episode reward: -307.706, mean reward: -3.108 [-100.000, 49.488], mean action: 1.606 [0.000, 3.000], mean observation: -0.009 [-1.650, 4.675], loss: 1.644859, mean_absolute_error: 10.778343, mean_q: 1.161265, mean_eps: 0.911930
   98018/2000000: episode: 1029, duration: 1.600s, episode steps: 111, steps per second: 69, episode reward: -150.041, mean reward: -1.352 [-100.000, 13.885], mean action: 1.703 [0.000, 3.000], mean observation: 0.032 [-1.395, 3.296], loss: 1.415525, mean_absolute_error: 11.165403, mean_q: 1.033003, mean_eps: 0.911834
   98101/2000000: episode: 1030, duration: 1.224s, episode steps: 83, steps per second: 68, episode reward: -107.098, mean reward: -1.290 [-100.000, 43.509], mean action: 1.542 [0.000, 3.000], mean observation: -0.041 [-4.609, 1.000], loss: 1.507308, mean_absolute_error: 11.526438, mean_q: 0.145298, mean_eps: 0.911746
   98192/2000000: episode: 1031, duration: 1.336s, episode steps: 91, steps per second: 68, episode reward: -239.905, mean reward: -2.636 [-100.000, 28.068], mean action: 1.462 [0.000, 3.000], mean observation: -0.060 [-1.471, 4.443], loss: 1.676432, mean_absolute_error: 11.343173, mean_q: 1.436364, mean_eps: 0.911669
   98326/2000000: episode: 1032, duration: 1.925s, episode steps: 134, steps per second: 70, episode reward: -316.061, mean reward: -2.359 [-100.000, 98.022], mean action: 1.500 [0.000, 3.000], mean observation: -0.163 [-2.922, 1.097], loss: 1.170084, mean_absolute_error: 10.354559, mean_q: 2.475527, mean_eps: 0.911568
   98472/2000000: episode: 1033, duration: 2.123s, episode steps: 146, steps per second: 69, episode reward: -242.557, mean reward: -1.661 [-100.000, 80.925], mean action: 1.562 [0.000, 3.000], mean observation: -0.171 [-2.679, 1.000], loss: 1.659410, mean_absolute_error: 11.312917, mean_q: 0.426972, mean_eps: 0.911442
   98558/2000000: episode: 1034, duration: 1.252s, episode steps: 86, steps per second: 69, episode reward: -173.334, mean reward: -2.016 [-100.000, 12.641], mean action: 1.523 [0.000, 3.000], mean observation: 0.017 [-4.202, 1.000], loss: 0.991609, mean_absolute_error: 9.739873, mean_q: 3.489291, mean_eps: 0.911337
   98691/2000000: episode: 1035, duration: 1.901s, episode steps: 133, steps per second: 70, episode reward: -189.389, mean reward: -1.424 [-100.000, 11.819], mean action: 1.594 [0.000, 3.000], mean observation: 0.141 [-1.184, 1.018], loss: 1.965538, mean_absolute_error: 11.928008, mean_q: 0.009818, mean_eps: 0.911238
   98755/2000000: episode: 1036, duration: 0.936s, episode steps: 64, steps per second: 68, episode reward: -139.943, mean reward: -2.187 [-100.000, 25.205], mean action: 1.344 [0.000, 3.000], mean observation: 0.023 [-2.388, 1.000], loss: 1.623521, mean_absolute_error: 12.760338, mean_q: -1.494720, mean_eps: 0.911150
   98894/2000000: episode: 1037, duration: 1.973s, episode steps: 139, steps per second: 70, episode reward: -145.862, mean reward: -1.049 [-100.000, 9.726], mean action: 1.518 [0.000, 3.000], mean observation: 0.018 [-4.301, 1.013], loss: 2.082888, mean_absolute_error: 11.175343, mean_q: 1.264663, mean_eps: 0.911058
   98993/2000000: episode: 1038, duration: 1.431s, episode steps: 99, steps per second: 69, episode reward: -389.009, mean reward: -3.929 [-100.000, 0.258], mean action: 1.515 [0.000, 3.000], mean observation: 0.117 [-1.308, 1.783], loss: 1.841252, mean_absolute_error: 11.302691, mean_q: 0.705878, mean_eps: 0.910950
   99083/2000000: episode: 1039, duration: 1.243s, episode steps: 90, steps per second: 72, episode reward: -97.609, mean reward: -1.085 [-100.000, 79.705], mean action: 1.544 [0.000, 3.000], mean observation: 0.040 [-3.040, 1.000], loss: 1.600064, mean_absolute_error: 11.273574, mean_q: 0.997013, mean_eps: 0.910866
   99163/2000000: episode: 1040, duration: 1.142s, episode steps: 80, steps per second: 70, episode reward: -137.364, mean reward: -1.717 [-100.000, 12.418], mean action: 1.575 [0.000, 3.000], mean observation: -0.082 [-4.260, 1.000], loss: 1.593162, mean_absolute_error: 11.144941, mean_q: 1.119247, mean_eps: 0.910790
   99235/2000000: episode: 1041, duration: 1.055s, episode steps: 72, steps per second: 68, episode reward: -122.452, mean reward: -1.701 [-100.000, 6.705], mean action: 1.528 [0.000, 3.000], mean observation: 0.096 [-1.242, 4.627], loss: 1.237231, mean_absolute_error: 11.154423, mean_q: 1.507958, mean_eps: 0.910722
   99365/2000000: episode: 1042, duration: 1.886s, episode steps: 130, steps per second: 69, episode reward: -164.914, mean reward: -1.269 [-100.000, 15.921], mean action: 1.546 [0.000, 3.000], mean observation: -0.066 [-1.243, 1.042], loss: 1.345216, mean_absolute_error: 11.036075, mean_q: 0.655502, mean_eps: 0.910630
   99448/2000000: episode: 1043, duration: 1.202s, episode steps: 83, steps per second: 69, episode reward: -136.736, mean reward: -1.647 [-100.000, 14.511], mean action: 1.470 [0.000, 3.000], mean observation: -0.053 [-4.403, 1.000], loss: 1.467614, mean_absolute_error: 10.809362, mean_q: 1.915323, mean_eps: 0.910535
   99524/2000000: episode: 1044, duration: 1.118s, episode steps: 76, steps per second: 68, episode reward: -238.171, mean reward: -3.134 [-100.000, 5.801], mean action: 1.539 [0.000, 3.000], mean observation: 0.012 [-1.582, 3.860], loss: 1.628775, mean_absolute_error: 10.692117, mean_q: 1.858976, mean_eps: 0.910464
   99604/2000000: episode: 1045, duration: 1.185s, episode steps: 80, steps per second: 68, episode reward: -145.359, mean reward: -1.817 [-100.000, 19.572], mean action: 1.525 [0.000, 3.000], mean observation: 0.078 [-1.476, 1.000], loss: 1.473383, mean_absolute_error: 11.023501, mean_q: 1.524810, mean_eps: 0.910394
   99717/2000000: episode: 1046, duration: 1.647s, episode steps: 113, steps per second: 69, episode reward: -163.160, mean reward: -1.444 [-100.000, 16.705], mean action: 1.602 [0.000, 3.000], mean observation: 0.123 [-1.212, 1.000], loss: 1.863600, mean_absolute_error: 11.382280, mean_q: 0.356828, mean_eps: 0.910306
   99845/2000000: episode: 1047, duration: 1.812s, episode steps: 128, steps per second: 71, episode reward: -270.633, mean reward: -2.114 [-100.000, 3.688], mean action: 1.492 [0.000, 3.000], mean observation: 0.210 [-1.582, 1.048], loss: 1.403698, mean_absolute_error: 10.727328, mean_q: 1.494404, mean_eps: 0.910196
   99927/2000000: episode: 1048, duration: 1.152s, episode steps: 82, steps per second: 71, episode reward: -159.074, mean reward: -1.940 [-100.000, 13.066], mean action: 1.524 [0.000, 3.000], mean observation: 0.108 [-4.040, 1.000], loss: 1.444334, mean_absolute_error: 11.249617, mean_q: 0.192373, mean_eps: 0.910103
  100014/2000000: episode: 1049, duration: 1.268s, episode steps: 87, steps per second: 69, episode reward: -160.281, mean reward: -1.842 [-100.000, 23.910], mean action: 1.621 [0.000, 3.000], mean observation: -0.062 [-2.753, 1.000], loss: 2.100349, mean_absolute_error: 11.632556, mean_q: 0.570348, mean_eps: 0.910027
  100081/2000000: episode: 1050, duration: 0.977s, episode steps: 67, steps per second: 69, episode reward: -106.341, mean reward: -1.587 [-100.000, 21.534], mean action: 1.597 [0.000, 3.000], mean observation: -0.072 [-4.594, 1.000], loss: 1.672702, mean_absolute_error: 11.078876, mean_q: 3.879548, mean_eps: 0.909957
  100203/2000000: episode: 1051, duration: 1.678s, episode steps: 122, steps per second: 73, episode reward: -239.907, mean reward: -1.966 [-100.000, 8.035], mean action: 1.607 [0.000, 3.000], mean observation: 0.033 [-1.354, 1.000], loss: 1.401070, mean_absolute_error: 11.771152, mean_q: 1.635131, mean_eps: 0.909872
  100270/2000000: episode: 1052, duration: 0.973s, episode steps: 67, steps per second: 69, episode reward: -147.526, mean reward: -2.202 [-100.000, 29.820], mean action: 1.433 [0.000, 3.000], mean observation: -0.072 [-1.431, 2.773], loss: 1.526034, mean_absolute_error: 10.703557, mean_q: 3.714763, mean_eps: 0.909788
  100373/2000000: episode: 1053, duration: 1.473s, episode steps: 103, steps per second: 70, episode reward: -146.398, mean reward: -1.421 [-100.000, 21.464], mean action: 1.485 [0.000, 3.000], mean observation: 0.100 [-3.938, 1.000], loss: 1.465548, mean_absolute_error: 11.816064, mean_q: 1.596520, mean_eps: 0.909710
  100454/2000000: episode: 1054, duration: 1.132s, episode steps: 81, steps per second: 72, episode reward: -146.657, mean reward: -1.811 [-100.000, 7.140], mean action: 1.481 [0.000, 3.000], mean observation: -0.022 [-1.259, 4.454], loss: 1.737828, mean_absolute_error: 12.301476, mean_q: 2.180682, mean_eps: 0.909627
  100529/2000000: episode: 1055, duration: 1.064s, episode steps: 75, steps per second: 71, episode reward: -140.933, mean reward: -1.879 [-100.000, 11.986], mean action: 1.573 [0.000, 3.000], mean observation: 0.126 [-4.344, 1.000], loss: 1.633222, mean_absolute_error: 12.410755, mean_q: 1.283456, mean_eps: 0.909557
  100627/2000000: episode: 1056, duration: 1.359s, episode steps: 98, steps per second: 72, episode reward: -129.267, mean reward: -1.319 [-100.000, 8.534], mean action: 1.684 [0.000, 3.000], mean observation: 0.053 [-1.287, 1.000], loss: 2.420463, mean_absolute_error: 12.081545, mean_q: 2.257289, mean_eps: 0.909480
  100726/2000000: episode: 1057, duration: 1.433s, episode steps: 99, steps per second: 69, episode reward: -319.948, mean reward: -3.232 [-100.000, 38.920], mean action: 1.545 [0.000, 3.000], mean observation: 0.101 [-1.560, 3.469], loss: 1.837937, mean_absolute_error: 11.403331, mean_q: 3.432895, mean_eps: 0.909392
  100790/2000000: episode: 1058, duration: 0.897s, episode steps: 64, steps per second: 71, episode reward: -158.522, mean reward: -2.477 [-100.000, 9.126], mean action: 1.516 [0.000, 3.000], mean observation: -0.052 [-1.509, 1.000], loss: 1.967885, mean_absolute_error: 13.635579, mean_q: -0.894807, mean_eps: 0.909318
  100861/2000000: episode: 1059, duration: 1.023s, episode steps: 71, steps per second: 69, episode reward: -89.297, mean reward: -1.258 [-100.000, 84.189], mean action: 1.507 [0.000, 3.000], mean observation: -0.025 [-2.728, 1.050], loss: 1.370752, mean_absolute_error: 11.727888, mean_q: 2.514438, mean_eps: 0.909257
  100937/2000000: episode: 1060, duration: 1.075s, episode steps: 76, steps per second: 71, episode reward: -151.868, mean reward: -1.998 [-100.000, 15.449], mean action: 1.368 [0.000, 3.000], mean observation: 0.083 [-1.324, 1.000], loss: 1.745205, mean_absolute_error: 11.106740, mean_q: 3.625039, mean_eps: 0.909190
  101018/2000000: episode: 1061, duration: 1.138s, episode steps: 81, steps per second: 71, episode reward: -121.264, mean reward: -1.497 [-100.000, 8.735], mean action: 1.444 [0.000, 3.000], mean observation: 0.088 [-1.218, 1.000], loss: 1.968199, mean_absolute_error: 12.324035, mean_q: 1.185954, mean_eps: 0.909120
  101130/2000000: episode: 1062, duration: 1.589s, episode steps: 112, steps per second: 70, episode reward: -150.929, mean reward: -1.348 [-100.000, 7.883], mean action: 1.402 [0.000, 3.000], mean observation: 0.040 [-4.632, 1.000], loss: 2.296856, mean_absolute_error: 12.931575, mean_q: 1.746005, mean_eps: 0.909033
  101214/2000000: episode: 1063, duration: 1.180s, episode steps: 84, steps per second: 71, episode reward: -166.451, mean reward: -1.982 [-100.000, 9.259], mean action: 1.607 [0.000, 3.000], mean observation: -0.054 [-3.640, 1.000], loss: 2.057648, mean_absolute_error: 11.865229, mean_q: 1.847262, mean_eps: 0.908945
  101356/2000000: episode: 1064, duration: 2.048s, episode steps: 142, steps per second: 69, episode reward: -118.423, mean reward: -0.834 [-100.000, 6.382], mean action: 1.535 [0.000, 3.000], mean observation: 0.078 [-1.070, 4.194], loss: 1.806103, mean_absolute_error: 11.917510, mean_q: 1.705896, mean_eps: 0.908844
  101504/2000000: episode: 1065, duration: 2.125s, episode steps: 148, steps per second: 70, episode reward: -181.054, mean reward: -1.223 [-100.000, 24.699], mean action: 1.473 [0.000, 3.000], mean observation: -0.015 [-3.460, 1.032], loss: 1.556789, mean_absolute_error: 12.360665, mean_q: 1.105354, mean_eps: 0.908715
  101580/2000000: episode: 1066, duration: 1.117s, episode steps: 76, steps per second: 68, episode reward: -194.911, mean reward: -2.565 [-100.000, 14.507], mean action: 1.658 [0.000, 3.000], mean observation: -0.001 [-1.486, 3.757], loss: 1.148481, mean_absolute_error: 12.179236, mean_q: 2.935543, mean_eps: 0.908614
  101659/2000000: episode: 1067, duration: 1.126s, episode steps: 79, steps per second: 70, episode reward: -168.288, mean reward: -2.130 [-100.000, 4.884], mean action: 1.734 [0.000, 3.000], mean observation: -0.150 [-1.480, 4.350], loss: 1.127808, mean_absolute_error: 11.145214, mean_q: 2.541921, mean_eps: 0.908544
  101760/2000000: episode: 1068, duration: 1.470s, episode steps: 101, steps per second: 69, episode reward: -116.650, mean reward: -1.155 [-100.000, 10.572], mean action: 1.683 [0.000, 3.000], mean observation: -0.132 [-1.234, 4.203], loss: 1.745419, mean_absolute_error: 13.444747, mean_q: 0.295178, mean_eps: 0.908463
  101874/2000000: episode: 1069, duration: 1.617s, episode steps: 114, steps per second: 71, episode reward: -142.569, mean reward: -1.251 [-100.000, 13.931], mean action: 1.579 [0.000, 3.000], mean observation: -0.097 [-1.237, 4.065], loss: 1.522873, mean_absolute_error: 12.019840, mean_q: 2.524150, mean_eps: 0.908366
  101944/2000000: episode: 1070, duration: 1.030s, episode steps: 70, steps per second: 68, episode reward: -124.549, mean reward: -1.779 [-100.000, 9.766], mean action: 1.557 [0.000, 3.000], mean observation: -0.139 [-1.379, 3.377], loss: 1.235786, mean_absolute_error: 10.779192, mean_q: 2.752095, mean_eps: 0.908283
  102045/2000000: episode: 1071, duration: 1.467s, episode steps: 101, steps per second: 69, episode reward: -123.712, mean reward: -1.225 [-100.000, 8.427], mean action: 1.574 [0.000, 3.000], mean observation: -0.030 [-3.715, 1.000], loss: 1.377504, mean_absolute_error: 11.099310, mean_q: 2.302343, mean_eps: 0.908205
  102122/2000000: episode: 1072, duration: 1.098s, episode steps: 77, steps per second: 70, episode reward: -161.439, mean reward: -2.097 [-100.000, 39.942], mean action: 1.649 [0.000, 3.000], mean observation: 0.002 [-1.322, 1.818], loss: 1.811968, mean_absolute_error: 11.995207, mean_q: 0.880199, mean_eps: 0.908124
  102230/2000000: episode: 1073, duration: 1.510s, episode steps: 108, steps per second: 72, episode reward: -291.644, mean reward: -2.700 [-100.000, 1.342], mean action: 1.565 [0.000, 3.000], mean observation: 0.175 [-1.524, 1.254], loss: 1.527849, mean_absolute_error: 11.485989, mean_q: 2.386393, mean_eps: 0.908042
  102311/2000000: episode: 1074, duration: 1.134s, episode steps: 81, steps per second: 71, episode reward: -150.188, mean reward: -1.854 [-100.000, 20.960], mean action: 1.765 [0.000, 3.000], mean observation: -0.145 [-1.347, 1.000], loss: 2.047400, mean_absolute_error: 11.930025, mean_q: 1.181965, mean_eps: 0.907957
  102443/2000000: episode: 1075, duration: 1.849s, episode steps: 132, steps per second: 71, episode reward: -171.715, mean reward: -1.301 [-100.000, 8.519], mean action: 1.455 [0.000, 3.000], mean observation: -0.001 [-1.257, 4.600], loss: 1.610463, mean_absolute_error: 11.059191, mean_q: 3.318874, mean_eps: 0.907862
  102553/2000000: episode: 1076, duration: 1.565s, episode steps: 110, steps per second: 70, episode reward: -129.119, mean reward: -1.174 [-100.000, 24.351], mean action: 1.573 [0.000, 3.000], mean observation: 0.092 [-1.446, 4.536], loss: 1.260060, mean_absolute_error: 10.854857, mean_q: 3.513724, mean_eps: 0.907752
  102622/2000000: episode: 1077, duration: 0.967s, episode steps: 69, steps per second: 71, episode reward: -274.138, mean reward: -3.973 [-100.000, 4.701], mean action: 1.797 [0.000, 3.000], mean observation: -0.010 [-2.328, 1.032], loss: 1.326924, mean_absolute_error: 10.929880, mean_q: 2.670685, mean_eps: 0.907671
  102777/2000000: episode: 1078, duration: 2.207s, episode steps: 155, steps per second: 70, episode reward: -215.185, mean reward: -1.388 [-100.000, 24.929], mean action: 1.484 [0.000, 3.000], mean observation: 0.188 [-1.466, 5.181], loss: 1.751560, mean_absolute_error: 12.796960, mean_q: 1.222085, mean_eps: 0.907570
  102905/2000000: episode: 1079, duration: 1.824s, episode steps: 128, steps per second: 70, episode reward: -354.269, mean reward: -2.768 [-100.000, 30.236], mean action: 1.500 [0.000, 3.000], mean observation: 0.030 [-1.594, 6.480], loss: 1.666106, mean_absolute_error: 11.448350, mean_q: 2.710324, mean_eps: 0.907442
  103036/2000000: episode: 1080, duration: 1.856s, episode steps: 131, steps per second: 71, episode reward: -266.373, mean reward: -2.033 [-100.000, 5.568], mean action: 1.664 [0.000, 3.000], mean observation: 0.026 [-1.426, 3.501], loss: 2.206230, mean_absolute_error: 12.429243, mean_q: 2.219611, mean_eps: 0.907327
  103156/2000000: episode: 1081, duration: 1.761s, episode steps: 120, steps per second: 68, episode reward: -271.426, mean reward: -2.262 [-100.000, 6.493], mean action: 1.475 [0.000, 3.000], mean observation: 0.153 [-1.456, 1.369], loss: 1.798743, mean_absolute_error: 10.738767, mean_q: 3.295453, mean_eps: 0.907215
  103233/2000000: episode: 1082, duration: 1.122s, episode steps: 77, steps per second: 69, episode reward: -148.396, mean reward: -1.927 [-100.000, 8.167], mean action: 1.597 [0.000, 3.000], mean observation: -0.170 [-1.239, 1.000], loss: 1.984559, mean_absolute_error: 12.024060, mean_q: 2.150641, mean_eps: 0.907125
  103332/2000000: episode: 1083, duration: 1.408s, episode steps: 99, steps per second: 70, episode reward: -136.172, mean reward: -1.375 [-100.000, 10.369], mean action: 1.667 [0.000, 3.000], mean observation: -0.067 [-3.888, 1.000], loss: 1.611967, mean_absolute_error: 10.622121, mean_q: 3.887108, mean_eps: 0.907046
  103407/2000000: episode: 1084, duration: 1.095s, episode steps: 75, steps per second: 69, episode reward: -146.218, mean reward: -1.950 [-100.000, 7.377], mean action: 1.467 [0.000, 3.000], mean observation: -0.138 [-1.454, 4.490], loss: 1.515943, mean_absolute_error: 10.987889, mean_q: 3.143440, mean_eps: 0.906969
  103499/2000000: episode: 1085, duration: 1.312s, episode steps: 92, steps per second: 70, episode reward: -174.535, mean reward: -1.897 [-100.000, 7.774], mean action: 1.446 [0.000, 3.000], mean observation: -0.010 [-1.443, 5.152], loss: 1.609167, mean_absolute_error: 11.668773, mean_q: 2.517315, mean_eps: 0.906893
  103634/2000000: episode: 1086, duration: 1.920s, episode steps: 135, steps per second: 70, episode reward: -148.016, mean reward: -1.096 [-100.000, 6.591], mean action: 1.689 [0.000, 3.000], mean observation: -0.075 [-1.466, 3.785], loss: 1.766925, mean_absolute_error: 11.742262, mean_q: 2.601034, mean_eps: 0.906791
  103748/2000000: episode: 1087, duration: 1.638s, episode steps: 114, steps per second: 70, episode reward: -185.786, mean reward: -1.630 [-100.000, 6.031], mean action: 1.526 [0.000, 3.000], mean observation: 0.115 [-4.227, 1.000], loss: 1.939996, mean_absolute_error: 11.363024, mean_q: 2.843251, mean_eps: 0.906679
  103844/2000000: episode: 1088, duration: 1.524s, episode steps: 96, steps per second: 63, episode reward: -151.037, mean reward: -1.573 [-100.000, 10.255], mean action: 1.760 [0.000, 3.000], mean observation: -0.026 [-4.082, 1.000], loss: 1.906758, mean_absolute_error: 11.849308, mean_q: 2.569240, mean_eps: 0.906585
  103910/2000000: episode: 1089, duration: 0.995s, episode steps: 66, steps per second: 66, episode reward: -127.395, mean reward: -1.930 [-100.000, 35.382], mean action: 1.348 [0.000, 3.000], mean observation: -0.010 [-1.467, 1.967], loss: 1.562984, mean_absolute_error: 12.159462, mean_q: 0.997438, mean_eps: 0.906512
  104048/2000000: episode: 1090, duration: 2.024s, episode steps: 138, steps per second: 68, episode reward: -187.991, mean reward: -1.362 [-100.000, 8.618], mean action: 1.703 [0.000, 3.000], mean observation: 0.033 [-1.409, 4.220], loss: 1.624033, mean_absolute_error: 12.104027, mean_q: 1.736314, mean_eps: 0.906420
  104189/2000000: episode: 1091, duration: 2.082s, episode steps: 141, steps per second: 68, episode reward: -106.024, mean reward: -0.752 [-100.000, 32.109], mean action: 1.638 [0.000, 3.000], mean observation: -0.060 [-1.359, 1.319], loss: 1.519121, mean_absolute_error: 11.283980, mean_q: 2.674443, mean_eps: 0.906294
  104324/2000000: episode: 1092, duration: 1.942s, episode steps: 135, steps per second: 70, episode reward: -125.918, mean reward: -0.933 [-100.000, 20.930], mean action: 1.770 [0.000, 3.000], mean observation: -0.004 [-1.303, 3.982], loss: 1.937310, mean_absolute_error: 12.729956, mean_q: 1.220342, mean_eps: 0.906170
  104432/2000000: episode: 1093, duration: 1.589s, episode steps: 108, steps per second: 68, episode reward: -161.427, mean reward: -1.495 [-100.000, 18.464], mean action: 1.593 [0.000, 3.000], mean observation: 0.003 [-1.392, 5.088], loss: 1.628320, mean_absolute_error: 11.174382, mean_q: 2.687711, mean_eps: 0.906062
  104517/2000000: episode: 1094, duration: 1.253s, episode steps: 85, steps per second: 68, episode reward: -130.532, mean reward: -1.536 [-100.000, 9.483], mean action: 1.624 [0.000, 3.000], mean observation: 0.022 [-1.415, 1.000], loss: 1.416911, mean_absolute_error: 11.928902, mean_q: 1.299698, mean_eps: 0.905973
  104604/2000000: episode: 1095, duration: 1.253s, episode steps: 87, steps per second: 69, episode reward: -101.593, mean reward: -1.168 [-100.000, 6.992], mean action: 1.563 [0.000, 3.000], mean observation: 0.030 [-4.023, 1.000], loss: 1.390518, mean_absolute_error: 11.805645, mean_q: 2.362257, mean_eps: 0.905896
  104724/2000000: episode: 1096, duration: 1.764s, episode steps: 120, steps per second: 68, episode reward: -173.723, mean reward: -1.448 [-100.000, 24.480], mean action: 1.667 [0.000, 3.000], mean observation: 0.140 [-3.194, 1.000], loss: 1.492971, mean_absolute_error: 11.368784, mean_q: 3.704038, mean_eps: 0.905804
  104838/2000000: episode: 1097, duration: 1.662s, episode steps: 114, steps per second: 69, episode reward: -215.626, mean reward: -1.891 [-100.000, 3.109], mean action: 1.518 [0.000, 3.000], mean observation: -0.149 [-1.282, 0.974], loss: 1.566344, mean_absolute_error: 10.811454, mean_q: 3.518877, mean_eps: 0.905698
  104951/2000000: episode: 1098, duration: 1.596s, episode steps: 113, steps per second: 71, episode reward: -364.394, mean reward: -3.225 [-100.000, 1.377], mean action: 1.451 [0.000, 3.000], mean observation: 0.135 [-1.469, 1.441], loss: 1.583266, mean_absolute_error: 12.153811, mean_q: 0.737226, mean_eps: 0.905595
  105022/2000000: episode: 1099, duration: 1.026s, episode steps: 71, steps per second: 69, episode reward: -147.427, mean reward: -2.076 [-100.000, 7.263], mean action: 1.408 [0.000, 3.000], mean observation: -0.046 [-4.277, 1.000], loss: 1.316926, mean_absolute_error: 10.899300, mean_q: 3.421488, mean_eps: 0.905513
  105141/2000000: episode: 1100, duration: 1.680s, episode steps: 119, steps per second: 71, episode reward: -168.162, mean reward: -1.413 [-100.000, 21.172], mean action: 1.479 [0.000, 3.000], mean observation: -0.134 [-3.371, 1.000], loss: 1.586362, mean_absolute_error: 11.226751, mean_q: 3.066795, mean_eps: 0.905426
  105268/2000000: episode: 1101, duration: 1.829s, episode steps: 127, steps per second: 69, episode reward: -214.306, mean reward: -1.687 [-100.000, 7.036], mean action: 1.583 [0.000, 3.000], mean observation: 0.119 [-1.233, 2.588], loss: 1.226455, mean_absolute_error: 11.713572, mean_q: 1.911867, mean_eps: 0.905316
  105399/2000000: episode: 1102, duration: 1.880s, episode steps: 131, steps per second: 70, episode reward: -197.621, mean reward: -1.509 [-100.000, 18.669], mean action: 1.573 [0.000, 3.000], mean observation: -0.045 [-2.680, 1.000], loss: 1.937886, mean_absolute_error: 12.042440, mean_q: 1.362175, mean_eps: 0.905201
  105503/2000000: episode: 1103, duration: 1.501s, episode steps: 104, steps per second: 69, episode reward: -138.355, mean reward: -1.330 [-100.000, 8.194], mean action: 1.596 [0.000, 3.000], mean observation: -0.123 [-1.227, 4.253], loss: 1.525576, mean_absolute_error: 11.598352, mean_q: 1.859499, mean_eps: 0.905095
  105591/2000000: episode: 1104, duration: 1.271s, episode steps: 88, steps per second: 69, episode reward: -213.948, mean reward: -2.431 [-100.000, 6.697], mean action: 1.534 [0.000, 3.000], mean observation: -0.035 [-4.184, 1.000], loss: 1.449303, mean_absolute_error: 10.841693, mean_q: 1.478748, mean_eps: 0.905009
  105673/2000000: episode: 1105, duration: 1.189s, episode steps: 82, steps per second: 69, episode reward: -128.525, mean reward: -1.567 [-100.000, 7.793], mean action: 1.476 [0.000, 3.000], mean observation: -0.135 [-1.214, 4.572], loss: 1.881657, mean_absolute_error: 11.328743, mean_q: 3.431415, mean_eps: 0.904931
  105737/2000000: episode: 1106, duration: 0.907s, episode steps: 64, steps per second: 71, episode reward: -147.517, mean reward: -2.305 [-100.000, 6.842], mean action: 1.438 [0.000, 3.000], mean observation: -0.055 [-4.637, 1.000], loss: 1.644819, mean_absolute_error: 10.877685, mean_q: 2.131729, mean_eps: 0.904865
  105837/2000000: episode: 1107, duration: 1.425s, episode steps: 100, steps per second: 70, episode reward: -112.571, mean reward: -1.126 [-100.000, 21.290], mean action: 1.550 [0.000, 3.000], mean observation: -0.013 [-4.425, 1.000], loss: 1.323553, mean_absolute_error: 11.369379, mean_q: 3.392858, mean_eps: 0.904791
  105932/2000000: episode: 1108, duration: 1.366s, episode steps: 95, steps per second: 70, episode reward: -133.659, mean reward: -1.407 [-100.000, 23.483], mean action: 1.400 [0.000, 3.000], mean observation: 0.018 [-1.248, 1.000], loss: 1.706334, mean_absolute_error: 12.196609, mean_q: 0.360802, mean_eps: 0.904704
  106026/2000000: episode: 1109, duration: 1.371s, episode steps: 94, steps per second: 69, episode reward: -183.811, mean reward: -1.955 [-100.000, 9.652], mean action: 1.649 [0.000, 3.000], mean observation: -0.041 [-1.426, 4.175], loss: 1.978338, mean_absolute_error: 11.758049, mean_q: 2.679041, mean_eps: 0.904620
  106133/2000000: episode: 1110, duration: 1.541s, episode steps: 107, steps per second: 69, episode reward: -166.603, mean reward: -1.557 [-100.000, 22.860], mean action: 1.682 [0.000, 3.000], mean observation: -0.067 [-3.319, 1.000], loss: 1.734161, mean_absolute_error: 11.326313, mean_q: 3.769582, mean_eps: 0.904528
  106230/2000000: episode: 1111, duration: 1.361s, episode steps: 97, steps per second: 71, episode reward: -204.463, mean reward: -2.108 [-100.000, 36.324], mean action: 1.577 [0.000, 3.000], mean observation: 0.078 [-1.595, 2.511], loss: 1.936236, mean_absolute_error: 11.855302, mean_q: 2.462157, mean_eps: 0.904436
  106332/2000000: episode: 1112, duration: 1.487s, episode steps: 102, steps per second: 69, episode reward: -148.721, mean reward: -1.458 [-100.000, 9.829], mean action: 1.539 [0.000, 3.000], mean observation: -0.020 [-4.172, 1.000], loss: 1.448081, mean_absolute_error: 10.879475, mean_q: 3.650460, mean_eps: 0.904348
  106401/2000000: episode: 1113, duration: 1.019s, episode steps: 69, steps per second: 68, episode reward: -74.967, mean reward: -1.086 [-100.000, 50.873], mean action: 1.319 [0.000, 3.000], mean observation: -0.069 [-3.472, 1.000], loss: 1.746449, mean_absolute_error: 11.500222, mean_q: 3.352809, mean_eps: 0.904271
  106545/2000000: episode: 1114, duration: 2.028s, episode steps: 144, steps per second: 71, episode reward: -225.131, mean reward: -1.563 [-100.000, 41.808], mean action: 1.410 [0.000, 3.000], mean observation: -0.105 [-3.059, 1.016], loss: 1.680888, mean_absolute_error: 11.425903, mean_q: 3.081477, mean_eps: 0.904173
  106667/2000000: episode: 1115, duration: 1.700s, episode steps: 122, steps per second: 72, episode reward: -171.241, mean reward: -1.404 [-100.000, 55.134], mean action: 1.492 [0.000, 3.000], mean observation: -0.069 [-1.434, 1.042], loss: 1.883092, mean_absolute_error: 11.651946, mean_q: 2.604764, mean_eps: 0.904055
  106778/2000000: episode: 1116, duration: 1.607s, episode steps: 111, steps per second: 69, episode reward: -188.594, mean reward: -1.699 [-100.000, 5.490], mean action: 1.505 [0.000, 3.000], mean observation: 0.163 [-3.664, 1.281], loss: 1.539420, mean_absolute_error: 11.776911, mean_q: 2.779177, mean_eps: 0.903950
  106912/2000000: episode: 1117, duration: 1.944s, episode steps: 134, steps per second: 69, episode reward: -146.079, mean reward: -1.090 [-100.000, 41.928], mean action: 1.515 [0.000, 3.000], mean observation: -0.137 [-1.421, 2.015], loss: 1.047195, mean_absolute_error: 11.199191, mean_q: 2.612822, mean_eps: 0.903840
  107016/2000000: episode: 1118, duration: 1.517s, episode steps: 104, steps per second: 69, episode reward: -174.004, mean reward: -1.673 [-100.000, 8.426], mean action: 1.596 [0.000, 3.000], mean observation: -0.053 [-1.265, 4.216], loss: 1.308642, mean_absolute_error: 10.963699, mean_q: 3.560826, mean_eps: 0.903734
  107123/2000000: episode: 1119, duration: 1.543s, episode steps: 107, steps per second: 69, episode reward: -128.891, mean reward: -1.205 [-100.000, 8.852], mean action: 1.346 [0.000, 3.000], mean observation: 0.127 [-1.224, 1.000], loss: 2.127707, mean_absolute_error: 12.585119, mean_q: 1.699949, mean_eps: 0.903639
  107229/2000000: episode: 1120, duration: 1.505s, episode steps: 106, steps per second: 70, episode reward: -197.118, mean reward: -1.860 [-100.000, 4.661], mean action: 1.434 [0.000, 3.000], mean observation: 0.092 [-1.529, 1.000], loss: 1.779138, mean_absolute_error: 11.683855, mean_q: 2.413441, mean_eps: 0.903542
  107319/2000000: episode: 1121, duration: 1.256s, episode steps: 90, steps per second: 72, episode reward: -172.371, mean reward: -1.915 [-100.000, 7.234], mean action: 1.533 [0.000, 3.000], mean observation: -0.103 [-1.499, 1.988], loss: 1.264275, mean_absolute_error: 10.965690, mean_q: 3.490940, mean_eps: 0.903453
  107424/2000000: episode: 1122, duration: 1.508s, episode steps: 105, steps per second: 70, episode reward: -136.136, mean reward: -1.297 [-100.000, 18.925], mean action: 1.476 [0.000, 3.000], mean observation: -0.000 [-1.435, 1.000], loss: 1.426525, mean_absolute_error: 11.119018, mean_q: 2.972615, mean_eps: 0.903367
  107495/2000000: episode: 1123, duration: 1.023s, episode steps: 71, steps per second: 69, episode reward: -109.079, mean reward: -1.536 [-100.000, 7.850], mean action: 1.479 [0.000, 3.000], mean observation: -0.000 [-4.381, 1.000], loss: 1.440293, mean_absolute_error: 11.429162, mean_q: 2.408352, mean_eps: 0.903288
  107654/2000000: episode: 1124, duration: 2.274s, episode steps: 159, steps per second: 70, episode reward: -37.331, mean reward: -0.235 [-100.000, 108.789], mean action: 1.491 [0.000, 3.000], mean observation: 0.081 [-1.614, 1.505], loss: 1.756673, mean_absolute_error: 11.759925, mean_q: 3.106468, mean_eps: 0.903183
  107725/2000000: episode: 1125, duration: 1.024s, episode steps: 71, steps per second: 69, episode reward: -118.952, mean reward: -1.675 [-100.000, 6.593], mean action: 1.296 [0.000, 3.000], mean observation: 0.097 [-1.255, 4.845], loss: 1.507501, mean_absolute_error: 11.844408, mean_q: 3.139143, mean_eps: 0.903079
  107816/2000000: episode: 1126, duration: 1.312s, episode steps: 91, steps per second: 69, episode reward: -117.504, mean reward: -1.291 [-100.000, 72.313], mean action: 1.516 [0.000, 3.000], mean observation: -0.016 [-1.432, 2.733], loss: 1.837614, mean_absolute_error: 12.488930, mean_q: 0.812772, mean_eps: 0.903007
  107898/2000000: episode: 1127, duration: 1.186s, episode steps: 82, steps per second: 69, episode reward: -25.076, mean reward: -0.306 [-100.000, 91.275], mean action: 1.488 [0.000, 3.000], mean observation: -0.039 [-1.668, 1.800], loss: 1.669231, mean_absolute_error: 11.534914, mean_q: 3.018648, mean_eps: 0.902930
  107994/2000000: episode: 1128, duration: 1.365s, episode steps: 96, steps per second: 70, episode reward: -141.711, mean reward: -1.476 [-100.000, 7.621], mean action: 1.438 [0.000, 3.000], mean observation: 0.111 [-1.046, 1.000], loss: 1.328186, mean_absolute_error: 11.720666, mean_q: 2.050876, mean_eps: 0.902849
  108111/2000000: episode: 1129, duration: 1.663s, episode steps: 117, steps per second: 70, episode reward: -138.550, mean reward: -1.184 [-100.000, 18.837], mean action: 1.624 [0.000, 3.000], mean observation: 0.052 [-1.316, 1.000], loss: 1.337548, mean_absolute_error: 11.495946, mean_q: 1.562610, mean_eps: 0.902753
  108177/2000000: episode: 1130, duration: 0.975s, episode steps: 66, steps per second: 68, episode reward: -91.271, mean reward: -1.383 [-100.000, 17.344], mean action: 1.606 [0.000, 3.000], mean observation: -0.042 [-1.419, 1.000], loss: 1.497426, mean_absolute_error: 12.255865, mean_q: 1.419462, mean_eps: 0.902670
  108273/2000000: episode: 1131, duration: 1.371s, episode steps: 96, steps per second: 70, episode reward: -151.761, mean reward: -1.581 [-100.000, 6.233], mean action: 1.646 [0.000, 3.000], mean observation: -0.145 [-1.125, 4.168], loss: 1.330623, mean_absolute_error: 11.347791, mean_q: 2.675117, mean_eps: 0.902597
  108374/2000000: episode: 1132, duration: 1.453s, episode steps: 101, steps per second: 69, episode reward: -374.757, mean reward: -3.710 [-100.000, 88.523], mean action: 1.693 [0.000, 3.000], mean observation: -0.247 [-3.242, 1.072], loss: 1.970151, mean_absolute_error: 11.692801, mean_q: 3.189248, mean_eps: 0.902508
  108464/2000000: episode: 1133, duration: 1.289s, episode steps: 90, steps per second: 70, episode reward: -154.897, mean reward: -1.721 [-100.000, 9.065], mean action: 1.489 [0.000, 3.000], mean observation: -0.033 [-4.724, 1.000], loss: 1.342507, mean_absolute_error: 11.269006, mean_q: 2.818532, mean_eps: 0.902424
  108569/2000000: episode: 1134, duration: 1.538s, episode steps: 105, steps per second: 68, episode reward: -162.765, mean reward: -1.550 [-100.000, 15.605], mean action: 1.448 [0.000, 3.000], mean observation: -0.062 [-1.315, 1.000], loss: 1.937282, mean_absolute_error: 11.726845, mean_q: 2.607500, mean_eps: 0.902336
  108689/2000000: episode: 1135, duration: 1.709s, episode steps: 120, steps per second: 70, episode reward: -143.660, mean reward: -1.197 [-100.000, 7.085], mean action: 1.575 [0.000, 3.000], mean observation: -0.079 [-1.106, 2.446], loss: 1.191308, mean_absolute_error: 11.191430, mean_q: 2.788818, mean_eps: 0.902233
  108814/2000000: episode: 1136, duration: 1.758s, episode steps: 125, steps per second: 71, episode reward: -181.315, mean reward: -1.451 [-100.000, 8.029], mean action: 1.592 [0.000, 3.000], mean observation: -0.011 [-3.467, 1.002], loss: 1.881833, mean_absolute_error: 11.164446, mean_q: 3.436414, mean_eps: 0.902123
  108913/2000000: episode: 1137, duration: 1.425s, episode steps: 99, steps per second: 69, episode reward: -120.506, mean reward: -1.217 [-100.000, 79.368], mean action: 1.606 [0.000, 3.000], mean observation: -0.135 [-1.550, 1.561], loss: 1.408702, mean_absolute_error: 11.254013, mean_q: 2.820004, mean_eps: 0.902022
  108990/2000000: episode: 1138, duration: 1.097s, episode steps: 77, steps per second: 70, episode reward: -200.631, mean reward: -2.606 [-100.000, 32.810], mean action: 1.481 [0.000, 3.000], mean observation: -0.013 [-5.334, 1.000], loss: 1.484749, mean_absolute_error: 11.886601, mean_q: 1.876803, mean_eps: 0.901943
  109109/2000000: episode: 1139, duration: 1.700s, episode steps: 119, steps per second: 70, episode reward: -317.688, mean reward: -2.670 [-100.000, 32.881], mean action: 1.706 [0.000, 3.000], mean observation: -0.194 [-2.362, 1.000], loss: 1.440551, mean_absolute_error: 11.361561, mean_q: 3.539796, mean_eps: 0.901855
  109183/2000000: episode: 1140, duration: 1.038s, episode steps: 74, steps per second: 71, episode reward: -249.485, mean reward: -3.371 [-100.000, 6.917], mean action: 1.459 [0.000, 3.000], mean observation: -0.118 [-4.571, 1.000], loss: 1.353154, mean_absolute_error: 10.863457, mean_q: 3.641975, mean_eps: 0.901769
  109279/2000000: episode: 1141, duration: 1.377s, episode steps: 96, steps per second: 70, episode reward: -233.438, mean reward: -2.432 [-100.000, 15.029], mean action: 1.406 [0.000, 3.000], mean observation: 0.144 [-1.370, 4.377], loss: 1.661306, mean_absolute_error: 11.829763, mean_q: 2.835131, mean_eps: 0.901693
  109419/2000000: episode: 1142, duration: 1.969s, episode steps: 140, steps per second: 71, episode reward: -230.681, mean reward: -1.648 [-100.000, 22.572], mean action: 1.671 [0.000, 3.000], mean observation: 0.247 [-1.444, 3.658], loss: 1.709423, mean_absolute_error: 11.741690, mean_q: 2.855434, mean_eps: 0.901587
  109489/2000000: episode: 1143, duration: 1.031s, episode steps: 70, steps per second: 68, episode reward: -164.970, mean reward: -2.357 [-100.000, 7.559], mean action: 1.629 [0.000, 3.000], mean observation: -0.034 [-4.598, 1.000], loss: 1.708544, mean_absolute_error: 11.971035, mean_q: 1.622573, mean_eps: 0.901491
  109572/2000000: episode: 1144, duration: 1.177s, episode steps: 83, steps per second: 71, episode reward: -171.498, mean reward: -2.066 [-100.000, 5.707], mean action: 1.325 [0.000, 3.000], mean observation: 0.059 [-4.596, 1.000], loss: 1.623479, mean_absolute_error: 11.075208, mean_q: 3.127620, mean_eps: 0.901423
  109675/2000000: episode: 1145, duration: 1.488s, episode steps: 103, steps per second: 69, episode reward: -162.627, mean reward: -1.579 [-100.000, 7.479], mean action: 1.738 [0.000, 3.000], mean observation: -0.144 [-4.904, 1.000], loss: 1.409824, mean_absolute_error: 12.437353, mean_q: 1.704457, mean_eps: 0.901340
  109772/2000000: episode: 1146, duration: 1.429s, episode steps: 97, steps per second: 68, episode reward: -146.525, mean reward: -1.511 [-100.000, 16.669], mean action: 1.608 [0.000, 3.000], mean observation: -0.056 [-1.267, 3.959], loss: 1.647625, mean_absolute_error: 11.455213, mean_q: 3.199751, mean_eps: 0.901250
  109879/2000000: episode: 1147, duration: 1.528s, episode steps: 107, steps per second: 70, episode reward: -152.867, mean reward: -1.429 [-100.000, 46.774], mean action: 1.738 [0.000, 3.000], mean observation: -0.037 [-2.352, 1.000], loss: 1.373935, mean_absolute_error: 11.153814, mean_q: 3.278322, mean_eps: 0.901158
  109962/2000000: episode: 1148, duration: 1.198s, episode steps: 83, steps per second: 69, episode reward: -181.517, mean reward: -2.187 [-100.000, 18.236], mean action: 1.542 [0.000, 3.000], mean observation: 0.083 [-1.399, 3.186], loss: 1.744207, mean_absolute_error: 11.876053, mean_q: 2.402513, mean_eps: 0.901072
  110036/2000000: episode: 1149, duration: 1.080s, episode steps: 74, steps per second: 69, episode reward: 5.988, mean reward: 0.081 [-100.000, 111.519], mean action: 1.473 [0.000, 3.000], mean observation: -0.105 [-1.492, 1.437], loss: 1.832393, mean_absolute_error: 11.233716, mean_q: 2.460385, mean_eps: 0.901002
  110157/2000000: episode: 1150, duration: 1.718s, episode steps: 121, steps per second: 70, episode reward: -145.344, mean reward: -1.201 [-100.000, 9.903], mean action: 1.579 [0.000, 3.000], mean observation: -0.008 [-1.298, 4.522], loss: 1.899013, mean_absolute_error: 13.053778, mean_q: 3.124828, mean_eps: 0.900914
  110280/2000000: episode: 1151, duration: 1.759s, episode steps: 123, steps per second: 70, episode reward: -168.407, mean reward: -1.369 [-100.000, 21.793], mean action: 1.585 [0.000, 3.000], mean observation: -0.151 [-1.590, 1.174], loss: 1.583014, mean_absolute_error: 12.776198, mean_q: 3.401473, mean_eps: 0.900804
  110362/2000000: episode: 1152, duration: 1.183s, episode steps: 82, steps per second: 69, episode reward: -164.106, mean reward: -2.001 [-100.000, 20.738], mean action: 1.707 [0.000, 3.000], mean observation: -0.062 [-3.188, 1.000], loss: 1.803757, mean_absolute_error: 12.995892, mean_q: 3.713900, mean_eps: 0.900712
  110497/2000000: episode: 1153, duration: 1.912s, episode steps: 135, steps per second: 71, episode reward: -128.736, mean reward: -0.954 [-100.000, 35.470], mean action: 1.622 [0.000, 3.000], mean observation: -0.119 [-2.038, 1.654], loss: 1.492822, mean_absolute_error: 12.695313, mean_q: 3.007149, mean_eps: 0.900613
  110588/2000000: episode: 1154, duration: 1.298s, episode steps: 91, steps per second: 70, episode reward: -193.357, mean reward: -2.125 [-100.000, 21.150], mean action: 1.549 [0.000, 3.000], mean observation: 0.125 [-1.444, 3.816], loss: 1.717525, mean_absolute_error: 13.246102, mean_q: 2.991892, mean_eps: 0.900512
  110684/2000000: episode: 1155, duration: 1.405s, episode steps: 96, steps per second: 68, episode reward: -309.750, mean reward: -3.227 [-100.000, 53.169], mean action: 1.417 [0.000, 3.000], mean observation: -0.159 [-2.695, 1.000], loss: 1.470487, mean_absolute_error: 12.124497, mean_q: 3.390873, mean_eps: 0.900429
  110824/2000000: episode: 1156, duration: 2.024s, episode steps: 140, steps per second: 69, episode reward: -188.754, mean reward: -1.348 [-100.000, 63.761], mean action: 1.500 [0.000, 3.000], mean observation: 0.184 [-1.426, 2.394], loss: 1.524628, mean_absolute_error: 12.472736, mean_q: 3.519043, mean_eps: 0.900323
  110941/2000000: episode: 1157, duration: 1.698s, episode steps: 117, steps per second: 69, episode reward: -213.293, mean reward: -1.823 [-100.000, 7.795], mean action: 1.607 [0.000, 3.000], mean observation: 0.058 [-1.235, 3.837], loss: 1.472339, mean_absolute_error: 13.069262, mean_q: 1.765691, mean_eps: 0.900206
  111011/2000000: episode: 1158, duration: 0.970s, episode steps: 70, steps per second: 72, episode reward: -166.023, mean reward: -2.372 [-100.000, 7.473], mean action: 1.443 [0.000, 3.000], mean observation: 0.038 [-1.393, 4.682], loss: 2.288501, mean_absolute_error: 12.735881, mean_q: 3.090677, mean_eps: 0.900122
  111099/2000000: episode: 1159, duration: 1.244s, episode steps: 88, steps per second: 71, episode reward: -242.337, mean reward: -2.754 [-100.000, 7.962], mean action: 1.773 [0.000, 3.000], mean observation: -0.037 [-1.405, 4.136], loss: 1.452486, mean_absolute_error: 11.566950, mean_q: 2.496625, mean_eps: 0.900051
  111203/2000000: episode: 1160, duration: 1.491s, episode steps: 104, steps per second: 70, episode reward: -280.688, mean reward: -2.699 [-100.000, 0.860], mean action: 1.625 [0.000, 3.000], mean observation: -0.083 [-1.336, 0.991], loss: 1.762203, mean_absolute_error: 12.394576, mean_q: 2.658234, mean_eps: 0.899965
  111344/2000000: episode: 1161, duration: 2.051s, episode steps: 141, steps per second: 69, episode reward: -177.604, mean reward: -1.260 [-100.000, 14.418], mean action: 1.652 [0.000, 3.000], mean observation: -0.001 [-1.345, 4.195], loss: 1.807926, mean_absolute_error: 13.352949, mean_q: 2.677475, mean_eps: 0.899855
  111421/2000000: episode: 1162, duration: 1.140s, episode steps: 77, steps per second: 68, episode reward: -122.655, mean reward: -1.593 [-100.000, 11.380], mean action: 1.455 [0.000, 3.000], mean observation: -0.136 [-4.602, 1.000], loss: 1.694855, mean_absolute_error: 13.626620, mean_q: 2.518072, mean_eps: 0.899756
  111531/2000000: episode: 1163, duration: 1.534s, episode steps: 110, steps per second: 72, episode reward: -296.333, mean reward: -2.694 [-100.000, 5.981], mean action: 1.491 [0.000, 3.000], mean observation: 0.065 [-1.578, 2.108], loss: 1.895253, mean_absolute_error: 12.938536, mean_q: 3.086114, mean_eps: 0.899672
  111621/2000000: episode: 1164, duration: 1.296s, episode steps: 90, steps per second: 69, episode reward: -129.184, mean reward: -1.435 [-100.000, 16.174], mean action: 1.611 [0.000, 3.000], mean observation: -0.033 [-1.315, 4.323], loss: 1.601537, mean_absolute_error: 12.567463, mean_q: 3.400945, mean_eps: 0.899582
  111711/2000000: episode: 1165, duration: 1.274s, episode steps: 90, steps per second: 71, episode reward: -162.782, mean reward: -1.809 [-100.000, 19.451], mean action: 1.678 [0.000, 3.000], mean observation: -0.052 [-1.304, 2.532], loss: 1.827567, mean_absolute_error: 12.050919, mean_q: 3.641860, mean_eps: 0.899501
  111790/2000000: episode: 1166, duration: 1.250s, episode steps: 79, steps per second: 63, episode reward: -108.050, mean reward: -1.368 [-100.000, 8.621], mean action: 1.430 [0.000, 3.000], mean observation: -0.059 [-1.314, 1.000], loss: 1.827400, mean_absolute_error: 13.187541, mean_q: 2.149732, mean_eps: 0.899425
  111858/2000000: episode: 1167, duration: 0.972s, episode steps: 68, steps per second: 70, episode reward: -139.180, mean reward: -2.047 [-100.000, 14.609], mean action: 1.735 [0.000, 3.000], mean observation: -0.086 [-1.412, 1.000], loss: 1.639749, mean_absolute_error: 12.682941, mean_q: 3.637848, mean_eps: 0.899358
  111921/2000000: episode: 1168, duration: 0.927s, episode steps: 63, steps per second: 68, episode reward: -134.509, mean reward: -2.135 [-100.000, 18.253], mean action: 1.619 [0.000, 3.000], mean observation: -0.008 [-1.480, 1.000], loss: 1.914744, mean_absolute_error: 12.839817, mean_q: 1.749754, mean_eps: 0.899299
  112044/2000000: episode: 1169, duration: 1.735s, episode steps: 123, steps per second: 71, episode reward: -126.128, mean reward: -1.025 [-100.000, 8.271], mean action: 1.683 [0.000, 3.000], mean observation: -0.037 [-1.033, 3.439], loss: 1.150591, mean_absolute_error: 11.957453, mean_q: 2.996095, mean_eps: 0.899216
  112158/2000000: episode: 1170, duration: 1.732s, episode steps: 114, steps per second: 66, episode reward: -112.268, mean reward: -0.985 [-100.000, 21.261], mean action: 1.579 [0.000, 3.000], mean observation: 0.025 [-1.431, 1.013], loss: 2.045295, mean_absolute_error: 12.727761, mean_q: 3.745955, mean_eps: 0.899110
  112265/2000000: episode: 1171, duration: 1.545s, episode steps: 107, steps per second: 69, episode reward: -118.115, mean reward: -1.104 [-100.000, 51.060], mean action: 1.505 [0.000, 3.000], mean observation: -0.120 [-1.618, 2.621], loss: 1.412012, mean_absolute_error: 11.808010, mean_q: 3.566077, mean_eps: 0.899009
  112377/2000000: episode: 1172, duration: 1.588s, episode steps: 112, steps per second: 71, episode reward: -172.701, mean reward: -1.542 [-100.000, 12.208], mean action: 1.643 [0.000, 3.000], mean observation: -0.105 [-1.243, 4.198], loss: 2.226101, mean_absolute_error: 13.356907, mean_q: 1.955761, mean_eps: 0.898910
  112440/2000000: episode: 1173, duration: 0.908s, episode steps: 63, steps per second: 69, episode reward: -119.590, mean reward: -1.898 [-100.000, 17.466], mean action: 1.429 [0.000, 3.000], mean observation: 0.092 [-1.395, 4.408], loss: 1.722214, mean_absolute_error: 13.092098, mean_q: 2.876209, mean_eps: 0.898833
  112532/2000000: episode: 1174, duration: 1.363s, episode steps: 92, steps per second: 67, episode reward: -167.594, mean reward: -1.822 [-100.000, 17.735], mean action: 1.446 [0.000, 3.000], mean observation: 0.006 [-1.453, 3.439], loss: 1.540819, mean_absolute_error: 12.558914, mean_q: 2.931973, mean_eps: 0.898764
  112643/2000000: episode: 1175, duration: 1.568s, episode steps: 111, steps per second: 71, episode reward: -294.285, mean reward: -2.651 [-100.000, 62.253], mean action: 1.450 [0.000, 3.000], mean observation: 0.153 [-1.531, 4.105], loss: 1.357750, mean_absolute_error: 12.534780, mean_q: 4.247205, mean_eps: 0.898673
  112743/2000000: episode: 1176, duration: 1.407s, episode steps: 100, steps per second: 71, episode reward: -155.051, mean reward: -1.551 [-100.000, 11.143], mean action: 1.570 [0.000, 3.000], mean observation: -0.013 [-1.367, 4.607], loss: 1.675284, mean_absolute_error: 12.131117, mean_q: 3.048799, mean_eps: 0.898577
  112865/2000000: episode: 1177, duration: 1.743s, episode steps: 122, steps per second: 70, episode reward: -110.322, mean reward: -0.904 [-100.000, 11.575], mean action: 1.557 [0.000, 3.000], mean observation: -0.031 [-1.332, 4.316], loss: 1.674105, mean_absolute_error: 12.924653, mean_q: 2.341573, mean_eps: 0.898476
  113011/2000000: episode: 1178, duration: 2.032s, episode steps: 146, steps per second: 72, episode reward: -109.040, mean reward: -0.747 [-100.000, 9.352], mean action: 1.568 [0.000, 3.000], mean observation: 0.007 [-1.274, 4.502], loss: 1.503258, mean_absolute_error: 11.773229, mean_q: 4.773476, mean_eps: 0.898356
  113140/2000000: episode: 1179, duration: 1.853s, episode steps: 129, steps per second: 70, episode reward: -197.164, mean reward: -1.528 [-100.000, 10.525], mean action: 1.380 [0.000, 3.000], mean observation: 0.105 [-4.022, 1.037], loss: 1.720582, mean_absolute_error: 12.766245, mean_q: 3.530363, mean_eps: 0.898233
  113216/2000000: episode: 1180, duration: 1.149s, episode steps: 76, steps per second: 66, episode reward: -108.796, mean reward: -1.432 [-100.000, 18.939], mean action: 1.421 [0.000, 3.000], mean observation: -0.013 [-1.382, 1.000], loss: 1.969061, mean_absolute_error: 12.376355, mean_q: 2.695542, mean_eps: 0.898142
  113352/2000000: episode: 1181, duration: 1.968s, episode steps: 136, steps per second: 69, episode reward: -123.179, mean reward: -0.906 [-100.000, 11.053], mean action: 1.471 [0.000, 3.000], mean observation: 0.018 [-4.153, 1.000], loss: 1.681712, mean_absolute_error: 11.954272, mean_q: 3.141623, mean_eps: 0.898046
  113438/2000000: episode: 1182, duration: 1.290s, episode steps: 86, steps per second: 67, episode reward: -259.028, mean reward: -3.012 [-100.000, 80.199], mean action: 1.651 [0.000, 3.000], mean observation: -0.009 [-3.217, 1.000], loss: 1.530220, mean_absolute_error: 12.489480, mean_q: 3.477940, mean_eps: 0.897945
  113514/2000000: episode: 1183, duration: 1.083s, episode steps: 76, steps per second: 70, episode reward: -117.345, mean reward: -1.544 [-100.000, 8.824], mean action: 1.605 [0.000, 3.000], mean observation: 0.061 [-3.946, 1.000], loss: 1.515750, mean_absolute_error: 12.641671, mean_q: 4.137669, mean_eps: 0.897872
  113578/2000000: episode: 1184, duration: 0.932s, episode steps: 64, steps per second: 69, episode reward: -104.194, mean reward: -1.628 [-100.000, 16.531], mean action: 1.578 [0.000, 3.000], mean observation: -0.133 [-1.189, 1.000], loss: 1.884893, mean_absolute_error: 12.447071, mean_q: 2.796156, mean_eps: 0.897809
  113643/2000000: episode: 1185, duration: 0.893s, episode steps: 65, steps per second: 73, episode reward: -107.494, mean reward: -1.654 [-100.000, 12.938], mean action: 1.677 [0.000, 3.000], mean observation: -0.102 [-1.429, 4.154], loss: 1.762416, mean_absolute_error: 12.594500, mean_q: 3.412901, mean_eps: 0.897751
  113713/2000000: episode: 1186, duration: 1.021s, episode steps: 70, steps per second: 69, episode reward: -194.587, mean reward: -2.780 [-100.000, 8.595], mean action: 1.600 [0.000, 3.000], mean observation: -0.154 [-4.962, 1.000], loss: 1.834650, mean_absolute_error: 12.077332, mean_q: 3.945364, mean_eps: 0.897690
  113822/2000000: episode: 1187, duration: 1.554s, episode steps: 109, steps per second: 70, episode reward: -197.289, mean reward: -1.810 [-100.000, 9.206], mean action: 1.734 [0.000, 3.000], mean observation: -0.092 [-1.292, 3.094], loss: 1.319000, mean_absolute_error: 12.758861, mean_q: 2.429437, mean_eps: 0.897609
  113943/2000000: episode: 1188, duration: 1.937s, episode steps: 121, steps per second: 62, episode reward: -249.581, mean reward: -2.063 [-100.000, 15.925], mean action: 1.504 [0.000, 3.000], mean observation: 0.131 [-1.307, 4.245], loss: 1.615938, mean_absolute_error: 12.610533, mean_q: 4.167149, mean_eps: 0.897506
  114061/2000000: episode: 1189, duration: 1.752s, episode steps: 118, steps per second: 67, episode reward: -148.600, mean reward: -1.259 [-100.000, 17.486], mean action: 1.585 [0.000, 3.000], mean observation: -0.037 [-1.262, 4.193], loss: 1.658194, mean_absolute_error: 14.113829, mean_q: 1.433433, mean_eps: 0.897398
  114134/2000000: episode: 1190, duration: 1.044s, episode steps: 73, steps per second: 70, episode reward: -88.559, mean reward: -1.213 [-100.000, 18.101], mean action: 1.548 [0.000, 3.000], mean observation: -0.024 [-1.418, 1.000], loss: 1.472886, mean_absolute_error: 13.215873, mean_q: 3.061342, mean_eps: 0.897312
  114238/2000000: episode: 1191, duration: 1.462s, episode steps: 104, steps per second: 71, episode reward: -154.309, mean reward: -1.484 [-100.000, 11.458], mean action: 1.404 [0.000, 3.000], mean observation: 0.063 [-1.322, 4.142], loss: 1.601798, mean_absolute_error: 12.620712, mean_q: 2.840335, mean_eps: 0.897233
  114376/2000000: episode: 1192, duration: 1.985s, episode steps: 138, steps per second: 70, episode reward: -177.452, mean reward: -1.286 [-100.000, 9.851], mean action: 1.522 [0.000, 3.000], mean observation: 0.049 [-1.207, 3.849], loss: 1.580594, mean_absolute_error: 12.668136, mean_q: 4.057996, mean_eps: 0.897125
  114493/2000000: episode: 1193, duration: 1.708s, episode steps: 117, steps per second: 68, episode reward: -138.294, mean reward: -1.182 [-100.000, 8.423], mean action: 1.496 [0.000, 3.000], mean observation: -0.001 [-1.240, 4.467], loss: 1.808466, mean_absolute_error: 12.824553, mean_q: 2.426675, mean_eps: 0.897009
  114612/2000000: episode: 1194, duration: 1.714s, episode steps: 119, steps per second: 69, episode reward: -43.267, mean reward: -0.364 [-100.000, 86.376], mean action: 1.664 [0.000, 3.000], mean observation: 0.035 [-1.703, 1.085], loss: 1.418950, mean_absolute_error: 12.408302, mean_q: 3.822093, mean_eps: 0.896903
  114704/2000000: episode: 1195, duration: 1.347s, episode steps: 92, steps per second: 68, episode reward: -172.405, mean reward: -1.874 [-100.000, 30.154], mean action: 1.522 [0.000, 3.000], mean observation: -0.041 [-1.200, 2.992], loss: 1.846625, mean_absolute_error: 11.807521, mean_q: 3.287836, mean_eps: 0.896810
  114777/2000000: episode: 1196, duration: 1.072s, episode steps: 73, steps per second: 68, episode reward: -134.824, mean reward: -1.847 [-100.000, 6.373], mean action: 1.466 [0.000, 3.000], mean observation: -0.017 [-4.700, 1.000], loss: 1.823993, mean_absolute_error: 12.551303, mean_q: 3.664628, mean_eps: 0.896734
  114856/2000000: episode: 1197, duration: 1.154s, episode steps: 79, steps per second: 68, episode reward: -157.792, mean reward: -1.997 [-100.000, 8.465], mean action: 1.734 [0.000, 3.000], mean observation: -0.106 [-1.288, 3.233], loss: 1.868495, mean_absolute_error: 12.652603, mean_q: 3.802841, mean_eps: 0.896666
  114918/2000000: episode: 1198, duration: 0.908s, episode steps: 62, steps per second: 68, episode reward: -122.521, mean reward: -1.976 [-100.000, 8.003], mean action: 1.484 [0.000, 3.000], mean observation: -0.017 [-1.450, 4.121], loss: 1.776959, mean_absolute_error: 13.025188, mean_q: 2.331467, mean_eps: 0.896603
  115027/2000000: episode: 1199, duration: 1.567s, episode steps: 109, steps per second: 70, episode reward: -322.111, mean reward: -2.955 [-100.000, 8.783], mean action: 1.706 [0.000, 3.000], mean observation: -0.110 [-4.545, 1.075], loss: 1.421111, mean_absolute_error: 12.487922, mean_q: 2.870048, mean_eps: 0.896525
  115097/2000000: episode: 1200, duration: 1.051s, episode steps: 70, steps per second: 67, episode reward: -149.665, mean reward: -2.138 [-100.000, 10.567], mean action: 1.457 [0.000, 3.000], mean observation: -0.102 [-1.501, 1.000], loss: 1.341421, mean_absolute_error: 11.540589, mean_q: 4.820881, mean_eps: 0.896444
  115176/2000000: episode: 1201, duration: 1.124s, episode steps: 79, steps per second: 70, episode reward: -184.285, mean reward: -2.333 [-100.000, 7.612], mean action: 1.608 [0.000, 3.000], mean observation: 0.086 [-1.322, 4.563], loss: 1.312661, mean_absolute_error: 12.725995, mean_q: 3.265228, mean_eps: 0.896378
  115267/2000000: episode: 1202, duration: 1.317s, episode steps: 91, steps per second: 69, episode reward: -175.317, mean reward: -1.927 [-100.000, 7.388], mean action: 1.462 [0.000, 3.000], mean observation: -0.045 [-1.532, 1.218], loss: 1.427504, mean_absolute_error: 12.245154, mean_q: 4.093947, mean_eps: 0.896302
  115350/2000000: episode: 1203, duration: 1.207s, episode steps: 83, steps per second: 69, episode reward: -238.347, mean reward: -2.872 [-100.000, 6.710], mean action: 1.458 [0.000, 3.000], mean observation: 0.065 [-1.645, 5.245], loss: 1.647728, mean_absolute_error: 12.754923, mean_q: 2.927780, mean_eps: 0.896223
  115412/2000000: episode: 1204, duration: 0.905s, episode steps: 62, steps per second: 69, episode reward: -162.066, mean reward: -2.614 [-100.000, 10.494], mean action: 1.565 [0.000, 3.000], mean observation: -0.046 [-1.458, 4.203], loss: 1.757833, mean_absolute_error: 11.996714, mean_q: 3.783827, mean_eps: 0.896158
  115509/2000000: episode: 1205, duration: 1.403s, episode steps: 97, steps per second: 69, episode reward: -60.945, mean reward: -0.628 [-100.000, 63.808], mean action: 1.660 [0.000, 3.000], mean observation: 0.016 [-2.663, 1.000], loss: 1.764313, mean_absolute_error: 12.547132, mean_q: 3.831059, mean_eps: 0.896086
  115599/2000000: episode: 1206, duration: 1.248s, episode steps: 90, steps per second: 72, episode reward: -152.086, mean reward: -1.690 [-100.000, 9.191], mean action: 1.444 [0.000, 3.000], mean observation: 0.082 [-1.371, 5.028], loss: 1.118852, mean_absolute_error: 12.113523, mean_q: 3.245589, mean_eps: 0.896001
  115705/2000000: episode: 1207, duration: 1.506s, episode steps: 106, steps per second: 70, episode reward: -136.825, mean reward: -1.291 [-100.000, 10.707], mean action: 1.538 [0.000, 3.000], mean observation: -0.035 [-1.305, 1.000], loss: 1.366766, mean_absolute_error: 12.199242, mean_q: 5.540998, mean_eps: 0.895913
  115822/2000000: episode: 1208, duration: 1.654s, episode steps: 117, steps per second: 71, episode reward: -284.147, mean reward: -2.429 [-100.000, 118.826], mean action: 1.368 [0.000, 3.000], mean observation: -0.031 [-2.360, 1.000], loss: 1.291279, mean_absolute_error: 12.419886, mean_q: 4.649983, mean_eps: 0.895812
  115931/2000000: episode: 1209, duration: 1.552s, episode steps: 109, steps per second: 70, episode reward: -204.194, mean reward: -1.873 [-100.000, 3.090], mean action: 1.725 [0.000, 3.000], mean observation: -0.067 [-1.489, 4.679], loss: 1.526606, mean_absolute_error: 11.994760, mean_q: 4.182448, mean_eps: 0.895712
  116022/2000000: episode: 1210, duration: 1.285s, episode steps: 91, steps per second: 71, episode reward: -29.622, mean reward: -0.326 [-100.000, 77.765], mean action: 1.659 [0.000, 3.000], mean observation: 0.013 [-1.675, 1.216], loss: 1.341173, mean_absolute_error: 11.244719, mean_q: 4.854649, mean_eps: 0.895622
  116088/2000000: episode: 1211, duration: 0.970s, episode steps: 66, steps per second: 68, episode reward: -114.716, mean reward: -1.738 [-100.000, 18.664], mean action: 1.697 [0.000, 3.000], mean observation: -0.148 [-1.312, 1.000], loss: 1.730937, mean_absolute_error: 13.335822, mean_q: 2.053072, mean_eps: 0.895551
  116168/2000000: episode: 1212, duration: 1.168s, episode steps: 80, steps per second: 69, episode reward: -133.847, mean reward: -1.673 [-100.000, 7.964], mean action: 1.538 [0.000, 3.000], mean observation: -0.092 [-1.393, 4.202], loss: 1.684183, mean_absolute_error: 13.336413, mean_q: 2.878391, mean_eps: 0.895487
  116272/2000000: episode: 1213, duration: 1.532s, episode steps: 104, steps per second: 68, episode reward: -156.302, mean reward: -1.503 [-100.000, 7.251], mean action: 1.481 [0.000, 3.000], mean observation: -0.113 [-4.200, 1.000], loss: 1.789989, mean_absolute_error: 12.791764, mean_q: 3.759021, mean_eps: 0.895404
  116378/2000000: episode: 1214, duration: 1.497s, episode steps: 106, steps per second: 71, episode reward: -153.069, mean reward: -1.444 [-100.000, 6.102], mean action: 1.660 [0.000, 3.000], mean observation: -0.043 [-1.151, 3.945], loss: 1.293812, mean_absolute_error: 11.884254, mean_q: 4.513846, mean_eps: 0.895308
  116472/2000000: episode: 1215, duration: 1.354s, episode steps: 94, steps per second: 69, episode reward: -140.688, mean reward: -1.497 [-100.000, 6.908], mean action: 1.436 [0.000, 3.000], mean observation: 0.031 [-4.542, 1.000], loss: 1.674218, mean_absolute_error: 12.112164, mean_q: 4.517825, mean_eps: 0.895218
  116619/2000000: episode: 1216, duration: 2.106s, episode steps: 147, steps per second: 70, episode reward: -321.848, mean reward: -2.189 [-100.000, 19.993], mean action: 1.612 [0.000, 3.000], mean observation: 0.038 [-3.910, 1.000], loss: 1.329903, mean_absolute_error: 13.202098, mean_q: 3.089821, mean_eps: 0.895110
  116717/2000000: episode: 1217, duration: 1.415s, episode steps: 98, steps per second: 69, episode reward: -160.509, mean reward: -1.638 [-100.000, 6.980], mean action: 1.551 [0.000, 3.000], mean observation: -0.059 [-1.460, 4.466], loss: 1.478945, mean_absolute_error: 13.411471, mean_q: 2.801522, mean_eps: 0.894999
  116794/2000000: episode: 1218, duration: 1.077s, episode steps: 77, steps per second: 72, episode reward: -96.855, mean reward: -1.258 [-100.000, 9.642], mean action: 1.429 [0.000, 3.000], mean observation: -0.012 [-3.870, 1.000], loss: 1.481802, mean_absolute_error: 12.238255, mean_q: 3.497996, mean_eps: 0.894920
  116882/2000000: episode: 1219, duration: 1.232s, episode steps: 88, steps per second: 71, episode reward: -100.487, mean reward: -1.142 [-100.000, 5.860], mean action: 1.614 [0.000, 3.000], mean observation: -0.064 [-3.459, 1.000], loss: 1.377476, mean_absolute_error: 12.643665, mean_q: 3.913830, mean_eps: 0.894846
  116977/2000000: episode: 1220, duration: 1.373s, episode steps: 95, steps per second: 69, episode reward: -161.875, mean reward: -1.704 [-100.000, 12.608], mean action: 1.432 [0.000, 3.000], mean observation: -0.044 [-1.454, 4.813], loss: 1.711923, mean_absolute_error: 12.677354, mean_q: 2.749599, mean_eps: 0.894763
  117103/2000000: episode: 1221, duration: 1.745s, episode steps: 126, steps per second: 72, episode reward: -46.177, mean reward: -0.366 [-100.000, 65.711], mean action: 1.603 [0.000, 3.000], mean observation: 0.035 [-1.360, 1.226], loss: 1.827965, mean_absolute_error: 13.047836, mean_q: 2.919673, mean_eps: 0.894664
  117198/2000000: episode: 1222, duration: 1.383s, episode steps: 95, steps per second: 69, episode reward: -198.994, mean reward: -2.095 [-100.000, 6.665], mean action: 1.526 [0.000, 3.000], mean observation: 0.142 [-3.054, 1.009], loss: 1.426120, mean_absolute_error: 12.475854, mean_q: 3.521192, mean_eps: 0.894565
  117279/2000000: episode: 1223, duration: 1.156s, episode steps: 81, steps per second: 70, episode reward: -117.854, mean reward: -1.455 [-100.000, 19.959], mean action: 1.420 [0.000, 3.000], mean observation: 0.041 [-1.385, 1.000], loss: 1.793438, mean_absolute_error: 12.328999, mean_q: 2.842956, mean_eps: 0.894486
  117388/2000000: episode: 1224, duration: 1.596s, episode steps: 109, steps per second: 68, episode reward: -119.230, mean reward: -1.094 [-100.000, 14.466], mean action: 1.615 [0.000, 3.000], mean observation: -0.111 [-1.160, 1.000], loss: 1.691774, mean_absolute_error: 13.140521, mean_q: 3.376902, mean_eps: 0.894401
  117496/2000000: episode: 1225, duration: 1.571s, episode steps: 108, steps per second: 69, episode reward: -73.913, mean reward: -0.684 [-100.000, 74.259], mean action: 1.704 [0.000, 3.000], mean observation: -0.074 [-1.360, 2.233], loss: 1.412110, mean_absolute_error: 13.038714, mean_q: 2.513127, mean_eps: 0.894304
  117574/2000000: episode: 1226, duration: 1.141s, episode steps: 78, steps per second: 68, episode reward: -121.166, mean reward: -1.553 [-100.000, 13.325], mean action: 1.513 [0.000, 3.000], mean observation: -0.054 [-1.406, 1.000], loss: 1.483339, mean_absolute_error: 12.499306, mean_q: 4.760045, mean_eps: 0.894219
  117669/2000000: episode: 1227, duration: 1.382s, episode steps: 95, steps per second: 69, episode reward: -315.184, mean reward: -3.318 [-100.000, 0.077], mean action: 1.674 [0.000, 3.000], mean observation: -0.052 [-1.587, 0.982], loss: 1.696988, mean_absolute_error: 12.990721, mean_q: 2.689733, mean_eps: 0.894140
  117805/2000000: episode: 1228, duration: 2.308s, episode steps: 136, steps per second: 59, episode reward: -96.688, mean reward: -0.711 [-100.000, 19.196], mean action: 1.603 [0.000, 3.000], mean observation: 0.012 [-1.399, 1.036], loss: 1.245767, mean_absolute_error: 12.177501, mean_q: 3.874820, mean_eps: 0.894036
  117884/2000000: episode: 1229, duration: 1.158s, episode steps: 79, steps per second: 68, episode reward: -127.628, mean reward: -1.616 [-100.000, 8.028], mean action: 1.506 [0.000, 3.000], mean observation: 0.027 [-4.180, 1.000], loss: 1.522469, mean_absolute_error: 11.430093, mean_q: 4.013083, mean_eps: 0.893940
  117948/2000000: episode: 1230, duration: 0.987s, episode steps: 64, steps per second: 65, episode reward: -193.435, mean reward: -3.022 [-100.000, 5.623], mean action: 1.469 [0.000, 3.000], mean observation: -0.058 [-1.454, 3.658], loss: 1.696147, mean_absolute_error: 13.805877, mean_q: 3.318290, mean_eps: 0.893877
  118085/2000000: episode: 1231, duration: 1.981s, episode steps: 137, steps per second: 69, episode reward: -146.177, mean reward: -1.067 [-100.000, 6.337], mean action: 1.460 [0.000, 3.000], mean observation: 0.080 [-3.905, 1.000], loss: 1.643421, mean_absolute_error: 12.734944, mean_q: 3.425661, mean_eps: 0.893786
  118154/2000000: episode: 1232, duration: 0.990s, episode steps: 69, steps per second: 70, episode reward: -109.520, mean reward: -1.587 [-100.000, 9.314], mean action: 1.435 [0.000, 3.000], mean observation: 0.034 [-4.323, 1.000], loss: 1.469509, mean_absolute_error: 12.924668, mean_q: 3.004615, mean_eps: 0.893692
  118221/2000000: episode: 1233, duration: 0.995s, episode steps: 67, steps per second: 67, episode reward: -134.157, mean reward: -2.002 [-100.000, 7.499], mean action: 1.597 [0.000, 3.000], mean observation: -0.116 [-5.521, 1.000], loss: 1.246137, mean_absolute_error: 13.518731, mean_q: 3.655136, mean_eps: 0.893631
  118369/2000000: episode: 1234, duration: 2.116s, episode steps: 148, steps per second: 70, episode reward: -39.006, mean reward: -0.264 [-100.000, 102.237], mean action: 1.635 [0.000, 3.000], mean observation: 0.129 [-1.695, 1.081], loss: 1.338905, mean_absolute_error: 12.365447, mean_q: 4.622979, mean_eps: 0.893534
  118433/2000000: episode: 1235, duration: 0.923s, episode steps: 64, steps per second: 69, episode reward: -121.859, mean reward: -1.904 [-100.000, 8.129], mean action: 1.469 [0.000, 3.000], mean observation: -0.022 [-4.615, 1.000], loss: 1.819829, mean_absolute_error: 13.546122, mean_q: 2.192759, mean_eps: 0.893438
  118560/2000000: episode: 1236, duration: 1.816s, episode steps: 127, steps per second: 70, episode reward: -169.198, mean reward: -1.332 [-100.000, 11.282], mean action: 1.654 [0.000, 3.000], mean observation: 0.014 [-1.179, 3.487], loss: 1.869707, mean_absolute_error: 12.404812, mean_q: 2.803879, mean_eps: 0.893354
  118637/2000000: episode: 1237, duration: 1.142s, episode steps: 77, steps per second: 67, episode reward: -156.311, mean reward: -2.030 [-100.000, 14.427], mean action: 1.390 [0.000, 3.000], mean observation: 0.042 [-2.595, 1.000], loss: 2.151520, mean_absolute_error: 13.393594, mean_q: 2.783841, mean_eps: 0.893262
  118779/2000000: episode: 1238, duration: 2.004s, episode steps: 142, steps per second: 71, episode reward: -258.055, mean reward: -1.817 [-100.000, 84.744], mean action: 1.423 [0.000, 3.000], mean observation: 0.150 [-1.757, 2.994], loss: 1.361346, mean_absolute_error: 12.341042, mean_q: 3.868333, mean_eps: 0.893163
  118852/2000000: episode: 1239, duration: 1.082s, episode steps: 73, steps per second: 67, episode reward: -119.699, mean reward: -1.640 [-100.000, 12.165], mean action: 1.589 [0.000, 3.000], mean observation: 0.018 [-4.448, 1.000], loss: 1.218919, mean_absolute_error: 11.747830, mean_q: 3.470346, mean_eps: 0.893067
  118935/2000000: episode: 1240, duration: 1.186s, episode steps: 83, steps per second: 70, episode reward: -158.043, mean reward: -1.904 [-100.000, 16.528], mean action: 1.349 [0.000, 3.000], mean observation: 0.081 [-1.379, 1.000], loss: 1.351314, mean_absolute_error: 12.658290, mean_q: 2.980290, mean_eps: 0.892997
  119039/2000000: episode: 1241, duration: 1.484s, episode steps: 104, steps per second: 70, episode reward: -155.728, mean reward: -1.497 [-100.000, 12.961], mean action: 1.462 [0.000, 3.000], mean observation: 0.120 [-1.510, 1.024], loss: 1.699938, mean_absolute_error: 13.036256, mean_q: 3.441122, mean_eps: 0.892913
  119139/2000000: episode: 1242, duration: 1.436s, episode steps: 100, steps per second: 70, episode reward: -77.898, mean reward: -0.779 [-100.000, 100.125], mean action: 1.590 [0.000, 3.000], mean observation: 0.020 [-1.413, 1.984], loss: 1.333674, mean_absolute_error: 12.265232, mean_q: 3.500287, mean_eps: 0.892821
  119218/2000000: episode: 1243, duration: 1.136s, episode steps: 79, steps per second: 70, episode reward: -135.712, mean reward: -1.718 [-100.000, 14.943], mean action: 1.544 [0.000, 3.000], mean observation: 0.124 [-1.363, 1.000], loss: 1.601745, mean_absolute_error: 12.548467, mean_q: 2.779666, mean_eps: 0.892740
  119288/2000000: episode: 1244, duration: 1.020s, episode steps: 70, steps per second: 69, episode reward: -101.744, mean reward: -1.453 [-100.000, 6.566], mean action: 1.800 [0.000, 3.000], mean observation: -0.157 [-1.054, 2.227], loss: 1.530925, mean_absolute_error: 13.303576, mean_q: 3.591974, mean_eps: 0.892673
  119421/2000000: episode: 1245, duration: 1.936s, episode steps: 133, steps per second: 69, episode reward: -281.935, mean reward: -2.120 [-100.000, 34.792], mean action: 1.549 [0.000, 3.000], mean observation: -0.151 [-4.091, 1.040], loss: 1.323188, mean_absolute_error: 12.173394, mean_q: 4.319040, mean_eps: 0.892581
  119536/2000000: episode: 1246, duration: 1.646s, episode steps: 115, steps per second: 70, episode reward: -192.051, mean reward: -1.670 [-100.000, 10.209], mean action: 1.313 [0.000, 3.000], mean observation: 0.148 [-1.279, 1.000], loss: 1.671676, mean_absolute_error: 13.183038, mean_q: 2.632245, mean_eps: 0.892470
  119639/2000000: episode: 1247, duration: 1.468s, episode steps: 103, steps per second: 70, episode reward: -126.386, mean reward: -1.227 [-100.000, 10.326], mean action: 1.417 [0.000, 3.000], mean observation: 0.108 [-1.169, 1.000], loss: 1.578469, mean_absolute_error: 11.673829, mean_q: 4.740116, mean_eps: 0.892373
  119735/2000000: episode: 1248, duration: 1.371s, episode steps: 96, steps per second: 70, episode reward: -156.831, mean reward: -1.634 [-100.000, 10.380], mean action: 1.698 [0.000, 3.000], mean observation: -0.048 [-1.309, 4.062], loss: 1.457815, mean_absolute_error: 12.190438, mean_q: 4.275647, mean_eps: 0.892283
  119803/2000000: episode: 1249, duration: 0.958s, episode steps: 68, steps per second: 71, episode reward: -146.639, mean reward: -2.156 [-100.000, 7.192], mean action: 1.735 [0.000, 3.000], mean observation: -0.059 [-1.348, 4.498], loss: 1.587467, mean_absolute_error: 12.679589, mean_q: 3.397247, mean_eps: 0.892209
  119940/2000000: episode: 1250, duration: 1.979s, episode steps: 137, steps per second: 69, episode reward: -139.703, mean reward: -1.020 [-100.000, 10.769], mean action: 1.569 [0.000, 3.000], mean observation: 0.094 [-2.406, 1.000], loss: 1.491373, mean_absolute_error: 12.599134, mean_q: 3.792406, mean_eps: 0.892117
  120003/2000000: episode: 1251, duration: 0.919s, episode steps: 63, steps per second: 69, episode reward: -210.460, mean reward: -3.341 [-100.000, 33.815], mean action: 1.492 [0.000, 3.000], mean observation: -0.082 [-1.587, 5.180], loss: 1.399091, mean_absolute_error: 12.641178, mean_q: 3.943578, mean_eps: 0.892027
  120070/2000000: episode: 1252, duration: 0.976s, episode steps: 67, steps per second: 69, episode reward: -170.203, mean reward: -2.540 [-100.000, 7.439], mean action: 1.701 [0.000, 3.000], mean observation: -0.054 [-1.445, 3.699], loss: 2.159307, mean_absolute_error: 14.013539, mean_q: 3.477344, mean_eps: 0.891968
  120135/2000000: episode: 1253, duration: 0.946s, episode steps: 65, steps per second: 69, episode reward: -144.846, mean reward: -2.228 [-100.000, 13.400], mean action: 1.738 [0.000, 3.000], mean observation: -0.061 [-4.664, 1.000], loss: 1.508807, mean_absolute_error: 13.811074, mean_q: 3.171595, mean_eps: 0.891908
  120250/2000000: episode: 1254, duration: 1.645s, episode steps: 115, steps per second: 70, episode reward: -169.196, mean reward: -1.471 [-100.000, 5.756], mean action: 1.452 [0.000, 3.000], mean observation: 0.064 [-4.932, 1.000], loss: 1.613845, mean_absolute_error: 13.346040, mean_q: 3.891805, mean_eps: 0.891827
  120388/2000000: episode: 1255, duration: 2.036s, episode steps: 138, steps per second: 68, episode reward: -86.856, mean reward: -0.629 [-100.000, 17.643], mean action: 1.645 [0.000, 3.000], mean observation: 0.065 [-1.303, 1.025], loss: 1.352160, mean_absolute_error: 12.790741, mean_q: 5.613550, mean_eps: 0.891714
  120502/2000000: episode: 1256, duration: 1.638s, episode steps: 114, steps per second: 70, episode reward: -154.080, mean reward: -1.352 [-100.000, 9.402], mean action: 1.491 [0.000, 3.000], mean observation: -0.071 [-1.281, 1.028], loss: 2.011269, mean_absolute_error: 13.819850, mean_q: 4.191656, mean_eps: 0.891600
  120635/2000000: episode: 1257, duration: 1.898s, episode steps: 133, steps per second: 70, episode reward: -190.013, mean reward: -1.429 [-100.000, 21.482], mean action: 1.504 [0.000, 3.000], mean observation: 0.053 [-1.275, 2.746], loss: 1.298555, mean_absolute_error: 12.784791, mean_q: 5.191484, mean_eps: 0.891489
  120722/2000000: episode: 1258, duration: 1.255s, episode steps: 87, steps per second: 69, episode reward: -155.707, mean reward: -1.790 [-100.000, 5.806], mean action: 1.678 [0.000, 3.000], mean observation: -0.170 [-1.154, 2.877], loss: 1.682943, mean_absolute_error: 13.006951, mean_q: 4.742553, mean_eps: 0.891390
  120784/2000000: episode: 1259, duration: 0.902s, episode steps: 62, steps per second: 69, episode reward: -152.081, mean reward: -2.453 [-100.000, 7.045], mean action: 1.387 [0.000, 3.000], mean observation: -0.003 [-5.030, 1.000], loss: 1.787397, mean_absolute_error: 13.677524, mean_q: 4.849441, mean_eps: 0.891323
  120881/2000000: episode: 1260, duration: 1.419s, episode steps: 97, steps per second: 68, episode reward: -162.829, mean reward: -1.679 [-100.000, 7.792], mean action: 1.608 [0.000, 3.000], mean observation: 0.070 [-1.573, 4.753], loss: 1.827432, mean_absolute_error: 14.173173, mean_q: 3.805519, mean_eps: 0.891251
  120967/2000000: episode: 1261, duration: 1.194s, episode steps: 86, steps per second: 72, episode reward: -56.108, mean reward: -0.652 [-100.000, 13.937], mean action: 1.523 [0.000, 3.000], mean observation: -0.054 [-1.152, 1.000], loss: 1.466825, mean_absolute_error: 14.429019, mean_q: 4.669480, mean_eps: 0.891168
  121059/2000000: episode: 1262, duration: 1.292s, episode steps: 92, steps per second: 71, episode reward: -160.345, mean reward: -1.743 [-100.000, 17.040], mean action: 1.576 [0.000, 3.000], mean observation: 0.017 [-1.350, 1.000], loss: 1.755109, mean_absolute_error: 14.522736, mean_q: 4.145653, mean_eps: 0.891089
  121140/2000000: episode: 1263, duration: 1.192s, episode steps: 81, steps per second: 68, episode reward: -130.996, mean reward: -1.617 [-100.000, 6.118], mean action: 1.716 [0.000, 3.000], mean observation: -0.058 [-1.207, 4.199], loss: 1.806478, mean_absolute_error: 14.213345, mean_q: 3.454179, mean_eps: 0.891012
  121228/2000000: episode: 1264, duration: 1.299s, episode steps: 88, steps per second: 68, episode reward: -175.711, mean reward: -1.997 [-100.000, 8.351], mean action: 1.727 [0.000, 3.000], mean observation: -0.114 [-1.464, 4.445], loss: 1.600477, mean_absolute_error: 13.182465, mean_q: 4.469850, mean_eps: 0.890936
  121369/2000000: episode: 1265, duration: 2.038s, episode steps: 141, steps per second: 69, episode reward: -368.321, mean reward: -2.612 [-100.000, 98.947], mean action: 1.475 [0.000, 3.000], mean observation: 0.224 [-1.476, 2.755], loss: 1.718992, mean_absolute_error: 13.281838, mean_q: 4.748007, mean_eps: 0.890832
  121472/2000000: episode: 1266, duration: 1.486s, episode steps: 103, steps per second: 69, episode reward: -137.306, mean reward: -1.333 [-100.000, 6.567], mean action: 1.563 [0.000, 3.000], mean observation: -0.140 [-1.179, 3.453], loss: 1.247259, mean_absolute_error: 13.705853, mean_q: 4.423690, mean_eps: 0.890722
  121551/2000000: episode: 1267, duration: 1.134s, episode steps: 79, steps per second: 70, episode reward: -161.109, mean reward: -2.039 [-100.000, 7.125], mean action: 1.418 [0.000, 3.000], mean observation: 0.095 [-4.568, 1.000], loss: 1.983315, mean_absolute_error: 13.082957, mean_q: 4.243304, mean_eps: 0.890641
  121616/2000000: episode: 1268, duration: 0.971s, episode steps: 65, steps per second: 67, episode reward: -155.847, mean reward: -2.398 [-100.000, 56.831], mean action: 1.538 [0.000, 3.000], mean observation: 0.044 [-1.455, 3.453], loss: 1.643434, mean_absolute_error: 14.527392, mean_q: 2.059890, mean_eps: 0.890576
  121681/2000000: episode: 1269, duration: 0.957s, episode steps: 65, steps per second: 68, episode reward: -187.780, mean reward: -2.889 [-100.000, 7.425], mean action: 1.569 [0.000, 3.000], mean observation: 0.013 [-1.570, 5.156], loss: 1.527606, mean_absolute_error: 13.245227, mean_q: 4.309762, mean_eps: 0.890517
  121792/2000000: episode: 1270, duration: 1.559s, episode steps: 111, steps per second: 71, episode reward: -164.973, mean reward: -1.486 [-100.000, 7.588], mean action: 1.495 [0.000, 3.000], mean observation: 0.074 [-3.864, 1.000], loss: 2.127283, mean_absolute_error: 14.269155, mean_q: 2.259759, mean_eps: 0.890438
  121898/2000000: episode: 1271, duration: 1.538s, episode steps: 106, steps per second: 69, episode reward: -172.221, mean reward: -1.625 [-100.000, 7.377], mean action: 1.491 [0.000, 3.000], mean observation: 0.005 [-1.244, 3.142], loss: 1.820474, mean_absolute_error: 13.290686, mean_q: 4.167001, mean_eps: 0.890340
  121970/2000000: episode: 1272, duration: 0.999s, episode steps: 72, steps per second: 72, episode reward: -124.570, mean reward: -1.730 [-100.000, 16.981], mean action: 1.431 [0.000, 3.000], mean observation: -0.089 [-2.736, 1.000], loss: 1.735272, mean_absolute_error: 14.135969, mean_q: 4.014250, mean_eps: 0.890259
  122090/2000000: episode: 1273, duration: 1.726s, episode steps: 120, steps per second: 70, episode reward: -161.377, mean reward: -1.345 [-100.000, 6.607], mean action: 1.558 [0.000, 3.000], mean observation: 0.098 [-4.423, 1.000], loss: 2.152952, mean_absolute_error: 13.634080, mean_q: 4.070490, mean_eps: 0.890173
  122478/2000000: episode: 1274, duration: 5.651s, episode steps: 388, steps per second: 69, episode reward: -222.076, mean reward: -0.572 [-100.000, 23.760], mean action: 1.479 [0.000, 3.000], mean observation: 0.143 [-1.437, 1.615], loss: 1.786056, mean_absolute_error: 13.567943, mean_q: 4.621489, mean_eps: 0.889944
  122605/2000000: episode: 1275, duration: 1.818s, episode steps: 127, steps per second: 70, episode reward: -154.186, mean reward: -1.214 [-100.000, 14.304], mean action: 1.669 [0.000, 3.000], mean observation: -0.123 [-1.281, 3.795], loss: 1.502326, mean_absolute_error: 12.645129, mean_q: 4.962213, mean_eps: 0.889712
  122677/2000000: episode: 1276, duration: 1.013s, episode steps: 72, steps per second: 71, episode reward: -109.290, mean reward: -1.518 [-100.000, 15.594], mean action: 1.653 [0.000, 3.000], mean observation: -0.138 [-4.498, 1.000], loss: 1.336887, mean_absolute_error: 13.974253, mean_q: 4.643483, mean_eps: 0.889622
  122771/2000000: episode: 1277, duration: 1.313s, episode steps: 94, steps per second: 72, episode reward: -67.940, mean reward: -0.723 [-100.000, 113.536], mean action: 1.521 [0.000, 3.000], mean observation: -0.097 [-1.413, 1.713], loss: 1.295092, mean_absolute_error: 14.036119, mean_q: 4.303015, mean_eps: 0.889548
  122854/2000000: episode: 1278, duration: 1.204s, episode steps: 83, steps per second: 69, episode reward: -115.273, mean reward: -1.389 [-100.000, 13.509], mean action: 1.518 [0.000, 3.000], mean observation: 0.049 [-4.426, 1.000], loss: 2.482105, mean_absolute_error: 15.104358, mean_q: 2.475303, mean_eps: 0.889469
  122929/2000000: episode: 1279, duration: 1.066s, episode steps: 75, steps per second: 70, episode reward: -66.223, mean reward: -0.883 [-100.000, 19.406], mean action: 1.640 [0.000, 3.000], mean observation: -0.041 [-1.177, 1.000], loss: 1.971784, mean_absolute_error: 14.448346, mean_q: 4.074201, mean_eps: 0.889397
  123037/2000000: episode: 1280, duration: 1.515s, episode steps: 108, steps per second: 71, episode reward: -126.789, mean reward: -1.174 [-100.000, 7.877], mean action: 1.713 [0.000, 3.000], mean observation: 0.057 [-4.193, 1.000], loss: 1.367812, mean_absolute_error: 12.855733, mean_q: 5.985154, mean_eps: 0.889314
  123118/2000000: episode: 1281, duration: 1.163s, episode steps: 81, steps per second: 70, episode reward: -124.487, mean reward: -1.537 [-100.000, 58.588], mean action: 1.679 [0.000, 3.000], mean observation: -0.070 [-1.421, 5.230], loss: 1.918171, mean_absolute_error: 14.064425, mean_q: 5.253406, mean_eps: 0.889230
  123255/2000000: episode: 1282, duration: 1.936s, episode steps: 137, steps per second: 71, episode reward: -134.755, mean reward: -0.984 [-100.000, 15.817], mean action: 1.599 [0.000, 3.000], mean observation: -0.019 [-1.179, 3.707], loss: 1.935125, mean_absolute_error: 14.126057, mean_q: 4.016539, mean_eps: 0.889133
  123354/2000000: episode: 1283, duration: 1.429s, episode steps: 99, steps per second: 69, episode reward: -127.903, mean reward: -1.292 [-100.000, 7.206], mean action: 1.606 [0.000, 3.000], mean observation: -0.001 [-3.970, 1.000], loss: 1.809285, mean_absolute_error: 14.604234, mean_q: 3.332381, mean_eps: 0.889026
  123473/2000000: episode: 1284, duration: 1.717s, episode steps: 119, steps per second: 69, episode reward: -135.246, mean reward: -1.137 [-100.000, 11.812], mean action: 1.588 [0.000, 3.000], mean observation: 0.068 [-1.118, 3.456], loss: 1.663515, mean_absolute_error: 13.063882, mean_q: 4.440012, mean_eps: 0.888927
  123620/2000000: episode: 1285, duration: 2.127s, episode steps: 147, steps per second: 69, episode reward: -300.926, mean reward: -2.047 [-100.000, 21.048], mean action: 1.639 [0.000, 3.000], mean observation: -0.050 [-3.585, 1.034], loss: 1.197431, mean_absolute_error: 12.749150, mean_q: 5.874903, mean_eps: 0.888809
  123741/2000000: episode: 1286, duration: 1.763s, episode steps: 121, steps per second: 69, episode reward: -176.245, mean reward: -1.457 [-100.000, 22.334], mean action: 1.603 [0.000, 3.000], mean observation: 0.063 [-1.384, 3.916], loss: 2.026477, mean_absolute_error: 14.121955, mean_q: 3.301097, mean_eps: 0.888688
  123864/2000000: episode: 1287, duration: 1.745s, episode steps: 123, steps per second: 70, episode reward: -144.062, mean reward: -1.171 [-100.000, 9.839], mean action: 1.626 [0.000, 3.000], mean observation: 0.060 [-1.216, 4.055], loss: 1.706683, mean_absolute_error: 14.197254, mean_q: 3.905023, mean_eps: 0.888578
  123961/2000000: episode: 1288, duration: 1.407s, episode steps: 97, steps per second: 69, episode reward: -293.487, mean reward: -3.026 [-100.000, 0.389], mean action: 1.474 [0.000, 3.000], mean observation: 0.175 [-1.199, 1.391], loss: 2.107633, mean_absolute_error: 14.577774, mean_q: 4.348412, mean_eps: 0.888479
  124036/2000000: episode: 1289, duration: 1.075s, episode steps: 75, steps per second: 70, episode reward: -142.725, mean reward: -1.903 [-100.000, 16.834], mean action: 1.813 [0.000, 3.000], mean observation: -0.181 [-1.536, 1.000], loss: 1.695758, mean_absolute_error: 13.504378, mean_q: 3.600411, mean_eps: 0.888402
  124113/2000000: episode: 1290, duration: 1.142s, episode steps: 77, steps per second: 67, episode reward: -116.888, mean reward: -1.518 [-100.000, 6.374], mean action: 1.494 [0.000, 3.000], mean observation: -0.060 [-4.703, 1.000], loss: 1.998418, mean_absolute_error: 14.213399, mean_q: 3.418988, mean_eps: 0.888333
  124276/2000000: episode: 1291, duration: 2.328s, episode steps: 163, steps per second: 70, episode reward: -307.678, mean reward: -1.888 [-100.000, 48.940], mean action: 1.595 [0.000, 3.000], mean observation: 0.066 [-1.089, 3.000], loss: 1.203917, mean_absolute_error: 13.104301, mean_q: 6.038083, mean_eps: 0.888225
  124411/2000000: episode: 1292, duration: 1.935s, episode steps: 135, steps per second: 70, episode reward: -140.356, mean reward: -1.040 [-100.000, 12.801], mean action: 1.563 [0.000, 3.000], mean observation: 0.049 [-4.337, 1.045], loss: 1.655087, mean_absolute_error: 13.256391, mean_q: 4.278805, mean_eps: 0.888092
  124475/2000000: episode: 1293, duration: 0.906s, episode steps: 64, steps per second: 71, episode reward: -116.903, mean reward: -1.827 [-100.000, 5.803], mean action: 1.594 [0.000, 3.000], mean observation: -0.154 [-5.022, 1.000], loss: 1.645969, mean_absolute_error: 13.541649, mean_q: 4.661143, mean_eps: 0.888002
  124591/2000000: episode: 1294, duration: 1.649s, episode steps: 116, steps per second: 70, episode reward: -155.937, mean reward: -1.344 [-100.000, 7.719], mean action: 1.560 [0.000, 3.000], mean observation: -0.090 [-3.780, 1.000], loss: 1.624006, mean_absolute_error: 14.000982, mean_q: 3.822194, mean_eps: 0.887921
  124675/2000000: episode: 1295, duration: 1.201s, episode steps: 84, steps per second: 70, episode reward: -91.721, mean reward: -1.092 [-100.000, 20.688], mean action: 1.583 [0.000, 3.000], mean observation: 0.093 [-1.168, 1.000], loss: 1.436650, mean_absolute_error: 13.401642, mean_q: 3.621009, mean_eps: 0.887831
  124773/2000000: episode: 1296, duration: 1.413s, episode steps: 98, steps per second: 69, episode reward: -263.488, mean reward: -2.689 [-100.000, 8.403], mean action: 1.429 [0.000, 3.000], mean observation: 0.036 [-6.025, 1.000], loss: 1.561482, mean_absolute_error: 13.614033, mean_q: 4.668882, mean_eps: 0.887748
  124870/2000000: episode: 1297, duration: 1.393s, episode steps: 97, steps per second: 70, episode reward: -140.699, mean reward: -1.451 [-100.000, 9.676], mean action: 1.732 [0.000, 3.000], mean observation: -0.091 [-1.150, 1.000], loss: 1.932123, mean_absolute_error: 14.219673, mean_q: 4.737238, mean_eps: 0.887660
  124979/2000000: episode: 1298, duration: 1.529s, episode steps: 109, steps per second: 71, episode reward: -154.315, mean reward: -1.416 [-100.000, 9.564], mean action: 1.532 [0.000, 3.000], mean observation: -0.019 [-1.199, 4.377], loss: 1.767810, mean_absolute_error: 13.661990, mean_q: 4.618125, mean_eps: 0.887568
  125054/2000000: episode: 1299, duration: 1.077s, episode steps: 75, steps per second: 70, episode reward: -150.934, mean reward: -2.012 [-100.000, 6.481], mean action: 1.547 [0.000, 3.000], mean observation: -0.048 [-1.449, 4.830], loss: 1.286088, mean_absolute_error: 13.047852, mean_q: 4.004141, mean_eps: 0.887486
  125183/2000000: episode: 1300, duration: 1.800s, episode steps: 129, steps per second: 72, episode reward: -109.632, mean reward: -0.850 [-100.000, 18.672], mean action: 1.496 [0.000, 3.000], mean observation: 0.002 [-1.214, 4.218], loss: 1.456374, mean_absolute_error: 12.722875, mean_q: 5.008815, mean_eps: 0.887394
  125250/2000000: episode: 1301, duration: 0.986s, episode steps: 67, steps per second: 68, episode reward: -134.910, mean reward: -2.014 [-100.000, 78.208], mean action: 1.522 [0.000, 3.000], mean observation: -0.118 [-3.839, 1.000], loss: 1.877319, mean_absolute_error: 12.923556, mean_q: 6.471159, mean_eps: 0.887306
  125324/2000000: episode: 1302, duration: 1.080s, episode steps: 74, steps per second: 69, episode reward: -131.261, mean reward: -1.774 [-100.000, 6.111], mean action: 1.676 [0.000, 3.000], mean observation: -0.156 [-1.251, 4.590], loss: 1.566450, mean_absolute_error: 13.029892, mean_q: 4.373220, mean_eps: 0.887243
  125440/2000000: episode: 1303, duration: 1.687s, episode steps: 116, steps per second: 69, episode reward: -239.783, mean reward: -2.067 [-100.000, 8.369], mean action: 1.491 [0.000, 3.000], mean observation: 0.117 [-1.507, 5.775], loss: 1.452091, mean_absolute_error: 13.582113, mean_q: 4.119819, mean_eps: 0.887158
  125517/2000000: episode: 1304, duration: 1.131s, episode steps: 77, steps per second: 68, episode reward: -174.373, mean reward: -2.265 [-100.000, 11.436], mean action: 1.545 [0.000, 3.000], mean observation: -0.028 [-5.447, 1.000], loss: 1.451104, mean_absolute_error: 12.838935, mean_q: 4.997075, mean_eps: 0.887070
  125642/2000000: episode: 1305, duration: 1.779s, episode steps: 125, steps per second: 70, episode reward: -122.997, mean reward: -0.984 [-100.000, 17.086], mean action: 1.616 [0.000, 3.000], mean observation: 0.034 [-1.350, 1.000], loss: 1.547203, mean_absolute_error: 12.974401, mean_q: 5.570115, mean_eps: 0.886978
  125759/2000000: episode: 1306, duration: 1.662s, episode steps: 117, steps per second: 70, episode reward: -150.688, mean reward: -1.288 [-100.000, 11.095], mean action: 1.701 [0.000, 3.000], mean observation: -0.047 [-4.045, 1.000], loss: 1.458118, mean_absolute_error: 12.464585, mean_q: 5.729905, mean_eps: 0.886870
  125823/2000000: episode: 1307, duration: 0.913s, episode steps: 64, steps per second: 70, episode reward: -149.043, mean reward: -2.329 [-100.000, 8.249], mean action: 1.531 [0.000, 3.000], mean observation: -0.087 [-1.633, 5.681], loss: 2.114297, mean_absolute_error: 14.150463, mean_q: 4.416023, mean_eps: 0.886789
  125900/2000000: episode: 1308, duration: 1.120s, episode steps: 77, steps per second: 69, episode reward: -300.976, mean reward: -3.909 [-100.000, 118.830], mean action: 1.390 [0.000, 3.000], mean observation: -0.075 [-3.945, 1.000], loss: 1.646035, mean_absolute_error: 12.923764, mean_q: 5.920315, mean_eps: 0.886726
  125978/2000000: episode: 1309, duration: 1.138s, episode steps: 78, steps per second: 69, episode reward: -144.682, mean reward: -1.855 [-100.000, 7.257], mean action: 1.705 [0.000, 3.000], mean observation: -0.097 [-1.343, 4.513], loss: 1.256586, mean_absolute_error: 12.760984, mean_q: 5.633307, mean_eps: 0.886656
  126068/2000000: episode: 1310, duration: 1.277s, episode steps: 90, steps per second: 70, episode reward: -233.794, mean reward: -2.598 [-100.000, 112.625], mean action: 1.456 [0.000, 3.000], mean observation: -0.047 [-2.865, 1.000], loss: 1.888688, mean_absolute_error: 13.923656, mean_q: 4.264355, mean_eps: 0.886580
  126140/2000000: episode: 1311, duration: 1.078s, episode steps: 72, steps per second: 67, episode reward: -168.167, mean reward: -2.336 [-100.000, 4.713], mean action: 1.833 [0.000, 3.000], mean observation: -0.188 [-1.372, 1.000], loss: 1.528660, mean_absolute_error: 13.318639, mean_q: 5.558696, mean_eps: 0.886508
  126248/2000000: episode: 1312, duration: 1.596s, episode steps: 108, steps per second: 68, episode reward: -334.499, mean reward: -3.097 [-100.000, 1.711], mean action: 1.722 [0.000, 3.000], mean observation: -0.154 [-1.947, 1.000], loss: 1.314173, mean_absolute_error: 14.009622, mean_q: 3.727885, mean_eps: 0.886427
  126369/2000000: episode: 1313, duration: 1.747s, episode steps: 121, steps per second: 69, episode reward: -118.075, mean reward: -0.976 [-100.000, 12.759], mean action: 1.620 [0.000, 3.000], mean observation: -0.016 [-1.295, 3.569], loss: 1.775787, mean_absolute_error: 12.578650, mean_q: 6.001116, mean_eps: 0.886323
  126442/2000000: episode: 1314, duration: 1.035s, episode steps: 73, steps per second: 71, episode reward: -137.439, mean reward: -1.883 [-100.000, 6.613], mean action: 1.589 [0.000, 3.000], mean observation: 0.038 [-1.369, 4.419], loss: 1.494763, mean_absolute_error: 13.361884, mean_q: 4.376257, mean_eps: 0.886235
  126559/2000000: episode: 1315, duration: 1.638s, episode steps: 117, steps per second: 71, episode reward: -268.830, mean reward: -2.298 [-100.000, 21.290], mean action: 1.538 [0.000, 3.000], mean observation: -0.011 [-5.998, 1.040], loss: 1.763490, mean_absolute_error: 13.650564, mean_q: 3.752733, mean_eps: 0.886150
  126630/2000000: episode: 1316, duration: 1.005s, episode steps: 71, steps per second: 71, episode reward: -170.707, mean reward: -2.404 [-100.000, 7.664], mean action: 1.690 [0.000, 3.000], mean observation: -0.045 [-4.024, 1.000], loss: 2.162428, mean_absolute_error: 13.635420, mean_q: 4.186981, mean_eps: 0.886065
  126744/2000000: episode: 1317, duration: 1.645s, episode steps: 114, steps per second: 69, episode reward: -327.061, mean reward: -2.869 [-100.000, 3.435], mean action: 1.728 [0.000, 3.000], mean observation: -0.132 [-1.661, 3.193], loss: 1.497373, mean_absolute_error: 14.423106, mean_q: 3.760582, mean_eps: 0.885983
  126825/2000000: episode: 1318, duration: 1.195s, episode steps: 81, steps per second: 68, episode reward: -165.955, mean reward: -2.049 [-100.000, 8.573], mean action: 1.469 [0.000, 3.000], mean observation: 0.050 [-1.402, 2.671], loss: 2.245779, mean_absolute_error: 14.077189, mean_q: 2.810709, mean_eps: 0.885894
  126952/2000000: episode: 1319, duration: 1.796s, episode steps: 127, steps per second: 71, episode reward: -161.135, mean reward: -1.269 [-100.000, 18.994], mean action: 1.378 [0.000, 3.000], mean observation: 0.066 [-3.816, 1.050], loss: 1.522236, mean_absolute_error: 13.431251, mean_q: 5.249833, mean_eps: 0.885801
  127110/2000000: episode: 1320, duration: 2.236s, episode steps: 158, steps per second: 71, episode reward: -377.106, mean reward: -2.387 [-100.000, 92.212], mean action: 1.443 [0.000, 3.000], mean observation: 0.281 [-1.440, 3.152], loss: 1.816771, mean_absolute_error: 13.630248, mean_q: 3.905184, mean_eps: 0.885673
  127228/2000000: episode: 1321, duration: 1.689s, episode steps: 118, steps per second: 70, episode reward: -159.601, mean reward: -1.353 [-100.000, 22.524], mean action: 1.610 [0.000, 3.000], mean observation: 0.082 [-4.375, 1.000], loss: 1.377558, mean_absolute_error: 13.456341, mean_q: 5.459675, mean_eps: 0.885549
  127297/2000000: episode: 1322, duration: 1.005s, episode steps: 69, steps per second: 69, episode reward: -113.309, mean reward: -1.642 [-100.000, 6.225], mean action: 1.449 [0.000, 3.000], mean observation: 0.018 [-4.153, 1.000], loss: 1.296101, mean_absolute_error: 13.303141, mean_q: 5.426562, mean_eps: 0.885464
  127417/2000000: episode: 1323, duration: 1.677s, episode steps: 120, steps per second: 72, episode reward: -69.467, mean reward: -0.579 [-100.000, 77.304], mean action: 1.500 [0.000, 3.000], mean observation: -0.039 [-1.626, 1.507], loss: 1.097868, mean_absolute_error: 13.197826, mean_q: 4.740580, mean_eps: 0.885378
  127509/2000000: episode: 1324, duration: 1.310s, episode steps: 92, steps per second: 70, episode reward: -411.643, mean reward: -4.474 [-100.000, 62.944], mean action: 1.467 [0.000, 3.000], mean observation: 0.015 [-3.590, 1.198], loss: 1.687541, mean_absolute_error: 12.431446, mean_q: 5.589405, mean_eps: 0.885282
  127621/2000000: episode: 1325, duration: 1.565s, episode steps: 112, steps per second: 72, episode reward: -145.225, mean reward: -1.297 [-100.000, 13.218], mean action: 1.652 [0.000, 3.000], mean observation: 0.135 [-3.262, 1.000], loss: 1.482731, mean_absolute_error: 13.257923, mean_q: 4.312967, mean_eps: 0.885191
  127706/2000000: episode: 1326, duration: 1.186s, episode steps: 85, steps per second: 72, episode reward: -123.482, mean reward: -1.453 [-100.000, 11.666], mean action: 1.424 [0.000, 3.000], mean observation: 0.082 [-2.961, 1.000], loss: 1.565223, mean_absolute_error: 13.587124, mean_q: 6.002139, mean_eps: 0.885102
  127816/2000000: episode: 1327, duration: 1.597s, episode steps: 110, steps per second: 69, episode reward: -29.354, mean reward: -0.267 [-100.000, 67.623], mean action: 1.655 [0.000, 3.000], mean observation: -0.096 [-1.667, 1.406], loss: 1.505403, mean_absolute_error: 11.940694, mean_q: 5.310304, mean_eps: 0.885016
  127899/2000000: episode: 1328, duration: 1.182s, episode steps: 83, steps per second: 70, episode reward: -147.943, mean reward: -1.782 [-100.000, 11.835], mean action: 1.675 [0.000, 3.000], mean observation: -0.075 [-1.308, 3.944], loss: 1.952216, mean_absolute_error: 13.209060, mean_q: 3.952251, mean_eps: 0.884930
  128005/2000000: episode: 1329, duration: 1.514s, episode steps: 106, steps per second: 70, episode reward: -347.267, mean reward: -3.276 [-100.000, 18.102], mean action: 1.575 [0.000, 3.000], mean observation: 0.038 [-4.447, 1.301], loss: 1.274632, mean_absolute_error: 13.049605, mean_q: 4.683051, mean_eps: 0.884843
  128072/2000000: episode: 1330, duration: 0.987s, episode steps: 67, steps per second: 68, episode reward: -125.841, mean reward: -1.878 [-100.000, 9.321], mean action: 1.642 [0.000, 3.000], mean observation: -0.039 [-1.339, 4.651], loss: 2.051875, mean_absolute_error: 14.255727, mean_q: 3.920573, mean_eps: 0.884766
  128166/2000000: episode: 1331, duration: 1.374s, episode steps: 94, steps per second: 68, episode reward: -160.359, mean reward: -1.706 [-100.000, 46.914], mean action: 1.394 [0.000, 3.000], mean observation: 0.115 [-1.235, 2.774], loss: 1.228456, mean_absolute_error: 13.120933, mean_q: 5.590294, mean_eps: 0.884694
  128251/2000000: episode: 1332, duration: 1.238s, episode steps: 85, steps per second: 69, episode reward: -154.953, mean reward: -1.823 [-100.000, 5.558], mean action: 1.494 [0.000, 3.000], mean observation: -0.012 [-4.222, 1.000], loss: 1.039450, mean_absolute_error: 13.202359, mean_q: 4.787967, mean_eps: 0.884613
  128373/2000000: episode: 1333, duration: 1.906s, episode steps: 122, steps per second: 64, episode reward: -141.984, mean reward: -1.164 [-100.000, 6.375], mean action: 1.607 [0.000, 3.000], mean observation: 0.083 [-3.248, 1.000], loss: 1.508255, mean_absolute_error: 13.468843, mean_q: 4.808400, mean_eps: 0.884519
  128445/2000000: episode: 1334, duration: 1.036s, episode steps: 72, steps per second: 69, episode reward: -94.355, mean reward: -1.310 [-100.000, 12.105], mean action: 1.542 [0.000, 3.000], mean observation: 0.072 [-1.215, 3.517], loss: 1.600563, mean_absolute_error: 12.479600, mean_q: 6.379097, mean_eps: 0.884431
  128511/2000000: episode: 1335, duration: 1.008s, episode steps: 66, steps per second: 65, episode reward: -117.507, mean reward: -1.780 [-100.000, 17.029], mean action: 1.606 [0.000, 3.000], mean observation: -0.154 [-1.403, 1.000], loss: 1.408916, mean_absolute_error: 13.654097, mean_q: 3.466274, mean_eps: 0.884370
  128592/2000000: episode: 1336, duration: 1.309s, episode steps: 81, steps per second: 62, episode reward: -151.034, mean reward: -1.865 [-100.000, 15.525], mean action: 1.753 [0.000, 3.000], mean observation: -0.129 [-4.823, 1.000], loss: 1.681048, mean_absolute_error: 14.929767, mean_q: 3.349831, mean_eps: 0.884305
  128672/2000000: episode: 1337, duration: 1.194s, episode steps: 80, steps per second: 67, episode reward: -130.028, mean reward: -1.625 [-100.000, 11.483], mean action: 1.688 [0.000, 3.000], mean observation: -0.053 [-3.964, 1.000], loss: 1.453646, mean_absolute_error: 12.934158, mean_q: 5.131442, mean_eps: 0.884233
  128739/2000000: episode: 1338, duration: 1.055s, episode steps: 67, steps per second: 64, episode reward: -92.959, mean reward: -1.387 [-100.000, 10.649], mean action: 1.567 [0.000, 3.000], mean observation: 0.038 [-1.224, 1.000], loss: 1.947144, mean_absolute_error: 13.416195, mean_q: 4.490062, mean_eps: 0.884166
  128820/2000000: episode: 1339, duration: 1.210s, episode steps: 81, steps per second: 67, episode reward: -166.739, mean reward: -2.059 [-100.000, 5.933], mean action: 1.296 [0.000, 3.000], mean observation: 0.084 [-4.913, 1.000], loss: 1.571275, mean_absolute_error: 14.041176, mean_q: 5.801527, mean_eps: 0.884100
  128898/2000000: episode: 1340, duration: 1.173s, episode steps: 78, steps per second: 67, episode reward: -159.262, mean reward: -2.042 [-100.000, 7.760], mean action: 1.385 [0.000, 3.000], mean observation: -0.093 [-2.288, 1.000], loss: 1.437182, mean_absolute_error: 13.569025, mean_q: 4.982874, mean_eps: 0.884028
  129006/2000000: episode: 1341, duration: 1.584s, episode steps: 108, steps per second: 68, episode reward: -143.501, mean reward: -1.329 [-100.000, 17.872], mean action: 1.620 [0.000, 3.000], mean observation: 0.044 [-4.603, 1.000], loss: 1.895331, mean_absolute_error: 13.394433, mean_q: 4.859205, mean_eps: 0.883943
  129100/2000000: episode: 1342, duration: 1.395s, episode steps: 94, steps per second: 67, episode reward: -133.975, mean reward: -1.425 [-100.000, 7.429], mean action: 1.638 [0.000, 3.000], mean observation: -0.093 [-3.789, 1.000], loss: 1.502087, mean_absolute_error: 13.544721, mean_q: 4.969480, mean_eps: 0.883853
  129224/2000000: episode: 1343, duration: 1.838s, episode steps: 124, steps per second: 67, episode reward: -66.230, mean reward: -0.534 [-100.000, 40.238], mean action: 1.460 [0.000, 3.000], mean observation: 0.166 [-1.277, 1.689], loss: 1.563607, mean_absolute_error: 13.704324, mean_q: 4.819633, mean_eps: 0.883756
  129331/2000000: episode: 1344, duration: 1.568s, episode steps: 107, steps per second: 68, episode reward: -128.407, mean reward: -1.200 [-100.000, 8.647], mean action: 1.542 [0.000, 3.000], mean observation: -0.065 [-1.144, 4.435], loss: 1.969427, mean_absolute_error: 13.481984, mean_q: 6.230342, mean_eps: 0.883652
  129394/2000000: episode: 1345, duration: 0.933s, episode steps: 63, steps per second: 68, episode reward: -111.835, mean reward: -1.775 [-100.000, 12.895], mean action: 1.873 [0.000, 3.000], mean observation: -0.152 [-1.312, 1.000], loss: 1.300385, mean_absolute_error: 13.441129, mean_q: 4.408698, mean_eps: 0.883574
  129498/2000000: episode: 1346, duration: 1.495s, episode steps: 104, steps per second: 70, episode reward: -157.037, mean reward: -1.510 [-100.000, 33.896], mean action: 1.587 [0.000, 3.000], mean observation: -0.035 [-1.156, 1.519], loss: 1.432546, mean_absolute_error: 12.362124, mean_q: 4.964493, mean_eps: 0.883499
  129613/2000000: episode: 1347, duration: 1.653s, episode steps: 115, steps per second: 70, episode reward: -309.495, mean reward: -2.691 [-100.000, 12.552], mean action: 1.522 [0.000, 3.000], mean observation: 0.086 [-1.471, 3.309], loss: 1.403547, mean_absolute_error: 13.701800, mean_q: 5.144515, mean_eps: 0.883400
  129695/2000000: episode: 1348, duration: 1.151s, episode steps: 82, steps per second: 71, episode reward: -128.740, mean reward: -1.570 [-100.000, 9.993], mean action: 1.573 [0.000, 3.000], mean observation: -0.114 [-4.129, 1.000], loss: 1.837244, mean_absolute_error: 13.083707, mean_q: 5.237372, mean_eps: 0.883311
  129790/2000000: episode: 1349, duration: 1.391s, episode steps: 95, steps per second: 68, episode reward: -95.311, mean reward: -1.003 [-100.000, 15.285], mean action: 1.632 [0.000, 3.000], mean observation: -0.038 [-3.636, 1.000], loss: 1.527720, mean_absolute_error: 12.716111, mean_q: 5.224640, mean_eps: 0.883232
  129898/2000000: episode: 1350, duration: 1.714s, episode steps: 108, steps per second: 63, episode reward: -350.737, mean reward: -3.248 [-100.000, 1.011], mean action: 1.500 [0.000, 3.000], mean observation: 0.120 [-1.530, 1.379], loss: 1.952469, mean_absolute_error: 13.424508, mean_q: 4.938722, mean_eps: 0.883140
  130009/2000000: episode: 1351, duration: 1.622s, episode steps: 111, steps per second: 68, episode reward: -56.958, mean reward: -0.513 [-100.000, 90.416], mean action: 1.775 [0.000, 3.000], mean observation: -0.079 [-2.028, 1.481], loss: 1.332563, mean_absolute_error: 13.137513, mean_q: 5.589673, mean_eps: 0.883041
  130099/2000000: episode: 1352, duration: 1.275s, episode steps: 90, steps per second: 71, episode reward: -140.239, mean reward: -1.558 [-100.000, 15.019], mean action: 1.578 [0.000, 3.000], mean observation: -0.050 [-1.265, 1.000], loss: 1.794583, mean_absolute_error: 14.255173, mean_q: 5.870188, mean_eps: 0.882951
  130197/2000000: episode: 1353, duration: 1.435s, episode steps: 98, steps per second: 68, episode reward: -166.562, mean reward: -1.700 [-100.000, 15.008], mean action: 1.653 [0.000, 3.000], mean observation: -0.116 [-4.355, 1.000], loss: 1.870004, mean_absolute_error: 15.133928, mean_q: 6.264011, mean_eps: 0.882867
  130318/2000000: episode: 1354, duration: 1.713s, episode steps: 121, steps per second: 71, episode reward: -122.424, mean reward: -1.012 [-100.000, 15.965], mean action: 1.455 [0.000, 3.000], mean observation: 0.078 [-1.277, 1.000], loss: 1.387904, mean_absolute_error: 14.205102, mean_q: 6.225561, mean_eps: 0.882768
  130407/2000000: episode: 1355, duration: 1.285s, episode steps: 89, steps per second: 69, episode reward: -144.359, mean reward: -1.622 [-100.000, 5.406], mean action: 1.674 [0.000, 3.000], mean observation: -0.054 [-1.218, 4.154], loss: 1.737202, mean_absolute_error: 13.557924, mean_q: 5.710875, mean_eps: 0.882674
  130494/2000000: episode: 1356, duration: 1.280s, episode steps: 87, steps per second: 68, episode reward: -145.533, mean reward: -1.673 [-100.000, 3.527], mean action: 1.506 [0.000, 3.000], mean observation: -0.075 [-1.116, 2.902], loss: 2.006354, mean_absolute_error: 14.217254, mean_q: 5.317772, mean_eps: 0.882595
  130561/2000000: episode: 1357, duration: 0.990s, episode steps: 67, steps per second: 68, episode reward: -148.148, mean reward: -2.211 [-100.000, 5.883], mean action: 1.761 [0.000, 3.000], mean observation: -0.112 [-1.381, 4.357], loss: 2.139866, mean_absolute_error: 14.917221, mean_q: 5.343867, mean_eps: 0.882525
  130635/2000000: episode: 1358, duration: 1.037s, episode steps: 74, steps per second: 71, episode reward: -125.739, mean reward: -1.699 [-100.000, 9.119], mean action: 1.581 [0.000, 3.000], mean observation: -0.018 [-1.174, 3.197], loss: 1.403636, mean_absolute_error: 13.450827, mean_q: 6.072139, mean_eps: 0.882462
  130804/2000000: episode: 1359, duration: 2.463s, episode steps: 169, steps per second: 69, episode reward: -247.253, mean reward: -1.463 [-100.000, 79.626], mean action: 1.509 [0.000, 3.000], mean observation: -0.059 [-1.928, 1.057], loss: 1.434819, mean_absolute_error: 14.585645, mean_q: 4.766446, mean_eps: 0.882354
  130895/2000000: episode: 1360, duration: 1.328s, episode steps: 91, steps per second: 69, episode reward: -265.115, mean reward: -2.913 [-100.000, 5.281], mean action: 1.527 [0.000, 3.000], mean observation: 0.053 [-1.499, 4.199], loss: 1.870943, mean_absolute_error: 14.361055, mean_q: 4.578390, mean_eps: 0.882237
  131017/2000000: episode: 1361, duration: 1.781s, episode steps: 122, steps per second: 68, episode reward: -224.810, mean reward: -1.843 [-100.000, 7.026], mean action: 1.590 [0.000, 3.000], mean observation: -0.112 [-3.195, 1.044], loss: 2.055723, mean_absolute_error: 16.305548, mean_q: 1.967253, mean_eps: 0.882140
  131130/2000000: episode: 1362, duration: 1.660s, episode steps: 113, steps per second: 68, episode reward: -329.554, mean reward: -2.916 [-100.000, 2.618], mean action: 1.752 [0.000, 3.000], mean observation: -0.149 [-1.999, 1.033], loss: 1.607853, mean_absolute_error: 14.502694, mean_q: 7.002083, mean_eps: 0.882033
  131249/2000000: episode: 1363, duration: 1.738s, episode steps: 119, steps per second: 68, episode reward: -277.876, mean reward: -2.335 [-100.000, 7.167], mean action: 1.513 [0.000, 3.000], mean observation: -0.079 [-4.625, 1.000], loss: 1.299847, mean_absolute_error: 13.441641, mean_q: 7.910601, mean_eps: 0.881929
  131371/2000000: episode: 1364, duration: 1.713s, episode steps: 122, steps per second: 71, episode reward: -181.337, mean reward: -1.486 [-100.000, 6.735], mean action: 1.557 [0.000, 3.000], mean observation: -0.084 [-1.492, 4.911], loss: 1.447352, mean_absolute_error: 14.469658, mean_q: 5.402601, mean_eps: 0.881821
  131466/2000000: episode: 1365, duration: 1.367s, episode steps: 95, steps per second: 70, episode reward: -182.687, mean reward: -1.923 [-100.000, 7.927], mean action: 1.505 [0.000, 3.000], mean observation: -0.018 [-1.424, 4.987], loss: 1.840175, mean_absolute_error: 14.916302, mean_q: 4.371909, mean_eps: 0.881724
  131536/2000000: episode: 1366, duration: 1.023s, episode steps: 70, steps per second: 68, episode reward: -146.643, mean reward: -2.095 [-100.000, 6.937], mean action: 1.414 [0.000, 3.000], mean observation: 0.039 [-1.419, 4.948], loss: 1.888062, mean_absolute_error: 15.459995, mean_q: 5.307346, mean_eps: 0.881650
  131621/2000000: episode: 1367, duration: 1.252s, episode steps: 85, steps per second: 68, episode reward: -187.870, mean reward: -2.210 [-100.000, 6.835], mean action: 1.565 [0.000, 3.000], mean observation: -0.037 [-1.426, 1.000], loss: 2.055841, mean_absolute_error: 15.021083, mean_q: 4.750185, mean_eps: 0.881580
  131691/2000000: episode: 1368, duration: 0.980s, episode steps: 70, steps per second: 71, episode reward: -173.716, mean reward: -2.482 [-100.000, 12.725], mean action: 1.400 [0.000, 3.000], mean observation: 0.027 [-1.610, 6.923], loss: 1.849379, mean_absolute_error: 13.948488, mean_q: 6.039421, mean_eps: 0.881510
  131777/2000000: episode: 1369, duration: 1.381s, episode steps: 86, steps per second: 62, episode reward: -76.838, mean reward: -0.893 [-100.000, 13.354], mean action: 1.593 [0.000, 3.000], mean observation: -0.056 [-3.382, 1.000], loss: 1.996962, mean_absolute_error: 15.593248, mean_q: 4.028036, mean_eps: 0.881439
  131900/2000000: episode: 1370, duration: 1.765s, episode steps: 123, steps per second: 70, episode reward: -144.248, mean reward: -1.173 [-100.000, 7.231], mean action: 1.650 [0.000, 3.000], mean observation: 0.133 [-3.722, 1.024], loss: 1.498886, mean_absolute_error: 13.212932, mean_q: 5.595490, mean_eps: 0.881346
  131998/2000000: episode: 1371, duration: 1.426s, episode steps: 98, steps per second: 69, episode reward: -208.922, mean reward: -2.132 [-100.000, 6.760], mean action: 1.541 [0.000, 3.000], mean observation: 0.128 [-1.180, 4.647], loss: 1.569023, mean_absolute_error: 15.059697, mean_q: 3.580870, mean_eps: 0.881247
  132088/2000000: episode: 1372, duration: 1.302s, episode steps: 90, steps per second: 69, episode reward: -283.082, mean reward: -3.145 [-100.000, 25.750], mean action: 1.600 [0.000, 3.000], mean observation: -0.034 [-5.828, 1.000], loss: 1.320554, mean_absolute_error: 13.248801, mean_q: 6.975323, mean_eps: 0.881162
  132143/2000000: episode: 1373, duration: 0.785s, episode steps: 55, steps per second: 70, episode reward: -126.064, mean reward: -2.292 [-100.000, 22.385], mean action: 1.491 [0.000, 3.000], mean observation: -0.152 [-3.129, 1.000], loss: 1.366590, mean_absolute_error: 13.152422, mean_q: 5.716375, mean_eps: 0.881097
  132213/2000000: episode: 1374, duration: 1.026s, episode steps: 70, steps per second: 68, episode reward: -128.533, mean reward: -1.836 [-100.000, 21.906], mean action: 1.571 [0.000, 3.000], mean observation: 0.058 [-1.454, 5.040], loss: 1.643171, mean_absolute_error: 15.974286, mean_q: 3.243698, mean_eps: 0.881040
  132297/2000000: episode: 1375, duration: 1.194s, episode steps: 84, steps per second: 70, episode reward: -175.964, mean reward: -2.095 [-100.000, 8.647], mean action: 1.833 [0.000, 3.000], mean observation: -0.159 [-1.362, 4.145], loss: 1.940314, mean_absolute_error: 15.156698, mean_q: 4.958929, mean_eps: 0.880970
  132376/2000000: episode: 1376, duration: 1.139s, episode steps: 79, steps per second: 69, episode reward: -211.716, mean reward: -2.680 [-100.000, 8.399], mean action: 1.418 [0.000, 3.000], mean observation: 0.077 [-1.502, 1.000], loss: 1.999800, mean_absolute_error: 14.563581, mean_q: 5.308480, mean_eps: 0.880898
  132456/2000000: episode: 1377, duration: 1.191s, episode steps: 80, steps per second: 67, episode reward: -314.041, mean reward: -3.926 [-100.000, 6.497], mean action: 1.675 [0.000, 3.000], mean observation: -0.055 [-1.613, 4.551], loss: 1.566612, mean_absolute_error: 14.239834, mean_q: 4.365488, mean_eps: 0.880827
  132598/2000000: episode: 1378, duration: 2.047s, episode steps: 142, steps per second: 69, episode reward: -123.342, mean reward: -0.869 [-100.000, 10.336], mean action: 1.627 [0.000, 3.000], mean observation: 0.046 [-1.311, 1.027], loss: 1.674905, mean_absolute_error: 14.787700, mean_q: 6.223221, mean_eps: 0.880727
  132710/2000000: episode: 1379, duration: 1.569s, episode steps: 112, steps per second: 71, episode reward: -204.516, mean reward: -1.826 [-100.000, 26.893], mean action: 1.527 [0.000, 3.000], mean observation: -0.142 [-4.046, 1.000], loss: 1.625105, mean_absolute_error: 14.888336, mean_q: 4.725041, mean_eps: 0.880611
  132798/2000000: episode: 1380, duration: 1.235s, episode steps: 88, steps per second: 71, episode reward: -3.141, mean reward: -0.036 [-100.000, 113.284], mean action: 1.489 [0.000, 3.000], mean observation: -0.010 [-2.026, 1.039], loss: 1.350974, mean_absolute_error: 14.995011, mean_q: 3.413092, mean_eps: 0.880521
  132875/2000000: episode: 1381, duration: 1.073s, episode steps: 77, steps per second: 72, episode reward: -109.753, mean reward: -1.425 [-100.000, 43.623], mean action: 1.286 [0.000, 3.000], mean observation: 0.093 [-3.152, 1.000], loss: 1.655495, mean_absolute_error: 14.802457, mean_q: 4.858940, mean_eps: 0.880448
  132947/2000000: episode: 1382, duration: 1.013s, episode steps: 72, steps per second: 71, episode reward: -72.273, mean reward: -1.004 [-100.000, 17.586], mean action: 1.514 [0.000, 3.000], mean observation: -0.072 [-1.275, 1.000], loss: 1.574100, mean_absolute_error: 14.879112, mean_q: 5.416446, mean_eps: 0.880381
  133076/2000000: episode: 1383, duration: 1.833s, episode steps: 129, steps per second: 70, episode reward: -121.653, mean reward: -0.943 [-100.000, 6.591], mean action: 1.558 [0.000, 3.000], mean observation: 0.112 [-1.470, 1.005], loss: 1.769599, mean_absolute_error: 14.583664, mean_q: 5.689405, mean_eps: 0.880291
  133146/2000000: episode: 1384, duration: 1.033s, episode steps: 70, steps per second: 68, episode reward: -144.613, mean reward: -2.066 [-100.000, 8.133], mean action: 1.657 [0.000, 3.000], mean observation: -0.151 [-5.451, 1.000], loss: 1.362597, mean_absolute_error: 14.659707, mean_q: 4.822912, mean_eps: 0.880201
  133246/2000000: episode: 1385, duration: 1.422s, episode steps: 100, steps per second: 70, episode reward: -138.412, mean reward: -1.384 [-100.000, 19.318], mean action: 1.680 [0.000, 3.000], mean observation: -0.118 [-4.255, 1.000], loss: 1.603306, mean_absolute_error: 14.169471, mean_q: 5.682026, mean_eps: 0.880124
  133340/2000000: episode: 1386, duration: 1.352s, episode steps: 94, steps per second: 70, episode reward: -138.831, mean reward: -1.477 [-100.000, 6.890], mean action: 1.426 [0.000, 3.000], mean observation: 0.041 [-4.398, 1.000], loss: 2.228860, mean_absolute_error: 15.133562, mean_q: 4.474858, mean_eps: 0.880037
  133422/2000000: episode: 1387, duration: 1.174s, episode steps: 82, steps per second: 70, episode reward: -173.976, mean reward: -2.122 [-100.000, 5.921], mean action: 1.707 [0.000, 3.000], mean observation: -0.175 [-1.337, 4.819], loss: 1.548690, mean_absolute_error: 15.349564, mean_q: 5.979260, mean_eps: 0.879958
  133527/2000000: episode: 1388, duration: 1.472s, episode steps: 105, steps per second: 71, episode reward: -171.716, mean reward: -1.635 [-100.000, 12.329], mean action: 1.333 [0.000, 3.000], mean observation: 0.134 [-4.256, 1.000], loss: 1.264397, mean_absolute_error: 14.891114, mean_q: 3.800330, mean_eps: 0.879873
  133605/2000000: episode: 1389, duration: 1.125s, episode steps: 78, steps per second: 69, episode reward: -123.986, mean reward: -1.590 [-100.000, 15.902], mean action: 1.513 [0.000, 3.000], mean observation: 0.019 [-1.333, 1.000], loss: 1.776607, mean_absolute_error: 14.655128, mean_q: 4.741207, mean_eps: 0.879791
  133710/2000000: episode: 1390, duration: 1.481s, episode steps: 105, steps per second: 71, episode reward: -371.838, mean reward: -3.541 [-100.000, 2.404], mean action: 1.686 [0.000, 3.000], mean observation: -0.108 [-2.005, 3.784], loss: 1.613251, mean_absolute_error: 15.181082, mean_q: 5.075103, mean_eps: 0.879708
  133782/2000000: episode: 1391, duration: 1.022s, episode steps: 72, steps per second: 70, episode reward: -153.593, mean reward: -2.133 [-100.000, 14.147], mean action: 1.625 [0.000, 3.000], mean observation: -0.061 [-3.409, 1.000], loss: 1.290295, mean_absolute_error: 14.996212, mean_q: 4.916572, mean_eps: 0.879629
  133875/2000000: episode: 1392, duration: 1.312s, episode steps: 93, steps per second: 71, episode reward: -84.592, mean reward: -0.910 [-100.000, 16.852], mean action: 1.473 [0.000, 3.000], mean observation: 0.005 [-1.074, 1.000], loss: 1.630029, mean_absolute_error: 13.763709, mean_q: 6.260409, mean_eps: 0.879555
  133974/2000000: episode: 1393, duration: 1.446s, episode steps: 99, steps per second: 68, episode reward: -230.181, mean reward: -2.325 [-100.000, 6.778], mean action: 1.707 [0.000, 3.000], mean observation: -0.151 [-1.580, 1.000], loss: 2.080403, mean_absolute_error: 14.895316, mean_q: 4.950170, mean_eps: 0.879468
  134053/2000000: episode: 1394, duration: 1.134s, episode steps: 79, steps per second: 70, episode reward: -139.046, mean reward: -1.760 [-100.000, 7.164], mean action: 1.633 [0.000, 3.000], mean observation: -0.039 [-1.340, 4.132], loss: 1.697277, mean_absolute_error: 13.583314, mean_q: 7.415619, mean_eps: 0.879387
  134186/2000000: episode: 1395, duration: 1.884s, episode steps: 133, steps per second: 71, episode reward: -153.667, mean reward: -1.155 [-100.000, 7.174], mean action: 1.549 [0.000, 3.000], mean observation: 0.184 [-1.364, 4.497], loss: 1.031640, mean_absolute_error: 13.740682, mean_q: 6.734826, mean_eps: 0.879292
  134313/2000000: episode: 1396, duration: 1.832s, episode steps: 127, steps per second: 69, episode reward: -160.667, mean reward: -1.265 [-100.000, 8.176], mean action: 1.654 [0.000, 3.000], mean observation: -0.014 [-1.296, 4.661], loss: 1.245183, mean_absolute_error: 14.325914, mean_q: 6.094426, mean_eps: 0.879175
  134423/2000000: episode: 1397, duration: 1.545s, episode steps: 110, steps per second: 71, episode reward: -172.278, mean reward: -1.566 [-100.000, 4.176], mean action: 1.545 [0.000, 3.000], mean observation: 0.141 [-1.166, 3.520], loss: 1.479988, mean_absolute_error: 14.322123, mean_q: 6.249191, mean_eps: 0.879069
  134499/2000000: episode: 1398, duration: 1.087s, episode steps: 76, steps per second: 70, episode reward: -132.135, mean reward: -1.739 [-100.000, 9.458], mean action: 1.500 [0.000, 3.000], mean observation: -0.001 [-4.927, 1.000], loss: 1.915504, mean_absolute_error: 14.096024, mean_q: 5.422145, mean_eps: 0.878986
  134570/2000000: episode: 1399, duration: 1.044s, episode steps: 71, steps per second: 68, episode reward: -76.708, mean reward: -1.080 [-100.000, 13.330], mean action: 1.592 [0.000, 3.000], mean observation: -0.021 [-1.212, 1.000], loss: 2.031616, mean_absolute_error: 15.249670, mean_q: 3.609271, mean_eps: 0.878919
  134651/2000000: episode: 1400, duration: 1.155s, episode steps: 81, steps per second: 70, episode reward: -228.683, mean reward: -2.823 [-100.000, 19.719], mean action: 1.494 [0.000, 3.000], mean observation: -0.080 [-3.718, 1.000], loss: 1.779991, mean_absolute_error: 15.021419, mean_q: 6.219147, mean_eps: 0.878851
  134726/2000000: episode: 1401, duration: 1.097s, episode steps: 75, steps per second: 68, episode reward: -138.695, mean reward: -1.849 [-100.000, 5.327], mean action: 1.480 [0.000, 3.000], mean observation: -0.144 [-4.573, 1.000], loss: 1.268197, mean_absolute_error: 13.446419, mean_q: 5.886402, mean_eps: 0.878781
  134792/2000000: episode: 1402, duration: 0.959s, episode steps: 66, steps per second: 69, episode reward: -103.640, mean reward: -1.570 [-100.000, 6.261], mean action: 1.485 [0.000, 3.000], mean observation: -0.010 [-1.384, 1.000], loss: 1.344727, mean_absolute_error: 14.078224, mean_q: 6.653448, mean_eps: 0.878718
  134886/2000000: episode: 1403, duration: 1.349s, episode steps: 94, steps per second: 70, episode reward: -181.495, mean reward: -1.931 [-100.000, 5.579], mean action: 1.255 [0.000, 3.000], mean observation: 0.112 [-4.999, 1.000], loss: 1.724825, mean_absolute_error: 13.943206, mean_q: 5.603292, mean_eps: 0.878646
  134989/2000000: episode: 1404, duration: 1.471s, episode steps: 103, steps per second: 70, episode reward: -153.697, mean reward: -1.492 [-100.000, 22.085], mean action: 1.456 [0.000, 3.000], mean observation: 0.161 [-3.503, 1.005], loss: 1.180867, mean_absolute_error: 14.052556, mean_q: 4.923861, mean_eps: 0.878556
  135058/2000000: episode: 1405, duration: 0.967s, episode steps: 69, steps per second: 71, episode reward: -140.004, mean reward: -2.029 [-100.000, 15.587], mean action: 1.725 [0.000, 3.000], mean observation: -0.089 [-1.518, 1.000], loss: 1.232441, mean_absolute_error: 14.809840, mean_q: 4.594176, mean_eps: 0.878478
  135150/2000000: episode: 1406, duration: 1.299s, episode steps: 92, steps per second: 71, episode reward: -303.128, mean reward: -3.295 [-100.000, 59.357], mean action: 1.489 [0.000, 3.000], mean observation: 0.079 [-3.547, 1.295], loss: 1.581809, mean_absolute_error: 15.105919, mean_q: 3.176199, mean_eps: 0.878406
  135229/2000000: episode: 1407, duration: 1.156s, episode steps: 79, steps per second: 68, episode reward: -163.952, mean reward: -2.075 [-100.000, 15.774], mean action: 1.519 [0.000, 3.000], mean observation: 0.048 [-2.860, 1.000], loss: 1.432684, mean_absolute_error: 14.165022, mean_q: 4.575006, mean_eps: 0.878329
  135325/2000000: episode: 1408, duration: 1.352s, episode steps: 96, steps per second: 71, episode reward: -193.243, mean reward: -2.013 [-100.000, 11.878], mean action: 1.448 [0.000, 3.000], mean observation: 0.151 [-1.902, 2.388], loss: 1.599789, mean_absolute_error: 14.263797, mean_q: 6.018133, mean_eps: 0.878250
  135400/2000000: episode: 1409, duration: 1.091s, episode steps: 75, steps per second: 69, episode reward: -199.293, mean reward: -2.657 [-100.000, 11.677], mean action: 1.760 [0.000, 3.000], mean observation: -0.210 [-1.485, 5.854], loss: 1.436661, mean_absolute_error: 15.192267, mean_q: 5.331032, mean_eps: 0.878174
  135504/2000000: episode: 1410, duration: 1.521s, episode steps: 104, steps per second: 68, episode reward: -165.273, mean reward: -1.589 [-100.000, 12.080], mean action: 1.596 [0.000, 3.000], mean observation: -0.029 [-1.286, 3.903], loss: 2.104994, mean_absolute_error: 14.683117, mean_q: 5.874326, mean_eps: 0.878095
  135620/2000000: episode: 1411, duration: 1.684s, episode steps: 116, steps per second: 69, episode reward: -151.584, mean reward: -1.307 [-100.000, 5.598], mean action: 1.733 [0.000, 3.000], mean observation: -0.059 [-1.222, 1.410], loss: 1.646187, mean_absolute_error: 14.346001, mean_q: 6.056072, mean_eps: 0.877996
  135676/2000000: episode: 1412, duration: 0.857s, episode steps: 56, steps per second: 65, episode reward: -121.944, mean reward: -2.178 [-100.000, 6.348], mean action: 1.232 [0.000, 3.000], mean observation: 0.062 [-1.492, 5.604], loss: 2.252557, mean_absolute_error: 15.029941, mean_q: 6.407753, mean_eps: 0.877919
  135780/2000000: episode: 1413, duration: 1.527s, episode steps: 104, steps per second: 68, episode reward: -142.035, mean reward: -1.366 [-100.000, 9.600], mean action: 1.606 [0.000, 3.000], mean observation: -0.038 [-4.718, 1.000], loss: 1.294676, mean_absolute_error: 14.602060, mean_q: 5.456893, mean_eps: 0.877847
  135893/2000000: episode: 1414, duration: 1.643s, episode steps: 113, steps per second: 69, episode reward: -163.199, mean reward: -1.444 [-100.000, 11.018], mean action: 1.699 [0.000, 3.000], mean observation: 0.006 [-4.565, 1.000], loss: 1.436584, mean_absolute_error: 14.361737, mean_q: 5.181577, mean_eps: 0.877748
  135979/2000000: episode: 1415, duration: 1.219s, episode steps: 86, steps per second: 71, episode reward: -119.452, mean reward: -1.389 [-100.000, 10.077], mean action: 1.430 [0.000, 3.000], mean observation: 0.063 [-3.407, 1.000], loss: 1.758393, mean_absolute_error: 16.466714, mean_q: 3.195116, mean_eps: 0.877658
  136039/2000000: episode: 1416, duration: 0.856s, episode steps: 60, steps per second: 70, episode reward: -165.288, mean reward: -2.755 [-100.000, 8.350], mean action: 1.350 [0.000, 3.000], mean observation: 0.022 [-1.410, 4.728], loss: 2.193321, mean_absolute_error: 16.062752, mean_q: 4.524573, mean_eps: 0.877593
  136128/2000000: episode: 1417, duration: 1.303s, episode steps: 89, steps per second: 68, episode reward: -170.461, mean reward: -1.915 [-100.000, 6.028], mean action: 1.404 [0.000, 3.000], mean observation: 0.124 [-4.341, 1.000], loss: 1.462008, mean_absolute_error: 13.482922, mean_q: 7.760727, mean_eps: 0.877526
  136206/2000000: episode: 1418, duration: 1.153s, episode steps: 78, steps per second: 68, episode reward: -139.415, mean reward: -1.787 [-100.000, 17.152], mean action: 1.641 [0.000, 3.000], mean observation: -0.030 [-4.834, 1.000], loss: 1.693293, mean_absolute_error: 14.678670, mean_q: 3.356072, mean_eps: 0.877451
  136336/2000000: episode: 1419, duration: 1.849s, episode steps: 130, steps per second: 70, episode reward: -134.705, mean reward: -1.036 [-100.000, 22.181], mean action: 1.538 [0.000, 3.000], mean observation: 0.020 [-3.697, 1.033], loss: 1.273289, mean_absolute_error: 14.230325, mean_q: 5.952711, mean_eps: 0.877357
  136450/2000000: episode: 1420, duration: 1.656s, episode steps: 114, steps per second: 69, episode reward: -302.379, mean reward: -2.652 [-100.000, 5.526], mean action: 1.719 [0.000, 3.000], mean observation: -0.107 [-4.514, 1.000], loss: 1.796222, mean_absolute_error: 15.094626, mean_q: 4.187355, mean_eps: 0.877247
  136520/2000000: episode: 1421, duration: 1.028s, episode steps: 70, steps per second: 68, episode reward: -128.041, mean reward: -1.829 [-100.000, 9.844], mean action: 1.429 [0.000, 3.000], mean observation: 0.065 [-1.468, 1.000], loss: 2.125904, mean_absolute_error: 15.021929, mean_q: 4.613953, mean_eps: 0.877164
  136598/2000000: episode: 1422, duration: 1.145s, episode steps: 78, steps per second: 68, episode reward: -152.522, mean reward: -1.955 [-100.000, 16.062], mean action: 1.359 [0.000, 3.000], mean observation: -0.075 [-1.409, 1.000], loss: 1.882912, mean_absolute_error: 14.636675, mean_q: 6.159598, mean_eps: 0.877098
  136670/2000000: episode: 1423, duration: 1.048s, episode steps: 72, steps per second: 69, episode reward: -223.538, mean reward: -3.105 [-100.000, 6.610], mean action: 1.583 [0.000, 3.000], mean observation: -0.126 [-3.886, 1.000], loss: 1.224860, mean_absolute_error: 14.404585, mean_q: 6.154224, mean_eps: 0.877029
  136730/2000000: episode: 1424, duration: 0.849s, episode steps: 60, steps per second: 71, episode reward: -117.405, mean reward: -1.957 [-100.000, 12.500], mean action: 1.283 [0.000, 3.000], mean observation: -0.023 [-1.502, 4.474], loss: 2.090852, mean_absolute_error: 15.701504, mean_q: 4.020154, mean_eps: 0.876970
  136833/2000000: episode: 1425, duration: 1.482s, episode steps: 103, steps per second: 70, episode reward: -282.296, mean reward: -2.741 [-100.000, 6.229], mean action: 1.379 [0.000, 3.000], mean observation: 0.022 [-1.379, 4.510], loss: 1.259567, mean_absolute_error: 13.379920, mean_q: 7.811054, mean_eps: 0.876896
  136930/2000000: episode: 1426, duration: 1.385s, episode steps: 97, steps per second: 70, episode reward: -110.751, mean reward: -1.142 [-100.000, 20.322], mean action: 1.732 [0.000, 3.000], mean observation: -0.126 [-1.006, 1.957], loss: 1.633132, mean_absolute_error: 15.300802, mean_q: 5.133686, mean_eps: 0.876806
  137015/2000000: episode: 1427, duration: 1.357s, episode steps: 85, steps per second: 63, episode reward: -136.346, mean reward: -1.604 [-100.000, 55.587], mean action: 1.294 [0.000, 3.000], mean observation: -0.038 [-3.910, 1.000], loss: 1.288140, mean_absolute_error: 14.365676, mean_q: 6.257200, mean_eps: 0.876725
  137117/2000000: episode: 1428, duration: 1.487s, episode steps: 102, steps per second: 69, episode reward: -75.700, mean reward: -0.742 [-100.000, 27.087], mean action: 1.608 [0.000, 3.000], mean observation: -0.001 [-1.093, 1.000], loss: 1.816571, mean_absolute_error: 14.100260, mean_q: 5.791352, mean_eps: 0.876641
  137219/2000000: episode: 1429, duration: 1.425s, episode steps: 102, steps per second: 72, episode reward: -190.086, mean reward: -1.864 [-100.000, 5.817], mean action: 1.471 [0.000, 3.000], mean observation: 0.183 [-1.233, 1.000], loss: 1.723959, mean_absolute_error: 14.232452, mean_q: 5.056637, mean_eps: 0.876549
  137332/2000000: episode: 1430, duration: 1.633s, episode steps: 113, steps per second: 69, episode reward: -106.739, mean reward: -0.945 [-100.000, 13.991], mean action: 1.496 [0.000, 3.000], mean observation: -0.040 [-3.437, 1.000], loss: 1.678834, mean_absolute_error: 14.569015, mean_q: 5.443476, mean_eps: 0.876453
  137405/2000000: episode: 1431, duration: 1.096s, episode steps: 73, steps per second: 67, episode reward: -171.646, mean reward: -2.351 [-100.000, 21.501], mean action: 1.603 [0.000, 3.000], mean observation: -0.098 [-3.993, 1.000], loss: 1.759710, mean_absolute_error: 14.979446, mean_q: 4.382499, mean_eps: 0.876369
  137522/2000000: episode: 1432, duration: 1.649s, episode steps: 117, steps per second: 71, episode reward: -122.469, mean reward: -1.047 [-100.000, 76.709], mean action: 1.795 [0.000, 3.000], mean observation: -0.002 [-2.028, 1.039], loss: 1.988160, mean_absolute_error: 15.030168, mean_q: 5.071505, mean_eps: 0.876282
  137597/2000000: episode: 1433, duration: 1.068s, episode steps: 75, steps per second: 70, episode reward: 13.959, mean reward: 0.186 [-100.000, 118.269], mean action: 1.440 [0.000, 3.000], mean observation: -0.074 [-1.618, 2.044], loss: 1.715856, mean_absolute_error: 14.980441, mean_q: 5.197455, mean_eps: 0.876196
  137710/2000000: episode: 1434, duration: 1.585s, episode steps: 113, steps per second: 71, episode reward: -251.072, mean reward: -2.222 [-100.000, 5.142], mean action: 1.319 [0.000, 3.000], mean observation: 0.199 [-1.486, 1.978], loss: 1.886322, mean_absolute_error: 15.098332, mean_q: 3.984347, mean_eps: 0.876111
  137809/2000000: episode: 1435, duration: 1.440s, episode steps: 99, steps per second: 69, episode reward: -117.147, mean reward: -1.183 [-100.000, 17.800], mean action: 1.616 [0.000, 3.000], mean observation: -0.002 [-1.338, 1.000], loss: 1.466911, mean_absolute_error: 14.788825, mean_q: 5.241872, mean_eps: 0.876016
  137896/2000000: episode: 1436, duration: 1.265s, episode steps: 87, steps per second: 69, episode reward: -175.925, mean reward: -2.022 [-100.000, 18.006], mean action: 1.563 [0.000, 3.000], mean observation: 0.010 [-1.383, 3.631], loss: 1.614912, mean_absolute_error: 15.235047, mean_q: 4.091749, mean_eps: 0.875933
  137962/2000000: episode: 1437, duration: 0.962s, episode steps: 66, steps per second: 69, episode reward: -215.780, mean reward: -3.269 [-100.000, 4.912], mean action: 1.227 [0.000, 3.000], mean observation: -0.070 [-4.322, 1.000], loss: 0.953678, mean_absolute_error: 13.540823, mean_q: 6.270213, mean_eps: 0.875865
  138030/2000000: episode: 1438, duration: 0.978s, episode steps: 68, steps per second: 70, episode reward: -149.149, mean reward: -2.193 [-100.000, 21.677], mean action: 1.456 [0.000, 3.000], mean observation: 0.009 [-1.564, 1.000], loss: 2.580385, mean_absolute_error: 16.270676, mean_q: 3.775739, mean_eps: 0.875804
  138093/2000000: episode: 1439, duration: 0.909s, episode steps: 63, steps per second: 69, episode reward: -135.798, mean reward: -2.156 [-100.000, 11.607], mean action: 1.714 [0.000, 3.000], mean observation: -0.107 [-3.204, 1.000], loss: 0.917729, mean_absolute_error: 13.459526, mean_q: 6.409754, mean_eps: 0.875744
  138182/2000000: episode: 1440, duration: 1.251s, episode steps: 89, steps per second: 71, episode reward: -119.801, mean reward: -1.346 [-100.000, 22.026], mean action: 1.528 [0.000, 3.000], mean observation: -0.016 [-1.301, 1.000], loss: 1.068678, mean_absolute_error: 14.776236, mean_q: 5.859353, mean_eps: 0.875676
  138263/2000000: episode: 1441, duration: 1.126s, episode steps: 81, steps per second: 72, episode reward: -124.363, mean reward: -1.535 [-100.000, 9.914], mean action: 1.679 [0.000, 3.000], mean observation: -0.140 [-4.180, 1.000], loss: 1.309057, mean_absolute_error: 14.538381, mean_q: 4.676132, mean_eps: 0.875600
  138341/2000000: episode: 1442, duration: 1.131s, episode steps: 78, steps per second: 69, episode reward: -159.502, mean reward: -2.045 [-100.000, 7.366], mean action: 1.628 [0.000, 3.000], mean observation: -0.065 [-1.414, 1.000], loss: 1.809136, mean_absolute_error: 14.518440, mean_q: 4.087409, mean_eps: 0.875528
  138407/2000000: episode: 1443, duration: 0.926s, episode steps: 66, steps per second: 71, episode reward: -169.347, mean reward: -2.566 [-100.000, 7.973], mean action: 1.727 [0.000, 3.000], mean observation: -0.124 [-1.640, 4.360], loss: 1.062193, mean_absolute_error: 13.854708, mean_q: 6.226449, mean_eps: 0.875463
  138521/2000000: episode: 1444, duration: 1.608s, episode steps: 114, steps per second: 71, episode reward: -377.261, mean reward: -3.309 [-100.000, 106.121], mean action: 1.711 [0.000, 3.000], mean observation: -0.207 [-3.286, 1.000], loss: 1.560799, mean_absolute_error: 15.734788, mean_q: 3.119540, mean_eps: 0.875382
  138609/2000000: episode: 1445, duration: 1.247s, episode steps: 88, steps per second: 71, episode reward: -109.355, mean reward: -1.243 [-100.000, 7.619], mean action: 1.511 [0.000, 3.000], mean observation: 0.000 [-1.168, 4.260], loss: 1.284798, mean_absolute_error: 14.040157, mean_q: 6.331240, mean_eps: 0.875291
  138720/2000000: episode: 1446, duration: 1.858s, episode steps: 111, steps per second: 60, episode reward: -170.981, mean reward: -1.540 [-100.000, 6.547], mean action: 1.550 [0.000, 3.000], mean observation: 0.057 [-1.075, 3.233], loss: 1.423502, mean_absolute_error: 13.984344, mean_q: 6.433925, mean_eps: 0.875202
  138813/2000000: episode: 1447, duration: 1.520s, episode steps: 93, steps per second: 61, episode reward: -146.257, mean reward: -1.573 [-100.000, 25.226], mean action: 1.624 [0.000, 3.000], mean observation: 0.040 [-1.168, 1.887], loss: 1.481461, mean_absolute_error: 15.039541, mean_q: 4.075430, mean_eps: 0.875111
  138887/2000000: episode: 1448, duration: 1.046s, episode steps: 74, steps per second: 71, episode reward: -158.428, mean reward: -2.141 [-100.000, 8.565], mean action: 1.554 [0.000, 3.000], mean observation: -0.029 [-1.367, 4.624], loss: 1.623135, mean_absolute_error: 14.713136, mean_q: 4.374000, mean_eps: 0.875035
  138956/2000000: episode: 1449, duration: 1.073s, episode steps: 69, steps per second: 64, episode reward: -132.974, mean reward: -1.927 [-100.000, 12.259], mean action: 1.710 [0.000, 3.000], mean observation: -0.153 [-1.266, 1.000], loss: 1.251745, mean_absolute_error: 14.196690, mean_q: 5.941628, mean_eps: 0.874972
  139065/2000000: episode: 1450, duration: 1.619s, episode steps: 109, steps per second: 67, episode reward: -291.387, mean reward: -2.673 [-100.000, 56.987], mean action: 1.706 [0.000, 3.000], mean observation: -0.041 [-1.319, 2.109], loss: 1.842975, mean_absolute_error: 15.735332, mean_q: 4.755407, mean_eps: 0.874891
  139181/2000000: episode: 1451, duration: 1.635s, episode steps: 116, steps per second: 71, episode reward: -302.595, mean reward: -2.609 [-100.000, 7.708], mean action: 1.422 [0.000, 3.000], mean observation: 0.165 [-1.723, 6.630], loss: 1.553311, mean_absolute_error: 14.467335, mean_q: 5.591027, mean_eps: 0.874788
  139258/2000000: episode: 1452, duration: 1.085s, episode steps: 77, steps per second: 71, episode reward: -151.547, mean reward: -1.968 [-100.000, 17.736], mean action: 1.909 [0.000, 3.000], mean observation: -0.054 [-1.192, 2.460], loss: 1.848484, mean_absolute_error: 14.719333, mean_q: 5.215633, mean_eps: 0.874702
  139363/2000000: episode: 1453, duration: 1.495s, episode steps: 105, steps per second: 70, episode reward: -155.176, mean reward: -1.478 [-100.000, 16.830], mean action: 1.486 [0.000, 3.000], mean observation: -0.045 [-1.428, 1.000], loss: 1.360609, mean_absolute_error: 14.925966, mean_q: 5.709085, mean_eps: 0.874621
  139475/2000000: episode: 1454, duration: 1.598s, episode steps: 112, steps per second: 70, episode reward: -175.226, mean reward: -1.565 [-100.000, 20.472], mean action: 1.348 [0.000, 3.000], mean observation: 0.089 [-1.354, 5.222], loss: 1.787030, mean_absolute_error: 15.077559, mean_q: 4.705053, mean_eps: 0.874524
  139577/2000000: episode: 1455, duration: 1.478s, episode steps: 102, steps per second: 69, episode reward: -187.645, mean reward: -1.840 [-100.000, 9.936], mean action: 1.706 [0.000, 3.000], mean observation: -0.075 [-1.619, 1.000], loss: 1.635366, mean_absolute_error: 14.674266, mean_q: 5.145740, mean_eps: 0.874427
  139657/2000000: episode: 1456, duration: 1.156s, episode steps: 80, steps per second: 69, episode reward: -135.245, mean reward: -1.691 [-100.000, 13.519], mean action: 1.525 [0.000, 3.000], mean observation: -0.040 [-4.010, 1.000], loss: 1.417595, mean_absolute_error: 13.433203, mean_q: 5.764999, mean_eps: 0.874344
  139786/2000000: episode: 1457, duration: 1.820s, episode steps: 129, steps per second: 71, episode reward: -182.316, mean reward: -1.413 [-100.000, 6.467], mean action: 1.628 [0.000, 3.000], mean observation: -0.107 [-1.285, 3.104], loss: 1.173388, mean_absolute_error: 13.565797, mean_q: 6.524689, mean_eps: 0.874250
  139883/2000000: episode: 1458, duration: 1.363s, episode steps: 97, steps per second: 71, episode reward: -149.799, mean reward: -1.544 [-100.000, 12.958], mean action: 1.412 [0.000, 3.000], mean observation: 0.120 [-4.069, 1.000], loss: 1.939515, mean_absolute_error: 14.951372, mean_q: 5.259198, mean_eps: 0.874149
  139944/2000000: episode: 1459, duration: 0.890s, episode steps: 61, steps per second: 69, episode reward: -127.135, mean reward: -2.084 [-100.000, 9.564], mean action: 1.623 [0.000, 3.000], mean observation: 0.012 [-1.411, 4.171], loss: 1.053996, mean_absolute_error: 13.583819, mean_q: 6.379713, mean_eps: 0.874079
  140026/2000000: episode: 1460, duration: 1.191s, episode steps: 82, steps per second: 69, episode reward: -375.643, mean reward: -4.581 [-100.000, 5.234], mean action: 1.585 [0.000, 3.000], mean observation: -0.026 [-1.483, 4.032], loss: 0.937826, mean_absolute_error: 13.184692, mean_q: 6.828405, mean_eps: 0.874014
  140122/2000000: episode: 1461, duration: 1.362s, episode steps: 96, steps per second: 70, episode reward: -303.851, mean reward: -3.165 [-100.000, 0.752], mean action: 1.646 [0.000, 3.000], mean observation: -0.027 [-1.646, 1.030], loss: 1.679531, mean_absolute_error: 15.226077, mean_q: 7.173143, mean_eps: 0.873933
  140217/2000000: episode: 1462, duration: 1.421s, episode steps: 95, steps per second: 67, episode reward: -263.072, mean reward: -2.769 [-100.000, 16.498], mean action: 1.589 [0.000, 3.000], mean observation: 0.001 [-1.378, 5.416], loss: 2.089441, mean_absolute_error: 16.118490, mean_q: 3.981301, mean_eps: 0.873847
  140340/2000000: episode: 1463, duration: 1.890s, episode steps: 123, steps per second: 65, episode reward: -50.936, mean reward: -0.414 [-100.000, 84.003], mean action: 1.602 [0.000, 3.000], mean observation: 0.060 [-1.399, 1.001], loss: 1.831800, mean_absolute_error: 16.110032, mean_q: 6.160230, mean_eps: 0.873750
  140433/2000000: episode: 1464, duration: 1.339s, episode steps: 93, steps per second: 69, episode reward: -159.353, mean reward: -1.713 [-100.000, 11.169], mean action: 1.473 [0.000, 3.000], mean observation: -0.033 [-4.413, 1.000], loss: 1.490877, mean_absolute_error: 15.016194, mean_q: 8.622221, mean_eps: 0.873653
  140536/2000000: episode: 1465, duration: 1.496s, episode steps: 103, steps per second: 69, episode reward: -169.760, mean reward: -1.648 [-100.000, 19.736], mean action: 1.427 [0.000, 3.000], mean observation: 0.145 [-1.737, 1.000], loss: 1.616403, mean_absolute_error: 16.171278, mean_q: 5.585200, mean_eps: 0.873564
  140653/2000000: episode: 1466, duration: 1.659s, episode steps: 117, steps per second: 71, episode reward: -158.887, mean reward: -1.358 [-100.000, 8.343], mean action: 1.470 [0.000, 3.000], mean observation: 0.175 [-3.878, 1.000], loss: 2.010286, mean_absolute_error: 15.861812, mean_q: 5.786384, mean_eps: 0.873465
  140760/2000000: episode: 1467, duration: 1.563s, episode steps: 107, steps per second: 68, episode reward: -134.105, mean reward: -1.253 [-100.000, 36.428], mean action: 1.645 [0.000, 3.000], mean observation: 0.072 [-1.007, 2.119], loss: 1.743376, mean_absolute_error: 15.758575, mean_q: 5.795891, mean_eps: 0.873365
  140860/2000000: episode: 1468, duration: 1.465s, episode steps: 100, steps per second: 68, episode reward: -194.472, mean reward: -1.945 [-100.000, 9.689], mean action: 1.440 [0.000, 3.000], mean observation: 0.167 [-1.511, 1.828], loss: 1.675319, mean_absolute_error: 15.136759, mean_q: 7.148926, mean_eps: 0.873273
  140928/2000000: episode: 1469, duration: 1.027s, episode steps: 68, steps per second: 66, episode reward: -99.184, mean reward: -1.459 [-100.000, 8.723], mean action: 1.618 [0.000, 3.000], mean observation: -0.093 [-1.203, 3.897], loss: 1.542331, mean_absolute_error: 15.216242, mean_q: 7.355190, mean_eps: 0.873197
  140992/2000000: episode: 1470, duration: 0.964s, episode steps: 64, steps per second: 66, episode reward: -140.416, mean reward: -2.194 [-100.000, 7.724], mean action: 1.688 [0.000, 3.000], mean observation: -0.177 [-5.662, 1.000], loss: 2.179507, mean_absolute_error: 16.403741, mean_q: 6.824810, mean_eps: 0.873138
  141128/2000000: episode: 1471, duration: 2.003s, episode steps: 136, steps per second: 68, episode reward: -339.461, mean reward: -2.496 [-100.000, 83.575], mean action: 1.551 [0.000, 3.000], mean observation: -0.142 [-2.997, 1.000], loss: 1.698878, mean_absolute_error: 15.608812, mean_q: 6.176704, mean_eps: 0.873048
  141199/2000000: episode: 1472, duration: 1.019s, episode steps: 71, steps per second: 70, episode reward: -146.703, mean reward: -2.066 [-100.000, 10.701], mean action: 1.479 [0.000, 3.000], mean observation: 0.029 [-1.369, 4.839], loss: 1.806387, mean_absolute_error: 16.296450, mean_q: 7.119454, mean_eps: 0.872954
  141269/2000000: episode: 1473, duration: 1.012s, episode steps: 70, steps per second: 69, episode reward: -152.617, mean reward: -2.180 [-100.000, 11.755], mean action: 1.471 [0.000, 3.000], mean observation: -0.000 [-4.400, 1.000], loss: 1.349394, mean_absolute_error: 14.785289, mean_q: 6.601207, mean_eps: 0.872889
  141377/2000000: episode: 1474, duration: 1.530s, episode steps: 108, steps per second: 71, episode reward: -136.641, mean reward: -1.265 [-100.000, 14.278], mean action: 1.537 [0.000, 3.000], mean observation: 0.070 [-1.475, 5.373], loss: 1.855416, mean_absolute_error: 17.104290, mean_q: 5.261663, mean_eps: 0.872808
  141465/2000000: episode: 1475, duration: 1.253s, episode steps: 88, steps per second: 70, episode reward: -120.169, mean reward: -1.366 [-100.000, 12.143], mean action: 1.682 [0.000, 3.000], mean observation: -0.129 [-1.133, 4.002], loss: 1.561773, mean_absolute_error: 15.761049, mean_q: 5.455156, mean_eps: 0.872720
  141599/2000000: episode: 1476, duration: 1.861s, episode steps: 134, steps per second: 72, episode reward: -107.448, mean reward: -0.802 [-100.000, 22.693], mean action: 1.709 [0.000, 3.000], mean observation: -0.003 [-1.246, 1.082], loss: 1.619758, mean_absolute_error: 16.212679, mean_q: 6.669408, mean_eps: 0.872621
  141725/2000000: episode: 1477, duration: 1.801s, episode steps: 126, steps per second: 70, episode reward: -45.760, mean reward: -0.363 [-100.000, 92.908], mean action: 1.460 [0.000, 3.000], mean observation: 0.105 [-1.880, 1.076], loss: 1.377171, mean_absolute_error: 15.040339, mean_q: 6.334450, mean_eps: 0.872504
  141855/2000000: episode: 1478, duration: 1.800s, episode steps: 130, steps per second: 72, episode reward: -153.300, mean reward: -1.179 [-100.000, 8.663], mean action: 1.631 [0.000, 3.000], mean observation: 0.024 [-4.347, 1.000], loss: 1.992101, mean_absolute_error: 16.409563, mean_q: 4.609069, mean_eps: 0.872389
  141954/2000000: episode: 1479, duration: 1.417s, episode steps: 99, steps per second: 70, episode reward: -133.909, mean reward: -1.353 [-100.000, 19.814], mean action: 1.616 [0.000, 3.000], mean observation: -0.082 [-1.205, 1.000], loss: 1.817886, mean_absolute_error: 14.169994, mean_q: 7.228690, mean_eps: 0.872286
  142046/2000000: episode: 1480, duration: 1.281s, episode steps: 92, steps per second: 72, episode reward: -145.285, mean reward: -1.579 [-100.000, 28.271], mean action: 1.587 [0.000, 3.000], mean observation: -0.094 [-2.974, 1.000], loss: 1.657959, mean_absolute_error: 15.266827, mean_q: 6.146286, mean_eps: 0.872200
  142126/2000000: episode: 1481, duration: 1.186s, episode steps: 80, steps per second: 67, episode reward: -182.822, mean reward: -2.285 [-100.000, 14.884], mean action: 1.650 [0.000, 3.000], mean observation: -0.099 [-1.495, 5.115], loss: 1.954238, mean_absolute_error: 16.281052, mean_q: 5.650581, mean_eps: 0.872123
  142211/2000000: episode: 1482, duration: 1.177s, episode steps: 85, steps per second: 72, episode reward: -133.234, mean reward: -1.567 [-100.000, 7.806], mean action: 1.518 [0.000, 3.000], mean observation: -0.050 [-4.375, 1.000], loss: 1.174809, mean_absolute_error: 14.573617, mean_q: 8.201430, mean_eps: 0.872049
  142311/2000000: episode: 1483, duration: 1.410s, episode steps: 100, steps per second: 71, episode reward: -98.638, mean reward: -0.986 [-100.000, 13.565], mean action: 1.580 [0.000, 3.000], mean observation: -0.122 [-3.172, 1.000], loss: 1.111637, mean_absolute_error: 15.002888, mean_q: 7.304901, mean_eps: 0.871966
  142418/2000000: episode: 1484, duration: 1.527s, episode steps: 107, steps per second: 70, episode reward: -138.964, mean reward: -1.299 [-100.000, 11.337], mean action: 1.570 [0.000, 3.000], mean observation: 0.071 [-1.119, 1.000], loss: 1.449053, mean_absolute_error: 16.516621, mean_q: 4.634872, mean_eps: 0.871872
  142498/2000000: episode: 1485, duration: 1.131s, episode steps: 80, steps per second: 71, episode reward: -165.238, mean reward: -2.065 [-100.000, 7.394], mean action: 1.512 [0.000, 3.000], mean observation: -0.013 [-5.000, 1.000], loss: 1.796639, mean_absolute_error: 15.189757, mean_q: 7.613522, mean_eps: 0.871788
  142595/2000000: episode: 1486, duration: 1.367s, episode steps: 97, steps per second: 71, episode reward: -173.790, mean reward: -1.792 [-100.000, 6.010], mean action: 1.546 [0.000, 3.000], mean observation: 0.104 [-1.539, 5.726], loss: 1.370079, mean_absolute_error: 14.267891, mean_q: 7.988141, mean_eps: 0.871709
  142717/2000000: episode: 1487, duration: 1.743s, episode steps: 122, steps per second: 70, episode reward: -332.068, mean reward: -2.722 [-100.000, 70.835], mean action: 1.574 [0.000, 3.000], mean observation: -0.106 [-2.241, 1.000], loss: 2.050501, mean_absolute_error: 16.839901, mean_q: 5.492528, mean_eps: 0.871610
  142844/2000000: episode: 1488, duration: 1.881s, episode steps: 127, steps per second: 68, episode reward: -347.883, mean reward: -2.739 [-100.000, 2.319], mean action: 1.638 [0.000, 3.000], mean observation: -0.073 [-1.937, 1.053], loss: 1.854452, mean_absolute_error: 16.312561, mean_q: 4.594550, mean_eps: 0.871498
  142915/2000000: episode: 1489, duration: 1.024s, episode steps: 71, steps per second: 69, episode reward: -118.407, mean reward: -1.668 [-100.000, 7.766], mean action: 1.451 [0.000, 3.000], mean observation: -0.012 [-4.822, 1.000], loss: 2.490131, mean_absolute_error: 16.796971, mean_q: 5.141616, mean_eps: 0.871410
  142984/2000000: episode: 1490, duration: 1.023s, episode steps: 69, steps per second: 67, episode reward: -111.754, mean reward: -1.620 [-100.000, 14.827], mean action: 1.754 [0.000, 3.000], mean observation: -0.088 [-1.414, 1.000], loss: 1.408967, mean_absolute_error: 14.548226, mean_q: 6.349677, mean_eps: 0.871347
  143080/2000000: episode: 1491, duration: 1.405s, episode steps: 96, steps per second: 68, episode reward: -223.064, mean reward: -2.324 [-100.000, 13.610], mean action: 1.854 [0.000, 3.000], mean observation: -0.179 [-3.789, 1.000], loss: 1.684182, mean_absolute_error: 15.184477, mean_q: 7.950177, mean_eps: 0.871273
  143134/2000000: episode: 1492, duration: 0.791s, episode steps: 54, steps per second: 68, episode reward: -134.059, mean reward: -2.483 [-100.000, 7.592], mean action: 1.407 [0.000, 3.000], mean observation: 0.023 [-1.628, 4.682], loss: 1.704198, mean_absolute_error: 15.389641, mean_q: 5.892642, mean_eps: 0.871205
  143287/2000000: episode: 1493, duration: 2.147s, episode steps: 153, steps per second: 71, episode reward: -175.983, mean reward: -1.150 [-100.000, 18.865], mean action: 1.556 [0.000, 3.000], mean observation: 0.131 [-3.422, 1.130], loss: 1.662542, mean_absolute_error: 15.321127, mean_q: 6.611460, mean_eps: 0.871111
  143354/2000000: episode: 1494, duration: 0.972s, episode steps: 67, steps per second: 69, episode reward: -107.874, mean reward: -1.610 [-100.000, 12.966], mean action: 1.597 [0.000, 3.000], mean observation: -0.127 [-1.461, 1.000], loss: 1.712321, mean_absolute_error: 15.844160, mean_q: 7.104610, mean_eps: 0.871012
  143419/2000000: episode: 1495, duration: 0.931s, episode steps: 65, steps per second: 70, episode reward: -140.851, mean reward: -2.167 [-100.000, 7.680], mean action: 1.600 [0.000, 3.000], mean observation: -0.069 [-1.412, 4.741], loss: 1.112613, mean_absolute_error: 15.554929, mean_q: 6.005800, mean_eps: 0.870953
  143533/2000000: episode: 1496, duration: 1.647s, episode steps: 114, steps per second: 69, episode reward: -127.657, mean reward: -1.120 [-100.000, 11.840], mean action: 1.509 [0.000, 3.000], mean observation: 0.046 [-3.630, 1.000], loss: 2.062275, mean_absolute_error: 15.857158, mean_q: 6.376545, mean_eps: 0.870872
  143634/2000000: episode: 1497, duration: 1.414s, episode steps: 101, steps per second: 71, episode reward: -157.681, mean reward: -1.561 [-100.000, 12.701], mean action: 1.594 [0.000, 3.000], mean observation: 0.071 [-4.419, 1.000], loss: 2.198291, mean_absolute_error: 15.878849, mean_q: 5.844691, mean_eps: 0.870774
  143698/2000000: episode: 1498, duration: 0.916s, episode steps: 64, steps per second: 70, episode reward: -201.927, mean reward: -3.155 [-100.000, 9.478], mean action: 1.719 [0.000, 3.000], mean observation: 0.026 [-1.574, 1.434], loss: 1.820041, mean_absolute_error: 15.251210, mean_q: 6.875583, mean_eps: 0.870701
  143803/2000000: episode: 1499, duration: 1.457s, episode steps: 105, steps per second: 72, episode reward: -282.794, mean reward: -2.693 [-100.000, 6.605], mean action: 1.495 [0.000, 3.000], mean observation: -0.086 [-5.366, 1.000], loss: 1.791405, mean_absolute_error: 15.982442, mean_q: 5.903773, mean_eps: 0.870625
  143884/2000000: episode: 1500, duration: 1.188s, episode steps: 81, steps per second: 68, episode reward: -120.223, mean reward: -1.484 [-100.000, 6.779], mean action: 1.432 [0.000, 3.000], mean observation: 0.016 [-4.452, 1.000], loss: 1.947585, mean_absolute_error: 15.957987, mean_q: 6.018965, mean_eps: 0.870542
  143951/2000000: episode: 1501, duration: 0.965s, episode steps: 67, steps per second: 69, episode reward: -127.448, mean reward: -1.902 [-100.000, 5.474], mean action: 1.701 [0.000, 3.000], mean observation: -0.148 [-1.399, 4.568], loss: 1.836656, mean_absolute_error: 16.071247, mean_q: 6.474172, mean_eps: 0.870476
  144049/2000000: episode: 1502, duration: 1.392s, episode steps: 98, steps per second: 70, episode reward: -175.707, mean reward: -1.793 [-100.000, 13.867], mean action: 1.469 [0.000, 3.000], mean observation: 0.026 [-4.200, 1.000], loss: 1.660938, mean_absolute_error: 15.224016, mean_q: 5.427911, mean_eps: 0.870400
  144178/2000000: episode: 1503, duration: 1.814s, episode steps: 129, steps per second: 71, episode reward: -80.352, mean reward: -0.623 [-100.000, 22.723], mean action: 1.581 [0.000, 3.000], mean observation: 0.038 [-1.044, 1.000], loss: 1.861830, mean_absolute_error: 15.336083, mean_q: 6.280457, mean_eps: 0.870297
  144256/2000000: episode: 1504, duration: 1.149s, episode steps: 78, steps per second: 68, episode reward: -152.896, mean reward: -1.960 [-100.000, 3.481], mean action: 1.526 [0.000, 3.000], mean observation: -0.098 [-1.368, 1.417], loss: 2.016113, mean_absolute_error: 16.180344, mean_q: 6.005731, mean_eps: 0.870206
  144312/2000000: episode: 1505, duration: 0.840s, episode steps: 56, steps per second: 67, episode reward: -109.234, mean reward: -1.951 [-100.000, 12.266], mean action: 1.304 [0.000, 3.000], mean observation: 0.008 [-1.465, 5.083], loss: 2.126079, mean_absolute_error: 15.865244, mean_q: 4.996567, mean_eps: 0.870146
  144396/2000000: episode: 1506, duration: 1.243s, episode steps: 84, steps per second: 68, episode reward: -116.788, mean reward: -1.390 [-100.000, 20.778], mean action: 1.440 [0.000, 3.000], mean observation: 0.094 [-1.078, 1.000], loss: 1.682468, mean_absolute_error: 15.626478, mean_q: 5.441470, mean_eps: 0.870083
  144492/2000000: episode: 1507, duration: 1.412s, episode steps: 96, steps per second: 68, episode reward: -139.224, mean reward: -1.450 [-100.000, 7.486], mean action: 1.385 [0.000, 3.000], mean observation: 0.104 [-1.098, 1.000], loss: 2.096589, mean_absolute_error: 16.628218, mean_q: 4.611933, mean_eps: 0.870002
  144579/2000000: episode: 1508, duration: 1.250s, episode steps: 87, steps per second: 70, episode reward: -371.638, mean reward: -4.272 [-100.000, 95.798], mean action: 1.713 [0.000, 3.000], mean observation: 0.066 [-1.353, 3.123], loss: 1.508651, mean_absolute_error: 15.151021, mean_q: 5.679322, mean_eps: 0.869919
  144669/2000000: episode: 1509, duration: 1.291s, episode steps: 90, steps per second: 70, episode reward: -118.529, mean reward: -1.317 [-100.000, 18.924], mean action: 1.522 [0.000, 3.000], mean observation: -0.087 [-1.207, 1.000], loss: 1.316229, mean_absolute_error: 15.180065, mean_q: 8.431235, mean_eps: 0.869838
  144788/2000000: episode: 1510, duration: 1.693s, episode steps: 119, steps per second: 70, episode reward: -118.166, mean reward: -0.993 [-100.000, 8.035], mean action: 1.664 [0.000, 3.000], mean observation: -0.104 [-1.215, 3.058], loss: 1.868594, mean_absolute_error: 15.949004, mean_q: 6.353077, mean_eps: 0.869745
  144920/2000000: episode: 1511, duration: 1.920s, episode steps: 132, steps per second: 69, episode reward: -156.645, mean reward: -1.187 [-100.000, 9.357], mean action: 1.621 [0.000, 3.000], mean observation: 0.079 [-4.840, 1.033], loss: 1.576304, mean_absolute_error: 15.140574, mean_q: 7.001494, mean_eps: 0.869633
  145005/2000000: episode: 1512, duration: 1.241s, episode steps: 85, steps per second: 68, episode reward: -133.552, mean reward: -1.571 [-100.000, 37.395], mean action: 1.353 [0.000, 3.000], mean observation: 0.109 [-1.857, 1.000], loss: 2.093492, mean_absolute_error: 15.631027, mean_q: 6.918045, mean_eps: 0.869534
  145100/2000000: episode: 1513, duration: 1.366s, episode steps: 95, steps per second: 70, episode reward: -209.264, mean reward: -2.203 [-100.000, 6.606], mean action: 1.484 [0.000, 3.000], mean observation: 0.050 [-1.745, 1.194], loss: 1.834328, mean_absolute_error: 15.336099, mean_q: 6.756218, mean_eps: 0.869453
  145202/2000000: episode: 1514, duration: 1.465s, episode steps: 102, steps per second: 70, episode reward: -193.018, mean reward: -1.892 [-100.000, 27.712], mean action: 1.676 [0.000, 3.000], mean observation: -0.047 [-1.342, 1.392], loss: 1.812639, mean_absolute_error: 16.916835, mean_q: 4.936734, mean_eps: 0.869365
  145291/2000000: episode: 1515, duration: 1.388s, episode steps: 89, steps per second: 64, episode reward: -223.152, mean reward: -2.507 [-100.000, 6.203], mean action: 1.393 [0.000, 3.000], mean observation: 0.090 [-1.330, 1.589], loss: 1.278708, mean_absolute_error: 14.771624, mean_q: 7.621163, mean_eps: 0.869279
  145397/2000000: episode: 1516, duration: 1.540s, episode steps: 106, steps per second: 69, episode reward: -143.515, mean reward: -1.354 [-100.000, 10.341], mean action: 1.708 [0.000, 3.000], mean observation: -0.003 [-4.351, 1.000], loss: 1.837356, mean_absolute_error: 16.370807, mean_q: 4.636593, mean_eps: 0.869190
  145540/2000000: episode: 1517, duration: 2.039s, episode steps: 143, steps per second: 70, episode reward: -98.273, mean reward: -0.687 [-100.000, 43.637], mean action: 1.476 [0.000, 3.000], mean observation: 0.180 [-1.595, 1.672], loss: 1.604921, mean_absolute_error: 15.089324, mean_q: 6.452700, mean_eps: 0.869079
  145627/2000000: episode: 1518, duration: 1.241s, episode steps: 87, steps per second: 70, episode reward: -129.603, mean reward: -1.490 [-100.000, 6.605], mean action: 1.414 [0.000, 3.000], mean observation: 0.074 [-1.363, 4.695], loss: 1.596820, mean_absolute_error: 15.591520, mean_q: 6.671489, mean_eps: 0.868976
  145692/2000000: episode: 1519, duration: 0.954s, episode steps: 65, steps per second: 68, episode reward: -145.285, mean reward: -2.235 [-100.000, 4.438], mean action: 1.631 [0.000, 3.000], mean observation: 0.036 [-3.667, 1.000], loss: 1.225811, mean_absolute_error: 14.166038, mean_q: 9.019036, mean_eps: 0.868908
  145766/2000000: episode: 1520, duration: 1.090s, episode steps: 74, steps per second: 68, episode reward: -168.377, mean reward: -2.275 [-100.000, 8.251], mean action: 1.730 [0.000, 3.000], mean observation: -0.154 [-5.486, 1.000], loss: 1.576438, mean_absolute_error: 14.761244, mean_q: 7.407773, mean_eps: 0.868845
  145906/2000000: episode: 1521, duration: 1.956s, episode steps: 140, steps per second: 72, episode reward: -119.472, mean reward: -0.853 [-100.000, 12.119], mean action: 1.429 [0.000, 3.000], mean observation: 0.062 [-3.252, 1.120], loss: 1.215826, mean_absolute_error: 14.852476, mean_q: 7.516852, mean_eps: 0.868748
  146019/2000000: episode: 1522, duration: 1.592s, episode steps: 113, steps per second: 71, episode reward: -238.893, mean reward: -2.114 [-100.000, 6.900], mean action: 1.513 [0.000, 3.000], mean observation: -0.116 [-5.422, 1.000], loss: 1.988333, mean_absolute_error: 17.134973, mean_q: 5.757329, mean_eps: 0.868634
  146107/2000000: episode: 1523, duration: 1.242s, episode steps: 88, steps per second: 71, episode reward: -334.566, mean reward: -3.802 [-100.000, -0.344], mean action: 1.477 [0.000, 3.000], mean observation: 0.117 [-1.266, 1.771], loss: 1.549068, mean_absolute_error: 15.640265, mean_q: 5.148050, mean_eps: 0.868544
  146198/2000000: episode: 1524, duration: 1.291s, episode steps: 91, steps per second: 70, episode reward: -196.440, mean reward: -2.159 [-100.000, 7.399], mean action: 1.681 [0.000, 3.000], mean observation: 0.027 [-5.075, 1.000], loss: 1.681636, mean_absolute_error: 16.655388, mean_q: 4.989511, mean_eps: 0.868463
  146323/2000000: episode: 1525, duration: 1.762s, episode steps: 125, steps per second: 71, episode reward: -227.651, mean reward: -1.821 [-100.000, 4.607], mean action: 1.704 [0.000, 3.000], mean observation: -0.095 [-1.502, 1.020], loss: 1.533725, mean_absolute_error: 15.102839, mean_q: 7.750780, mean_eps: 0.868366
  146465/2000000: episode: 1526, duration: 2.034s, episode steps: 142, steps per second: 70, episode reward: -126.225, mean reward: -0.889 [-100.000, 13.346], mean action: 1.620 [0.000, 3.000], mean observation: 0.105 [-2.196, 1.000], loss: 1.770971, mean_absolute_error: 15.520119, mean_q: 6.192998, mean_eps: 0.868245
  146561/2000000: episode: 1527, duration: 1.356s, episode steps: 96, steps per second: 71, episode reward: -147.532, mean reward: -1.537 [-100.000, 21.870], mean action: 1.385 [0.000, 3.000], mean observation: 0.150 [-1.272, 1.000], loss: 1.092372, mean_absolute_error: 15.506813, mean_q: 7.960289, mean_eps: 0.868137
  146703/2000000: episode: 1528, duration: 1.957s, episode steps: 142, steps per second: 73, episode reward: -151.918, mean reward: -1.070 [-100.000, 9.032], mean action: 1.500 [0.000, 3.000], mean observation: 0.022 [-1.562, 4.712], loss: 1.574785, mean_absolute_error: 15.957444, mean_q: 5.592400, mean_eps: 0.868031
  146826/2000000: episode: 1529, duration: 1.766s, episode steps: 123, steps per second: 70, episode reward: -138.313, mean reward: -1.124 [-100.000, 7.656], mean action: 1.431 [0.000, 3.000], mean observation: 0.065 [-4.513, 1.000], loss: 1.467182, mean_absolute_error: 15.839536, mean_q: 6.957335, mean_eps: 0.867912
  146892/2000000: episode: 1530, duration: 0.968s, episode steps: 66, steps per second: 68, episode reward: -116.539, mean reward: -1.766 [-100.000, 5.645], mean action: 1.727 [0.000, 3.000], mean observation: 0.036 [-1.168, 3.462], loss: 1.714577, mean_absolute_error: 17.326498, mean_q: 4.608838, mean_eps: 0.867828
  146950/2000000: episode: 1531, duration: 0.853s, episode steps: 58, steps per second: 68, episode reward: -126.169, mean reward: -2.175 [-100.000, 35.547], mean action: 1.466 [0.000, 3.000], mean observation: 0.039 [-1.409, 2.918], loss: 1.937067, mean_absolute_error: 15.797586, mean_q: 7.133624, mean_eps: 0.867772
  147078/2000000: episode: 1532, duration: 1.816s, episode steps: 128, steps per second: 71, episode reward: -162.621, mean reward: -1.270 [-100.000, 7.393], mean action: 1.633 [0.000, 3.000], mean observation: 0.038 [-1.206, 4.341], loss: 1.476164, mean_absolute_error: 14.859182, mean_q: 6.236397, mean_eps: 0.867687
  147181/2000000: episode: 1533, duration: 1.466s, episode steps: 103, steps per second: 70, episode reward: -162.741, mean reward: -1.580 [-100.000, 11.702], mean action: 1.757 [0.000, 3.000], mean observation: -0.029 [-1.159, 1.828], loss: 1.391494, mean_absolute_error: 15.286419, mean_q: 6.737487, mean_eps: 0.867583
  147262/2000000: episode: 1534, duration: 1.137s, episode steps: 81, steps per second: 71, episode reward: -161.078, mean reward: -1.989 [-100.000, 3.417], mean action: 1.617 [0.000, 3.000], mean observation: -0.111 [-4.406, 1.000], loss: 1.539364, mean_absolute_error: 14.956109, mean_q: 7.772348, mean_eps: 0.867500
  147345/2000000: episode: 1535, duration: 1.190s, episode steps: 83, steps per second: 70, episode reward: -153.714, mean reward: -1.852 [-100.000, 7.174], mean action: 1.687 [0.000, 3.000], mean observation: 0.015 [-1.194, 4.303], loss: 1.704931, mean_absolute_error: 15.095039, mean_q: 6.075268, mean_eps: 0.867426
  147456/2000000: episode: 1536, duration: 1.577s, episode steps: 111, steps per second: 70, episode reward: -264.760, mean reward: -2.385 [-100.000, 1.180], mean action: 1.505 [0.000, 3.000], mean observation: 0.157 [-1.466, 1.221], loss: 1.607473, mean_absolute_error: 16.055416, mean_q: 5.723772, mean_eps: 0.867340
  147558/2000000: episode: 1537, duration: 1.456s, episode steps: 102, steps per second: 70, episode reward: -316.213, mean reward: -3.100 [-100.000, 38.504], mean action: 1.578 [0.000, 3.000], mean observation: -0.148 [-3.210, 1.000], loss: 2.077195, mean_absolute_error: 16.241928, mean_q: 5.507352, mean_eps: 0.867245
  147683/2000000: episode: 1538, duration: 1.756s, episode steps: 125, steps per second: 71, episode reward: -119.646, mean reward: -0.957 [-100.000, 14.443], mean action: 1.512 [0.000, 3.000], mean observation: -0.031 [-1.332, 4.487], loss: 1.993559, mean_absolute_error: 15.856169, mean_q: 5.699057, mean_eps: 0.867142
  147795/2000000: episode: 1539, duration: 1.595s, episode steps: 112, steps per second: 70, episode reward: -161.763, mean reward: -1.444 [-100.000, 10.621], mean action: 1.679 [0.000, 3.000], mean observation: 0.049 [-2.748, 1.000], loss: 1.573836, mean_absolute_error: 15.265609, mean_q: 7.409312, mean_eps: 0.867036
  147888/2000000: episode: 1540, duration: 1.346s, episode steps: 93, steps per second: 69, episode reward: -216.872, mean reward: -2.332 [-100.000, 81.950], mean action: 1.624 [0.000, 3.000], mean observation: -0.154 [-2.292, 1.000], loss: 1.886532, mean_absolute_error: 16.239958, mean_q: 6.253741, mean_eps: 0.866944
  147999/2000000: episode: 1541, duration: 1.550s, episode steps: 111, steps per second: 72, episode reward: -170.592, mean reward: -1.537 [-100.000, 12.986], mean action: 1.622 [0.000, 3.000], mean observation: 0.038 [-5.331, 1.010], loss: 1.406122, mean_absolute_error: 15.528030, mean_q: 6.359939, mean_eps: 0.866852
  148102/2000000: episode: 1542, duration: 1.453s, episode steps: 103, steps per second: 71, episode reward: -100.661, mean reward: -0.977 [-100.000, 17.764], mean action: 1.699 [0.000, 3.000], mean observation: -0.057 [-1.217, 1.000], loss: 1.311079, mean_absolute_error: 15.034721, mean_q: 7.412400, mean_eps: 0.866755
  148174/2000000: episode: 1543, duration: 1.005s, episode steps: 72, steps per second: 72, episode reward: -116.827, mean reward: -1.623 [-100.000, 7.625], mean action: 1.583 [0.000, 3.000], mean observation: 0.013 [-1.208, 4.396], loss: 1.980618, mean_absolute_error: 17.274474, mean_q: 4.257788, mean_eps: 0.866676
  148323/2000000: episode: 1544, duration: 2.083s, episode steps: 149, steps per second: 72, episode reward: -128.730, mean reward: -0.864 [-100.000, 51.675], mean action: 1.497 [0.000, 3.000], mean observation: 0.198 [-1.634, 1.588], loss: 1.338100, mean_absolute_error: 15.435585, mean_q: 7.956621, mean_eps: 0.866577
  148393/2000000: episode: 1545, duration: 1.013s, episode steps: 70, steps per second: 69, episode reward: -120.206, mean reward: -1.717 [-100.000, 6.233], mean action: 1.743 [0.000, 3.000], mean observation: -0.183 [-4.953, 1.000], loss: 1.573005, mean_absolute_error: 14.924787, mean_q: 6.552507, mean_eps: 0.866478
  148453/2000000: episode: 1546, duration: 0.851s, episode steps: 60, steps per second: 71, episode reward: -126.091, mean reward: -2.102 [-100.000, 6.415], mean action: 1.600 [0.000, 3.000], mean observation: 0.008 [-4.361, 1.036], loss: 1.635571, mean_absolute_error: 16.945530, mean_q: 4.262934, mean_eps: 0.866418
  148528/2000000: episode: 1547, duration: 1.073s, episode steps: 75, steps per second: 70, episode reward: -143.983, mean reward: -1.920 [-100.000, 8.234], mean action: 1.440 [0.000, 3.000], mean observation: 0.052 [-1.431, 1.131], loss: 1.358325, mean_absolute_error: 13.794857, mean_q: 8.880692, mean_eps: 0.866359
  148592/2000000: episode: 1548, duration: 0.931s, episode steps: 64, steps per second: 69, episode reward: -134.694, mean reward: -2.105 [-100.000, 5.837], mean action: 1.234 [0.000, 3.000], mean observation: -0.005 [-5.181, 1.000], loss: 1.215566, mean_absolute_error: 15.637949, mean_q: 6.062829, mean_eps: 0.866298
  148683/2000000: episode: 1549, duration: 1.272s, episode steps: 91, steps per second: 72, episode reward: -127.178, mean reward: -1.398 [-100.000, 7.104], mean action: 1.484 [0.000, 3.000], mean observation: 0.049 [-1.154, 3.747], loss: 1.931490, mean_absolute_error: 15.952331, mean_q: 6.085117, mean_eps: 0.866228
  148762/2000000: episode: 1550, duration: 1.132s, episode steps: 79, steps per second: 70, episode reward: -186.754, mean reward: -2.364 [-100.000, 11.735], mean action: 1.608 [0.000, 3.000], mean observation: -0.094 [-1.513, 4.849], loss: 1.848758, mean_absolute_error: 15.699651, mean_q: 6.070391, mean_eps: 0.866150
  148850/2000000: episode: 1551, duration: 1.241s, episode steps: 88, steps per second: 71, episode reward: -181.528, mean reward: -2.063 [-100.000, 7.803], mean action: 1.682 [0.000, 3.000], mean observation: -0.011 [-1.322, 1.000], loss: 1.802331, mean_absolute_error: 14.853205, mean_q: 6.817245, mean_eps: 0.866075
  148965/2000000: episode: 1552, duration: 1.625s, episode steps: 115, steps per second: 71, episode reward: -179.027, mean reward: -1.557 [-100.000, 28.760], mean action: 1.591 [0.000, 3.000], mean observation: 0.085 [-2.460, 1.000], loss: 2.143174, mean_absolute_error: 17.037063, mean_q: 5.493676, mean_eps: 0.865983
  149063/2000000: episode: 1553, duration: 1.372s, episode steps: 98, steps per second: 71, episode reward: -167.947, mean reward: -1.714 [-100.000, 6.394], mean action: 1.684 [0.000, 3.000], mean observation: 0.059 [-1.418, 1.000], loss: 1.440848, mean_absolute_error: 15.488386, mean_q: 6.479177, mean_eps: 0.865887
  149187/2000000: episode: 1554, duration: 1.752s, episode steps: 124, steps per second: 71, episode reward: -292.605, mean reward: -2.360 [-100.000, 49.122], mean action: 1.444 [0.000, 3.000], mean observation: 0.256 [-1.341, 3.458], loss: 1.896296, mean_absolute_error: 15.981201, mean_q: 6.288527, mean_eps: 0.865788
  149301/2000000: episode: 1555, duration: 1.646s, episode steps: 114, steps per second: 69, episode reward: -117.257, mean reward: -1.029 [-100.000, 11.937], mean action: 1.640 [0.000, 3.000], mean observation: 0.012 [-3.458, 1.000], loss: 1.403138, mean_absolute_error: 15.183268, mean_q: 7.544113, mean_eps: 0.865680
  149424/2000000: episode: 1556, duration: 1.731s, episode steps: 123, steps per second: 71, episode reward: -109.848, mean reward: -0.893 [-100.000, 46.066], mean action: 1.569 [0.000, 3.000], mean observation: 0.035 [-2.819, 1.011], loss: 1.368200, mean_absolute_error: 14.847256, mean_q: 7.264910, mean_eps: 0.865574
  149556/2000000: episode: 1557, duration: 1.888s, episode steps: 132, steps per second: 70, episode reward: -128.587, mean reward: -0.974 [-100.000, 21.918], mean action: 1.636 [0.000, 3.000], mean observation: -0.115 [-1.604, 1.232], loss: 1.518838, mean_absolute_error: 15.971374, mean_q: 5.953567, mean_eps: 0.865461
  149637/2000000: episode: 1558, duration: 1.181s, episode steps: 81, steps per second: 69, episode reward: -96.497, mean reward: -1.191 [-100.000, 9.920], mean action: 1.568 [0.000, 3.000], mean observation: 0.028 [-3.562, 1.000], loss: 1.339719, mean_absolute_error: 16.263064, mean_q: 6.002638, mean_eps: 0.865364
  149770/2000000: episode: 1559, duration: 1.869s, episode steps: 133, steps per second: 71, episode reward: -128.616, mean reward: -0.967 [-100.000, 7.474], mean action: 1.579 [0.000, 3.000], mean observation: -0.048 [-1.129, 3.132], loss: 1.425578, mean_absolute_error: 15.813039, mean_q: 6.456597, mean_eps: 0.865266
  149875/2000000: episode: 1560, duration: 1.480s, episode steps: 105, steps per second: 71, episode reward: -264.155, mean reward: -2.516 [-100.000, 7.436], mean action: 1.552 [0.000, 3.000], mean observation: -0.048 [-4.443, 1.019], loss: 1.592875, mean_absolute_error: 15.494628, mean_q: 7.145916, mean_eps: 0.865160
  149983/2000000: episode: 1561, duration: 1.614s, episode steps: 108, steps per second: 67, episode reward: -174.163, mean reward: -1.613 [-100.000, 5.798], mean action: 1.509 [0.000, 3.000], mean observation: 0.126 [-2.682, 1.000], loss: 2.005526, mean_absolute_error: 16.394104, mean_q: 4.909323, mean_eps: 0.865065
  150058/2000000: episode: 1562, duration: 1.088s, episode steps: 75, steps per second: 69, episode reward: -244.705, mean reward: -3.263 [-100.000, 6.878], mean action: 1.320 [0.000, 3.000], mean observation: 0.030 [-1.557, 1.000], loss: 1.483406, mean_absolute_error: 16.187054, mean_q: 8.174781, mean_eps: 0.864982
  150139/2000000: episode: 1563, duration: 1.121s, episode steps: 81, steps per second: 72, episode reward: -156.159, mean reward: -1.928 [-100.000, 17.740], mean action: 1.321 [0.000, 3.000], mean observation: 0.065 [-1.487, 1.000], loss: 2.237906, mean_absolute_error: 17.981379, mean_q: 5.963151, mean_eps: 0.864912
  150263/2000000: episode: 1564, duration: 1.747s, episode steps: 124, steps per second: 71, episode reward: -120.710, mean reward: -0.973 [-100.000, 11.947], mean action: 1.605 [0.000, 3.000], mean observation: 0.056 [-3.994, 1.000], loss: 1.478874, mean_absolute_error: 15.861981, mean_q: 8.480887, mean_eps: 0.864820
  150340/2000000: episode: 1565, duration: 1.122s, episode steps: 77, steps per second: 69, episode reward: -240.374, mean reward: -3.122 [-100.000, 7.541], mean action: 1.481 [0.000, 3.000], mean observation: 0.042 [-5.758, 1.000], loss: 1.807117, mean_absolute_error: 18.560396, mean_q: 7.445682, mean_eps: 0.864730
  150442/2000000: episode: 1566, duration: 1.460s, episode steps: 102, steps per second: 70, episode reward: -149.244, mean reward: -1.463 [-100.000, 23.153], mean action: 1.647 [0.000, 3.000], mean observation: -0.087 [-1.283, 2.404], loss: 1.625898, mean_absolute_error: 16.410940, mean_q: 8.336252, mean_eps: 0.864649
  150534/2000000: episode: 1567, duration: 1.306s, episode steps: 92, steps per second: 70, episode reward: -193.326, mean reward: -2.101 [-100.000, 12.496], mean action: 1.543 [0.000, 3.000], mean observation: -0.125 [-4.488, 1.000], loss: 2.010049, mean_absolute_error: 16.811863, mean_q: 8.132796, mean_eps: 0.864561
  150602/2000000: episode: 1568, duration: 0.968s, episode steps: 68, steps per second: 70, episode reward: -116.237, mean reward: -1.709 [-100.000, 6.961], mean action: 1.588 [0.000, 3.000], mean observation: 0.084 [-1.316, 4.855], loss: 1.538027, mean_absolute_error: 15.470764, mean_q: 8.116054, mean_eps: 0.864489
  150761/2000000: episode: 1569, duration: 2.230s, episode steps: 159, steps per second: 71, episode reward: -156.036, mean reward: -0.981 [-100.000, 6.256], mean action: 1.472 [0.000, 3.000], mean observation: -0.029 [-2.648, 1.050], loss: 1.810506, mean_absolute_error: 17.374313, mean_q: 6.922959, mean_eps: 0.864386
  150920/2000000: episode: 1570, duration: 2.266s, episode steps: 159, steps per second: 70, episode reward: -146.931, mean reward: -0.924 [-100.000, 101.247], mean action: 1.610 [0.000, 3.000], mean observation: 0.004 [-1.943, 1.000], loss: 1.470691, mean_absolute_error: 16.566448, mean_q: 8.584782, mean_eps: 0.864244
  150984/2000000: episode: 1571, duration: 0.952s, episode steps: 64, steps per second: 67, episode reward: -166.954, mean reward: -2.609 [-100.000, 18.303], mean action: 1.594 [0.000, 3.000], mean observation: 0.029 [-1.422, 4.088], loss: 0.984397, mean_absolute_error: 16.263537, mean_q: 9.845882, mean_eps: 0.864145
  151062/2000000: episode: 1572, duration: 1.135s, episode steps: 78, steps per second: 69, episode reward: -149.204, mean reward: -1.913 [-100.000, 11.783], mean action: 1.526 [0.000, 3.000], mean observation: -0.041 [-2.814, 1.000], loss: 1.770474, mean_absolute_error: 16.961909, mean_q: 7.626230, mean_eps: 0.864080
  151131/2000000: episode: 1573, duration: 1.051s, episode steps: 69, steps per second: 66, episode reward: -144.362, mean reward: -2.092 [-100.000, 7.552], mean action: 1.464 [0.000, 3.000], mean observation: -0.045 [-4.863, 1.000], loss: 1.324414, mean_absolute_error: 15.723715, mean_q: 10.490699, mean_eps: 0.864014
  151199/2000000: episode: 1574, duration: 1.115s, episode steps: 68, steps per second: 61, episode reward: -120.753, mean reward: -1.776 [-100.000, 8.549], mean action: 1.603 [0.000, 3.000], mean observation: -0.042 [-4.815, 1.000], loss: 2.093116, mean_absolute_error: 17.878543, mean_q: 8.728545, mean_eps: 0.863952
  151285/2000000: episode: 1575, duration: 1.306s, episode steps: 86, steps per second: 66, episode reward: -150.739, mean reward: -1.753 [-100.000, 6.090], mean action: 1.709 [0.000, 3.000], mean observation: -0.147 [-1.143, 3.923], loss: 2.331549, mean_absolute_error: 17.323033, mean_q: 6.156171, mean_eps: 0.863882
  151385/2000000: episode: 1576, duration: 1.423s, episode steps: 100, steps per second: 70, episode reward: -121.092, mean reward: -1.211 [-100.000, 11.670], mean action: 1.690 [0.000, 3.000], mean observation: -0.018 [-1.200, 1.000], loss: 1.681422, mean_absolute_error: 16.465589, mean_q: 7.832217, mean_eps: 0.863798
  151457/2000000: episode: 1577, duration: 1.020s, episode steps: 72, steps per second: 71, episode reward: -150.678, mean reward: -2.093 [-100.000, 3.760], mean action: 1.250 [0.000, 3.000], mean observation: -0.114 [-4.964, 1.000], loss: 1.770474, mean_absolute_error: 17.090707, mean_q: 5.769535, mean_eps: 0.863720
  151581/2000000: episode: 1578, duration: 1.758s, episode steps: 124, steps per second: 71, episode reward: -186.963, mean reward: -1.508 [-100.000, 5.541], mean action: 1.645 [0.000, 3.000], mean observation: -0.089 [-1.366, 4.328], loss: 1.861405, mean_absolute_error: 16.642702, mean_q: 8.494754, mean_eps: 0.863632
  151654/2000000: episode: 1579, duration: 1.041s, episode steps: 73, steps per second: 70, episode reward: -108.072, mean reward: -1.480 [-100.000, 13.270], mean action: 1.699 [0.000, 3.000], mean observation: -0.155 [-1.207, 1.000], loss: 1.580898, mean_absolute_error: 16.794466, mean_q: 6.972596, mean_eps: 0.863544
  151753/2000000: episode: 1580, duration: 1.393s, episode steps: 99, steps per second: 71, episode reward: -146.635, mean reward: -1.481 [-100.000, 14.857], mean action: 1.667 [0.000, 3.000], mean observation: -0.094 [-1.116, 3.612], loss: 1.572963, mean_absolute_error: 17.515764, mean_q: 8.154014, mean_eps: 0.863466
  151822/2000000: episode: 1581, duration: 0.961s, episode steps: 69, steps per second: 72, episode reward: -108.469, mean reward: -1.572 [-100.000, 16.058], mean action: 1.652 [0.000, 3.000], mean observation: -0.130 [-1.200, 1.000], loss: 1.207115, mean_absolute_error: 16.374385, mean_q: 8.858129, mean_eps: 0.863391
  151888/2000000: episode: 1582, duration: 0.967s, episode steps: 66, steps per second: 68, episode reward: -164.567, mean reward: -2.493 [-100.000, 19.147], mean action: 1.803 [0.000, 3.000], mean observation: -0.132 [-1.569, 1.000], loss: 1.782670, mean_absolute_error: 16.001772, mean_q: 8.411166, mean_eps: 0.863331
  151954/2000000: episode: 1583, duration: 0.960s, episode steps: 66, steps per second: 69, episode reward: -111.919, mean reward: -1.696 [-100.000, 7.078], mean action: 1.606 [0.000, 3.000], mean observation: -0.073 [-1.235, 4.291], loss: 1.999946, mean_absolute_error: 18.024655, mean_q: 5.270323, mean_eps: 0.863272
  152018/2000000: episode: 1584, duration: 0.915s, episode steps: 64, steps per second: 70, episode reward: -105.321, mean reward: -1.646 [-100.000, 12.241], mean action: 1.531 [0.000, 3.000], mean observation: -0.130 [-4.262, 1.000], loss: 1.867705, mean_absolute_error: 17.681651, mean_q: 8.745402, mean_eps: 0.863213
  152137/2000000: episode: 1585, duration: 1.669s, episode steps: 119, steps per second: 71, episode reward: -155.958, mean reward: -1.311 [-100.000, 9.150], mean action: 1.597 [0.000, 3.000], mean observation: 0.077 [-5.412, 1.000], loss: 1.955042, mean_absolute_error: 16.760849, mean_q: 8.328281, mean_eps: 0.863130
  152209/2000000: episode: 1586, duration: 1.004s, episode steps: 72, steps per second: 72, episode reward: -113.870, mean reward: -1.582 [-100.000, 15.576], mean action: 1.431 [0.000, 3.000], mean observation: 0.005 [-1.306, 1.000], loss: 2.302683, mean_absolute_error: 18.455897, mean_q: 6.551002, mean_eps: 0.863043
  152328/2000000: episode: 1587, duration: 1.678s, episode steps: 119, steps per second: 71, episode reward: -205.370, mean reward: -1.726 [-100.000, 15.337], mean action: 1.630 [0.000, 3.000], mean observation: -0.130 [-1.359, 2.912], loss: 1.708546, mean_absolute_error: 17.469704, mean_q: 6.993189, mean_eps: 0.862959
  152438/2000000: episode: 1588, duration: 1.563s, episode steps: 110, steps per second: 70, episode reward: -168.616, mean reward: -1.533 [-100.000, 11.348], mean action: 1.509 [0.000, 3.000], mean observation: -0.131 [-1.248, 3.547], loss: 2.340662, mean_absolute_error: 18.236742, mean_q: 7.331917, mean_eps: 0.862856
  153438/2000000: episode: 1589, duration: 15.592s, episode steps: 1000, steps per second: 64, episode reward: 32.200, mean reward: 0.032 [-24.499, 68.479], mean action: 1.547 [0.000, 3.000], mean observation: 0.134 [-2.098, 1.186], loss: 1.695916, mean_absolute_error: 17.093018, mean_q: 7.892456, mean_eps: 0.862356
  153563/2000000: episode: 1590, duration: 1.775s, episode steps: 125, steps per second: 70, episode reward: -33.750, mean reward: -0.270 [-100.000, 63.352], mean action: 1.464 [0.000, 3.000], mean observation: 0.029 [-0.993, 1.646], loss: 1.623595, mean_absolute_error: 16.568748, mean_q: 8.732760, mean_eps: 0.861850
  153654/2000000: episode: 1591, duration: 1.390s, episode steps: 91, steps per second: 65, episode reward: -170.736, mean reward: -1.876 [-100.000, 6.509], mean action: 1.527 [0.000, 3.000], mean observation: -0.077 [-1.424, 4.745], loss: 1.955763, mean_absolute_error: 16.262800, mean_q: 9.038124, mean_eps: 0.861753
  153743/2000000: episode: 1592, duration: 1.301s, episode steps: 89, steps per second: 68, episode reward: -121.959, mean reward: -1.370 [-100.000, 5.572], mean action: 1.742 [0.000, 3.000], mean observation: -0.081 [-0.986, 3.554], loss: 1.310917, mean_absolute_error: 17.242135, mean_q: 8.495751, mean_eps: 0.861672
  153812/2000000: episode: 1593, duration: 1.023s, episode steps: 69, steps per second: 67, episode reward: -111.356, mean reward: -1.614 [-100.000, 11.892], mean action: 1.435 [0.000, 3.000], mean observation: -0.017 [-4.288, 1.000], loss: 1.263944, mean_absolute_error: 16.374584, mean_q: 7.660318, mean_eps: 0.861602
  153897/2000000: episode: 1594, duration: 1.227s, episode steps: 85, steps per second: 69, episode reward: -160.825, mean reward: -1.892 [-100.000, 12.345], mean action: 1.647 [0.000, 3.000], mean observation: -0.167 [-1.299, 3.236], loss: 1.654108, mean_absolute_error: 16.876874, mean_q: 9.149519, mean_eps: 0.861531
  153999/2000000: episode: 1595, duration: 1.450s, episode steps: 102, steps per second: 70, episode reward: -68.543, mean reward: -0.672 [-100.000, 115.440], mean action: 1.569 [0.000, 3.000], mean observation: 0.118 [-1.578, 1.252], loss: 1.632933, mean_absolute_error: 16.503692, mean_q: 8.608261, mean_eps: 0.861447
  154092/2000000: episode: 1596, duration: 1.358s, episode steps: 93, steps per second: 68, episode reward: -161.456, mean reward: -1.736 [-100.000, 6.722], mean action: 1.312 [0.000, 3.000], mean observation: 0.120 [-4.376, 1.017], loss: 1.821348, mean_absolute_error: 16.804054, mean_q: 7.351537, mean_eps: 0.861360
  154178/2000000: episode: 1597, duration: 1.235s, episode steps: 86, steps per second: 70, episode reward: -94.155, mean reward: -1.095 [-100.000, 7.718], mean action: 1.395 [0.000, 3.000], mean observation: 0.035 [-0.983, 1.000], loss: 1.455804, mean_absolute_error: 16.485054, mean_q: 6.475637, mean_eps: 0.861279
  154306/2000000: episode: 1598, duration: 1.802s, episode steps: 128, steps per second: 71, episode reward: -113.937, mean reward: -0.890 [-100.000, 17.236], mean action: 1.578 [0.000, 3.000], mean observation: 0.018 [-1.414, 1.029], loss: 1.738543, mean_absolute_error: 17.168734, mean_q: 7.790986, mean_eps: 0.861182
  154433/2000000: episode: 1599, duration: 1.809s, episode steps: 127, steps per second: 70, episode reward: -87.367, mean reward: -0.688 [-100.000, 107.898], mean action: 1.417 [0.000, 3.000], mean observation: 0.144 [-1.677, 1.547], loss: 1.360954, mean_absolute_error: 16.805303, mean_q: 8.587029, mean_eps: 0.861067
  154542/2000000: episode: 1600, duration: 1.535s, episode steps: 109, steps per second: 71, episode reward: -148.933, mean reward: -1.366 [-100.000, 10.757], mean action: 1.349 [0.000, 3.000], mean observation: 0.076 [-3.744, 1.000], loss: 1.982002, mean_absolute_error: 18.068531, mean_q: 7.175942, mean_eps: 0.860961
  154615/2000000: episode: 1601, duration: 1.034s, episode steps: 73, steps per second: 71, episode reward: -126.474, mean reward: -1.733 [-100.000, 8.823], mean action: 1.466 [0.000, 3.000], mean observation: 0.016 [-4.190, 1.000], loss: 2.055477, mean_absolute_error: 16.829852, mean_q: 8.218749, mean_eps: 0.860880
  154703/2000000: episode: 1602, duration: 1.264s, episode steps: 88, steps per second: 70, episode reward: -146.634, mean reward: -1.666 [-100.000, 10.303], mean action: 1.523 [0.000, 3.000], mean observation: 0.089 [-1.213, 2.170], loss: 1.866638, mean_absolute_error: 17.229210, mean_q: 7.589104, mean_eps: 0.860808
  154785/2000000: episode: 1603, duration: 1.181s, episode steps: 82, steps per second: 69, episode reward: -134.579, mean reward: -1.641 [-100.000, 6.415], mean action: 1.317 [0.000, 3.000], mean observation: 0.044 [-4.300, 1.000], loss: 1.591710, mean_absolute_error: 16.327462, mean_q: 8.646362, mean_eps: 0.860730
  154862/2000000: episode: 1604, duration: 1.081s, episode steps: 77, steps per second: 71, episode reward: -71.276, mean reward: -0.926 [-100.000, 10.237], mean action: 1.519 [0.000, 3.000], mean observation: 0.012 [-1.076, 1.000], loss: 2.586725, mean_absolute_error: 17.130752, mean_q: 7.928383, mean_eps: 0.860658
  154956/2000000: episode: 1605, duration: 1.337s, episode steps: 94, steps per second: 70, episode reward: -145.370, mean reward: -1.546 [-100.000, 14.706], mean action: 1.543 [0.000, 3.000], mean observation: 0.147 [-2.832, 1.000], loss: 1.614552, mean_absolute_error: 16.311722, mean_q: 7.908137, mean_eps: 0.860583
  155030/2000000: episode: 1606, duration: 1.077s, episode steps: 74, steps per second: 69, episode reward: -122.921, mean reward: -1.661 [-100.000, 16.366], mean action: 1.635 [0.000, 3.000], mean observation: -0.101 [-1.283, 1.000], loss: 2.218377, mean_absolute_error: 17.411373, mean_q: 7.007495, mean_eps: 0.860507
  155126/2000000: episode: 1607, duration: 1.364s, episode steps: 96, steps per second: 70, episode reward: -210.087, mean reward: -2.188 [-100.000, 50.750], mean action: 1.500 [0.000, 3.000], mean observation: 0.144 [-1.152, 2.956], loss: 1.713031, mean_absolute_error: 17.443277, mean_q: 6.836521, mean_eps: 0.860430
  155192/2000000: episode: 1608, duration: 0.959s, episode steps: 66, steps per second: 69, episode reward: -162.009, mean reward: -2.455 [-100.000, 5.857], mean action: 1.500 [0.000, 3.000], mean observation: 0.038 [-3.329, 1.000], loss: 1.998021, mean_absolute_error: 16.503795, mean_q: 9.999611, mean_eps: 0.860358
  155250/2000000: episode: 1609, duration: 0.838s, episode steps: 58, steps per second: 69, episode reward: -97.790, mean reward: -1.686 [-100.000, 13.609], mean action: 1.397 [0.000, 3.000], mean observation: 0.009 [-1.491, 1.000], loss: 1.100925, mean_absolute_error: 16.546766, mean_q: 9.099832, mean_eps: 0.860302
  155343/2000000: episode: 1610, duration: 1.331s, episode steps: 93, steps per second: 70, episode reward: -160.363, mean reward: -1.724 [-100.000, 7.145], mean action: 1.624 [0.000, 3.000], mean observation: -0.037 [-4.031, 1.000], loss: 1.584240, mean_absolute_error: 16.571290, mean_q: 8.893427, mean_eps: 0.860234
  155431/2000000: episode: 1611, duration: 1.254s, episode steps: 88, steps per second: 70, episode reward: -134.151, mean reward: -1.524 [-100.000, 16.355], mean action: 1.568 [0.000, 3.000], mean observation: -0.019 [-4.155, 1.000], loss: 1.459511, mean_absolute_error: 16.869735, mean_q: 9.586485, mean_eps: 0.860153
  155560/2000000: episode: 1612, duration: 1.842s, episode steps: 129, steps per second: 70, episode reward: -202.216, mean reward: -1.568 [-100.000, 3.759], mean action: 1.512 [0.000, 3.000], mean observation: 0.175 [-1.281, 1.053], loss: 2.186443, mean_absolute_error: 17.428392, mean_q: 7.543052, mean_eps: 0.860055
  155634/2000000: episode: 1613, duration: 1.075s, episode steps: 74, steps per second: 69, episode reward: -133.417, mean reward: -1.803 [-100.000, 5.847], mean action: 1.297 [0.000, 3.000], mean observation: 0.086 [-1.248, 3.586], loss: 1.580103, mean_absolute_error: 16.017133, mean_q: 8.203586, mean_eps: 0.859964
  155789/2000000: episode: 1614, duration: 2.199s, episode steps: 155, steps per second: 70, episode reward: -124.752, mean reward: -0.805 [-100.000, 7.635], mean action: 1.568 [0.000, 3.000], mean observation: -0.000 [-1.269, 4.073], loss: 1.473014, mean_absolute_error: 17.045414, mean_q: 7.148143, mean_eps: 0.859859
  155919/2000000: episode: 1615, duration: 1.839s, episode steps: 130, steps per second: 71, episode reward: -122.803, mean reward: -0.945 [-100.000, 47.877], mean action: 1.515 [0.000, 3.000], mean observation: 0.189 [-2.935, 1.000], loss: 1.459216, mean_absolute_error: 16.891458, mean_q: 8.897472, mean_eps: 0.859731
  156005/2000000: episode: 1616, duration: 1.411s, episode steps: 86, steps per second: 61, episode reward: -158.710, mean reward: -1.845 [-100.000, 12.318], mean action: 1.500 [0.000, 3.000], mean observation: 0.043 [-1.497, 5.061], loss: 1.771460, mean_absolute_error: 17.214533, mean_q: 6.780825, mean_eps: 0.859634
  156107/2000000: episode: 1617, duration: 1.580s, episode steps: 102, steps per second: 65, episode reward: -121.131, mean reward: -1.188 [-100.000, 18.753], mean action: 1.569 [0.000, 3.000], mean observation: -0.010 [-3.533, 1.000], loss: 2.058484, mean_absolute_error: 16.243569, mean_q: 8.342467, mean_eps: 0.859550
  156232/2000000: episode: 1618, duration: 1.944s, episode steps: 125, steps per second: 64, episode reward: -228.168, mean reward: -1.825 [-100.000, 1.060], mean action: 1.400 [0.000, 3.000], mean observation: 0.173 [-1.222, 1.071], loss: 1.669229, mean_absolute_error: 17.106187, mean_q: 7.708160, mean_eps: 0.859449
  156304/2000000: episode: 1619, duration: 1.081s, episode steps: 72, steps per second: 67, episode reward: -114.562, mean reward: -1.591 [-100.000, 10.028], mean action: 1.639 [0.000, 3.000], mean observation: -0.096 [-4.338, 1.000], loss: 1.985379, mean_absolute_error: 16.839125, mean_q: 8.342684, mean_eps: 0.859361
  156427/2000000: episode: 1620, duration: 1.743s, episode steps: 123, steps per second: 71, episode reward: -162.061, mean reward: -1.318 [-100.000, 9.822], mean action: 1.407 [0.000, 3.000], mean observation: 0.128 [-1.827, 1.064], loss: 1.911611, mean_absolute_error: 16.813859, mean_q: 8.658503, mean_eps: 0.859272
  156544/2000000: episode: 1621, duration: 1.652s, episode steps: 117, steps per second: 71, episode reward: -149.628, mean reward: -1.279 [-100.000, 7.254], mean action: 1.778 [0.000, 3.000], mean observation: -0.129 [-3.948, 1.000], loss: 1.720163, mean_absolute_error: 16.429318, mean_q: 7.372471, mean_eps: 0.859164
  156613/2000000: episode: 1622, duration: 1.023s, episode steps: 69, steps per second: 67, episode reward: -139.802, mean reward: -2.026 [-100.000, 6.401], mean action: 1.551 [0.000, 3.000], mean observation: -0.151 [-1.262, 2.895], loss: 2.096206, mean_absolute_error: 18.147556, mean_q: 5.720958, mean_eps: 0.859080
  156710/2000000: episode: 1623, duration: 1.360s, episode steps: 97, steps per second: 71, episode reward: -162.265, mean reward: -1.673 [-100.000, 9.792], mean action: 1.588 [0.000, 3.000], mean observation: -0.027 [-1.249, 3.795], loss: 1.737885, mean_absolute_error: 17.101079, mean_q: 8.016044, mean_eps: 0.859004
  156783/2000000: episode: 1624, duration: 1.015s, episode steps: 73, steps per second: 72, episode reward: -115.025, mean reward: -1.576 [-100.000, 15.398], mean action: 1.534 [0.000, 3.000], mean observation: -0.118 [-1.340, 1.000], loss: 1.792409, mean_absolute_error: 16.702495, mean_q: 7.720759, mean_eps: 0.858929
  156898/2000000: episode: 1625, duration: 1.638s, episode steps: 115, steps per second: 70, episode reward: -193.784, mean reward: -1.685 [-100.000, 13.681], mean action: 1.713 [0.000, 3.000], mean observation: -0.131 [-1.454, 2.616], loss: 1.445202, mean_absolute_error: 16.553014, mean_q: 7.761237, mean_eps: 0.858844
  156965/2000000: episode: 1626, duration: 0.961s, episode steps: 67, steps per second: 70, episode reward: -103.701, mean reward: -1.548 [-100.000, 10.758], mean action: 1.478 [0.000, 3.000], mean observation: -0.170 [-1.499, 1.000], loss: 1.442171, mean_absolute_error: 16.269808, mean_q: 7.132816, mean_eps: 0.858761
  157074/2000000: episode: 1627, duration: 1.518s, episode steps: 109, steps per second: 72, episode reward: -268.725, mean reward: -2.465 [-100.000, 6.509], mean action: 1.624 [0.000, 3.000], mean observation: -0.135 [-1.544, 1.270], loss: 1.517475, mean_absolute_error: 16.040337, mean_q: 9.113572, mean_eps: 0.858682
  157186/2000000: episode: 1628, duration: 1.591s, episode steps: 112, steps per second: 70, episode reward: -72.638, mean reward: -0.649 [-100.000, 18.938], mean action: 1.545 [0.000, 3.000], mean observation: -0.038 [-2.679, 1.000], loss: 1.392838, mean_absolute_error: 17.499225, mean_q: 7.093288, mean_eps: 0.858583
  157270/2000000: episode: 1629, duration: 1.163s, episode steps: 84, steps per second: 72, episode reward: -118.894, mean reward: -1.415 [-100.000, 16.744], mean action: 1.524 [0.000, 3.000], mean observation: -0.065 [-4.106, 1.000], loss: 1.099105, mean_absolute_error: 16.141929, mean_q: 7.233321, mean_eps: 0.858495
  157340/2000000: episode: 1630, duration: 1.030s, episode steps: 70, steps per second: 68, episode reward: -196.396, mean reward: -2.806 [-100.000, 7.612], mean action: 1.600 [0.000, 3.000], mean observation: 0.071 [-1.467, 4.726], loss: 1.822538, mean_absolute_error: 17.093642, mean_q: 8.485440, mean_eps: 0.858426
  157471/2000000: episode: 1631, duration: 1.868s, episode steps: 131, steps per second: 70, episode reward: -148.589, mean reward: -1.134 [-100.000, 14.634], mean action: 1.504 [0.000, 3.000], mean observation: 0.108 [-3.786, 1.000], loss: 1.443478, mean_absolute_error: 16.363876, mean_q: 7.934613, mean_eps: 0.858336
  157562/2000000: episode: 1632, duration: 1.336s, episode steps: 91, steps per second: 68, episode reward: -114.891, mean reward: -1.263 [-100.000, 41.748], mean action: 1.769 [0.000, 3.000], mean observation: -0.147 [-1.135, 2.774], loss: 1.439130, mean_absolute_error: 16.110696, mean_q: 8.339397, mean_eps: 0.858236
  157697/2000000: episode: 1633, duration: 1.966s, episode steps: 135, steps per second: 69, episode reward: -78.739, mean reward: -0.583 [-100.000, 65.348], mean action: 1.615 [0.000, 3.000], mean observation: -0.013 [-1.398, 1.238], loss: 1.750667, mean_absolute_error: 17.022890, mean_q: 8.025396, mean_eps: 0.858133
  157812/2000000: episode: 1634, duration: 1.667s, episode steps: 115, steps per second: 69, episode reward: -157.612, mean reward: -1.371 [-100.000, 7.689], mean action: 1.609 [0.000, 3.000], mean observation: -0.047 [-1.073, 2.815], loss: 1.536229, mean_absolute_error: 16.995444, mean_q: 8.234058, mean_eps: 0.858021
  157946/2000000: episode: 1635, duration: 1.942s, episode steps: 134, steps per second: 69, episode reward: -60.577, mean reward: -0.452 [-100.000, 94.622], mean action: 1.560 [0.000, 3.000], mean observation: -0.025 [-1.292, 1.590], loss: 1.888377, mean_absolute_error: 17.064695, mean_q: 7.748089, mean_eps: 0.857910
  158028/2000000: episode: 1636, duration: 1.192s, episode steps: 82, steps per second: 69, episode reward: -234.606, mean reward: -2.861 [-100.000, 7.179], mean action: 1.646 [0.000, 3.000], mean observation: 0.078 [-4.536, 1.123], loss: 1.439241, mean_absolute_error: 16.212135, mean_q: 9.667397, mean_eps: 0.857813
  158110/2000000: episode: 1637, duration: 1.191s, episode steps: 82, steps per second: 69, episode reward: -142.506, mean reward: -1.738 [-100.000, 18.823], mean action: 1.610 [0.000, 3.000], mean observation: 0.098 [-1.332, 4.563], loss: 1.647836, mean_absolute_error: 15.921453, mean_q: 10.112006, mean_eps: 0.857739
  158239/2000000: episode: 1638, duration: 1.832s, episode steps: 129, steps per second: 70, episode reward: -143.738, mean reward: -1.114 [-100.000, 10.844], mean action: 1.605 [0.000, 3.000], mean observation: -0.042 [-1.018, 3.724], loss: 1.725054, mean_absolute_error: 16.141348, mean_q: 8.352742, mean_eps: 0.857643
  158317/2000000: episode: 1639, duration: 1.126s, episode steps: 78, steps per second: 69, episode reward: -173.054, mean reward: -2.219 [-100.000, 29.710], mean action: 1.487 [0.000, 3.000], mean observation: 0.024 [-1.402, 1.544], loss: 1.810100, mean_absolute_error: 17.271219, mean_q: 9.236094, mean_eps: 0.857550
  158426/2000000: episode: 1640, duration: 1.534s, episode steps: 109, steps per second: 71, episode reward: -174.070, mean reward: -1.597 [-100.000, 32.013], mean action: 1.725 [0.000, 3.000], mean observation: -0.054 [-1.309, 2.939], loss: 1.942348, mean_absolute_error: 16.484470, mean_q: 8.982163, mean_eps: 0.857465
  158526/2000000: episode: 1641, duration: 1.409s, episode steps: 100, steps per second: 71, episode reward: -269.514, mean reward: -2.695 [-100.000, 7.882], mean action: 1.620 [0.000, 3.000], mean observation: -0.097 [-1.707, 1.000], loss: 1.737622, mean_absolute_error: 16.991647, mean_q: 8.897275, mean_eps: 0.857372
  158637/2000000: episode: 1642, duration: 1.579s, episode steps: 111, steps per second: 70, episode reward: -135.479, mean reward: -1.221 [-100.000, 21.804], mean action: 1.685 [0.000, 3.000], mean observation: -0.021 [-1.212, 4.109], loss: 1.672588, mean_absolute_error: 17.203451, mean_q: 6.541322, mean_eps: 0.857276
  158719/2000000: episode: 1643, duration: 1.137s, episode steps: 82, steps per second: 72, episode reward: -163.886, mean reward: -1.999 [-100.000, 15.086], mean action: 1.427 [0.000, 3.000], mean observation: 0.118 [-1.284, 1.000], loss: 1.962648, mean_absolute_error: 18.094635, mean_q: 6.317456, mean_eps: 0.857190
  158830/2000000: episode: 1644, duration: 1.586s, episode steps: 111, steps per second: 70, episode reward: -100.069, mean reward: -0.902 [-100.000, 11.351], mean action: 1.586 [0.000, 3.000], mean observation: 0.104 [-1.103, 3.818], loss: 1.886182, mean_absolute_error: 17.608336, mean_q: 7.642766, mean_eps: 0.857103
  158909/2000000: episode: 1645, duration: 1.145s, episode steps: 79, steps per second: 69, episode reward: -148.805, mean reward: -1.884 [-100.000, 10.979], mean action: 1.658 [0.000, 3.000], mean observation: -0.069 [-3.589, 1.000], loss: 1.523503, mean_absolute_error: 15.532659, mean_q: 8.648299, mean_eps: 0.857017
  159007/2000000: episode: 1646, duration: 1.346s, episode steps: 98, steps per second: 73, episode reward: -116.041, mean reward: -1.184 [-100.000, 12.200], mean action: 1.541 [0.000, 3.000], mean observation: 0.016 [-3.763, 1.000], loss: 1.326136, mean_absolute_error: 16.404137, mean_q: 10.051126, mean_eps: 0.856938
  159092/2000000: episode: 1647, duration: 1.227s, episode steps: 85, steps per second: 69, episode reward: -168.236, mean reward: -1.979 [-100.000, 11.589], mean action: 1.518 [0.000, 3.000], mean observation: -0.049 [-3.968, 1.000], loss: 0.895632, mean_absolute_error: 16.077957, mean_q: 8.847311, mean_eps: 0.856857
  159177/2000000: episode: 1648, duration: 1.230s, episode steps: 85, steps per second: 69, episode reward: -152.986, mean reward: -1.800 [-100.000, 11.612], mean action: 1.706 [0.000, 3.000], mean observation: -0.128 [-4.522, 1.000], loss: 1.517533, mean_absolute_error: 16.249124, mean_q: 8.562167, mean_eps: 0.856779
  159313/2000000: episode: 1649, duration: 1.907s, episode steps: 136, steps per second: 71, episode reward: -182.325, mean reward: -1.341 [-100.000, 12.432], mean action: 1.684 [0.000, 3.000], mean observation: -0.162 [-1.201, 2.924], loss: 2.514885, mean_absolute_error: 18.608822, mean_q: 5.782776, mean_eps: 0.856679
  159417/2000000: episode: 1650, duration: 1.482s, episode steps: 104, steps per second: 70, episode reward: -150.381, mean reward: -1.446 [-100.000, 12.671], mean action: 1.760 [0.000, 3.000], mean observation: -0.146 [-1.141, 3.935], loss: 1.693216, mean_absolute_error: 17.447943, mean_q: 7.576720, mean_eps: 0.856571
  159502/2000000: episode: 1651, duration: 1.208s, episode steps: 85, steps per second: 70, episode reward: -124.690, mean reward: -1.467 [-100.000, 7.965], mean action: 1.565 [0.000, 3.000], mean observation: -0.006 [-4.790, 1.000], loss: 1.476961, mean_absolute_error: 17.421105, mean_q: 7.882474, mean_eps: 0.856486
  159595/2000000: episode: 1652, duration: 1.315s, episode steps: 93, steps per second: 71, episode reward: -201.312, mean reward: -2.165 [-100.000, 14.543], mean action: 1.462 [0.000, 3.000], mean observation: 0.107 [-5.628, 1.000], loss: 1.510483, mean_absolute_error: 16.277403, mean_q: 8.701820, mean_eps: 0.856407
  159698/2000000: episode: 1653, duration: 1.456s, episode steps: 103, steps per second: 71, episode reward: -41.430, mean reward: -0.402 [-100.000, 85.579], mean action: 1.398 [0.000, 3.000], mean observation: 0.156 [-1.670, 1.483], loss: 1.488049, mean_absolute_error: 15.826765, mean_q: 10.550918, mean_eps: 0.856319
  159785/2000000: episode: 1654, duration: 1.247s, episode steps: 87, steps per second: 70, episode reward: -274.523, mean reward: -3.155 [-100.000, 7.988], mean action: 1.460 [0.000, 3.000], mean observation: 0.046 [-6.409, 1.228], loss: 1.960100, mean_absolute_error: 16.689525, mean_q: 8.331265, mean_eps: 0.856232
  159903/2000000: episode: 1655, duration: 1.633s, episode steps: 118, steps per second: 72, episode reward: -275.398, mean reward: -2.334 [-100.000, 22.854], mean action: 1.619 [0.000, 3.000], mean observation: -0.143 [-4.540, 1.000], loss: 1.723275, mean_absolute_error: 16.798522, mean_q: 7.786107, mean_eps: 0.856140
  159996/2000000: episode: 1656, duration: 1.333s, episode steps: 93, steps per second: 70, episode reward: -159.004, mean reward: -1.710 [-100.000, 6.458], mean action: 1.344 [0.000, 3.000], mean observation: 0.109 [-4.933, 1.000], loss: 0.966174, mean_absolute_error: 15.671664, mean_q: 9.588419, mean_eps: 0.856047
  160106/2000000: episode: 1657, duration: 1.581s, episode steps: 110, steps per second: 70, episode reward: -343.225, mean reward: -3.120 [-100.000, 110.397], mean action: 1.636 [0.000, 3.000], mean observation: -0.046 [-2.798, 1.000], loss: 2.016927, mean_absolute_error: 18.237828, mean_q: 7.127912, mean_eps: 0.855955
  160232/2000000: episode: 1658, duration: 1.843s, episode steps: 126, steps per second: 68, episode reward: -163.714, mean reward: -1.299 [-100.000, 12.814], mean action: 1.611 [0.000, 3.000], mean observation: -0.020 [-1.345, 3.689], loss: 2.199683, mean_absolute_error: 19.666179, mean_q: 6.816226, mean_eps: 0.855849
  160348/2000000: episode: 1659, duration: 1.718s, episode steps: 116, steps per second: 68, episode reward: -114.313, mean reward: -0.985 [-100.000, 35.864], mean action: 1.414 [0.000, 3.000], mean observation: 0.172 [-1.511, 4.680], loss: 2.452172, mean_absolute_error: 18.468190, mean_q: 7.259689, mean_eps: 0.855741
  160447/2000000: episode: 1660, duration: 1.424s, episode steps: 99, steps per second: 70, episode reward: -103.748, mean reward: -1.048 [-100.000, 11.918], mean action: 1.626 [0.000, 3.000], mean observation: 0.033 [-1.120, 3.747], loss: 1.213311, mean_absolute_error: 17.169674, mean_q: 9.200054, mean_eps: 0.855644
  160519/2000000: episode: 1661, duration: 1.031s, episode steps: 72, steps per second: 70, episode reward: -87.345, mean reward: -1.213 [-100.000, 15.934], mean action: 1.597 [0.000, 3.000], mean observation: -0.041 [-4.252, 1.000], loss: 1.718408, mean_absolute_error: 18.063223, mean_q: 8.121245, mean_eps: 0.855566
  160630/2000000: episode: 1662, duration: 1.588s, episode steps: 111, steps per second: 70, episode reward: -149.436, mean reward: -1.346 [-100.000, 10.716], mean action: 1.613 [0.000, 3.000], mean observation: -0.056 [-1.245, 2.436], loss: 1.448771, mean_absolute_error: 17.508509, mean_q: 10.274797, mean_eps: 0.855483
  160729/2000000: episode: 1663, duration: 1.420s, episode steps: 99, steps per second: 70, episode reward: -99.679, mean reward: -1.007 [-100.000, 72.624], mean action: 1.646 [0.000, 3.000], mean observation: -0.057 [-4.030, 1.000], loss: 1.882996, mean_absolute_error: 18.442594, mean_q: 6.766123, mean_eps: 0.855388
  160806/2000000: episode: 1664, duration: 1.091s, episode steps: 77, steps per second: 71, episode reward: -142.906, mean reward: -1.856 [-100.000, 13.595], mean action: 1.390 [0.000, 3.000], mean observation: -0.023 [-5.035, 1.000], loss: 1.118332, mean_absolute_error: 17.196596, mean_q: 10.313543, mean_eps: 0.855309
  160884/2000000: episode: 1665, duration: 1.133s, episode steps: 78, steps per second: 69, episode reward: -211.701, mean reward: -2.714 [-100.000, 7.558], mean action: 1.577 [0.000, 3.000], mean observation: -0.029 [-4.689, 1.000], loss: 1.535190, mean_absolute_error: 17.988875, mean_q: 9.509245, mean_eps: 0.855240
  161021/2000000: episode: 1666, duration: 1.979s, episode steps: 137, steps per second: 69, episode reward: -169.530, mean reward: -1.237 [-100.000, 11.199], mean action: 1.569 [0.000, 3.000], mean observation: 0.054 [-4.267, 1.000], loss: 2.085641, mean_absolute_error: 18.038494, mean_q: 7.839470, mean_eps: 0.855143
  161121/2000000: episode: 1667, duration: 1.413s, episode steps: 100, steps per second: 71, episode reward: -179.829, mean reward: -1.798 [-100.000, 6.678], mean action: 1.390 [0.000, 3.000], mean observation: 0.104 [-4.642, 1.000], loss: 1.795408, mean_absolute_error: 17.689623, mean_q: 8.184182, mean_eps: 0.855035
  161253/2000000: episode: 1668, duration: 1.864s, episode steps: 132, steps per second: 71, episode reward: -203.198, mean reward: -1.539 [-100.000, 11.428], mean action: 1.470 [0.000, 3.000], mean observation: -0.076 [-1.351, 3.808], loss: 1.753387, mean_absolute_error: 17.829788, mean_q: 7.990253, mean_eps: 0.854931
  161336/2000000: episode: 1669, duration: 1.196s, episode steps: 83, steps per second: 69, episode reward: -117.914, mean reward: -1.421 [-100.000, 9.018], mean action: 1.614 [0.000, 3.000], mean observation: -0.101 [-1.303, 3.736], loss: 2.405724, mean_absolute_error: 19.090056, mean_q: 7.691545, mean_eps: 0.854835
  161445/2000000: episode: 1670, duration: 1.584s, episode steps: 109, steps per second: 69, episode reward: -183.592, mean reward: -1.684 [-100.000, 9.494], mean action: 1.661 [0.000, 3.000], mean observation: -0.094 [-1.570, 1.011], loss: 1.542455, mean_absolute_error: 18.180982, mean_q: 9.379454, mean_eps: 0.854749
  161545/2000000: episode: 1671, duration: 1.425s, episode steps: 100, steps per second: 70, episode reward: -287.291, mean reward: -2.873 [-100.000, 74.486], mean action: 1.610 [0.000, 3.000], mean observation: -0.156 [-3.492, 1.000], loss: 1.816622, mean_absolute_error: 17.320284, mean_q: 8.750426, mean_eps: 0.854654
  161630/2000000: episode: 1672, duration: 1.209s, episode steps: 85, steps per second: 70, episode reward: -98.635, mean reward: -1.160 [-100.000, 7.605], mean action: 1.718 [0.000, 3.000], mean observation: -0.077 [-1.029, 3.854], loss: 1.941772, mean_absolute_error: 17.611534, mean_q: 8.565078, mean_eps: 0.854571
  161709/2000000: episode: 1673, duration: 1.141s, episode steps: 79, steps per second: 69, episode reward: -133.608, mean reward: -1.691 [-100.000, 11.845], mean action: 1.468 [0.000, 3.000], mean observation: -0.002 [-4.746, 1.000], loss: 1.340070, mean_absolute_error: 17.955333, mean_q: 8.668410, mean_eps: 0.854497
  161774/2000000: episode: 1674, duration: 0.905s, episode steps: 65, steps per second: 72, episode reward: -98.015, mean reward: -1.508 [-100.000, 10.701], mean action: 1.508 [0.000, 3.000], mean observation: -0.095 [-1.295, 1.000], loss: 2.658876, mean_absolute_error: 19.685601, mean_q: 6.744115, mean_eps: 0.854432
  161839/2000000: episode: 1675, duration: 0.913s, episode steps: 65, steps per second: 71, episode reward: -104.569, mean reward: -1.609 [-100.000, 6.798], mean action: 1.369 [0.000, 3.000], mean observation: 0.066 [-4.296, 1.000], loss: 1.515413, mean_absolute_error: 16.961905, mean_q: 10.360321, mean_eps: 0.854375
  161922/2000000: episode: 1676, duration: 1.283s, episode steps: 83, steps per second: 65, episode reward: -196.398, mean reward: -2.366 [-100.000, 54.388], mean action: 1.614 [0.000, 3.000], mean observation: 0.100 [-1.416, 1.302], loss: 1.965547, mean_absolute_error: 16.840373, mean_q: 10.895879, mean_eps: 0.854308
  162045/2000000: episode: 1677, duration: 1.746s, episode steps: 123, steps per second: 70, episode reward: -116.826, mean reward: -0.950 [-100.000, 10.326], mean action: 1.593 [0.000, 3.000], mean observation: 0.056 [-4.187, 1.000], loss: 2.021373, mean_absolute_error: 18.345408, mean_q: 8.419381, mean_eps: 0.854214
  162155/2000000: episode: 1678, duration: 1.546s, episode steps: 110, steps per second: 71, episode reward: -110.378, mean reward: -1.003 [-100.000, 5.899], mean action: 1.464 [0.000, 3.000], mean observation: -0.030 [-3.619, 1.000], loss: 1.824579, mean_absolute_error: 17.116196, mean_q: 9.355789, mean_eps: 0.854110
  162262/2000000: episode: 1679, duration: 1.529s, episode steps: 107, steps per second: 70, episode reward: -148.658, mean reward: -1.389 [-100.000, 12.542], mean action: 1.607 [0.000, 3.000], mean observation: -0.011 [-1.382, 4.660], loss: 2.103719, mean_absolute_error: 18.055041, mean_q: 9.031424, mean_eps: 0.854013
  162330/2000000: episode: 1680, duration: 0.977s, episode steps: 68, steps per second: 70, episode reward: -126.069, mean reward: -1.854 [-100.000, 7.280], mean action: 1.515 [0.000, 3.000], mean observation: 0.030 [-1.422, 5.248], loss: 1.257630, mean_absolute_error: 17.252914, mean_q: 10.552292, mean_eps: 0.853934
  162456/2000000: episode: 1681, duration: 1.800s, episode steps: 126, steps per second: 70, episode reward: -190.023, mean reward: -1.508 [-100.000, 15.525], mean action: 1.548 [0.000, 3.000], mean observation: 0.143 [-1.325, 1.022], loss: 1.840636, mean_absolute_error: 18.313312, mean_q: 7.886311, mean_eps: 0.853847
  162580/2000000: episode: 1682, duration: 1.832s, episode steps: 124, steps per second: 68, episode reward: -166.857, mean reward: -1.346 [-100.000, 7.129], mean action: 1.597 [0.000, 3.000], mean observation: 0.075 [-4.702, 1.000], loss: 1.284479, mean_absolute_error: 17.908444, mean_q: 9.252623, mean_eps: 0.853736
  162724/2000000: episode: 1683, duration: 2.082s, episode steps: 144, steps per second: 69, episode reward: -131.148, mean reward: -0.911 [-100.000, 12.419], mean action: 1.486 [0.000, 3.000], mean observation: 0.122 [-5.165, 1.053], loss: 1.683219, mean_absolute_error: 17.501312, mean_q: 8.546884, mean_eps: 0.853615
  162789/2000000: episode: 1684, duration: 0.971s, episode steps: 65, steps per second: 67, episode reward: -141.578, mean reward: -2.178 [-100.000, 7.903], mean action: 1.585 [0.000, 3.000], mean observation: -0.138 [-1.423, 4.790], loss: 1.873751, mean_absolute_error: 19.538831, mean_q: 7.181616, mean_eps: 0.853520
  162925/2000000: episode: 1685, duration: 1.949s, episode steps: 136, steps per second: 70, episode reward: -134.867, mean reward: -0.992 [-100.000, 15.724], mean action: 1.603 [0.000, 3.000], mean observation: 0.077 [-4.644, 1.000], loss: 1.913477, mean_absolute_error: 18.338746, mean_q: 7.674260, mean_eps: 0.853428
  163026/2000000: episode: 1686, duration: 1.432s, episode steps: 101, steps per second: 71, episode reward: -175.189, mean reward: -1.735 [-100.000, 9.806], mean action: 1.762 [0.000, 3.000], mean observation: -0.109 [-1.225, 3.400], loss: 1.590030, mean_absolute_error: 17.387780, mean_q: 9.515419, mean_eps: 0.853322
  163088/2000000: episode: 1687, duration: 0.905s, episode steps: 62, steps per second: 69, episode reward: -103.824, mean reward: -1.675 [-100.000, 14.624], mean action: 1.419 [0.000, 3.000], mean observation: -0.006 [-1.481, 1.000], loss: 3.078690, mean_absolute_error: 19.796376, mean_q: 4.812338, mean_eps: 0.853250
  163212/2000000: episode: 1688, duration: 1.798s, episode steps: 124, steps per second: 69, episode reward: -232.228, mean reward: -1.873 [-100.000, 1.341], mean action: 1.387 [0.000, 3.000], mean observation: 0.181 [-1.406, 1.063], loss: 1.750515, mean_absolute_error: 17.832122, mean_q: 8.948540, mean_eps: 0.853167
  163277/2000000: episode: 1689, duration: 0.968s, episode steps: 65, steps per second: 67, episode reward: -122.227, mean reward: -1.880 [-100.000, 8.218], mean action: 1.677 [0.000, 3.000], mean observation: 0.013 [-1.382, 4.499], loss: 1.770463, mean_absolute_error: 18.573134, mean_q: 9.754679, mean_eps: 0.853080
  163384/2000000: episode: 1690, duration: 1.526s, episode steps: 107, steps per second: 70, episode reward: -293.190, mean reward: -2.740 [-100.000, 6.127], mean action: 1.439 [0.000, 3.000], mean observation: 0.020 [-1.597, 1.172], loss: 1.919853, mean_absolute_error: 18.946537, mean_q: 6.569272, mean_eps: 0.853003
  163467/2000000: episode: 1691, duration: 1.188s, episode steps: 83, steps per second: 70, episode reward: -172.038, mean reward: -2.073 [-100.000, 17.077], mean action: 1.446 [0.000, 3.000], mean observation: 0.076 [-3.667, 1.000], loss: 2.579422, mean_absolute_error: 19.675721, mean_q: 8.411717, mean_eps: 0.852918
  163554/2000000: episode: 1692, duration: 1.233s, episode steps: 87, steps per second: 71, episode reward: -175.182, mean reward: -2.014 [-100.000, 11.502], mean action: 1.563 [0.000, 3.000], mean observation: 0.045 [-3.428, 1.000], loss: 1.827848, mean_absolute_error: 18.969405, mean_q: 8.261410, mean_eps: 0.852841
  163644/2000000: episode: 1693, duration: 1.307s, episode steps: 90, steps per second: 69, episode reward: -112.010, mean reward: -1.245 [-100.000, 11.173], mean action: 1.467 [0.000, 3.000], mean observation: 0.004 [-4.117, 1.000], loss: 1.194820, mean_absolute_error: 17.081068, mean_q: 9.661928, mean_eps: 0.852762
  163731/2000000: episode: 1694, duration: 1.261s, episode steps: 87, steps per second: 69, episode reward: -112.940, mean reward: -1.298 [-100.000, 104.669], mean action: 1.391 [0.000, 3.000], mean observation: 0.045 [-1.553, 1.674], loss: 1.536822, mean_absolute_error: 17.908230, mean_q: 8.699834, mean_eps: 0.852683
  163847/2000000: episode: 1695, duration: 1.749s, episode steps: 116, steps per second: 66, episode reward: -194.686, mean reward: -1.678 [-100.000, 12.242], mean action: 1.569 [0.000, 3.000], mean observation: 0.007 [-3.965, 1.000], loss: 1.869920, mean_absolute_error: 18.782306, mean_q: 8.706955, mean_eps: 0.852591
  163927/2000000: episode: 1696, duration: 1.163s, episode steps: 80, steps per second: 69, episode reward: -188.077, mean reward: -2.351 [-100.000, 9.008], mean action: 1.788 [0.000, 3.000], mean observation: -0.136 [-1.384, 3.813], loss: 1.411021, mean_absolute_error: 18.449022, mean_q: 8.556395, mean_eps: 0.852503
  164012/2000000: episode: 1697, duration: 1.227s, episode steps: 85, steps per second: 69, episode reward: -144.940, mean reward: -1.705 [-100.000, 9.576], mean action: 1.506 [0.000, 3.000], mean observation: -0.100 [-1.283, 3.902], loss: 1.696627, mean_absolute_error: 18.211568, mean_q: 6.048454, mean_eps: 0.852429
  164074/2000000: episode: 1698, duration: 0.898s, episode steps: 62, steps per second: 69, episode reward: -137.414, mean reward: -2.216 [-100.000, 7.168], mean action: 1.742 [0.000, 3.000], mean observation: -0.138 [-4.989, 1.000], loss: 1.470530, mean_absolute_error: 16.412429, mean_q: 10.249392, mean_eps: 0.852362
  164178/2000000: episode: 1699, duration: 1.469s, episode steps: 104, steps per second: 71, episode reward: -127.876, mean reward: -1.230 [-100.000, 22.693], mean action: 1.673 [0.000, 3.000], mean observation: -0.089 [-1.095, 3.891], loss: 1.135428, mean_absolute_error: 17.913345, mean_q: 7.874673, mean_eps: 0.852287
  164304/2000000: episode: 1700, duration: 1.813s, episode steps: 126, steps per second: 70, episode reward: -187.603, mean reward: -1.489 [-100.000, 21.352], mean action: 1.500 [0.000, 3.000], mean observation: 0.163 [-1.479, 1.009], loss: 1.808941, mean_absolute_error: 18.138084, mean_q: 8.954588, mean_eps: 0.852184
  164391/2000000: episode: 1701, duration: 1.243s, episode steps: 87, steps per second: 70, episode reward: -159.528, mean reward: -1.834 [-100.000, 6.515], mean action: 1.448 [0.000, 3.000], mean observation: 0.004 [-5.009, 1.000], loss: 1.620110, mean_absolute_error: 17.537577, mean_q: 9.544238, mean_eps: 0.852089
  164526/2000000: episode: 1702, duration: 1.905s, episode steps: 135, steps per second: 71, episode reward: -152.337, mean reward: -1.128 [-100.000, 9.638], mean action: 1.526 [0.000, 3.000], mean observation: 0.035 [-4.176, 1.000], loss: 1.888281, mean_absolute_error: 18.314790, mean_q: 7.957878, mean_eps: 0.851988
  164656/2000000: episode: 1703, duration: 1.870s, episode steps: 130, steps per second: 70, episode reward: -120.130, mean reward: -0.924 [-100.000, 15.051], mean action: 1.569 [0.000, 3.000], mean observation: 0.075 [-3.593, 1.000], loss: 2.146592, mean_absolute_error: 17.930009, mean_q: 9.224289, mean_eps: 0.851869
  164795/2000000: episode: 1704, duration: 1.944s, episode steps: 139, steps per second: 71, episode reward: -95.276, mean reward: -0.685 [-100.000, 17.978], mean action: 1.655 [0.000, 3.000], mean observation: 0.042 [-1.009, 1.000], loss: 1.386344, mean_absolute_error: 17.376202, mean_q: 9.705369, mean_eps: 0.851748
  164861/2000000: episode: 1705, duration: 0.950s, episode steps: 66, steps per second: 69, episode reward: -104.273, mean reward: -1.580 [-100.000, 16.856], mean action: 1.621 [0.000, 3.000], mean observation: -0.085 [-1.271, 1.000], loss: 2.551361, mean_absolute_error: 18.694497, mean_q: 6.535819, mean_eps: 0.851655
  164958/2000000: episode: 1706, duration: 1.356s, episode steps: 97, steps per second: 72, episode reward: -168.068, mean reward: -1.733 [-100.000, 6.563], mean action: 1.392 [0.000, 3.000], mean observation: 0.127 [-3.651, 1.034], loss: 1.831713, mean_absolute_error: 17.656281, mean_q: 9.722575, mean_eps: 0.851581
  165060/2000000: episode: 1707, duration: 1.482s, episode steps: 102, steps per second: 69, episode reward: -125.682, mean reward: -1.232 [-100.000, 19.252], mean action: 1.627 [0.000, 3.000], mean observation: 0.051 [-1.097, 1.000], loss: 1.339867, mean_absolute_error: 17.864340, mean_q: 8.420857, mean_eps: 0.851493
  165140/2000000: episode: 1708, duration: 1.167s, episode steps: 80, steps per second: 69, episode reward: -132.695, mean reward: -1.659 [-100.000, 12.355], mean action: 1.663 [0.000, 3.000], mean observation: -0.176 [-1.384, 1.000], loss: 1.807608, mean_absolute_error: 17.813521, mean_q: 8.564233, mean_eps: 0.851412
  165219/2000000: episode: 1709, duration: 1.131s, episode steps: 79, steps per second: 70, episode reward: -149.222, mean reward: -1.889 [-100.000, 11.830], mean action: 1.430 [0.000, 3.000], mean observation: 0.084 [-1.300, 1.000], loss: 1.041303, mean_absolute_error: 16.653995, mean_q: 10.186989, mean_eps: 0.851340
  165336/2000000: episode: 1710, duration: 1.703s, episode steps: 117, steps per second: 69, episode reward: -140.114, mean reward: -1.198 [-100.000, 7.204], mean action: 1.632 [0.000, 3.000], mean observation: -0.078 [-1.235, 4.335], loss: 1.827204, mean_absolute_error: 17.713009, mean_q: 7.843165, mean_eps: 0.851252
  165405/2000000: episode: 1711, duration: 1.020s, episode steps: 69, steps per second: 68, episode reward: -101.777, mean reward: -1.475 [-100.000, 10.946], mean action: 1.594 [0.000, 3.000], mean observation: -0.100 [-1.279, 1.000], loss: 1.362364, mean_absolute_error: 18.684143, mean_q: 9.096960, mean_eps: 0.851167
  165563/2000000: episode: 1712, duration: 2.164s, episode steps: 158, steps per second: 73, episode reward: -193.733, mean reward: -1.226 [-100.000, 10.131], mean action: 1.658 [0.000, 3.000], mean observation: 0.075 [-1.398, 5.005], loss: 1.434096, mean_absolute_error: 17.557951, mean_q: 9.579467, mean_eps: 0.851064
  165660/2000000: episode: 1713, duration: 1.379s, episode steps: 97, steps per second: 70, episode reward: -298.486, mean reward: -3.077 [-100.000, 0.426], mean action: 1.485 [0.000, 3.000], mean observation: 0.143 [-1.290, 1.615], loss: 2.181527, mean_absolute_error: 18.387905, mean_q: 7.397486, mean_eps: 0.850951
  165779/2000000: episode: 1714, duration: 1.768s, episode steps: 119, steps per second: 67, episode reward: -264.157, mean reward: -2.220 [-100.000, 27.355], mean action: 1.597 [0.000, 3.000], mean observation: -0.086 [-3.495, 1.000], loss: 2.066458, mean_absolute_error: 18.314045, mean_q: 8.255617, mean_eps: 0.850854
  165859/2000000: episode: 1715, duration: 1.157s, episode steps: 80, steps per second: 69, episode reward: -140.934, mean reward: -1.762 [-100.000, 13.115], mean action: 1.625 [0.000, 3.000], mean observation: -0.133 [-1.175, 2.832], loss: 1.602400, mean_absolute_error: 16.700397, mean_q: 10.145546, mean_eps: 0.850764
  165935/2000000: episode: 1716, duration: 1.063s, episode steps: 76, steps per second: 71, episode reward: -119.967, mean reward: -1.579 [-100.000, 6.881], mean action: 1.368 [0.000, 3.000], mean observation: 0.027 [-4.218, 1.000], loss: 1.765423, mean_absolute_error: 18.475188, mean_q: 5.931911, mean_eps: 0.850694
  166008/2000000: episode: 1717, duration: 1.058s, episode steps: 73, steps per second: 69, episode reward: -169.283, mean reward: -2.319 [-100.000, 8.077], mean action: 1.767 [0.000, 3.000], mean observation: -0.098 [-4.839, 1.000], loss: 1.276141, mean_absolute_error: 16.315388, mean_q: 9.218986, mean_eps: 0.850627
  166099/2000000: episode: 1718, duration: 1.299s, episode steps: 91, steps per second: 70, episode reward: -150.930, mean reward: -1.659 [-100.000, 10.107], mean action: 1.747 [0.000, 3.000], mean observation: -0.058 [-4.639, 1.000], loss: 1.602966, mean_absolute_error: 17.335212, mean_q: 7.937224, mean_eps: 0.850553
  166178/2000000: episode: 1719, duration: 1.118s, episode steps: 79, steps per second: 71, episode reward: -134.560, mean reward: -1.703 [-100.000, 7.468], mean action: 1.557 [0.000, 3.000], mean observation: 0.059 [-1.127, 3.775], loss: 1.507399, mean_absolute_error: 17.857581, mean_q: 8.654702, mean_eps: 0.850476
  166278/2000000: episode: 1720, duration: 1.409s, episode steps: 100, steps per second: 71, episode reward: -171.182, mean reward: -1.712 [-100.000, 14.249], mean action: 1.660 [0.000, 3.000], mean observation: -0.040 [-1.290, 4.036], loss: 2.134298, mean_absolute_error: 18.159153, mean_q: 8.033927, mean_eps: 0.850395
  166365/2000000: episode: 1721, duration: 1.267s, episode steps: 87, steps per second: 69, episode reward: -142.087, mean reward: -1.633 [-100.000, 18.824], mean action: 1.414 [0.000, 3.000], mean observation: 0.098 [-3.707, 1.000], loss: 1.635208, mean_absolute_error: 18.154826, mean_q: 8.869592, mean_eps: 0.850310
  166456/2000000: episode: 1722, duration: 1.317s, episode steps: 91, steps per second: 69, episode reward: -91.313, mean reward: -1.003 [-100.000, 17.508], mean action: 1.637 [0.000, 3.000], mean observation: 0.104 [-1.038, 3.324], loss: 1.411999, mean_absolute_error: 17.049084, mean_q: 10.208022, mean_eps: 0.850231
  166537/2000000: episode: 1723, duration: 1.180s, episode steps: 81, steps per second: 69, episode reward: -160.370, mean reward: -1.980 [-100.000, 20.063], mean action: 1.506 [0.000, 3.000], mean observation: 0.034 [-1.397, 5.782], loss: 1.831639, mean_absolute_error: 18.543310, mean_q: 9.375672, mean_eps: 0.850154
  166631/2000000: episode: 1724, duration: 1.305s, episode steps: 94, steps per second: 72, episode reward: -228.715, mean reward: -2.433 [-100.000, 8.226], mean action: 1.553 [0.000, 3.000], mean observation: 0.116 [-1.484, 4.828], loss: 1.824228, mean_absolute_error: 19.584883, mean_q: 5.685277, mean_eps: 0.850074
  166759/2000000: episode: 1725, duration: 1.821s, episode steps: 128, steps per second: 70, episode reward: -171.346, mean reward: -1.339 [-100.000, 8.854], mean action: 1.469 [0.000, 3.000], mean observation: -0.090 [-1.165, 4.018], loss: 1.434329, mean_absolute_error: 17.458558, mean_q: 9.075438, mean_eps: 0.849975
  166847/2000000: episode: 1726, duration: 1.253s, episode steps: 88, steps per second: 70, episode reward: -132.227, mean reward: -1.503 [-100.000, 10.745], mean action: 1.659 [0.000, 3.000], mean observation: -0.027 [-1.176, 3.854], loss: 1.762957, mean_absolute_error: 18.066857, mean_q: 8.930382, mean_eps: 0.849878
  166927/2000000: episode: 1727, duration: 1.142s, episode steps: 80, steps per second: 70, episode reward: -86.273, mean reward: -1.078 [-100.000, 16.148], mean action: 1.488 [0.000, 3.000], mean observation: -0.070 [-3.757, 1.000], loss: 2.025173, mean_absolute_error: 18.360532, mean_q: 6.372347, mean_eps: 0.849803
  167056/2000000: episode: 1728, duration: 1.828s, episode steps: 129, steps per second: 71, episode reward: -280.482, mean reward: -2.174 [-100.000, 33.542], mean action: 1.465 [0.000, 3.000], mean observation: 0.247 [-1.139, 3.103], loss: 2.161958, mean_absolute_error: 18.436869, mean_q: 7.662840, mean_eps: 0.849709
  167177/2000000: episode: 1729, duration: 1.757s, episode steps: 121, steps per second: 69, episode reward: -232.506, mean reward: -1.922 [-100.000, 6.006], mean action: 1.570 [0.000, 3.000], mean observation: -0.143 [-1.489, 4.772], loss: 1.472356, mean_absolute_error: 16.916538, mean_q: 9.947487, mean_eps: 0.849596
  167322/2000000: episode: 1730, duration: 2.052s, episode steps: 145, steps per second: 71, episode reward: -145.309, mean reward: -1.002 [-100.000, 6.331], mean action: 1.524 [0.000, 3.000], mean observation: -0.015 [-1.175, 3.755], loss: 1.573762, mean_absolute_error: 17.800242, mean_q: 9.649612, mean_eps: 0.849475
  167465/2000000: episode: 1731, duration: 2.047s, episode steps: 143, steps per second: 70, episode reward: -164.965, mean reward: -1.154 [-100.000, 15.599], mean action: 1.531 [0.000, 3.000], mean observation: 0.030 [-3.272, 1.039], loss: 1.380258, mean_absolute_error: 17.571303, mean_q: 9.271609, mean_eps: 0.849345
  167563/2000000: episode: 1732, duration: 1.525s, episode steps: 98, steps per second: 64, episode reward: -175.809, mean reward: -1.794 [-100.000, 8.205], mean action: 1.622 [0.000, 3.000], mean observation: -0.015 [-3.704, 1.000], loss: 2.207350, mean_absolute_error: 19.293846, mean_q: 5.610096, mean_eps: 0.849237
  167622/2000000: episode: 1733, duration: 0.851s, episode steps: 59, steps per second: 69, episode reward: -108.652, mean reward: -1.842 [-100.000, 20.052], mean action: 1.746 [0.000, 3.000], mean observation: -0.147 [-1.510, 2.376], loss: 1.659136, mean_absolute_error: 18.746674, mean_q: 7.484149, mean_eps: 0.849167
  167722/2000000: episode: 1734, duration: 1.418s, episode steps: 100, steps per second: 71, episode reward: -152.456, mean reward: -1.525 [-100.000, 13.182], mean action: 1.550 [0.000, 3.000], mean observation: 0.074 [-1.330, 1.000], loss: 1.277126, mean_absolute_error: 17.108469, mean_q: 8.872293, mean_eps: 0.849095
  167805/2000000: episode: 1735, duration: 1.198s, episode steps: 83, steps per second: 69, episode reward: -95.399, mean reward: -1.149 [-100.000, 8.032], mean action: 1.602 [0.000, 3.000], mean observation: 0.039 [-2.792, 1.000], loss: 1.975975, mean_absolute_error: 17.723737, mean_q: 8.791248, mean_eps: 0.849012
  167916/2000000: episode: 1736, duration: 1.599s, episode steps: 111, steps per second: 69, episode reward: -361.933, mean reward: -3.261 [-100.000, 80.338], mean action: 1.378 [0.000, 3.000], mean observation: 0.173 [-1.242, 3.407], loss: 1.524029, mean_absolute_error: 18.145383, mean_q: 7.755975, mean_eps: 0.848926
  167989/2000000: episode: 1737, duration: 1.075s, episode steps: 73, steps per second: 68, episode reward: -107.734, mean reward: -1.476 [-100.000, 13.709], mean action: 1.685 [0.000, 3.000], mean observation: -0.114 [-1.344, 3.866], loss: 2.778635, mean_absolute_error: 19.076891, mean_q: 7.153203, mean_eps: 0.848843
  168070/2000000: episode: 1738, duration: 1.143s, episode steps: 81, steps per second: 71, episode reward: -104.907, mean reward: -1.295 [-100.000, 16.223], mean action: 1.444 [0.000, 3.000], mean observation: -0.062 [-1.247, 1.000], loss: 1.083413, mean_absolute_error: 16.330637, mean_q: 8.061042, mean_eps: 0.848773
  168138/2000000: episode: 1739, duration: 1.034s, episode steps: 68, steps per second: 66, episode reward: -152.581, mean reward: -2.244 [-100.000, 5.576], mean action: 1.485 [0.000, 3.000], mean observation: 0.088 [-5.488, 1.000], loss: 1.676504, mean_absolute_error: 18.799459, mean_q: 6.857768, mean_eps: 0.848706
  168234/2000000: episode: 1740, duration: 1.376s, episode steps: 96, steps per second: 70, episode reward: -151.769, mean reward: -1.581 [-100.000, 6.738], mean action: 1.500 [0.000, 3.000], mean observation: 0.031 [-4.770, 1.000], loss: 1.949527, mean_absolute_error: 18.196342, mean_q: 8.470600, mean_eps: 0.848633
  168337/2000000: episode: 1741, duration: 1.481s, episode steps: 103, steps per second: 70, episode reward: -110.224, mean reward: -1.070 [-100.000, 17.116], mean action: 1.553 [0.000, 3.000], mean observation: 0.099 [-1.126, 1.000], loss: 1.552750, mean_absolute_error: 17.727754, mean_q: 7.824945, mean_eps: 0.848543
  168453/2000000: episode: 1742, duration: 1.687s, episode steps: 116, steps per second: 69, episode reward: -116.702, mean reward: -1.006 [-100.000, 10.224], mean action: 1.578 [0.000, 3.000], mean observation: -0.072 [-3.737, 1.000], loss: 1.691508, mean_absolute_error: 17.796550, mean_q: 6.956331, mean_eps: 0.848444
  168578/2000000: episode: 1743, duration: 1.784s, episode steps: 125, steps per second: 70, episode reward: -68.281, mean reward: -0.546 [-100.000, 79.240], mean action: 1.704 [0.000, 3.000], mean observation: 0.172 [-1.585, 1.000], loss: 1.480622, mean_absolute_error: 17.157511, mean_q: 10.251635, mean_eps: 0.848336
  168683/2000000: episode: 1744, duration: 1.507s, episode steps: 105, steps per second: 70, episode reward: -204.185, mean reward: -1.945 [-100.000, 8.061], mean action: 1.705 [0.000, 3.000], mean observation: -0.129 [-5.142, 1.000], loss: 1.530425, mean_absolute_error: 17.918858, mean_q: 8.295488, mean_eps: 0.848233
  168784/2000000: episode: 1745, duration: 1.470s, episode steps: 101, steps per second: 69, episode reward: -147.579, mean reward: -1.461 [-100.000, 10.575], mean action: 1.733 [0.000, 3.000], mean observation: -0.055 [-1.404, 5.104], loss: 1.649184, mean_absolute_error: 17.934754, mean_q: 7.800884, mean_eps: 0.848141
  168903/2000000: episode: 1746, duration: 1.731s, episode steps: 119, steps per second: 69, episode reward: -156.205, mean reward: -1.313 [-100.000, 7.132], mean action: 1.588 [0.000, 3.000], mean observation: 0.019 [-1.268, 4.527], loss: 1.785479, mean_absolute_error: 18.103263, mean_q: 8.217250, mean_eps: 0.848042
  168971/2000000: episode: 1747, duration: 0.998s, episode steps: 68, steps per second: 68, episode reward: -136.379, mean reward: -2.006 [-100.000, 7.357], mean action: 1.603 [0.000, 3.000], mean observation: -0.093 [-4.759, 1.000], loss: 1.264289, mean_absolute_error: 16.053345, mean_q: 10.344118, mean_eps: 0.847958
  169059/2000000: episode: 1748, duration: 1.264s, episode steps: 88, steps per second: 70, episode reward: -270.127, mean reward: -3.070 [-100.000, 41.955], mean action: 1.659 [0.000, 3.000], mean observation: -0.107 [-1.440, 4.515], loss: 1.385691, mean_absolute_error: 17.025173, mean_q: 12.140845, mean_eps: 0.847887
  169127/2000000: episode: 1749, duration: 0.973s, episode steps: 68, steps per second: 70, episode reward: -88.644, mean reward: -1.304 [-100.000, 6.670], mean action: 1.632 [0.000, 3.000], mean observation: -0.070 [-1.208, 4.252], loss: 1.431020, mean_absolute_error: 18.153460, mean_q: 9.114508, mean_eps: 0.847817
  169200/2000000: episode: 1750, duration: 1.066s, episode steps: 73, steps per second: 68, episode reward: -128.635, mean reward: -1.762 [-100.000, 12.464], mean action: 1.507 [0.000, 3.000], mean observation: 0.077 [-1.444, 5.118], loss: 1.212235, mean_absolute_error: 17.415003, mean_q: 10.431822, mean_eps: 0.847754
  169313/2000000: episode: 1751, duration: 1.685s, episode steps: 113, steps per second: 67, episode reward: -169.207, mean reward: -1.497 [-100.000, 54.263], mean action: 1.637 [0.000, 3.000], mean observation: -0.141 [-2.296, 1.000], loss: 1.660976, mean_absolute_error: 17.723331, mean_q: 8.164244, mean_eps: 0.847670
  169411/2000000: episode: 1752, duration: 1.338s, episode steps: 98, steps per second: 73, episode reward: -159.096, mean reward: -1.623 [-100.000, 7.857], mean action: 1.653 [0.000, 3.000], mean observation: -0.085 [-5.066, 1.000], loss: 1.707692, mean_absolute_error: 17.898540, mean_q: 7.518693, mean_eps: 0.847574
  169511/2000000: episode: 1753, duration: 1.401s, episode steps: 100, steps per second: 71, episode reward: -153.360, mean reward: -1.534 [-100.000, 6.181], mean action: 1.530 [0.000, 3.000], mean observation: 0.084 [-5.062, 1.000], loss: 1.339425, mean_absolute_error: 16.759307, mean_q: 8.130159, mean_eps: 0.847486
  169574/2000000: episode: 1754, duration: 0.889s, episode steps: 63, steps per second: 71, episode reward: -114.414, mean reward: -1.816 [-100.000, 63.789], mean action: 1.524 [0.000, 3.000], mean observation: 0.044 [-1.441, 3.977], loss: 1.525365, mean_absolute_error: 18.644664, mean_q: 7.113606, mean_eps: 0.847412
  169641/2000000: episode: 1755, duration: 0.965s, episode steps: 67, steps per second: 69, episode reward: -93.279, mean reward: -1.392 [-100.000, 56.424], mean action: 1.851 [0.000, 3.000], mean observation: -0.123 [-1.467, 4.232], loss: 1.672203, mean_absolute_error: 17.709708, mean_q: 7.393895, mean_eps: 0.847353
  169755/2000000: episode: 1756, duration: 1.567s, episode steps: 114, steps per second: 73, episode reward: -164.532, mean reward: -1.443 [-100.000, 14.791], mean action: 1.623 [0.000, 3.000], mean observation: 0.111 [-4.487, 1.000], loss: 1.540819, mean_absolute_error: 17.157799, mean_q: 9.623768, mean_eps: 0.847272
  169843/2000000: episode: 1757, duration: 1.221s, episode steps: 88, steps per second: 72, episode reward: -142.657, mean reward: -1.621 [-100.000, 6.428], mean action: 1.420 [0.000, 3.000], mean observation: -0.021 [-4.214, 1.000], loss: 1.914224, mean_absolute_error: 18.163435, mean_q: 8.751566, mean_eps: 0.847182
  169967/2000000: episode: 1758, duration: 1.759s, episode steps: 124, steps per second: 70, episode reward: -188.725, mean reward: -1.522 [-100.000, 32.814], mean action: 1.605 [0.000, 3.000], mean observation: 0.124 [-1.364, 5.189], loss: 2.164267, mean_absolute_error: 19.191442, mean_q: 5.464547, mean_eps: 0.847086
  170066/2000000: episode: 1759, duration: 1.420s, episode steps: 99, steps per second: 70, episode reward: -141.998, mean reward: -1.434 [-100.000, 13.964], mean action: 1.505 [0.000, 3.000], mean observation: 0.102 [-2.666, 1.000], loss: 1.686275, mean_absolute_error: 18.272276, mean_q: 10.348665, mean_eps: 0.846986
  170135/2000000: episode: 1760, duration: 0.989s, episode steps: 69, steps per second: 70, episode reward: -134.672, mean reward: -1.952 [-100.000, 8.676], mean action: 1.420 [0.000, 3.000], mean observation: 0.009 [-1.580, 1.000], loss: 1.583534, mean_absolute_error: 18.856213, mean_q: 7.196519, mean_eps: 0.846910
  170274/2000000: episode: 1761, duration: 2.082s, episode steps: 139, steps per second: 67, episode reward: -126.852, mean reward: -0.913 [-100.000, 7.407], mean action: 1.640 [0.000, 3.000], mean observation: 0.171 [-0.968, 3.299], loss: 1.377377, mean_absolute_error: 18.259386, mean_q: 8.465831, mean_eps: 0.846816
  170401/2000000: episode: 1762, duration: 1.847s, episode steps: 127, steps per second: 69, episode reward: -97.289, mean reward: -0.766 [-100.000, 12.477], mean action: 1.606 [0.000, 3.000], mean observation: -0.003 [-1.159, 1.000], loss: 2.000868, mean_absolute_error: 18.763081, mean_q: 8.361840, mean_eps: 0.846696
  170487/2000000: episode: 1763, duration: 1.215s, episode steps: 86, steps per second: 71, episode reward: -157.132, mean reward: -1.827 [-100.000, 8.976], mean action: 1.547 [0.000, 3.000], mean observation: 0.095 [-4.485, 1.030], loss: 1.241997, mean_absolute_error: 17.835527, mean_q: 10.332155, mean_eps: 0.846600
  170580/2000000: episode: 1764, duration: 1.338s, episode steps: 93, steps per second: 69, episode reward: -104.841, mean reward: -1.127 [-100.000, 13.398], mean action: 1.785 [0.000, 3.000], mean observation: -0.050 [-1.170, 1.000], loss: 1.073113, mean_absolute_error: 18.413859, mean_q: 9.243817, mean_eps: 0.846521
  170685/2000000: episode: 1765, duration: 1.509s, episode steps: 105, steps per second: 70, episode reward: -142.272, mean reward: -1.355 [-100.000, 8.184], mean action: 1.724 [0.000, 3.000], mean observation: -0.081 [-1.152, 1.205], loss: 1.880411, mean_absolute_error: 17.976741, mean_q: 9.056810, mean_eps: 0.846431
  170779/2000000: episode: 1766, duration: 1.318s, episode steps: 94, steps per second: 71, episode reward: -102.034, mean reward: -1.085 [-100.000, 9.909], mean action: 1.638 [0.000, 3.000], mean observation: 0.072 [-0.993, 3.379], loss: 2.017815, mean_absolute_error: 19.148670, mean_q: 7.930659, mean_eps: 0.846341
  170857/2000000: episode: 1767, duration: 1.130s, episode steps: 78, steps per second: 69, episode reward: -90.459, mean reward: -1.160 [-100.000, 16.459], mean action: 1.487 [0.000, 3.000], mean observation: -0.001 [-1.189, 1.000], loss: 1.870615, mean_absolute_error: 18.923446, mean_q: 8.137070, mean_eps: 0.846264
  170953/2000000: episode: 1768, duration: 1.358s, episode steps: 96, steps per second: 71, episode reward: -154.428, mean reward: -1.609 [-100.000, 8.633], mean action: 1.417 [0.000, 3.000], mean observation: -0.005 [-4.349, 1.000], loss: 1.934639, mean_absolute_error: 19.604087, mean_q: 7.787800, mean_eps: 0.846185
  171101/2000000: episode: 1769, duration: 2.072s, episode steps: 148, steps per second: 71, episode reward: -137.012, mean reward: -0.926 [-100.000, 6.981], mean action: 1.642 [0.000, 3.000], mean observation: 0.067 [-3.962, 1.056], loss: 1.554598, mean_absolute_error: 17.932434, mean_q: 11.678810, mean_eps: 0.846075
  171171/2000000: episode: 1770, duration: 0.975s, episode steps: 70, steps per second: 72, episode reward: -80.533, mean reward: -1.150 [-100.000, 16.729], mean action: 1.329 [0.000, 3.000], mean observation: -0.006 [-1.265, 1.000], loss: 1.511074, mean_absolute_error: 18.814257, mean_q: 9.698872, mean_eps: 0.845978
  171248/2000000: episode: 1771, duration: 1.113s, episode steps: 77, steps per second: 69, episode reward: -145.771, mean reward: -1.893 [-100.000, 6.939], mean action: 1.519 [0.000, 3.000], mean observation: -0.028 [-1.237, 4.103], loss: 1.992707, mean_absolute_error: 18.986290, mean_q: 9.271401, mean_eps: 0.845913
  171390/2000000: episode: 1772, duration: 2.029s, episode steps: 142, steps per second: 70, episode reward: -72.751, mean reward: -0.512 [-100.000, 42.551], mean action: 1.458 [0.000, 3.000], mean observation: 0.125 [-1.464, 1.643], loss: 1.411707, mean_absolute_error: 18.027273, mean_q: 9.391799, mean_eps: 0.845814
  171524/2000000: episode: 1773, duration: 2.014s, episode steps: 134, steps per second: 67, episode reward: -282.431, mean reward: -2.108 [-100.000, 3.669], mean action: 1.410 [0.000, 3.000], mean observation: 0.232 [-1.470, 1.089], loss: 1.739660, mean_absolute_error: 19.181545, mean_q: 9.375924, mean_eps: 0.845690
  171588/2000000: episode: 1774, duration: 0.999s, episode steps: 64, steps per second: 64, episode reward: -115.767, mean reward: -1.809 [-100.000, 11.571], mean action: 1.578 [0.000, 3.000], mean observation: 0.028 [-1.407, 5.001], loss: 1.431250, mean_absolute_error: 18.508049, mean_q: 10.313594, mean_eps: 0.845601
  171696/2000000: episode: 1775, duration: 1.551s, episode steps: 108, steps per second: 70, episode reward: -129.611, mean reward: -1.200 [-100.000, 19.977], mean action: 1.630 [0.000, 3.000], mean observation: 0.025 [-3.232, 1.000], loss: 1.782318, mean_absolute_error: 18.543348, mean_q: 9.231820, mean_eps: 0.845524
  171806/2000000: episode: 1776, duration: 1.559s, episode steps: 110, steps per second: 71, episode reward: -258.054, mean reward: -2.346 [-100.000, 7.496], mean action: 1.636 [0.000, 3.000], mean observation: -0.160 [-4.692, 1.000], loss: 2.036811, mean_absolute_error: 19.115640, mean_q: 7.069714, mean_eps: 0.845425
  171928/2000000: episode: 1777, duration: 1.708s, episode steps: 122, steps per second: 71, episode reward: -106.148, mean reward: -0.870 [-100.000, 12.209], mean action: 1.549 [0.000, 3.000], mean observation: 0.015 [-1.164, 1.000], loss: 1.400310, mean_absolute_error: 17.917768, mean_q: 9.650036, mean_eps: 0.845321
  172031/2000000: episode: 1778, duration: 1.481s, episode steps: 103, steps per second: 70, episode reward: -119.415, mean reward: -1.159 [-100.000, 13.480], mean action: 1.427 [0.000, 3.000], mean observation: 0.051 [-4.146, 1.000], loss: 1.664345, mean_absolute_error: 18.825199, mean_q: 9.731963, mean_eps: 0.845220
  172133/2000000: episode: 1779, duration: 1.439s, episode steps: 102, steps per second: 71, episode reward: -262.838, mean reward: -2.577 [-100.000, 30.609], mean action: 1.412 [0.000, 3.000], mean observation: 0.151 [-1.689, 6.198], loss: 1.649086, mean_absolute_error: 18.735970, mean_q: 8.674971, mean_eps: 0.845126
  172211/2000000: episode: 1780, duration: 1.088s, episode steps: 78, steps per second: 72, episode reward: -128.216, mean reward: -1.644 [-100.000, 15.837], mean action: 1.731 [0.000, 3.000], mean observation: -0.149 [-1.170, 1.000], loss: 1.158455, mean_absolute_error: 17.979092, mean_q: 9.391059, mean_eps: 0.845045
  172310/2000000: episode: 1781, duration: 1.425s, episode steps: 99, steps per second: 69, episode reward: -151.953, mean reward: -1.535 [-100.000, 9.861], mean action: 1.556 [0.000, 3.000], mean observation: 0.141 [-3.913, 1.000], loss: 1.525413, mean_absolute_error: 19.002131, mean_q: 7.855184, mean_eps: 0.844966
  172424/2000000: episode: 1782, duration: 1.605s, episode steps: 114, steps per second: 71, episode reward: -283.401, mean reward: -2.486 [-100.000, 35.356], mean action: 1.728 [0.000, 3.000], mean observation: -0.194 [-4.675, 1.013], loss: 1.452358, mean_absolute_error: 18.882722, mean_q: 9.078370, mean_eps: 0.844871
  172498/2000000: episode: 1783, duration: 1.096s, episode steps: 74, steps per second: 68, episode reward: -224.951, mean reward: -3.040 [-100.000, 26.559], mean action: 1.622 [0.000, 3.000], mean observation: -0.099 [-4.892, 1.000], loss: 1.169306, mean_absolute_error: 18.013542, mean_q: 11.564245, mean_eps: 0.844786
  172605/2000000: episode: 1784, duration: 1.540s, episode steps: 107, steps per second: 70, episode reward: -159.488, mean reward: -1.491 [-100.000, 7.485], mean action: 1.636 [0.000, 3.000], mean observation: -0.049 [-1.350, 4.520], loss: 1.469145, mean_absolute_error: 18.076106, mean_q: 8.190314, mean_eps: 0.844703
  172679/2000000: episode: 1785, duration: 1.019s, episode steps: 74, steps per second: 73, episode reward: -210.382, mean reward: -2.843 [-100.000, 31.663], mean action: 1.405 [0.000, 3.000], mean observation: 0.121 [-1.307, 3.723], loss: 1.195266, mean_absolute_error: 18.006443, mean_q: 9.623062, mean_eps: 0.844622
  172799/2000000: episode: 1786, duration: 1.701s, episode steps: 120, steps per second: 71, episode reward: -95.773, mean reward: -0.798 [-100.000, 11.898], mean action: 1.575 [0.000, 3.000], mean observation: -0.047 [-1.015, 2.926], loss: 1.210024, mean_absolute_error: 18.214856, mean_q: 8.339750, mean_eps: 0.844536
  172918/2000000: episode: 1787, duration: 1.693s, episode steps: 119, steps per second: 70, episode reward: -153.685, mean reward: -1.291 [-100.000, 19.993], mean action: 1.445 [0.000, 3.000], mean observation: 0.090 [-1.413, 1.027], loss: 1.864783, mean_absolute_error: 19.209737, mean_q: 9.960686, mean_eps: 0.844428
  173032/2000000: episode: 1788, duration: 1.625s, episode steps: 114, steps per second: 70, episode reward: -179.399, mean reward: -1.574 [-100.000, 11.214], mean action: 1.456 [0.000, 3.000], mean observation: 0.146 [-1.358, 1.012], loss: 1.144123, mean_absolute_error: 19.007810, mean_q: 10.275826, mean_eps: 0.844323
  173133/2000000: episode: 1789, duration: 1.460s, episode steps: 101, steps per second: 69, episode reward: -129.912, mean reward: -1.286 [-100.000, 5.661], mean action: 1.723 [0.000, 3.000], mean observation: -0.001 [-1.132, 4.022], loss: 1.598748, mean_absolute_error: 18.462486, mean_q: 9.717668, mean_eps: 0.844226
  173202/2000000: episode: 1790, duration: 1.022s, episode steps: 69, steps per second: 68, episode reward: -101.759, mean reward: -1.475 [-100.000, 16.696], mean action: 1.638 [0.000, 3.000], mean observation: 0.058 [-1.194, 1.000], loss: 2.469831, mean_absolute_error: 20.013204, mean_q: 7.055139, mean_eps: 0.844149
  173282/2000000: episode: 1791, duration: 1.149s, episode steps: 80, steps per second: 70, episode reward: -151.892, mean reward: -1.899 [-100.000, 5.315], mean action: 1.700 [0.000, 3.000], mean observation: -0.181 [-1.313, 4.183], loss: 1.722343, mean_absolute_error: 18.584534, mean_q: 7.412948, mean_eps: 0.844082
  173407/2000000: episode: 1792, duration: 1.764s, episode steps: 125, steps per second: 71, episode reward: -169.271, mean reward: -1.354 [-100.000, 12.562], mean action: 1.536 [0.000, 3.000], mean observation: -0.174 [-1.086, 3.489], loss: 1.681198, mean_absolute_error: 18.331806, mean_q: 9.905048, mean_eps: 0.843990
  173510/2000000: episode: 1793, duration: 1.482s, episode steps: 103, steps per second: 69, episode reward: -162.229, mean reward: -1.575 [-100.000, 10.598], mean action: 1.689 [0.000, 3.000], mean observation: -0.063 [-1.298, 3.894], loss: 1.394954, mean_absolute_error: 18.398987, mean_q: 8.994967, mean_eps: 0.843888
  173588/2000000: episode: 1794, duration: 1.141s, episode steps: 78, steps per second: 68, episode reward: -94.118, mean reward: -1.207 [-100.000, 6.906], mean action: 1.667 [0.000, 3.000], mean observation: 0.021 [-3.404, 1.000], loss: 1.881965, mean_absolute_error: 18.652494, mean_q: 8.754600, mean_eps: 0.843807
  173718/2000000: episode: 1795, duration: 1.849s, episode steps: 130, steps per second: 70, episode reward: -161.778, mean reward: -1.244 [-100.000, 5.464], mean action: 1.669 [0.000, 3.000], mean observation: -0.121 [-1.019, 4.260], loss: 1.895727, mean_absolute_error: 19.111785, mean_q: 8.452979, mean_eps: 0.843713
  173839/2000000: episode: 1796, duration: 1.703s, episode steps: 121, steps per second: 71, episode reward: -163.862, mean reward: -1.354 [-100.000, 8.087], mean action: 1.587 [0.000, 3.000], mean observation: 0.151 [-3.264, 1.000], loss: 1.706794, mean_absolute_error: 19.524923, mean_q: 7.322172, mean_eps: 0.843600
  173936/2000000: episode: 1797, duration: 1.386s, episode steps: 97, steps per second: 70, episode reward: -125.158, mean reward: -1.290 [-100.000, 5.780], mean action: 1.639 [0.000, 3.000], mean observation: 0.058 [-1.198, 1.000], loss: 1.650778, mean_absolute_error: 18.316861, mean_q: 9.337994, mean_eps: 0.843503
  174045/2000000: episode: 1798, duration: 1.589s, episode steps: 109, steps per second: 69, episode reward: -372.927, mean reward: -3.421 [-100.000, 106.823], mean action: 1.752 [0.000, 3.000], mean observation: -0.204 [-3.297, 1.000], loss: 1.771214, mean_absolute_error: 18.042706, mean_q: 9.940229, mean_eps: 0.843409
  174122/2000000: episode: 1799, duration: 1.094s, episode steps: 77, steps per second: 70, episode reward: -131.695, mean reward: -1.710 [-100.000, 4.398], mean action: 1.584 [0.000, 3.000], mean observation: 0.132 [-1.084, 3.459], loss: 1.989629, mean_absolute_error: 19.088116, mean_q: 9.754387, mean_eps: 0.843324
  174250/2000000: episode: 1800, duration: 1.814s, episode steps: 128, steps per second: 71, episode reward: -151.965, mean reward: -1.187 [-100.000, 7.510], mean action: 1.594 [0.000, 3.000], mean observation: 0.157 [-1.167, 4.452], loss: 1.180495, mean_absolute_error: 18.101291, mean_q: 9.278336, mean_eps: 0.843233
  174387/2000000: episode: 1801, duration: 1.920s, episode steps: 137, steps per second: 71, episode reward: -356.279, mean reward: -2.601 [-100.000, 88.266], mean action: 1.555 [0.000, 3.000], mean observation: 0.234 [-1.319, 3.169], loss: 1.401292, mean_absolute_error: 18.650111, mean_q: 10.202235, mean_eps: 0.843114
  174451/2000000: episode: 1802, duration: 0.921s, episode steps: 64, steps per second: 70, episode reward: -111.760, mean reward: -1.746 [-100.000, 16.106], mean action: 1.781 [0.000, 3.000], mean observation: -0.147 [-1.284, 1.000], loss: 1.678193, mean_absolute_error: 19.517310, mean_q: 8.950726, mean_eps: 0.843024
  174571/2000000: episode: 1803, duration: 1.699s, episode steps: 120, steps per second: 71, episode reward: -186.530, mean reward: -1.554 [-100.000, 7.504], mean action: 1.508 [0.000, 3.000], mean observation: 0.121 [-3.917, 1.000], loss: 1.528181, mean_absolute_error: 17.968095, mean_q: 9.983346, mean_eps: 0.842941
  174653/2000000: episode: 1804, duration: 1.181s, episode steps: 82, steps per second: 69, episode reward: -154.535, mean reward: -1.885 [-100.000, 8.188], mean action: 1.695 [0.000, 3.000], mean observation: -0.057 [-5.053, 1.000], loss: 1.319397, mean_absolute_error: 18.165189, mean_q: 10.345915, mean_eps: 0.842849
  174771/2000000: episode: 1805, duration: 1.662s, episode steps: 118, steps per second: 71, episode reward: -45.004, mean reward: -0.381 [-100.000, 61.746], mean action: 1.720 [0.000, 3.000], mean observation: -0.074 [-1.170, 2.359], loss: 1.632251, mean_absolute_error: 18.417501, mean_q: 9.046306, mean_eps: 0.842759
  174865/2000000: episode: 1806, duration: 1.376s, episode steps: 94, steps per second: 68, episode reward: -206.072, mean reward: -2.192 [-100.000, 6.653], mean action: 1.234 [0.000, 3.000], mean observation: -0.107 [-5.257, 1.000], loss: 1.835654, mean_absolute_error: 19.056094, mean_q: 10.786087, mean_eps: 0.842664
  174962/2000000: episode: 1807, duration: 1.398s, episode steps: 97, steps per second: 69, episode reward: -151.233, mean reward: -1.559 [-100.000, 17.092], mean action: 1.546 [0.000, 3.000], mean observation: -0.077 [-4.484, 1.000], loss: 1.184911, mean_absolute_error: 17.999359, mean_q: 9.143387, mean_eps: 0.842577
  175021/2000000: episode: 1808, duration: 0.855s, episode steps: 59, steps per second: 69, episode reward: -120.453, mean reward: -2.042 [-100.000, 16.554], mean action: 1.729 [0.000, 3.000], mean observation: -0.179 [-1.549, 1.000], loss: 1.296744, mean_absolute_error: 18.463629, mean_q: 8.761734, mean_eps: 0.842507
  175127/2000000: episode: 1809, duration: 1.467s, episode steps: 106, steps per second: 72, episode reward: -146.948, mean reward: -1.386 [-100.000, 12.284], mean action: 1.585 [0.000, 3.000], mean observation: 0.037 [-4.346, 1.000], loss: 1.568652, mean_absolute_error: 18.615427, mean_q: 10.748500, mean_eps: 0.842433
  175257/2000000: episode: 1810, duration: 1.883s, episode steps: 130, steps per second: 69, episode reward: -209.111, mean reward: -1.609 [-100.000, 12.113], mean action: 1.585 [0.000, 3.000], mean observation: 0.176 [-1.243, 1.000], loss: 1.933221, mean_absolute_error: 19.366847, mean_q: 8.205419, mean_eps: 0.842327
  175320/2000000: episode: 1811, duration: 0.926s, episode steps: 63, steps per second: 68, episode reward: -114.575, mean reward: -1.819 [-100.000, 13.211], mean action: 1.889 [0.000, 3.000], mean observation: -0.170 [-1.380, 1.000], loss: 1.466006, mean_absolute_error: 18.563300, mean_q: 10.561703, mean_eps: 0.842241
  175402/2000000: episode: 1812, duration: 1.177s, episode steps: 82, steps per second: 70, episode reward: -204.228, mean reward: -2.491 [-100.000, 15.807], mean action: 1.585 [0.000, 3.000], mean observation: 0.106 [-1.440, 1.338], loss: 1.136666, mean_absolute_error: 17.806228, mean_q: 11.570957, mean_eps: 0.842176
  175504/2000000: episode: 1813, duration: 1.468s, episode steps: 102, steps per second: 69, episode reward: -147.523, mean reward: -1.446 [-100.000, 6.963], mean action: 1.520 [0.000, 3.000], mean observation: -0.048 [-4.836, 1.000], loss: 1.177638, mean_absolute_error: 18.602538, mean_q: 10.099347, mean_eps: 0.842093
  175603/2000000: episode: 1814, duration: 1.424s, episode steps: 99, steps per second: 70, episode reward: -141.340, mean reward: -1.428 [-100.000, 9.975], mean action: 1.424 [0.000, 3.000], mean observation: 0.083 [-1.339, 4.772], loss: 1.627466, mean_absolute_error: 19.238608, mean_q: 7.188101, mean_eps: 0.842003
  175707/2000000: episode: 1815, duration: 1.447s, episode steps: 104, steps per second: 72, episode reward: -182.981, mean reward: -1.759 [-100.000, 87.006], mean action: 1.433 [0.000, 3.000], mean observation: 0.029 [-1.589, 1.000], loss: 1.268700, mean_absolute_error: 18.341225, mean_q: 10.716176, mean_eps: 0.841911
  175778/2000000: episode: 1816, duration: 1.032s, episode steps: 71, steps per second: 69, episode reward: -119.817, mean reward: -1.688 [-100.000, 13.038], mean action: 1.859 [0.000, 3.000], mean observation: -0.134 [-1.215, 1.000], loss: 1.685672, mean_absolute_error: 17.113568, mean_q: 10.699844, mean_eps: 0.841832
  175894/2000000: episode: 1817, duration: 1.669s, episode steps: 116, steps per second: 70, episode reward: -120.718, mean reward: -1.041 [-100.000, 14.947], mean action: 1.612 [0.000, 3.000], mean observation: -0.033 [-3.495, 1.000], loss: 1.685689, mean_absolute_error: 18.060889, mean_q: 8.846412, mean_eps: 0.841748
  175972/2000000: episode: 1818, duration: 1.132s, episode steps: 78, steps per second: 69, episode reward: -126.777, mean reward: -1.625 [-100.000, 6.605], mean action: 1.577 [0.000, 3.000], mean observation: 0.028 [-4.072, 1.000], loss: 1.616233, mean_absolute_error: 18.323649, mean_q: 8.907213, mean_eps: 0.841661
  176077/2000000: episode: 1819, duration: 1.533s, episode steps: 105, steps per second: 68, episode reward: -145.551, mean reward: -1.386 [-100.000, 6.600], mean action: 1.648 [0.000, 3.000], mean observation: -0.121 [-1.172, 4.332], loss: 1.985086, mean_absolute_error: 18.201490, mean_q: 9.432246, mean_eps: 0.841578
  176201/2000000: episode: 1820, duration: 1.769s, episode steps: 124, steps per second: 70, episode reward: -145.384, mean reward: -1.172 [-100.000, 24.736], mean action: 1.589 [0.000, 3.000], mean observation: -0.096 [-1.197, 2.840], loss: 1.921585, mean_absolute_error: 19.607628, mean_q: 8.491820, mean_eps: 0.841474
  176275/2000000: episode: 1821, duration: 1.046s, episode steps: 74, steps per second: 71, episode reward: -83.139, mean reward: -1.123 [-100.000, 123.198], mean action: 1.743 [0.000, 3.000], mean observation: -0.137 [-1.565, 1.000], loss: 1.631804, mean_absolute_error: 18.037709, mean_q: 9.754659, mean_eps: 0.841386
  176397/2000000: episode: 1822, duration: 1.749s, episode steps: 122, steps per second: 70, episode reward: -168.797, mean reward: -1.384 [-100.000, 5.899], mean action: 1.648 [0.000, 3.000], mean observation: -0.102 [-1.311, 4.606], loss: 1.311456, mean_absolute_error: 19.091375, mean_q: 9.620910, mean_eps: 0.841298
  176496/2000000: episode: 1823, duration: 1.422s, episode steps: 99, steps per second: 70, episode reward: -162.805, mean reward: -1.644 [-100.000, 16.708], mean action: 1.545 [0.000, 3.000], mean observation: 0.074 [-4.949, 1.000], loss: 2.431973, mean_absolute_error: 19.771039, mean_q: 8.015962, mean_eps: 0.841199
  176598/2000000: episode: 1824, duration: 1.459s, episode steps: 102, steps per second: 70, episode reward: -137.338, mean reward: -1.346 [-100.000, 14.303], mean action: 1.647 [0.000, 3.000], mean observation: 0.008 [-3.894, 1.000], loss: 1.835822, mean_absolute_error: 19.879998, mean_q: 7.760816, mean_eps: 0.841109
  176685/2000000: episode: 1825, duration: 1.229s, episode steps: 87, steps per second: 71, episode reward: -131.650, mean reward: -1.513 [-100.000, 12.534], mean action: 1.563 [0.000, 3.000], mean observation: -0.040 [-1.258, 4.145], loss: 1.108498, mean_absolute_error: 17.944529, mean_q: 9.785050, mean_eps: 0.841022
  176790/2000000: episode: 1826, duration: 1.487s, episode steps: 105, steps per second: 71, episode reward: -339.451, mean reward: -3.233 [-100.000, 102.158], mean action: 1.667 [0.000, 3.000], mean observation: 0.096 [-1.490, 2.491], loss: 1.326917, mean_absolute_error: 19.151761, mean_q: 8.368399, mean_eps: 0.840936
  176877/2000000: episode: 1827, duration: 1.256s, episode steps: 87, steps per second: 69, episode reward: -186.885, mean reward: -2.148 [-100.000, 41.803], mean action: 1.529 [0.000, 3.000], mean observation: 0.038 [-5.430, 1.024], loss: 1.415203, mean_absolute_error: 18.191793, mean_q: 8.489870, mean_eps: 0.840849
  176978/2000000: episode: 1828, duration: 1.450s, episode steps: 101, steps per second: 70, episode reward: -151.319, mean reward: -1.498 [-100.000, 11.180], mean action: 1.505 [0.000, 3.000], mean observation: 0.098 [-3.780, 1.000], loss: 1.659720, mean_absolute_error: 18.097371, mean_q: 9.257329, mean_eps: 0.840765
  177109/2000000: episode: 1829, duration: 1.867s, episode steps: 131, steps per second: 70, episode reward: -360.742, mean reward: -2.754 [-100.000, 3.330], mean action: 1.832 [0.000, 3.000], mean observation: -0.177 [-2.049, 1.052], loss: 1.448018, mean_absolute_error: 19.666662, mean_q: 7.544616, mean_eps: 0.840660
  177195/2000000: episode: 1830, duration: 1.194s, episode steps: 86, steps per second: 72, episode reward: -138.006, mean reward: -1.605 [-100.000, 8.278], mean action: 1.453 [0.000, 3.000], mean observation: 0.016 [-1.251, 1.000], loss: 1.913013, mean_absolute_error: 19.638414, mean_q: 8.536010, mean_eps: 0.840563
  177329/2000000: episode: 1831, duration: 1.875s, episode steps: 134, steps per second: 71, episode reward: -118.230, mean reward: -0.882 [-100.000, 9.820], mean action: 1.649 [0.000, 3.000], mean observation: -0.019 [-1.948, 1.013], loss: 1.341172, mean_absolute_error: 17.807973, mean_q: 9.994147, mean_eps: 0.840464
  177424/2000000: episode: 1832, duration: 1.342s, episode steps: 95, steps per second: 71, episode reward: -149.716, mean reward: -1.576 [-100.000, 6.499], mean action: 1.400 [0.000, 3.000], mean observation: -0.005 [-4.372, 1.000], loss: 1.630513, mean_absolute_error: 18.486554, mean_q: 10.243527, mean_eps: 0.840362
  177541/2000000: episode: 1833, duration: 1.678s, episode steps: 117, steps per second: 70, episode reward: -128.906, mean reward: -1.102 [-100.000, 8.927], mean action: 1.556 [0.000, 3.000], mean observation: 0.026 [-5.051, 1.040], loss: 1.380597, mean_absolute_error: 18.085354, mean_q: 10.485502, mean_eps: 0.840266
  177631/2000000: episode: 1834, duration: 1.257s, episode steps: 90, steps per second: 72, episode reward: -125.846, mean reward: -1.398 [-100.000, 14.251], mean action: 1.456 [0.000, 3.000], mean observation: 0.084 [-4.351, 1.000], loss: 1.354445, mean_absolute_error: 18.360408, mean_q: 8.564258, mean_eps: 0.840173
  177744/2000000: episode: 1835, duration: 1.629s, episode steps: 113, steps per second: 69, episode reward: -146.342, mean reward: -1.295 [-100.000, 16.853], mean action: 1.434 [0.000, 3.000], mean observation: 0.123 [-1.035, 1.000], loss: 1.822867, mean_absolute_error: 19.341673, mean_q: 9.005430, mean_eps: 0.840083
  177870/2000000: episode: 1836, duration: 1.807s, episode steps: 126, steps per second: 70, episode reward: -303.136, mean reward: -2.406 [-100.000, 1.989], mean action: 1.595 [0.000, 3.000], mean observation: -0.036 [-1.417, 1.065], loss: 1.815747, mean_absolute_error: 19.553690, mean_q: 7.999571, mean_eps: 0.839975
  177981/2000000: episode: 1837, duration: 1.567s, episode steps: 111, steps per second: 71, episode reward: -128.022, mean reward: -1.153 [-100.000, 14.790], mean action: 1.468 [0.000, 3.000], mean observation: 0.029 [-1.413, 1.000], loss: 1.576138, mean_absolute_error: 18.100634, mean_q: 9.946910, mean_eps: 0.839867
  178071/2000000: episode: 1838, duration: 1.240s, episode steps: 90, steps per second: 73, episode reward: -101.566, mean reward: -1.129 [-100.000, 19.750], mean action: 1.700 [0.000, 3.000], mean observation: 0.010 [-1.306, 4.037], loss: 1.279547, mean_absolute_error: 18.406615, mean_q: 10.345487, mean_eps: 0.839777
  178187/2000000: episode: 1839, duration: 1.626s, episode steps: 116, steps per second: 71, episode reward: -217.934, mean reward: -1.879 [-100.000, 2.075], mean action: 1.491 [0.000, 3.000], mean observation: 0.190 [-1.286, 1.014], loss: 1.201597, mean_absolute_error: 18.279653, mean_q: 10.665178, mean_eps: 0.839685
  178290/2000000: episode: 1840, duration: 1.450s, episode steps: 103, steps per second: 71, episode reward: -197.777, mean reward: -1.920 [-100.000, 11.494], mean action: 1.563 [0.000, 3.000], mean observation: 0.071 [-1.485, 4.830], loss: 1.520675, mean_absolute_error: 19.323237, mean_q: 9.415976, mean_eps: 0.839586
  178381/2000000: episode: 1841, duration: 1.307s, episode steps: 91, steps per second: 70, episode reward: -171.158, mean reward: -1.881 [-100.000, 6.654], mean action: 1.231 [0.000, 3.000], mean observation: 0.167 [-1.126, 1.000], loss: 1.948331, mean_absolute_error: 19.363899, mean_q: 6.587942, mean_eps: 0.839498
  178451/2000000: episode: 1842, duration: 0.958s, episode steps: 70, steps per second: 73, episode reward: -133.176, mean reward: -1.903 [-100.000, 12.254], mean action: 1.529 [0.000, 3.000], mean observation: -0.118 [-4.992, 1.000], loss: 1.814454, mean_absolute_error: 20.086434, mean_q: 7.766582, mean_eps: 0.839426
  178529/2000000: episode: 1843, duration: 1.121s, episode steps: 78, steps per second: 70, episode reward: -123.813, mean reward: -1.587 [-100.000, 13.415], mean action: 1.397 [0.000, 3.000], mean observation: -0.099 [-1.356, 4.723], loss: 1.330383, mean_absolute_error: 18.690639, mean_q: 8.029602, mean_eps: 0.839359
  178606/2000000: episode: 1844, duration: 1.070s, episode steps: 77, steps per second: 72, episode reward: -130.427, mean reward: -1.694 [-100.000, 12.141], mean action: 1.714 [0.000, 3.000], mean observation: -0.121 [-3.874, 1.000], loss: 1.919635, mean_absolute_error: 21.036367, mean_q: 5.299983, mean_eps: 0.839289
  178669/2000000: episode: 1845, duration: 1.006s, episode steps: 63, steps per second: 63, episode reward: -112.775, mean reward: -1.790 [-100.000, 7.563], mean action: 1.667 [0.000, 3.000], mean observation: -0.033 [-1.422, 4.760], loss: 1.278841, mean_absolute_error: 18.083128, mean_q: 10.592953, mean_eps: 0.839226
  178764/2000000: episode: 1846, duration: 1.349s, episode steps: 95, steps per second: 70, episode reward: -133.450, mean reward: -1.405 [-100.000, 6.568], mean action: 1.621 [0.000, 3.000], mean observation: 0.075 [-3.658, 1.000], loss: 1.635320, mean_absolute_error: 18.730482, mean_q: 10.486537, mean_eps: 0.839156
  178880/2000000: episode: 1847, duration: 1.692s, episode steps: 116, steps per second: 69, episode reward: -96.115, mean reward: -0.829 [-100.000, 8.359], mean action: 1.724 [0.000, 3.000], mean observation: -0.042 [-1.126, 2.368], loss: 1.500714, mean_absolute_error: 18.994660, mean_q: 9.264401, mean_eps: 0.839062
  178984/2000000: episode: 1848, duration: 1.501s, episode steps: 104, steps per second: 69, episode reward: -76.554, mean reward: -0.736 [-100.000, 10.454], mean action: 1.596 [0.000, 3.000], mean observation: -0.010 [-0.910, 1.000], loss: 1.711484, mean_absolute_error: 19.298748, mean_q: 8.538720, mean_eps: 0.838963
  179058/2000000: episode: 1849, duration: 1.062s, episode steps: 74, steps per second: 70, episode reward: -94.638, mean reward: -1.279 [-100.000, 16.907], mean action: 1.554 [0.000, 3.000], mean observation: -0.059 [-1.259, 4.424], loss: 1.200835, mean_absolute_error: 17.423984, mean_q: 10.242397, mean_eps: 0.838882
  179143/2000000: episode: 1850, duration: 1.195s, episode steps: 85, steps per second: 71, episode reward: -118.033, mean reward: -1.389 [-100.000, 7.079], mean action: 1.624 [0.000, 3.000], mean observation: -0.075 [-1.074, 3.591], loss: 1.975732, mean_absolute_error: 19.976167, mean_q: 8.496950, mean_eps: 0.838810
  179279/2000000: episode: 1851, duration: 1.901s, episode steps: 136, steps per second: 72, episode reward: -347.849, mean reward: -2.558 [-100.000, 23.231], mean action: 1.397 [0.000, 3.000], mean observation: 0.179 [-1.309, 3.681], loss: 1.599961, mean_absolute_error: 19.575823, mean_q: 8.921276, mean_eps: 0.838711
  179371/2000000: episode: 1852, duration: 1.318s, episode steps: 92, steps per second: 70, episode reward: -124.944, mean reward: -1.358 [-100.000, 10.355], mean action: 1.641 [0.000, 3.000], mean observation: -0.121 [-1.060, 3.171], loss: 1.977019, mean_absolute_error: 19.427073, mean_q: 7.398884, mean_eps: 0.838608
  179461/2000000: episode: 1853, duration: 1.282s, episode steps: 90, steps per second: 70, episode reward: -162.370, mean reward: -1.804 [-100.000, 9.225], mean action: 1.467 [0.000, 3.000], mean observation: 0.106 [-3.976, 1.088], loss: 1.913701, mean_absolute_error: 18.748660, mean_q: 8.143449, mean_eps: 0.838526
  179549/2000000: episode: 1854, duration: 1.251s, episode steps: 88, steps per second: 70, episode reward: -134.867, mean reward: -1.533 [-100.000, 8.851], mean action: 1.432 [0.000, 3.000], mean observation: 0.048 [-2.743, 1.000], loss: 2.207744, mean_absolute_error: 18.955155, mean_q: 8.704840, mean_eps: 0.838445
  179618/2000000: episode: 1855, duration: 0.951s, episode steps: 69, steps per second: 73, episode reward: -140.009, mean reward: -2.029 [-100.000, 9.616], mean action: 1.435 [0.000, 3.000], mean observation: 0.059 [-1.368, 1.000], loss: 1.317305, mean_absolute_error: 18.054365, mean_q: 12.004565, mean_eps: 0.838374
  179711/2000000: episode: 1856, duration: 1.309s, episode steps: 93, steps per second: 71, episode reward: -112.618, mean reward: -1.211 [-100.000, 14.792], mean action: 1.430 [0.000, 3.000], mean observation: -0.123 [-1.263, 1.000], loss: 2.343461, mean_absolute_error: 19.804275, mean_q: 7.497293, mean_eps: 0.838302
  179785/2000000: episode: 1857, duration: 1.094s, episode steps: 74, steps per second: 68, episode reward: -114.205, mean reward: -1.543 [-100.000, 10.954], mean action: 1.419 [0.000, 3.000], mean observation: -0.019 [-4.791, 1.000], loss: 1.891693, mean_absolute_error: 19.325827, mean_q: 8.672840, mean_eps: 0.838227
  179903/2000000: episode: 1858, duration: 1.624s, episode steps: 118, steps per second: 73, episode reward: -106.455, mean reward: -0.902 [-100.000, 20.793], mean action: 1.627 [0.000, 3.000], mean observation: 0.023 [-1.256, 1.014], loss: 1.222728, mean_absolute_error: 18.388042, mean_q: 11.039017, mean_eps: 0.838140
  180009/2000000: episode: 1859, duration: 1.510s, episode steps: 106, steps per second: 70, episode reward: -131.404, mean reward: -1.240 [-100.000, 9.341], mean action: 1.509 [0.000, 3.000], mean observation: 0.028 [-1.399, 5.045], loss: 1.228313, mean_absolute_error: 18.344062, mean_q: 9.867583, mean_eps: 0.838040
  180073/2000000: episode: 1860, duration: 0.913s, episode steps: 64, steps per second: 70, episode reward: -125.972, mean reward: -1.968 [-100.000, 12.180], mean action: 1.734 [0.000, 3.000], mean observation: 0.069 [-1.425, 5.309], loss: 2.090661, mean_absolute_error: 20.466149, mean_q: 9.207598, mean_eps: 0.837962
  180183/2000000: episode: 1861, duration: 1.526s, episode steps: 110, steps per second: 72, episode reward: -136.761, mean reward: -1.243 [-100.000, 8.303], mean action: 1.518 [0.000, 3.000], mean observation: 0.011 [-3.837, 1.000], loss: 2.158418, mean_absolute_error: 20.917061, mean_q: 6.873344, mean_eps: 0.837885
  180285/2000000: episode: 1862, duration: 1.458s, episode steps: 102, steps per second: 70, episode reward: -121.789, mean reward: -1.194 [-100.000, 12.496], mean action: 1.588 [0.000, 3.000], mean observation: 0.089 [-3.510, 1.000], loss: 2.114685, mean_absolute_error: 19.241794, mean_q: 10.376473, mean_eps: 0.837789
  180344/2000000: episode: 1863, duration: 0.856s, episode steps: 59, steps per second: 69, episode reward: -133.648, mean reward: -2.265 [-100.000, 7.135], mean action: 1.441 [0.000, 3.000], mean observation: -0.036 [-4.749, 1.000], loss: 1.648779, mean_absolute_error: 19.373311, mean_q: 10.571733, mean_eps: 0.837717
  180429/2000000: episode: 1864, duration: 1.310s, episode steps: 85, steps per second: 65, episode reward: -81.104, mean reward: -0.954 [-100.000, 15.812], mean action: 1.624 [0.000, 3.000], mean observation: -0.006 [-1.144, 1.000], loss: 2.458048, mean_absolute_error: 22.010295, mean_q: 7.165921, mean_eps: 0.837653
  180562/2000000: episode: 1865, duration: 1.944s, episode steps: 133, steps per second: 68, episode reward: -211.678, mean reward: -1.592 [-100.000, 34.276], mean action: 1.602 [0.000, 3.000], mean observation: -0.154 [-5.363, 1.036], loss: 1.395309, mean_absolute_error: 20.496145, mean_q: 8.670441, mean_eps: 0.837554
  180660/2000000: episode: 1866, duration: 1.723s, episode steps: 98, steps per second: 57, episode reward: -131.605, mean reward: -1.343 [-100.000, 14.209], mean action: 1.469 [0.000, 3.000], mean observation: -0.014 [-1.293, 4.555], loss: 1.965319, mean_absolute_error: 20.836881, mean_q: 9.187028, mean_eps: 0.837451
  180726/2000000: episode: 1867, duration: 1.081s, episode steps: 66, steps per second: 61, episode reward: -214.401, mean reward: -3.248 [-100.000, 8.136], mean action: 1.667 [0.000, 3.000], mean observation: 0.044 [-1.454, 1.000], loss: 1.267497, mean_absolute_error: 19.108226, mean_q: 11.056421, mean_eps: 0.837377
  180787/2000000: episode: 1868, duration: 0.869s, episode steps: 61, steps per second: 70, episode reward: -126.039, mean reward: -2.066 [-100.000, 16.445], mean action: 1.639 [0.000, 3.000], mean observation: -0.132 [-1.629, 1.000], loss: 1.784995, mean_absolute_error: 20.226036, mean_q: 8.457407, mean_eps: 0.837320
  180872/2000000: episode: 1869, duration: 1.255s, episode steps: 85, steps per second: 68, episode reward: -10.736, mean reward: -0.126 [-100.000, 93.820], mean action: 1.671 [0.000, 3.000], mean observation: -0.034 [-1.376, 1.755], loss: 1.581027, mean_absolute_error: 18.844675, mean_q: 10.857101, mean_eps: 0.837255
  180969/2000000: episode: 1870, duration: 1.401s, episode steps: 97, steps per second: 69, episode reward: -178.764, mean reward: -1.843 [-100.000, 15.222], mean action: 1.567 [0.000, 3.000], mean observation: 0.140 [-1.230, 1.000], loss: 1.565133, mean_absolute_error: 20.527365, mean_q: 9.365836, mean_eps: 0.837172
  181084/2000000: episode: 1871, duration: 1.741s, episode steps: 115, steps per second: 66, episode reward: -92.094, mean reward: -0.801 [-100.000, 17.500], mean action: 1.600 [0.000, 3.000], mean observation: 0.057 [-1.266, 1.000], loss: 1.593243, mean_absolute_error: 20.339224, mean_q: 10.699197, mean_eps: 0.837077
  181162/2000000: episode: 1872, duration: 1.135s, episode steps: 78, steps per second: 69, episode reward: -105.588, mean reward: -1.354 [-100.000, 15.007], mean action: 1.603 [0.000, 3.000], mean observation: -0.143 [-1.217, 3.273], loss: 1.893619, mean_absolute_error: 19.635742, mean_q: 10.813938, mean_eps: 0.836990
  181264/2000000: episode: 1873, duration: 1.477s, episode steps: 102, steps per second: 69, episode reward: -123.016, mean reward: -1.206 [-100.000, 7.702], mean action: 1.490 [0.000, 3.000], mean observation: -0.041 [-4.167, 1.000], loss: 2.140878, mean_absolute_error: 20.270448, mean_q: 8.351357, mean_eps: 0.836909
  181330/2000000: episode: 1874, duration: 0.978s, episode steps: 66, steps per second: 67, episode reward: -119.492, mean reward: -1.810 [-100.000, 7.363], mean action: 1.652 [0.000, 3.000], mean observation: -0.048 [-1.418, 5.178], loss: 1.652620, mean_absolute_error: 20.927676, mean_q: 7.893172, mean_eps: 0.836834
  181440/2000000: episode: 1875, duration: 1.602s, episode steps: 110, steps per second: 69, episode reward: -128.470, mean reward: -1.168 [-100.000, 6.130], mean action: 1.718 [0.000, 3.000], mean observation: -0.055 [-1.072, 3.771], loss: 1.576595, mean_absolute_error: 20.052145, mean_q: 7.742119, mean_eps: 0.836754
  181518/2000000: episode: 1876, duration: 1.130s, episode steps: 78, steps per second: 69, episode reward: -208.803, mean reward: -2.677 [-100.000, 8.134], mean action: 1.500 [0.000, 3.000], mean observation: 0.048 [-4.183, 1.000], loss: 1.779148, mean_absolute_error: 19.192770, mean_q: 10.368438, mean_eps: 0.836670
  181578/2000000: episode: 1877, duration: 0.852s, episode steps: 60, steps per second: 70, episode reward: -150.506, mean reward: -2.508 [-100.000, 11.650], mean action: 1.567 [0.000, 3.000], mean observation: -0.134 [-1.513, 4.279], loss: 2.404371, mean_absolute_error: 21.308090, mean_q: 7.603975, mean_eps: 0.836607
  181672/2000000: episode: 1878, duration: 1.339s, episode steps: 94, steps per second: 70, episode reward: -140.627, mean reward: -1.496 [-100.000, 18.263], mean action: 1.500 [0.000, 3.000], mean observation: 0.065 [-1.409, 4.824], loss: 1.300145, mean_absolute_error: 19.678476, mean_q: 9.997935, mean_eps: 0.836538
  181769/2000000: episode: 1879, duration: 1.447s, episode steps: 97, steps per second: 67, episode reward: -121.466, mean reward: -1.252 [-100.000, 13.085], mean action: 1.680 [0.000, 3.000], mean observation: -0.009 [-1.265, 1.000], loss: 1.277767, mean_absolute_error: 19.225180, mean_q: 12.630442, mean_eps: 0.836452
  181905/2000000: episode: 1880, duration: 1.914s, episode steps: 136, steps per second: 71, episode reward: -173.086, mean reward: -1.273 [-100.000, 18.709], mean action: 1.581 [0.000, 3.000], mean observation: 0.025 [-1.536, 3.870], loss: 1.715668, mean_absolute_error: 19.607938, mean_q: 9.782092, mean_eps: 0.836346
  181992/2000000: episode: 1881, duration: 1.276s, episode steps: 87, steps per second: 68, episode reward: -161.340, mean reward: -1.854 [-100.000, 9.186], mean action: 1.644 [0.000, 3.000], mean observation: -0.105 [-1.298, 4.714], loss: 1.139244, mean_absolute_error: 19.844748, mean_q: 9.669246, mean_eps: 0.836247
  182054/2000000: episode: 1882, duration: 0.918s, episode steps: 62, steps per second: 68, episode reward: -106.155, mean reward: -1.712 [-100.000, 12.750], mean action: 1.661 [0.000, 3.000], mean observation: 0.065 [-1.309, 4.926], loss: 1.234837, mean_absolute_error: 20.266828, mean_q: 9.724611, mean_eps: 0.836180
  182149/2000000: episode: 1883, duration: 1.352s, episode steps: 95, steps per second: 70, episode reward: -136.714, mean reward: -1.439 [-100.000, 6.434], mean action: 1.611 [0.000, 3.000], mean observation: -0.090 [-1.077, 1.000], loss: 2.202391, mean_absolute_error: 20.307771, mean_q: 10.493328, mean_eps: 0.836108
  182227/2000000: episode: 1884, duration: 1.087s, episode steps: 78, steps per second: 72, episode reward: -257.781, mean reward: -3.305 [-100.000, 7.109], mean action: 1.551 [0.000, 3.000], mean observation: 0.074 [-1.603, 1.185], loss: 1.184015, mean_absolute_error: 19.045226, mean_q: 11.898434, mean_eps: 0.836031
  182349/2000000: episode: 1885, duration: 1.749s, episode steps: 122, steps per second: 70, episode reward: -101.009, mean reward: -0.828 [-100.000, 11.891], mean action: 1.656 [0.000, 3.000], mean observation: -0.092 [-3.383, 1.000], loss: 2.257547, mean_absolute_error: 20.712989, mean_q: 7.779912, mean_eps: 0.835941
  182440/2000000: episode: 1886, duration: 1.309s, episode steps: 91, steps per second: 70, episode reward: -117.870, mean reward: -1.295 [-100.000, 6.547], mean action: 1.571 [0.000, 3.000], mean observation: -0.106 [-1.072, 2.575], loss: 1.508254, mean_absolute_error: 19.160868, mean_q: 11.304338, mean_eps: 0.835845
  182568/2000000: episode: 1887, duration: 1.869s, episode steps: 128, steps per second: 68, episode reward: -191.227, mean reward: -1.494 [-100.000, 4.760], mean action: 1.656 [0.000, 3.000], mean observation: -0.109 [-1.093, 1.930], loss: 1.572230, mean_absolute_error: 19.842669, mean_q: 9.784893, mean_eps: 0.835748
  182658/2000000: episode: 1888, duration: 1.319s, episode steps: 90, steps per second: 68, episode reward: -178.262, mean reward: -1.981 [-100.000, 5.792], mean action: 1.744 [0.000, 3.000], mean observation: -0.156 [-5.246, 1.000], loss: 1.597755, mean_absolute_error: 19.143938, mean_q: 10.446913, mean_eps: 0.835649
  182717/2000000: episode: 1889, duration: 0.864s, episode steps: 59, steps per second: 68, episode reward: -125.354, mean reward: -2.125 [-100.000, 7.421], mean action: 1.458 [0.000, 3.000], mean observation: 0.028 [-1.459, 5.102], loss: 1.569276, mean_absolute_error: 17.872376, mean_q: 11.082565, mean_eps: 0.835581
  182841/2000000: episode: 1890, duration: 1.773s, episode steps: 124, steps per second: 70, episode reward: -149.483, mean reward: -1.206 [-100.000, 10.022], mean action: 1.548 [0.000, 3.000], mean observation: -0.013 [-4.455, 1.000], loss: 1.950404, mean_absolute_error: 19.356446, mean_q: 10.806868, mean_eps: 0.835498
  182921/2000000: episode: 1891, duration: 1.151s, episode steps: 80, steps per second: 70, episode reward: -84.164, mean reward: -1.052 [-100.000, 13.432], mean action: 1.675 [0.000, 3.000], mean observation: -0.028 [-1.212, 1.000], loss: 2.472057, mean_absolute_error: 19.803734, mean_q: 7.714121, mean_eps: 0.835406
  183040/2000000: episode: 1892, duration: 1.713s, episode steps: 119, steps per second: 69, episode reward: -154.703, mean reward: -1.300 [-100.000, 6.055], mean action: 1.655 [0.000, 3.000], mean observation: -0.066 [-1.112, 3.695], loss: 1.541519, mean_absolute_error: 20.086385, mean_q: 9.626697, mean_eps: 0.835318
  183168/2000000: episode: 1893, duration: 1.860s, episode steps: 128, steps per second: 69, episode reward: -155.384, mean reward: -1.214 [-100.000, 7.377], mean action: 1.578 [0.000, 3.000], mean observation: 0.012 [-1.533, 5.198], loss: 1.819836, mean_absolute_error: 20.299044, mean_q: 10.105940, mean_eps: 0.835208
  183261/2000000: episode: 1894, duration: 1.362s, episode steps: 93, steps per second: 68, episode reward: -190.829, mean reward: -2.052 [-100.000, 3.631], mean action: 1.624 [0.000, 3.000], mean observation: -0.104 [-1.292, 2.662], loss: 1.615756, mean_absolute_error: 21.103212, mean_q: 9.201135, mean_eps: 0.835107
  183409/2000000: episode: 1895, duration: 2.094s, episode steps: 148, steps per second: 71, episode reward: -115.210, mean reward: -0.778 [-100.000, 12.314], mean action: 1.649 [0.000, 3.000], mean observation: 0.017 [-1.330, 1.016], loss: 1.316661, mean_absolute_error: 19.245704, mean_q: 10.511894, mean_eps: 0.834998
  183490/2000000: episode: 1896, duration: 1.153s, episode steps: 81, steps per second: 70, episode reward: -173.335, mean reward: -2.140 [-100.000, 9.656], mean action: 1.395 [0.000, 3.000], mean observation: -0.014 [-4.905, 1.000], loss: 1.298484, mean_absolute_error: 20.289329, mean_q: 9.260910, mean_eps: 0.834895
  183550/2000000: episode: 1897, duration: 0.861s, episode steps: 60, steps per second: 70, episode reward: -133.682, mean reward: -2.228 [-100.000, 11.698], mean action: 1.700 [0.000, 3.000], mean observation: -0.026 [-1.603, 4.845], loss: 1.545017, mean_absolute_error: 20.925044, mean_q: 9.062005, mean_eps: 0.834832
  183625/2000000: episode: 1898, duration: 1.080s, episode steps: 75, steps per second: 69, episode reward: -98.590, mean reward: -1.315 [-100.000, 12.813], mean action: 1.333 [0.000, 3.000], mean observation: 0.031 [-1.147, 1.000], loss: 1.567484, mean_absolute_error: 18.098654, mean_q: 10.273841, mean_eps: 0.834771
  183727/2000000: episode: 1899, duration: 1.439s, episode steps: 102, steps per second: 71, episode reward: -183.992, mean reward: -1.804 [-100.000, 11.842], mean action: 1.637 [0.000, 3.000], mean observation: -0.036 [-1.469, 4.487], loss: 1.229013, mean_absolute_error: 19.697365, mean_q: 10.073718, mean_eps: 0.834692
  183796/2000000: episode: 1900, duration: 1.014s, episode steps: 69, steps per second: 68, episode reward: -110.279, mean reward: -1.598 [-100.000, 8.939], mean action: 1.623 [0.000, 3.000], mean observation: -0.108 [-1.374, 1.000], loss: 1.642444, mean_absolute_error: 20.055832, mean_q: 7.709275, mean_eps: 0.834616
  183917/2000000: episode: 1901, duration: 1.752s, episode steps: 121, steps per second: 69, episode reward: -100.041, mean reward: -0.827 [-100.000, 22.686], mean action: 1.653 [0.000, 3.000], mean observation: 0.018 [-0.895, 1.000], loss: 1.549324, mean_absolute_error: 19.508300, mean_q: 9.870341, mean_eps: 0.834530
  184037/2000000: episode: 1902, duration: 1.720s, episode steps: 120, steps per second: 70, episode reward: -209.632, mean reward: -1.747 [-100.000, 17.330], mean action: 1.458 [0.000, 3.000], mean observation: 0.161 [-2.627, 1.016], loss: 1.279587, mean_absolute_error: 18.844279, mean_q: 12.538992, mean_eps: 0.834420
  184198/2000000: episode: 1903, duration: 2.281s, episode steps: 161, steps per second: 71, episode reward: -141.090, mean reward: -0.876 [-100.000, 22.135], mean action: 1.640 [0.000, 3.000], mean observation: 0.056 [-2.257, 1.032], loss: 1.546899, mean_absolute_error: 19.874101, mean_q: 11.398215, mean_eps: 0.834294
  184302/2000000: episode: 1904, duration: 1.512s, episode steps: 104, steps per second: 69, episode reward: -115.747, mean reward: -1.113 [-100.000, 10.152], mean action: 1.538 [0.000, 3.000], mean observation: 0.116 [-3.321, 1.000], loss: 1.772110, mean_absolute_error: 20.998642, mean_q: 8.087018, mean_eps: 0.834175
  184361/2000000: episode: 1905, duration: 0.856s, episode steps: 59, steps per second: 69, episode reward: -151.199, mean reward: -2.563 [-100.000, 15.811], mean action: 1.542 [0.000, 3.000], mean observation: -0.002 [-1.701, 1.000], loss: 1.253118, mean_absolute_error: 19.429768, mean_q: 10.028835, mean_eps: 0.834101
  184468/2000000: episode: 1906, duration: 1.545s, episode steps: 107, steps per second: 69, episode reward: -114.831, mean reward: -1.073 [-100.000, 10.088], mean action: 1.626 [0.000, 3.000], mean observation: -0.067 [-1.046, 1.000], loss: 1.579763, mean_absolute_error: 19.691968, mean_q: 9.318989, mean_eps: 0.834027
  184543/2000000: episode: 1907, duration: 1.114s, episode steps: 75, steps per second: 67, episode reward: -131.649, mean reward: -1.755 [-100.000, 8.413], mean action: 1.653 [0.000, 3.000], mean observation: -0.079 [-5.135, 1.000], loss: 1.794897, mean_absolute_error: 21.104970, mean_q: 9.349676, mean_eps: 0.833946
  184623/2000000: episode: 1908, duration: 1.157s, episode steps: 80, steps per second: 69, episode reward: -140.679, mean reward: -1.758 [-100.000, 20.658], mean action: 1.675 [0.000, 3.000], mean observation: -0.105 [-1.354, 1.000], loss: 1.380484, mean_absolute_error: 20.746597, mean_q: 8.476178, mean_eps: 0.833876
  184696/2000000: episode: 1909, duration: 1.075s, episode steps: 73, steps per second: 68, episode reward: -89.715, mean reward: -1.229 [-100.000, 15.321], mean action: 1.890 [0.000, 3.000], mean observation: -0.066 [-3.111, 1.000], loss: 1.492015, mean_absolute_error: 18.639539, mean_q: 10.872037, mean_eps: 0.833808
  184821/2000000: episode: 1910, duration: 1.793s, episode steps: 125, steps per second: 70, episode reward: -171.248, mean reward: -1.370 [-100.000, 13.880], mean action: 1.456 [0.000, 3.000], mean observation: 0.138 [-1.462, 1.113], loss: 1.604868, mean_absolute_error: 20.636896, mean_q: 9.388444, mean_eps: 0.833718
  184899/2000000: episode: 1911, duration: 1.108s, episode steps: 78, steps per second: 70, episode reward: -121.591, mean reward: -1.559 [-100.000, 6.138], mean action: 1.577 [0.000, 3.000], mean observation: 0.057 [-4.332, 1.000], loss: 1.464228, mean_absolute_error: 19.390095, mean_q: 11.206136, mean_eps: 0.833626
  185029/2000000: episode: 1912, duration: 1.885s, episode steps: 130, steps per second: 69, episode reward: -158.098, mean reward: -1.216 [-100.000, 12.649], mean action: 1.462 [0.000, 3.000], mean observation: 0.134 [-4.292, 1.000], loss: 1.888952, mean_absolute_error: 20.412569, mean_q: 9.371960, mean_eps: 0.833532
  185140/2000000: episode: 1913, duration: 1.596s, episode steps: 111, steps per second: 70, episode reward: -219.326, mean reward: -1.976 [-100.000, 28.845], mean action: 1.622 [0.000, 3.000], mean observation: -0.172 [-3.763, 1.000], loss: 2.109027, mean_absolute_error: 20.667115, mean_q: 9.639391, mean_eps: 0.833424
  185219/2000000: episode: 1914, duration: 1.172s, episode steps: 79, steps per second: 67, episode reward: -135.046, mean reward: -1.709 [-100.000, 13.289], mean action: 1.646 [0.000, 3.000], mean observation: -0.151 [-1.450, 1.000], loss: 1.774556, mean_absolute_error: 19.731785, mean_q: 10.160244, mean_eps: 0.833340
  185340/2000000: episode: 1915, duration: 1.792s, episode steps: 121, steps per second: 68, episode reward: -156.745, mean reward: -1.295 [-100.000, 22.310], mean action: 1.612 [0.000, 3.000], mean observation: -0.042 [-1.518, 3.788], loss: 1.826697, mean_absolute_error: 20.423581, mean_q: 8.687046, mean_eps: 0.833250
  185435/2000000: episode: 1916, duration: 1.368s, episode steps: 95, steps per second: 69, episode reward: -136.744, mean reward: -1.439 [-100.000, 6.806], mean action: 1.568 [0.000, 3.000], mean observation: -0.085 [-1.032, 4.565], loss: 1.814756, mean_absolute_error: 19.613825, mean_q: 11.235148, mean_eps: 0.833153
  185567/2000000: episode: 1917, duration: 1.902s, episode steps: 132, steps per second: 69, episode reward: -144.150, mean reward: -1.092 [-100.000, 12.226], mean action: 1.697 [0.000, 3.000], mean observation: -0.121 [-1.108, 3.839], loss: 1.490279, mean_absolute_error: 19.284895, mean_q: 10.628525, mean_eps: 0.833050
  185654/2000000: episode: 1918, duration: 1.251s, episode steps: 87, steps per second: 70, episode reward: -105.847, mean reward: -1.217 [-100.000, 10.019], mean action: 1.483 [0.000, 3.000], mean observation: 0.082 [-1.117, 4.192], loss: 1.429575, mean_absolute_error: 20.285657, mean_q: 9.783888, mean_eps: 0.832951
  185781/2000000: episode: 1919, duration: 1.820s, episode steps: 127, steps per second: 70, episode reward: -229.026, mean reward: -1.803 [-100.000, 14.295], mean action: 1.520 [0.000, 3.000], mean observation: 0.225 [-1.125, 1.320], loss: 1.667486, mean_absolute_error: 20.126261, mean_q: 9.522724, mean_eps: 0.832854
  185874/2000000: episode: 1920, duration: 1.322s, episode steps: 93, steps per second: 70, episode reward: -105.050, mean reward: -1.130 [-100.000, 12.041], mean action: 1.570 [0.000, 3.000], mean observation: -0.026 [-1.108, 3.621], loss: 1.646044, mean_absolute_error: 20.423888, mean_q: 9.449983, mean_eps: 0.832755
  185970/2000000: episode: 1921, duration: 1.391s, episode steps: 96, steps per second: 69, episode reward: -182.705, mean reward: -1.903 [-100.000, 12.876], mean action: 1.438 [0.000, 3.000], mean observation: 0.027 [-4.361, 1.000], loss: 1.592689, mean_absolute_error: 20.033107, mean_q: 10.307552, mean_eps: 0.832670
  186074/2000000: episode: 1922, duration: 1.488s, episode steps: 104, steps per second: 70, episode reward: -134.628, mean reward: -1.294 [-100.000, 18.756], mean action: 1.615 [0.000, 3.000], mean observation: 0.036 [-4.614, 1.000], loss: 1.470482, mean_absolute_error: 20.569157, mean_q: 9.784351, mean_eps: 0.832580
  186183/2000000: episode: 1923, duration: 1.550s, episode steps: 109, steps per second: 70, episode reward: -222.864, mean reward: -2.045 [-100.000, 4.979], mean action: 1.706 [0.000, 3.000], mean observation: -0.117 [-1.313, 4.594], loss: 1.148916, mean_absolute_error: 19.047023, mean_q: 11.988981, mean_eps: 0.832485
  186309/2000000: episode: 1924, duration: 1.827s, episode steps: 126, steps per second: 69, episode reward: -93.278, mean reward: -0.740 [-100.000, 15.148], mean action: 1.683 [0.000, 3.000], mean observation: 0.026 [-0.866, 2.933], loss: 2.081572, mean_absolute_error: 20.836214, mean_q: 8.828309, mean_eps: 0.832379
  186422/2000000: episode: 1925, duration: 1.598s, episode steps: 113, steps per second: 71, episode reward: -155.644, mean reward: -1.377 [-100.000, 7.461], mean action: 1.575 [0.000, 3.000], mean observation: 0.121 [-3.681, 1.000], loss: 1.331025, mean_absolute_error: 19.541510, mean_q: 10.309677, mean_eps: 0.832271
  186481/2000000: episode: 1926, duration: 0.864s, episode steps: 59, steps per second: 68, episode reward: -131.604, mean reward: -2.231 [-100.000, 6.130], mean action: 1.356 [0.000, 3.000], mean observation: 0.054 [-3.929, 1.000], loss: 2.551649, mean_absolute_error: 21.790089, mean_q: 8.281384, mean_eps: 0.832193
  186585/2000000: episode: 1927, duration: 1.493s, episode steps: 104, steps per second: 70, episode reward: -101.930, mean reward: -0.980 [-100.000, 11.211], mean action: 1.692 [0.000, 3.000], mean observation: -0.032 [-3.109, 1.000], loss: 2.078802, mean_absolute_error: 19.801128, mean_q: 11.560564, mean_eps: 0.832119
  186649/2000000: episode: 1928, duration: 0.925s, episode steps: 64, steps per second: 69, episode reward: -84.503, mean reward: -1.320 [-100.000, 17.755], mean action: 1.562 [0.000, 3.000], mean observation: 0.011 [-1.354, 4.166], loss: 1.748932, mean_absolute_error: 20.328295, mean_q: 9.313650, mean_eps: 0.832044
  186724/2000000: episode: 1929, duration: 1.105s, episode steps: 75, steps per second: 68, episode reward: -160.954, mean reward: -2.146 [-100.000, 6.560], mean action: 1.320 [0.000, 3.000], mean observation: 0.026 [-4.924, 1.000], loss: 2.117960, mean_absolute_error: 20.103003, mean_q: 11.606734, mean_eps: 0.831983
  186899/2000000: episode: 1930, duration: 2.527s, episode steps: 175, steps per second: 69, episode reward: -121.452, mean reward: -0.694 [-100.000, 14.977], mean action: 1.646 [0.000, 3.000], mean observation: 0.117 [-1.144, 1.000], loss: 1.516474, mean_absolute_error: 20.145591, mean_q: 9.553323, mean_eps: 0.831871
  186985/2000000: episode: 1931, duration: 1.382s, episode steps: 86, steps per second: 62, episode reward: -132.772, mean reward: -1.544 [-100.000, 15.132], mean action: 1.523 [0.000, 3.000], mean observation: -0.040 [-1.308, 1.000], loss: 1.824975, mean_absolute_error: 20.188925, mean_q: 8.213687, mean_eps: 0.831752
  187129/2000000: episode: 1932, duration: 2.054s, episode steps: 144, steps per second: 70, episode reward: -140.355, mean reward: -0.975 [-100.000, 9.698], mean action: 1.569 [0.000, 3.000], mean observation: -0.047 [-1.223, 4.686], loss: 1.713551, mean_absolute_error: 19.321688, mean_q: 10.537198, mean_eps: 0.831648
  187232/2000000: episode: 1933, duration: 1.466s, episode steps: 103, steps per second: 70, episode reward: -240.855, mean reward: -2.338 [-100.000, 81.373], mean action: 1.641 [0.000, 3.000], mean observation: -0.204 [-2.247, 1.000], loss: 1.210370, mean_absolute_error: 18.962497, mean_q: 10.409816, mean_eps: 0.831538
  187345/2000000: episode: 1934, duration: 1.644s, episode steps: 113, steps per second: 69, episode reward: -62.041, mean reward: -0.549 [-100.000, 82.576], mean action: 1.354 [0.000, 3.000], mean observation: 0.197 [-1.374, 1.926], loss: 1.487010, mean_absolute_error: 18.984992, mean_q: 10.833186, mean_eps: 0.831441
  187427/2000000: episode: 1935, duration: 1.146s, episode steps: 82, steps per second: 72, episode reward: -162.966, mean reward: -1.987 [-100.000, 5.040], mean action: 1.366 [0.000, 3.000], mean observation: 0.122 [-4.125, 1.000], loss: 2.185721, mean_absolute_error: 21.049350, mean_q: 8.798594, mean_eps: 0.831353
  187503/2000000: episode: 1936, duration: 1.066s, episode steps: 76, steps per second: 71, episode reward: -210.645, mean reward: -2.772 [-100.000, 16.238], mean action: 1.461 [0.000, 3.000], mean observation: 0.069 [-1.563, 4.486], loss: 1.750073, mean_absolute_error: 19.890232, mean_q: 8.685904, mean_eps: 0.831282
  187614/2000000: episode: 1937, duration: 1.583s, episode steps: 111, steps per second: 70, episode reward: -168.886, mean reward: -1.521 [-100.000, 7.094], mean action: 1.333 [0.000, 3.000], mean observation: 0.189 [-4.538, 1.008], loss: 1.286513, mean_absolute_error: 19.546488, mean_q: 11.556010, mean_eps: 0.831198
  187723/2000000: episode: 1938, duration: 1.527s, episode steps: 109, steps per second: 71, episode reward: -112.509, mean reward: -1.032 [-100.000, 13.703], mean action: 1.495 [0.000, 3.000], mean observation: 0.028 [-1.148, 1.000], loss: 1.853291, mean_absolute_error: 20.234410, mean_q: 8.535827, mean_eps: 0.831099
  187798/2000000: episode: 1939, duration: 1.095s, episode steps: 75, steps per second: 69, episode reward: -113.760, mean reward: -1.517 [-100.000, 12.194], mean action: 1.667 [0.000, 3.000], mean observation: -0.053 [-1.276, 4.111], loss: 1.542470, mean_absolute_error: 20.265099, mean_q: 9.848440, mean_eps: 0.831016
  187864/2000000: episode: 1940, duration: 0.954s, episode steps: 66, steps per second: 69, episode reward: -113.409, mean reward: -1.718 [-100.000, 12.830], mean action: 1.909 [0.000, 3.000], mean observation: -0.118 [-1.352, 4.389], loss: 1.018047, mean_absolute_error: 19.710674, mean_q: 9.028765, mean_eps: 0.830953
  187962/2000000: episode: 1941, duration: 1.432s, episode steps: 98, steps per second: 68, episode reward: -134.763, mean reward: -1.375 [-100.000, 12.091], mean action: 1.704 [0.000, 3.000], mean observation: -0.128 [-4.290, 1.000], loss: 1.053598, mean_absolute_error: 18.325441, mean_q: 12.299061, mean_eps: 0.830879
  188074/2000000: episode: 1942, duration: 1.577s, episode steps: 112, steps per second: 71, episode reward: -212.739, mean reward: -1.899 [-100.000, 20.678], mean action: 1.482 [0.000, 3.000], mean observation: 0.137 [-1.340, 5.025], loss: 1.210699, mean_absolute_error: 19.384348, mean_q: 9.208257, mean_eps: 0.830784
  188149/2000000: episode: 1943, duration: 1.100s, episode steps: 75, steps per second: 68, episode reward: -124.064, mean reward: -1.654 [-100.000, 7.751], mean action: 1.480 [0.000, 3.000], mean observation: -0.051 [-4.689, 1.000], loss: 1.376580, mean_absolute_error: 19.973980, mean_q: 10.088130, mean_eps: 0.830699
  188303/2000000: episode: 1944, duration: 2.130s, episode steps: 154, steps per second: 72, episode reward: -97.940, mean reward: -0.636 [-100.000, 6.991], mean action: 1.552 [0.000, 3.000], mean observation: 0.072 [-3.792, 1.048], loss: 1.424614, mean_absolute_error: 19.798983, mean_q: 10.396924, mean_eps: 0.830597
  188405/2000000: episode: 1945, duration: 1.478s, episode steps: 102, steps per second: 69, episode reward: -146.348, mean reward: -1.435 [-100.000, 17.026], mean action: 1.657 [0.000, 3.000], mean observation: -0.097 [-1.180, 1.000], loss: 1.659278, mean_absolute_error: 19.967041, mean_q: 10.314636, mean_eps: 0.830481
  188467/2000000: episode: 1946, duration: 0.867s, episode steps: 62, steps per second: 72, episode reward: -148.431, mean reward: -2.394 [-100.000, 7.407], mean action: 1.355 [0.000, 3.000], mean observation: -0.015 [-4.627, 1.000], loss: 1.459624, mean_absolute_error: 21.631193, mean_q: 8.672128, mean_eps: 0.830408
  188566/2000000: episode: 1947, duration: 1.423s, episode steps: 99, steps per second: 70, episode reward: -108.516, mean reward: -1.096 [-100.000, 20.847], mean action: 1.444 [0.000, 3.000], mean observation: -0.029 [-1.114, 1.000], loss: 1.296044, mean_absolute_error: 19.588137, mean_q: 10.001349, mean_eps: 0.830336
  188636/2000000: episode: 1948, duration: 1.036s, episode steps: 70, steps per second: 68, episode reward: -138.448, mean reward: -1.978 [-100.000, 18.017], mean action: 1.843 [0.000, 3.000], mean observation: -0.122 [-3.369, 1.000], loss: 1.025268, mean_absolute_error: 19.765076, mean_q: 10.508601, mean_eps: 0.830260
  188697/2000000: episode: 1949, duration: 0.911s, episode steps: 61, steps per second: 67, episode reward: -126.650, mean reward: -2.076 [-100.000, 15.958], mean action: 1.689 [0.000, 3.000], mean observation: -0.117 [-5.346, 1.000], loss: 1.237938, mean_absolute_error: 20.216693, mean_q: 10.884294, mean_eps: 0.830201
  188782/2000000: episode: 1950, duration: 1.206s, episode steps: 85, steps per second: 70, episode reward: -122.073, mean reward: -1.436 [-100.000, 10.972], mean action: 1.459 [0.000, 3.000], mean observation: -0.067 [-1.152, 3.755], loss: 2.156762, mean_absolute_error: 20.832553, mean_q: 8.340248, mean_eps: 0.830134
  188984/2000000: episode: 1951, duration: 2.910s, episode steps: 202, steps per second: 69, episode reward: -120.555, mean reward: -0.597 [-100.000, 16.396], mean action: 1.629 [0.000, 3.000], mean observation: 0.145 [-1.134, 1.000], loss: 1.886588, mean_absolute_error: 20.359795, mean_q: 9.275469, mean_eps: 0.830006
  189116/2000000: episode: 1952, duration: 1.913s, episode steps: 132, steps per second: 69, episode reward: -99.156, mean reward: -0.751 [-100.000, 13.018], mean action: 1.682 [0.000, 3.000], mean observation: -0.043 [-3.464, 1.000], loss: 1.995354, mean_absolute_error: 19.618646, mean_q: 9.352674, mean_eps: 0.829857
  189190/2000000: episode: 1953, duration: 1.077s, episode steps: 74, steps per second: 69, episode reward: -53.023, mean reward: -0.717 [-100.000, 16.985], mean action: 1.419 [0.000, 3.000], mean observation: -0.028 [-1.132, 1.000], loss: 2.322089, mean_absolute_error: 19.341113, mean_q: 11.413391, mean_eps: 0.829763
  189310/2000000: episode: 1954, duration: 1.713s, episode steps: 120, steps per second: 70, episode reward: -109.290, mean reward: -0.911 [-100.000, 12.734], mean action: 1.558 [0.000, 3.000], mean observation: -0.084 [-1.151, 3.652], loss: 1.772715, mean_absolute_error: 19.823132, mean_q: 10.088062, mean_eps: 0.829675
  189422/2000000: episode: 1955, duration: 1.616s, episode steps: 112, steps per second: 69, episode reward: -131.160, mean reward: -1.171 [-100.000, 13.595], mean action: 1.580 [0.000, 3.000], mean observation: -0.022 [-1.189, 1.000], loss: 1.475681, mean_absolute_error: 19.920344, mean_q: 11.258852, mean_eps: 0.829571
  189542/2000000: episode: 1956, duration: 1.722s, episode steps: 120, steps per second: 70, episode reward: -170.074, mean reward: -1.417 [-100.000, 7.365], mean action: 1.542 [0.000, 3.000], mean observation: -0.052 [-1.345, 1.010], loss: 1.678243, mean_absolute_error: 20.229236, mean_q: 8.656654, mean_eps: 0.829466
  189665/2000000: episode: 1957, duration: 1.770s, episode steps: 123, steps per second: 69, episode reward: -95.085, mean reward: -0.773 [-100.000, 12.890], mean action: 1.626 [0.000, 3.000], mean observation: 0.001 [-3.129, 1.000], loss: 1.646960, mean_absolute_error: 19.560952, mean_q: 9.744406, mean_eps: 0.829356
  189798/2000000: episode: 1958, duration: 1.898s, episode steps: 133, steps per second: 70, episode reward: -23.176, mean reward: -0.174 [-100.000, 85.001], mean action: 1.632 [0.000, 3.000], mean observation: 0.122 [-1.335, 1.281], loss: 1.563642, mean_absolute_error: 20.001638, mean_q: 10.323736, mean_eps: 0.829241
  189861/2000000: episode: 1959, duration: 0.937s, episode steps: 63, steps per second: 67, episode reward: -92.848, mean reward: -1.474 [-100.000, 17.625], mean action: 1.524 [0.000, 3.000], mean observation: -0.065 [-1.399, 1.000], loss: 0.750631, mean_absolute_error: 18.308522, mean_q: 11.181532, mean_eps: 0.829153
  189951/2000000: episode: 1960, duration: 1.273s, episode steps: 90, steps per second: 71, episode reward: -120.439, mean reward: -1.338 [-100.000, 18.150], mean action: 1.622 [0.000, 3.000], mean observation: 0.006 [-1.322, 1.000], loss: 1.332758, mean_absolute_error: 19.535738, mean_q: 9.471535, mean_eps: 0.829085
  190083/2000000: episode: 1961, duration: 1.908s, episode steps: 132, steps per second: 69, episode reward: -224.506, mean reward: -1.701 [-100.000, 6.726], mean action: 1.417 [0.000, 3.000], mean observation: 0.086 [-1.333, 1.788], loss: 1.685209, mean_absolute_error: 21.309568, mean_q: 9.260705, mean_eps: 0.828986
  190230/2000000: episode: 1962, duration: 2.129s, episode steps: 147, steps per second: 69, episode reward: -120.139, mean reward: -0.817 [-100.000, 18.377], mean action: 1.585 [0.000, 3.000], mean observation: 0.148 [-3.811, 1.041], loss: 1.545476, mean_absolute_error: 20.683547, mean_q: 12.515845, mean_eps: 0.828860
  190303/2000000: episode: 1963, duration: 0.997s, episode steps: 73, steps per second: 73, episode reward: -132.289, mean reward: -1.812 [-100.000, 8.783], mean action: 1.534 [0.000, 3.000], mean observation: -0.057 [-1.211, 4.164], loss: 2.785990, mean_absolute_error: 22.105088, mean_q: 7.823663, mean_eps: 0.828761
  190399/2000000: episode: 1964, duration: 1.379s, episode steps: 96, steps per second: 70, episode reward: -133.396, mean reward: -1.390 [-100.000, 6.975], mean action: 1.719 [0.000, 3.000], mean observation: -0.051 [-1.137, 3.753], loss: 1.612032, mean_absolute_error: 20.388037, mean_q: 12.393076, mean_eps: 0.828685
  190513/2000000: episode: 1965, duration: 1.665s, episode steps: 114, steps per second: 68, episode reward: -155.994, mean reward: -1.368 [-100.000, 12.776], mean action: 1.491 [0.000, 3.000], mean observation: 0.138 [-3.220, 1.000], loss: 1.298274, mean_absolute_error: 22.968547, mean_q: 8.019722, mean_eps: 0.828590
  190630/2000000: episode: 1966, duration: 1.654s, episode steps: 117, steps per second: 71, episode reward: -80.672, mean reward: -0.690 [-100.000, 23.325], mean action: 1.675 [0.000, 3.000], mean observation: -0.051 [-3.252, 1.000], loss: 1.682214, mean_absolute_error: 21.272019, mean_q: 11.576211, mean_eps: 0.828485
  190721/2000000: episode: 1967, duration: 1.342s, episode steps: 91, steps per second: 68, episode reward: -129.759, mean reward: -1.426 [-100.000, 11.895], mean action: 1.505 [0.000, 3.000], mean observation: 0.021 [-4.590, 1.000], loss: 1.475663, mean_absolute_error: 21.273883, mean_q: 9.364059, mean_eps: 0.828392
  190900/2000000: episode: 1968, duration: 2.589s, episode steps: 179, steps per second: 69, episode reward: -123.450, mean reward: -0.690 [-100.000, 9.405], mean action: 1.592 [0.000, 3.000], mean observation: 0.005 [-1.021, 1.100], loss: 1.590359, mean_absolute_error: 20.791029, mean_q: 11.911042, mean_eps: 0.828271
  190983/2000000: episode: 1969, duration: 1.201s, episode steps: 83, steps per second: 69, episode reward: -125.100, mean reward: -1.507 [-100.000, 6.622], mean action: 1.482 [0.000, 3.000], mean observation: 0.125 [-1.036, 3.899], loss: 1.550478, mean_absolute_error: 20.854164, mean_q: 11.900402, mean_eps: 0.828154
  191083/2000000: episode: 1970, duration: 1.427s, episode steps: 100, steps per second: 70, episode reward: -146.642, mean reward: -1.466 [-100.000, 10.277], mean action: 1.610 [0.000, 3.000], mean observation: -0.067 [-1.327, 4.705], loss: 1.893965, mean_absolute_error: 21.930085, mean_q: 10.213070, mean_eps: 0.828071
  191205/2000000: episode: 1971, duration: 1.764s, episode steps: 122, steps per second: 69, episode reward: -123.369, mean reward: -1.011 [-100.000, 9.549], mean action: 1.656 [0.000, 3.000], mean observation: 0.094 [-3.400, 1.000], loss: 1.381454, mean_absolute_error: 20.442372, mean_q: 12.879772, mean_eps: 0.827970
  191335/2000000: episode: 1972, duration: 1.845s, episode steps: 130, steps per second: 70, episode reward: -136.940, mean reward: -1.053 [-100.000, 33.428], mean action: 1.662 [0.000, 3.000], mean observation: -0.137 [-1.494, 1.819], loss: 1.108638, mean_absolute_error: 19.945966, mean_q: 12.318867, mean_eps: 0.827857
  191416/2000000: episode: 1973, duration: 1.215s, episode steps: 81, steps per second: 67, episode reward: -138.095, mean reward: -1.705 [-100.000, 3.543], mean action: 1.519 [0.000, 3.000], mean observation: 0.122 [-1.064, 4.104], loss: 1.270997, mean_absolute_error: 20.613250, mean_q: 12.612215, mean_eps: 0.827763
  191531/2000000: episode: 1974, duration: 1.675s, episode steps: 115, steps per second: 69, episode reward: -95.729, mean reward: -0.832 [-100.000, 17.094], mean action: 1.461 [0.000, 3.000], mean observation: -0.020 [-1.082, 1.000], loss: 1.534078, mean_absolute_error: 21.015067, mean_q: 10.292668, mean_eps: 0.827675
  191598/2000000: episode: 1975, duration: 0.990s, episode steps: 67, steps per second: 68, episode reward: -105.056, mean reward: -1.568 [-100.000, 9.435], mean action: 1.254 [0.000, 3.000], mean observation: -0.004 [-4.340, 1.000], loss: 2.026562, mean_absolute_error: 22.031869, mean_q: 10.384299, mean_eps: 0.827592
  191691/2000000: episode: 1976, duration: 1.339s, episode steps: 93, steps per second: 69, episode reward: -121.005, mean reward: -1.301 [-100.000, 12.586], mean action: 1.656 [0.000, 3.000], mean observation: -0.091 [-1.295, 1.000], loss: 1.314238, mean_absolute_error: 20.671565, mean_q: 9.946614, mean_eps: 0.827520
  191761/2000000: episode: 1977, duration: 1.016s, episode steps: 70, steps per second: 69, episode reward: -133.492, mean reward: -1.907 [-100.000, 63.155], mean action: 1.400 [0.000, 3.000], mean observation: 0.050 [-1.623, 2.025], loss: 1.442207, mean_absolute_error: 20.002212, mean_q: 10.901056, mean_eps: 0.827447
  191881/2000000: episode: 1978, duration: 1.696s, episode steps: 120, steps per second: 71, episode reward: -245.351, mean reward: -2.045 [-100.000, 18.112], mean action: 1.458 [0.000, 3.000], mean observation: 0.191 [-1.514, 3.630], loss: 1.380603, mean_absolute_error: 20.865989, mean_q: 9.677142, mean_eps: 0.827360
  192005/2000000: episode: 1979, duration: 1.756s, episode steps: 124, steps per second: 71, episode reward: -212.836, mean reward: -1.716 [-100.000, 3.308], mean action: 1.508 [0.000, 3.000], mean observation: 0.185 [-1.204, 1.009], loss: 1.430413, mean_absolute_error: 22.065717, mean_q: 7.950663, mean_eps: 0.827250
  192072/2000000: episode: 1980, duration: 0.970s, episode steps: 67, steps per second: 69, episode reward: -180.879, mean reward: -2.700 [-100.000, 7.285], mean action: 1.522 [0.000, 3.000], mean observation: -0.070 [-3.246, 1.000], loss: 1.641480, mean_absolute_error: 21.802699, mean_q: 8.417467, mean_eps: 0.827166
  192208/2000000: episode: 1981, duration: 1.974s, episode steps: 136, steps per second: 69, episode reward: -93.661, mean reward: -0.689 [-100.000, 15.652], mean action: 1.537 [0.000, 3.000], mean observation: 0.042 [-1.245, 1.000], loss: 1.413710, mean_absolute_error: 20.459495, mean_q: 12.317389, mean_eps: 0.827076
  192274/2000000: episode: 1982, duration: 0.985s, episode steps: 66, steps per second: 67, episode reward: -130.885, mean reward: -1.983 [-100.000, 14.029], mean action: 1.561 [0.000, 3.000], mean observation: 0.087 [-1.377, 1.000], loss: 1.398459, mean_absolute_error: 20.208115, mean_q: 12.161728, mean_eps: 0.826984
  192354/2000000: episode: 1983, duration: 1.150s, episode steps: 80, steps per second: 70, episode reward: -122.195, mean reward: -1.527 [-100.000, 25.924], mean action: 1.312 [0.000, 3.000], mean observation: 0.079 [-4.513, 1.049], loss: 1.295160, mean_absolute_error: 21.165767, mean_q: 10.174553, mean_eps: 0.826917
  192486/2000000: episode: 1984, duration: 1.942s, episode steps: 132, steps per second: 68, episode reward: -161.372, mean reward: -1.223 [-100.000, 9.291], mean action: 1.667 [0.000, 3.000], mean observation: 0.004 [-1.213, 3.877], loss: 1.320493, mean_absolute_error: 20.582479, mean_q: 11.599102, mean_eps: 0.826822
  192604/2000000: episode: 1985, duration: 1.702s, episode steps: 118, steps per second: 69, episode reward: -201.681, mean reward: -1.709 [-100.000, 10.543], mean action: 1.475 [0.000, 3.000], mean observation: -0.088 [-1.463, 1.000], loss: 1.821777, mean_absolute_error: 21.668381, mean_q: 9.658183, mean_eps: 0.826710
  192734/2000000: episode: 1986, duration: 1.884s, episode steps: 130, steps per second: 69, episode reward: -318.145, mean reward: -2.447 [-100.000, 1.665], mean action: 1.615 [0.000, 3.000], mean observation: 0.116 [-1.473, 1.352], loss: 1.547795, mean_absolute_error: 20.684350, mean_q: 10.917832, mean_eps: 0.826599
  192799/2000000: episode: 1987, duration: 0.926s, episode steps: 65, steps per second: 70, episode reward: -98.824, mean reward: -1.520 [-100.000, 11.129], mean action: 1.554 [0.000, 3.000], mean observation: 0.075 [-1.213, 4.285], loss: 1.311870, mean_absolute_error: 20.208057, mean_q: 12.046448, mean_eps: 0.826511
  192945/2000000: episode: 1988, duration: 2.110s, episode steps: 146, steps per second: 69, episode reward: -138.842, mean reward: -0.951 [-100.000, 7.409], mean action: 1.658 [0.000, 3.000], mean observation: 0.109 [-4.047, 1.000], loss: 2.091823, mean_absolute_error: 21.416740, mean_q: 9.883194, mean_eps: 0.826415
  193029/2000000: episode: 1989, duration: 1.219s, episode steps: 84, steps per second: 69, episode reward: -121.676, mean reward: -1.449 [-100.000, 12.486], mean action: 1.833 [0.000, 3.000], mean observation: -0.046 [-1.181, 4.070], loss: 1.394861, mean_absolute_error: 21.157253, mean_q: 11.372342, mean_eps: 0.826311
  193100/2000000: episode: 1990, duration: 1.020s, episode steps: 71, steps per second: 70, episode reward: -119.865, mean reward: -1.688 [-100.000, 11.432], mean action: 1.366 [0.000, 3.000], mean observation: 0.059 [-1.216, 1.000], loss: 1.698720, mean_absolute_error: 20.254660, mean_q: 11.850744, mean_eps: 0.826242
  193206/2000000: episode: 1991, duration: 1.521s, episode steps: 106, steps per second: 70, episode reward: -123.084, mean reward: -1.161 [-100.000, 10.340], mean action: 1.528 [0.000, 3.000], mean observation: 0.023 [-4.279, 1.000], loss: 2.060418, mean_absolute_error: 21.335211, mean_q: 9.471796, mean_eps: 0.826163
  193306/2000000: episode: 1992, duration: 1.435s, episode steps: 100, steps per second: 70, episode reward: -127.445, mean reward: -1.274 [-100.000, 19.640], mean action: 1.720 [0.000, 3.000], mean observation: -0.084 [-1.116, 3.548], loss: 1.521686, mean_absolute_error: 21.227163, mean_q: 10.813698, mean_eps: 0.826070
  193384/2000000: episode: 1993, duration: 1.146s, episode steps: 78, steps per second: 68, episode reward: -113.895, mean reward: -1.460 [-100.000, 15.700], mean action: 1.513 [0.000, 3.000], mean observation: -0.081 [-1.314, 1.000], loss: 1.356402, mean_absolute_error: 22.159420, mean_q: 8.972400, mean_eps: 0.825990
  193450/2000000: episode: 1994, duration: 0.965s, episode steps: 66, steps per second: 68, episode reward: -101.363, mean reward: -1.536 [-100.000, 14.726], mean action: 1.515 [0.000, 3.000], mean observation: -0.050 [-1.299, 4.765], loss: 1.401601, mean_absolute_error: 20.714867, mean_q: 12.095036, mean_eps: 0.825926
  193606/2000000: episode: 1995, duration: 2.250s, episode steps: 156, steps per second: 69, episode reward: -239.119, mean reward: -1.533 [-100.000, 4.966], mean action: 1.513 [0.000, 3.000], mean observation: 0.193 [-1.147, 4.069], loss: 1.388841, mean_absolute_error: 21.321477, mean_q: 11.055551, mean_eps: 0.825825
  193679/2000000: episode: 1996, duration: 1.054s, episode steps: 73, steps per second: 69, episode reward: -62.119, mean reward: -0.851 [-100.000, 21.275], mean action: 1.671 [0.000, 3.000], mean observation: -0.034 [-1.168, 1.000], loss: 1.572851, mean_absolute_error: 21.021968, mean_q: 11.002991, mean_eps: 0.825722
  193842/2000000: episode: 1997, duration: 2.340s, episode steps: 163, steps per second: 70, episode reward: -188.494, mean reward: -1.156 [-100.000, 28.925], mean action: 1.368 [0.000, 3.000], mean observation: 0.171 [-1.098, 1.379], loss: 1.567780, mean_absolute_error: 20.770349, mean_q: 10.009851, mean_eps: 0.825616
  193932/2000000: episode: 1998, duration: 1.325s, episode steps: 90, steps per second: 68, episode reward: -172.909, mean reward: -1.921 [-100.000, 9.510], mean action: 1.467 [0.000, 3.000], mean observation: 0.014 [-1.454, 5.339], loss: 1.930692, mean_absolute_error: 21.642902, mean_q: 11.633304, mean_eps: 0.825503
  194014/2000000: episode: 1999, duration: 1.185s, episode steps: 82, steps per second: 69, episode reward: -67.065, mean reward: -0.818 [-100.000, 17.085], mean action: 1.622 [0.000, 3.000], mean observation: -0.031 [-1.087, 1.000], loss: 1.606667, mean_absolute_error: 21.644104, mean_q: 11.366707, mean_eps: 0.825425
  194125/2000000: episode: 2000, duration: 1.599s, episode steps: 111, steps per second: 69, episode reward: -116.397, mean reward: -1.049 [-100.000, 10.814], mean action: 1.532 [0.000, 3.000], mean observation: 0.073 [-1.119, 3.887], loss: 1.015089, mean_absolute_error: 20.587408, mean_q: 10.092890, mean_eps: 0.825337
  194207/2000000: episode: 2001, duration: 1.160s, episode steps: 82, steps per second: 71, episode reward: -118.761, mean reward: -1.448 [-100.000, 14.745], mean action: 1.683 [0.000, 3.000], mean observation: -0.046 [-1.265, 1.000], loss: 1.356091, mean_absolute_error: 20.986533, mean_q: 10.640203, mean_eps: 0.825251
  194280/2000000: episode: 2002, duration: 1.076s, episode steps: 73, steps per second: 68, episode reward: -79.260, mean reward: -1.086 [-100.000, 14.778], mean action: 1.575 [0.000, 3.000], mean observation: -0.050 [-1.172, 1.000], loss: 2.337926, mean_absolute_error: 21.866829, mean_q: 8.097458, mean_eps: 0.825182
  194376/2000000: episode: 2003, duration: 1.422s, episode steps: 96, steps per second: 67, episode reward: -84.393, mean reward: -0.879 [-100.000, 17.129], mean action: 1.469 [0.000, 3.000], mean observation: -0.011 [-1.114, 1.000], loss: 1.073992, mean_absolute_error: 20.006053, mean_q: 13.318160, mean_eps: 0.825107
  194515/2000000: episode: 2004, duration: 1.985s, episode steps: 139, steps per second: 70, episode reward: -163.312, mean reward: -1.175 [-100.000, 10.896], mean action: 1.647 [0.000, 3.000], mean observation: 0.033 [-1.288, 4.430], loss: 2.078572, mean_absolute_error: 20.984385, mean_q: 10.799848, mean_eps: 0.825000
  194644/2000000: episode: 2005, duration: 1.874s, episode steps: 129, steps per second: 69, episode reward: -140.324, mean reward: -1.088 [-100.000, 16.133], mean action: 1.504 [0.000, 3.000], mean observation: 0.029 [-4.099, 1.013], loss: 1.515217, mean_absolute_error: 20.791791, mean_q: 10.952106, mean_eps: 0.824880
  194762/2000000: episode: 2006, duration: 1.702s, episode steps: 118, steps per second: 69, episode reward: -122.913, mean reward: -1.042 [-100.000, 13.155], mean action: 1.669 [0.000, 3.000], mean observation: -0.016 [-1.174, 3.990], loss: 1.570011, mean_absolute_error: 21.579741, mean_q: 10.177846, mean_eps: 0.824768
  194880/2000000: episode: 2007, duration: 1.723s, episode steps: 118, steps per second: 68, episode reward: -215.629, mean reward: -1.827 [-100.000, 90.281], mean action: 1.381 [0.000, 3.000], mean observation: 0.196 [-1.414, 1.875], loss: 1.740365, mean_absolute_error: 21.701642, mean_q: 8.843004, mean_eps: 0.824662
  194970/2000000: episode: 2008, duration: 1.319s, episode steps: 90, steps per second: 68, episode reward: -155.562, mean reward: -1.728 [-100.000, 9.729], mean action: 1.667 [0.000, 3.000], mean observation: -0.056 [-1.251, 3.709], loss: 1.679893, mean_absolute_error: 21.224168, mean_q: 10.943329, mean_eps: 0.824568
  195087/2000000: episode: 2009, duration: 1.691s, episode steps: 117, steps per second: 69, episode reward: -204.274, mean reward: -1.746 [-100.000, 6.954], mean action: 1.538 [0.000, 3.000], mean observation: 0.124 [-3.752, 1.054], loss: 2.286696, mean_absolute_error: 21.516649, mean_q: 9.837667, mean_eps: 0.824475
  195166/2000000: episode: 2010, duration: 1.142s, episode steps: 79, steps per second: 69, episode reward: -123.142, mean reward: -1.559 [-100.000, 6.409], mean action: 1.494 [0.000, 3.000], mean observation: 0.088 [-1.060, 3.972], loss: 1.344287, mean_absolute_error: 21.668284, mean_q: 9.989610, mean_eps: 0.824387
  195261/2000000: episode: 2011, duration: 1.524s, episode steps: 95, steps per second: 62, episode reward: -147.467, mean reward: -1.552 [-100.000, 7.035], mean action: 1.516 [0.000, 3.000], mean observation: 0.059 [-4.089, 1.000], loss: 2.041612, mean_absolute_error: 21.084718, mean_q: 10.215277, mean_eps: 0.824307
  195383/2000000: episode: 2012, duration: 1.737s, episode steps: 122, steps per second: 70, episode reward: -102.237, mean reward: -0.838 [-100.000, 17.283], mean action: 1.615 [0.000, 3.000], mean observation: 0.105 [-3.146, 1.000], loss: 1.607373, mean_absolute_error: 22.501607, mean_q: 8.827013, mean_eps: 0.824210
  195456/2000000: episode: 2013, duration: 1.078s, episode steps: 73, steps per second: 68, episode reward: -191.709, mean reward: -2.626 [-100.000, 16.139], mean action: 1.671 [0.000, 3.000], mean observation: -0.133 [-1.605, 1.000], loss: 2.056064, mean_absolute_error: 22.011932, mean_q: 7.600838, mean_eps: 0.824124
  195524/2000000: episode: 2014, duration: 1.008s, episode steps: 68, steps per second: 67, episode reward: -114.151, mean reward: -1.679 [-100.000, 7.655], mean action: 1.618 [0.000, 3.000], mean observation: -0.049 [-1.376, 4.895], loss: 1.661772, mean_absolute_error: 20.916525, mean_q: 11.879146, mean_eps: 0.824061
  195658/2000000: episode: 2015, duration: 1.948s, episode steps: 134, steps per second: 69, episode reward: -163.914, mean reward: -1.223 [-100.000, 3.127], mean action: 1.716 [0.000, 3.000], mean observation: 0.164 [-1.094, 3.288], loss: 1.860079, mean_absolute_error: 20.720238, mean_q: 10.780724, mean_eps: 0.823969
  195729/2000000: episode: 2016, duration: 1.057s, episode steps: 71, steps per second: 67, episode reward: -102.405, mean reward: -1.442 [-100.000, 6.342], mean action: 1.338 [0.000, 3.000], mean observation: 0.079 [-1.234, 4.808], loss: 1.884607, mean_absolute_error: 21.378596, mean_q: 10.578443, mean_eps: 0.823875
  195832/2000000: episode: 2017, duration: 1.608s, episode steps: 103, steps per second: 64, episode reward: -63.238, mean reward: -0.614 [-100.000, 23.888], mean action: 1.718 [0.000, 3.000], mean observation: -0.151 [-1.251, 1.171], loss: 1.319754, mean_absolute_error: 21.152326, mean_q: 11.811516, mean_eps: 0.823798
  195906/2000000: episode: 2018, duration: 1.075s, episode steps: 74, steps per second: 69, episode reward: -317.118, mean reward: -4.285 [-100.000, 5.059], mean action: 1.622 [0.000, 3.000], mean observation: -0.007 [-4.515, 1.384], loss: 1.244196, mean_absolute_error: 21.011145, mean_q: 10.272528, mean_eps: 0.823719
  196026/2000000: episode: 2019, duration: 1.716s, episode steps: 120, steps per second: 70, episode reward: -150.984, mean reward: -1.258 [-100.000, 6.319], mean action: 1.433 [0.000, 3.000], mean observation: 0.145 [-4.288, 1.011], loss: 1.521716, mean_absolute_error: 20.547628, mean_q: 11.131394, mean_eps: 0.823631
  196111/2000000: episode: 2020, duration: 1.213s, episode steps: 85, steps per second: 70, episode reward: -99.351, mean reward: -1.169 [-100.000, 8.235], mean action: 1.800 [0.000, 3.000], mean observation: -0.095 [-0.934, 3.139], loss: 1.566817, mean_absolute_error: 20.390159, mean_q: 12.611672, mean_eps: 0.823539
  196210/2000000: episode: 2021, duration: 1.403s, episode steps: 99, steps per second: 71, episode reward: -141.288, mean reward: -1.427 [-100.000, 8.546], mean action: 1.283 [0.000, 3.000], mean observation: 0.076 [-3.259, 1.000], loss: 1.688627, mean_absolute_error: 20.434455, mean_q: 12.647250, mean_eps: 0.823456
  196305/2000000: episode: 2022, duration: 1.397s, episode steps: 95, steps per second: 68, episode reward: -201.590, mean reward: -2.122 [-100.000, 19.051], mean action: 1.568 [0.000, 3.000], mean observation: -0.121 [-1.396, 1.000], loss: 1.873461, mean_absolute_error: 21.496221, mean_q: 8.640522, mean_eps: 0.823368
  196374/2000000: episode: 2023, duration: 0.981s, episode steps: 69, steps per second: 70, episode reward: -130.988, mean reward: -1.898 [-100.000, 11.594], mean action: 1.246 [0.000, 3.000], mean observation: 0.062 [-1.358, 4.993], loss: 2.012526, mean_absolute_error: 21.847791, mean_q: 9.917620, mean_eps: 0.823294
  196459/2000000: episode: 2024, duration: 1.216s, episode steps: 85, steps per second: 70, episode reward: -96.122, mean reward: -1.131 [-100.000, 18.292], mean action: 1.565 [0.000, 3.000], mean observation: -0.028 [-1.176, 1.000], loss: 1.816751, mean_absolute_error: 21.493193, mean_q: 9.878425, mean_eps: 0.823226
  196532/2000000: episode: 2025, duration: 1.085s, episode steps: 73, steps per second: 67, episode reward: -111.713, mean reward: -1.530 [-100.000, 6.855], mean action: 1.342 [0.000, 3.000], mean observation: 0.048 [-4.095, 1.000], loss: 1.882812, mean_absolute_error: 21.177031, mean_q: 11.931008, mean_eps: 0.823155
  196594/2000000: episode: 2026, duration: 0.925s, episode steps: 62, steps per second: 67, episode reward: -130.962, mean reward: -2.112 [-100.000, 14.907], mean action: 1.645 [0.000, 3.000], mean observation: -0.092 [-5.877, 1.000], loss: 1.479886, mean_absolute_error: 22.632557, mean_q: 9.359071, mean_eps: 0.823094
  196730/2000000: episode: 2027, duration: 1.935s, episode steps: 136, steps per second: 70, episode reward: -130.274, mean reward: -0.958 [-100.000, 6.797], mean action: 1.529 [0.000, 3.000], mean observation: 0.174 [-0.961, 3.336], loss: 1.973641, mean_absolute_error: 22.876233, mean_q: 9.413073, mean_eps: 0.823004
  196862/2000000: episode: 2028, duration: 1.883s, episode steps: 132, steps per second: 70, episode reward: -264.256, mean reward: -2.002 [-100.000, 6.351], mean action: 1.644 [0.000, 3.000], mean observation: -0.165 [-2.440, 1.051], loss: 1.732519, mean_absolute_error: 21.437746, mean_q: 9.878076, mean_eps: 0.822884
  196981/2000000: episode: 2029, duration: 1.758s, episode steps: 119, steps per second: 68, episode reward: -189.628, mean reward: -1.594 [-100.000, 3.756], mean action: 1.504 [0.000, 3.000], mean observation: 0.203 [-1.199, 4.018], loss: 2.005534, mean_absolute_error: 21.587772, mean_q: 8.665549, mean_eps: 0.822770
  197050/2000000: episode: 2030, duration: 0.970s, episode steps: 69, steps per second: 71, episode reward: -106.438, mean reward: -1.543 [-100.000, 13.499], mean action: 1.449 [0.000, 3.000], mean observation: 0.009 [-1.435, 1.000], loss: 1.202190, mean_absolute_error: 20.606222, mean_q: 11.083237, mean_eps: 0.822686
  197144/2000000: episode: 2031, duration: 1.378s, episode steps: 94, steps per second: 68, episode reward: -346.987, mean reward: -3.691 [-100.000, 112.950], mean action: 1.660 [0.000, 3.000], mean observation: -0.054 [-3.060, 1.000], loss: 1.747920, mean_absolute_error: 22.884570, mean_q: 8.310426, mean_eps: 0.822614
  197300/2000000: episode: 2032, duration: 2.276s, episode steps: 156, steps per second: 69, episode reward: -77.416, mean reward: -0.496 [-100.000, 19.408], mean action: 1.673 [0.000, 3.000], mean observation: 0.045 [-2.746, 1.037], loss: 1.525320, mean_absolute_error: 20.719733, mean_q: 10.461390, mean_eps: 0.822502
  197401/2000000: episode: 2033, duration: 1.475s, episode steps: 101, steps per second: 68, episode reward: -154.336, mean reward: -1.528 [-100.000, 7.948], mean action: 1.624 [0.000, 3.000], mean observation: -0.069 [-1.184, 4.361], loss: 1.367898, mean_absolute_error: 20.786209, mean_q: 11.464697, mean_eps: 0.822385
  197500/2000000: episode: 2034, duration: 1.401s, episode steps: 99, steps per second: 71, episode reward: -150.372, mean reward: -1.519 [-100.000, 7.815], mean action: 1.545 [0.000, 3.000], mean observation: 0.008 [-1.293, 4.653], loss: 1.648871, mean_absolute_error: 21.278290, mean_q: 11.369247, mean_eps: 0.822295
  197604/2000000: episode: 2035, duration: 1.533s, episode steps: 104, steps per second: 68, episode reward: -196.495, mean reward: -1.889 [-100.000, 7.660], mean action: 1.462 [0.000, 3.000], mean observation: 0.153 [-3.065, 1.365], loss: 1.868049, mean_absolute_error: 20.816252, mean_q: 13.053080, mean_eps: 0.822205
  197737/2000000: episode: 2036, duration: 1.933s, episode steps: 133, steps per second: 69, episode reward: -114.505, mean reward: -0.861 [-100.000, 17.359], mean action: 1.609 [0.000, 3.000], mean observation: 0.017 [-1.162, 1.000], loss: 1.458160, mean_absolute_error: 21.219090, mean_q: 11.689735, mean_eps: 0.822097
  197794/2000000: episode: 2037, duration: 0.809s, episode steps: 57, steps per second: 70, episode reward: -108.642, mean reward: -1.906 [-100.000, 15.617], mean action: 1.684 [0.000, 3.000], mean observation: -0.117 [-5.568, 1.000], loss: 1.915940, mean_absolute_error: 19.938214, mean_q: 13.242032, mean_eps: 0.822011
  197864/2000000: episode: 2038, duration: 1.017s, episode steps: 70, steps per second: 69, episode reward: -97.179, mean reward: -1.388 [-100.000, 7.664], mean action: 1.600 [0.000, 3.000], mean observation: -0.127 [-1.215, 4.096], loss: 1.842640, mean_absolute_error: 21.778991, mean_q: 8.709395, mean_eps: 0.821955
  197982/2000000: episode: 2039, duration: 1.700s, episode steps: 118, steps per second: 69, episode reward: -91.345, mean reward: -0.774 [-100.000, 13.091], mean action: 1.602 [0.000, 3.000], mean observation: 0.003 [-2.880, 1.000], loss: 1.971820, mean_absolute_error: 21.476744, mean_q: 11.030361, mean_eps: 0.821870
  198072/2000000: episode: 2040, duration: 1.305s, episode steps: 90, steps per second: 69, episode reward: -114.819, mean reward: -1.276 [-100.000, 29.938], mean action: 1.622 [0.000, 3.000], mean observation: -0.072 [-1.278, 1.000], loss: 1.536430, mean_absolute_error: 20.634798, mean_q: 10.156814, mean_eps: 0.821777
  198157/2000000: episode: 2041, duration: 1.244s, episode steps: 85, steps per second: 68, episode reward: -108.700, mean reward: -1.279 [-100.000, 11.656], mean action: 1.600 [0.000, 3.000], mean observation: 0.044 [-3.752, 1.000], loss: 2.194666, mean_absolute_error: 21.835190, mean_q: 8.742760, mean_eps: 0.821697
  198299/2000000: episode: 2042, duration: 2.000s, episode steps: 142, steps per second: 71, episode reward: -221.922, mean reward: -1.563 [-100.000, 7.225], mean action: 1.662 [0.000, 3.000], mean observation: -0.056 [-1.313, 1.000], loss: 1.187595, mean_absolute_error: 20.730233, mean_q: 12.158133, mean_eps: 0.821595
  198381/2000000: episode: 2043, duration: 1.231s, episode steps: 82, steps per second: 67, episode reward: -130.793, mean reward: -1.595 [-100.000, 12.702], mean action: 1.573 [0.000, 3.000], mean observation: -0.025 [-1.183, 3.840], loss: 1.534709, mean_absolute_error: 21.385120, mean_q: 9.641202, mean_eps: 0.821494
  198459/2000000: episode: 2044, duration: 1.093s, episode steps: 78, steps per second: 71, episode reward: -91.377, mean reward: -1.171 [-100.000, 17.224], mean action: 1.731 [0.000, 3.000], mean observation: -0.143 [-1.103, 1.000], loss: 1.947517, mean_absolute_error: 22.550230, mean_q: 10.442409, mean_eps: 0.821422
  198579/2000000: episode: 2045, duration: 1.708s, episode steps: 120, steps per second: 70, episode reward: -123.249, mean reward: -1.027 [-100.000, 15.867], mean action: 1.575 [0.000, 3.000], mean observation: -0.017 [-1.179, 1.000], loss: 1.963105, mean_absolute_error: 21.525719, mean_q: 9.576441, mean_eps: 0.821334
  198660/2000000: episode: 2046, duration: 1.200s, episode steps: 81, steps per second: 67, episode reward: -96.837, mean reward: -1.196 [-100.000, 10.876], mean action: 1.506 [0.000, 3.000], mean observation: 0.028 [-1.120, 3.970], loss: 1.359613, mean_absolute_error: 20.560632, mean_q: 12.136890, mean_eps: 0.821244
  198737/2000000: episode: 2047, duration: 1.149s, episode steps: 77, steps per second: 67, episode reward: -126.598, mean reward: -1.644 [-100.000, 7.245], mean action: 1.623 [0.000, 3.000], mean observation: -0.071 [-4.474, 1.000], loss: 1.636551, mean_absolute_error: 20.737361, mean_q: 12.700145, mean_eps: 0.821172
  198847/2000000: episode: 2048, duration: 1.640s, episode steps: 110, steps per second: 67, episode reward: -129.153, mean reward: -1.174 [-100.000, 15.844], mean action: 1.618 [0.000, 3.000], mean observation: 0.084 [-3.732, 1.000], loss: 1.620032, mean_absolute_error: 21.340740, mean_q: 11.434256, mean_eps: 0.821087
  198922/2000000: episode: 2049, duration: 1.100s, episode steps: 75, steps per second: 68, episode reward: -156.505, mean reward: -2.087 [-100.000, 7.389], mean action: 1.680 [0.000, 3.000], mean observation: -0.120 [-4.486, 1.000], loss: 2.853792, mean_absolute_error: 22.415786, mean_q: 9.225517, mean_eps: 0.821004
  199035/2000000: episode: 2050, duration: 1.637s, episode steps: 113, steps per second: 69, episode reward: -107.333, mean reward: -0.950 [-100.000, 13.049], mean action: 1.496 [0.000, 3.000], mean observation: 0.034 [-1.131, 1.000], loss: 1.550639, mean_absolute_error: 20.924372, mean_q: 10.636351, mean_eps: 0.820920
  199142/2000000: episode: 2051, duration: 1.552s, episode steps: 107, steps per second: 69, episode reward: -156.338, mean reward: -1.461 [-100.000, 6.613], mean action: 1.542 [0.000, 3.000], mean observation: -0.014 [-1.227, 1.000], loss: 1.553774, mean_absolute_error: 21.909454, mean_q: 11.258767, mean_eps: 0.820821
  199263/2000000: episode: 2052, duration: 1.740s, episode steps: 121, steps per second: 70, episode reward: -166.104, mean reward: -1.373 [-100.000, 12.193], mean action: 1.744 [0.000, 3.000], mean observation: -0.065 [-1.116, 3.334], loss: 1.641881, mean_absolute_error: 21.216380, mean_q: 9.592196, mean_eps: 0.820718
  199330/2000000: episode: 2053, duration: 0.978s, episode steps: 67, steps per second: 69, episode reward: -129.315, mean reward: -1.930 [-100.000, 6.212], mean action: 1.716 [0.000, 3.000], mean observation: -0.094 [-1.429, 4.824], loss: 1.629317, mean_absolute_error: 21.790571, mean_q: 9.890668, mean_eps: 0.820634
  199470/2000000: episode: 2054, duration: 2.002s, episode steps: 140, steps per second: 70, episode reward: -116.484, mean reward: -0.832 [-100.000, 35.187], mean action: 1.593 [0.000, 3.000], mean observation: -0.066 [-1.461, 1.992], loss: 1.310387, mean_absolute_error: 20.955693, mean_q: 10.455447, mean_eps: 0.820540
  199538/2000000: episode: 2055, duration: 0.972s, episode steps: 68, steps per second: 70, episode reward: -152.678, mean reward: -2.245 [-100.000, 12.147], mean action: 1.397 [0.000, 3.000], mean observation: 0.091 [-3.230, 1.000], loss: 1.307375, mean_absolute_error: 19.977777, mean_q: 13.251715, mean_eps: 0.820446
  199699/2000000: episode: 2056, duration: 2.320s, episode steps: 161, steps per second: 69, episode reward: -349.056, mean reward: -2.168 [-100.000, 3.566], mean action: 1.758 [0.000, 3.000], mean observation: 0.189 [-1.987, 1.589], loss: 1.794657, mean_absolute_error: 20.785650, mean_q: 10.685059, mean_eps: 0.820344
  199814/2000000: episode: 2057, duration: 1.674s, episode steps: 115, steps per second: 69, episode reward: -226.207, mean reward: -1.967 [-100.000, 50.177], mean action: 1.565 [0.000, 3.000], mean observation: -0.127 [-2.630, 1.000], loss: 1.700769, mean_absolute_error: 21.506329, mean_q: 12.785034, mean_eps: 0.820220
  199881/2000000: episode: 2058, duration: 0.989s, episode steps: 67, steps per second: 68, episode reward: -149.420, mean reward: -2.230 [-100.000, 8.728], mean action: 1.567 [0.000, 3.000], mean observation: 0.028 [-1.416, 4.105], loss: 1.267697, mean_absolute_error: 19.484573, mean_q: 14.415687, mean_eps: 0.820137
  200011/2000000: episode: 2059, duration: 1.861s, episode steps: 130, steps per second: 70, episode reward: -162.077, mean reward: -1.247 [-100.000, 9.184], mean action: 1.554 [0.000, 3.000], mean observation: 0.045 [-3.603, 1.000], loss: 1.330521, mean_absolute_error: 20.016415, mean_q: 11.933151, mean_eps: 0.820049
  200158/2000000: episode: 2060, duration: 2.128s, episode steps: 147, steps per second: 69, episode reward: -174.705, mean reward: -1.188 [-100.000, 6.775], mean action: 1.721 [0.000, 3.000], mean observation: 0.122 [-0.878, 3.544], loss: 1.826461, mean_absolute_error: 22.119687, mean_q: 11.514335, mean_eps: 0.819924
  200221/2000000: episode: 2061, duration: 0.932s, episode steps: 63, steps per second: 68, episode reward: -115.937, mean reward: -1.840 [-100.000, 8.713], mean action: 1.508 [0.000, 3.000], mean observation: 0.059 [-4.783, 1.000], loss: 1.386004, mean_absolute_error: 23.029569, mean_q: 11.540598, mean_eps: 0.819829
  200326/2000000: episode: 2062, duration: 1.501s, episode steps: 105, steps per second: 70, episode reward: -208.687, mean reward: -1.987 [-100.000, 20.050], mean action: 1.714 [0.000, 3.000], mean observation: -0.111 [-1.711, 1.000], loss: 1.982565, mean_absolute_error: 22.067481, mean_q: 10.456600, mean_eps: 0.819753
  200422/2000000: episode: 2063, duration: 1.396s, episode steps: 96, steps per second: 69, episode reward: -131.744, mean reward: -1.372 [-100.000, 12.787], mean action: 1.656 [0.000, 3.000], mean observation: -0.022 [-1.167, 4.009], loss: 1.960804, mean_absolute_error: 22.499776, mean_q: 10.934517, mean_eps: 0.819663
  200539/2000000: episode: 2064, duration: 1.662s, episode steps: 117, steps per second: 70, episode reward: -123.428, mean reward: -1.055 [-100.000, 18.243], mean action: 1.658 [0.000, 3.000], mean observation: -0.072 [-3.686, 1.000], loss: 1.359291, mean_absolute_error: 21.782992, mean_q: 12.180569, mean_eps: 0.819568
  200684/2000000: episode: 2065, duration: 2.098s, episode steps: 145, steps per second: 69, episode reward: -210.926, mean reward: -1.455 [-100.000, 6.808], mean action: 1.524 [0.000, 3.000], mean observation: 0.087 [-3.304, 1.066], loss: 1.479104, mean_absolute_error: 22.023682, mean_q: 11.095450, mean_eps: 0.819451
  200766/2000000: episode: 2066, duration: 1.214s, episode steps: 82, steps per second: 68, episode reward: -133.133, mean reward: -1.624 [-100.000, 10.645], mean action: 1.841 [0.000, 3.000], mean observation: -0.037 [-1.287, 3.933], loss: 1.101529, mean_absolute_error: 21.599255, mean_q: 11.901724, mean_eps: 0.819348
  200874/2000000: episode: 2067, duration: 1.557s, episode steps: 108, steps per second: 69, episode reward: -161.260, mean reward: -1.493 [-100.000, 7.288], mean action: 1.509 [0.000, 3.000], mean observation: 0.158 [-1.333, 1.000], loss: 2.330449, mean_absolute_error: 22.958043, mean_q: 10.901674, mean_eps: 0.819262
  200958/2000000: episode: 2068, duration: 1.181s, episode steps: 84, steps per second: 71, episode reward: -138.872, mean reward: -1.653 [-100.000, 8.792], mean action: 1.440 [0.000, 3.000], mean observation: 0.075 [-2.635, 1.000], loss: 1.534000, mean_absolute_error: 23.036591, mean_q: 7.843687, mean_eps: 0.819176
  201031/2000000: episode: 2069, duration: 1.038s, episode steps: 73, steps per second: 70, episode reward: -144.347, mean reward: -1.977 [-100.000, 7.613], mean action: 1.685 [0.000, 3.000], mean observation: -0.089 [-1.402, 4.672], loss: 1.172813, mean_absolute_error: 21.605133, mean_q: 12.759706, mean_eps: 0.819105
  201138/2000000: episode: 2070, duration: 1.791s, episode steps: 107, steps per second: 60, episode reward: -152.811, mean reward: -1.428 [-100.000, 9.893], mean action: 1.458 [0.000, 3.000], mean observation: 0.035 [-1.463, 4.794], loss: 2.038546, mean_absolute_error: 22.368757, mean_q: 10.785214, mean_eps: 0.819024
  201204/2000000: episode: 2071, duration: 1.023s, episode steps: 66, steps per second: 65, episode reward: -108.879, mean reward: -1.650 [-100.000, 12.513], mean action: 1.697 [0.000, 3.000], mean observation: -0.144 [-4.517, 1.000], loss: 1.630418, mean_absolute_error: 22.132131, mean_q: 11.368232, mean_eps: 0.818947
  201280/2000000: episode: 2072, duration: 1.128s, episode steps: 76, steps per second: 67, episode reward: -91.074, mean reward: -1.198 [-100.000, 12.824], mean action: 1.526 [0.000, 3.000], mean observation: -0.090 [-1.211, 1.000], loss: 2.033668, mean_absolute_error: 22.023487, mean_q: 10.086258, mean_eps: 0.818884
  201369/2000000: episode: 2073, duration: 1.359s, episode steps: 89, steps per second: 65, episode reward: -133.682, mean reward: -1.502 [-100.000, 12.263], mean action: 1.629 [0.000, 3.000], mean observation: -0.002 [-1.182, 3.787], loss: 1.827436, mean_absolute_error: 21.834431, mean_q: 11.129184, mean_eps: 0.818808
  201437/2000000: episode: 2074, duration: 1.093s, episode steps: 68, steps per second: 62, episode reward: -136.863, mean reward: -2.013 [-100.000, 9.241], mean action: 1.324 [0.000, 3.000], mean observation: 0.086 [-1.431, 1.000], loss: 2.159687, mean_absolute_error: 21.752294, mean_q: 13.165777, mean_eps: 0.818736
  201504/2000000: episode: 2075, duration: 1.013s, episode steps: 67, steps per second: 66, episode reward: -150.916, mean reward: -2.252 [-100.000, 9.046], mean action: 1.597 [0.000, 3.000], mean observation: -0.067 [-1.452, 4.830], loss: 1.436870, mean_absolute_error: 21.478279, mean_q: 12.862697, mean_eps: 0.818677
  201652/2000000: episode: 2076, duration: 2.319s, episode steps: 148, steps per second: 64, episode reward: -80.062, mean reward: -0.541 [-100.000, 15.072], mean action: 1.541 [0.000, 3.000], mean observation: -0.025 [-1.046, 3.792], loss: 1.415748, mean_absolute_error: 21.818144, mean_q: 10.288858, mean_eps: 0.818582
  201776/2000000: episode: 2077, duration: 2.140s, episode steps: 124, steps per second: 58, episode reward: -182.262, mean reward: -1.470 [-100.000, 6.116], mean action: 1.548 [0.000, 3.000], mean observation: 0.193 [-1.258, 1.000], loss: 1.590961, mean_absolute_error: 21.809317, mean_q: 11.642441, mean_eps: 0.818459
  201878/2000000: episode: 2078, duration: 1.501s, episode steps: 102, steps per second: 68, episode reward: -163.588, mean reward: -1.604 [-100.000, 7.849], mean action: 1.775 [0.000, 3.000], mean observation: -0.095 [-1.337, 3.664], loss: 1.659367, mean_absolute_error: 23.662209, mean_q: 10.135331, mean_eps: 0.818357
  201980/2000000: episode: 2079, duration: 1.473s, episode steps: 102, steps per second: 69, episode reward: -136.159, mean reward: -1.335 [-100.000, 6.994], mean action: 1.549 [0.000, 3.000], mean observation: 0.025 [-1.127, 3.958], loss: 1.784650, mean_absolute_error: 21.118219, mean_q: 12.646038, mean_eps: 0.818265
  202063/2000000: episode: 2080, duration: 1.177s, episode steps: 83, steps per second: 71, episode reward: -190.536, mean reward: -2.296 [-100.000, 8.512], mean action: 1.663 [0.000, 3.000], mean observation: -0.121 [-4.391, 1.000], loss: 1.685974, mean_absolute_error: 20.236316, mean_q: 14.235345, mean_eps: 0.818182
  202178/2000000: episode: 2081, duration: 1.639s, episode steps: 115, steps per second: 70, episode reward: -126.402, mean reward: -1.099 [-100.000, 7.290], mean action: 1.504 [0.000, 3.000], mean observation: 0.053 [-1.155, 1.000], loss: 1.489012, mean_absolute_error: 21.284431, mean_q: 10.716683, mean_eps: 0.818092
  202313/2000000: episode: 2082, duration: 1.919s, episode steps: 135, steps per second: 70, episode reward: -139.272, mean reward: -1.032 [-100.000, 6.950], mean action: 1.585 [0.000, 3.000], mean observation: 0.036 [-1.249, 4.284], loss: 1.789951, mean_absolute_error: 23.455003, mean_q: 10.036289, mean_eps: 0.817979
  202405/2000000: episode: 2083, duration: 1.303s, episode steps: 92, steps per second: 71, episode reward: -138.726, mean reward: -1.508 [-100.000, 10.512], mean action: 1.489 [0.000, 3.000], mean observation: 0.089 [-1.359, 1.000], loss: 1.526894, mean_absolute_error: 21.775465, mean_q: 12.377763, mean_eps: 0.817876
  202534/2000000: episode: 2084, duration: 1.820s, episode steps: 129, steps per second: 71, episode reward: -301.349, mean reward: -2.336 [-100.000, 65.764], mean action: 1.643 [0.000, 3.000], mean observation: -0.193 [-3.423, 1.007], loss: 1.695640, mean_absolute_error: 22.633353, mean_q: 10.075044, mean_eps: 0.817777
  202600/2000000: episode: 2085, duration: 0.967s, episode steps: 66, steps per second: 68, episode reward: -160.469, mean reward: -2.431 [-100.000, 5.080], mean action: 1.455 [0.000, 3.000], mean observation: 0.030 [-4.531, 1.029], loss: 2.039445, mean_absolute_error: 22.433606, mean_q: 11.062059, mean_eps: 0.817691
  202741/2000000: episode: 2086, duration: 2.028s, episode steps: 141, steps per second: 70, episode reward: -189.662, mean reward: -1.345 [-100.000, 6.972], mean action: 1.617 [0.000, 3.000], mean observation: -0.125 [-1.355, 4.691], loss: 1.651010, mean_absolute_error: 21.763743, mean_q: 10.461998, mean_eps: 0.817597
  202810/2000000: episode: 2087, duration: 0.980s, episode steps: 69, steps per second: 70, episode reward: -94.634, mean reward: -1.372 [-100.000, 12.855], mean action: 1.565 [0.000, 3.000], mean observation: 0.086 [-3.585, 1.000], loss: 0.945913, mean_absolute_error: 22.029346, mean_q: 12.578534, mean_eps: 0.817502
  202905/2000000: episode: 2088, duration: 1.340s, episode steps: 95, steps per second: 71, episode reward: -177.180, mean reward: -1.865 [-100.000, 5.678], mean action: 1.705 [0.000, 3.000], mean observation: -0.159 [-0.959, 3.218], loss: 1.703757, mean_absolute_error: 21.532644, mean_q: 10.882683, mean_eps: 0.817428
  203026/2000000: episode: 2089, duration: 1.687s, episode steps: 121, steps per second: 72, episode reward: -155.489, mean reward: -1.285 [-100.000, 14.599], mean action: 1.463 [0.000, 3.000], mean observation: 0.135 [-4.363, 1.020], loss: 1.127041, mean_absolute_error: 21.267786, mean_q: 10.691120, mean_eps: 0.817331
  203138/2000000: episode: 2090, duration: 1.598s, episode steps: 112, steps per second: 70, episode reward: -160.247, mean reward: -1.431 [-100.000, 18.174], mean action: 1.554 [0.000, 3.000], mean observation: -0.030 [-1.665, 1.015], loss: 1.561475, mean_absolute_error: 22.132491, mean_q: 11.920213, mean_eps: 0.817226
  203264/2000000: episode: 2091, duration: 1.822s, episode steps: 126, steps per second: 69, episode reward: -236.964, mean reward: -1.881 [-100.000, 25.185], mean action: 1.524 [0.000, 3.000], mean observation: -0.091 [-4.902, 1.051], loss: 1.392208, mean_absolute_error: 22.179907, mean_q: 9.997533, mean_eps: 0.817120
  203405/2000000: episode: 2092, duration: 2.001s, episode steps: 141, steps per second: 70, episode reward: -103.189, mean reward: -0.732 [-100.000, 6.480], mean action: 1.511 [0.000, 3.000], mean observation: 0.044 [-3.505, 1.000], loss: 1.828933, mean_absolute_error: 21.608046, mean_q: 10.904657, mean_eps: 0.816999
  203498/2000000: episode: 2093, duration: 1.413s, episode steps: 93, steps per second: 66, episode reward: -92.115, mean reward: -0.990 [-100.000, 14.105], mean action: 1.613 [0.000, 3.000], mean observation: -0.008 [-1.043, 1.000], loss: 1.601657, mean_absolute_error: 21.410624, mean_q: 14.262768, mean_eps: 0.816893
  203596/2000000: episode: 2094, duration: 1.412s, episode steps: 98, steps per second: 69, episode reward: -140.006, mean reward: -1.429 [-100.000, 16.627], mean action: 1.449 [0.000, 3.000], mean observation: 0.086 [-1.185, 1.000], loss: 1.172616, mean_absolute_error: 21.506222, mean_q: 12.585957, mean_eps: 0.816809
  203695/2000000: episode: 2095, duration: 1.406s, episode steps: 99, steps per second: 70, episode reward: -178.277, mean reward: -1.801 [-100.000, 5.561], mean action: 1.505 [0.000, 3.000], mean observation: -0.093 [-1.422, 4.126], loss: 1.703840, mean_absolute_error: 21.954891, mean_q: 12.479669, mean_eps: 0.816720
  203786/2000000: episode: 2096, duration: 1.316s, episode steps: 91, steps per second: 69, episode reward: -152.328, mean reward: -1.674 [-100.000, 6.895], mean action: 1.549 [0.000, 3.000], mean observation: 0.089 [-1.259, 4.820], loss: 1.693991, mean_absolute_error: 22.161983, mean_q: 11.225017, mean_eps: 0.816634
  203858/2000000: episode: 2097, duration: 1.013s, episode steps: 72, steps per second: 71, episode reward: -126.224, mean reward: -1.753 [-100.000, 11.161], mean action: 1.736 [0.000, 3.000], mean observation: -0.123 [-4.059, 1.000], loss: 1.362155, mean_absolute_error: 22.609916, mean_q: 11.835773, mean_eps: 0.816560
  203936/2000000: episode: 2098, duration: 1.140s, episode steps: 78, steps per second: 68, episode reward: -139.474, mean reward: -1.788 [-100.000, 7.928], mean action: 1.782 [0.000, 3.000], mean observation: -0.150 [-1.332, 4.140], loss: 1.583889, mean_absolute_error: 21.628669, mean_q: 10.550861, mean_eps: 0.816494
  204030/2000000: episode: 2099, duration: 1.344s, episode steps: 94, steps per second: 70, episode reward: -261.296, mean reward: -2.780 [-100.000, 7.788], mean action: 1.766 [0.000, 3.000], mean observation: -0.111 [-2.621, 1.000], loss: 2.486179, mean_absolute_error: 22.883324, mean_q: 9.850569, mean_eps: 0.816416
  204146/2000000: episode: 2100, duration: 1.641s, episode steps: 116, steps per second: 71, episode reward: -156.976, mean reward: -1.353 [-100.000, 11.574], mean action: 1.647 [0.000, 3.000], mean observation: -0.063 [-1.313, 3.856], loss: 1.389010, mean_absolute_error: 20.746229, mean_q: 13.955878, mean_eps: 0.816321
  204225/2000000: episode: 2101, duration: 1.144s, episode steps: 79, steps per second: 69, episode reward: -86.514, mean reward: -1.095 [-100.000, 6.960], mean action: 1.696 [0.000, 3.000], mean observation: -0.074 [-1.061, 3.890], loss: 2.239039, mean_absolute_error: 22.231996, mean_q: 11.324130, mean_eps: 0.816233
  204348/2000000: episode: 2102, duration: 1.738s, episode steps: 123, steps per second: 71, episode reward: -168.262, mean reward: -1.368 [-100.000, 10.920], mean action: 1.610 [0.000, 3.000], mean observation: 0.105 [-1.235, 4.148], loss: 1.744488, mean_absolute_error: 21.851695, mean_q: 11.791647, mean_eps: 0.816143
  204491/2000000: episode: 2103, duration: 2.042s, episode steps: 143, steps per second: 70, episode reward: -297.951, mean reward: -2.084 [-100.000, 76.593], mean action: 1.490 [0.000, 3.000], mean observation: 0.210 [-1.236, 3.199], loss: 1.410496, mean_absolute_error: 21.319552, mean_q: 11.257442, mean_eps: 0.816024
  204569/2000000: episode: 2104, duration: 1.127s, episode steps: 78, steps per second: 69, episode reward: -123.263, mean reward: -1.580 [-100.000, 20.886], mean action: 1.603 [0.000, 3.000], mean observation: -0.093 [-4.047, 1.000], loss: 1.638811, mean_absolute_error: 24.017200, mean_q: 8.879731, mean_eps: 0.815923
  204700/2000000: episode: 2105, duration: 1.853s, episode steps: 131, steps per second: 71, episode reward: -214.845, mean reward: -1.640 [-100.000, 8.146], mean action: 1.595 [0.000, 3.000], mean observation: 0.184 [-1.727, 1.057], loss: 1.846430, mean_absolute_error: 21.524427, mean_q: 10.633200, mean_eps: 0.815829
  204870/2000000: episode: 2106, duration: 2.404s, episode steps: 170, steps per second: 71, episode reward: -133.922, mean reward: -0.788 [-100.000, 6.262], mean action: 1.494 [0.000, 3.000], mean observation: 0.153 [-1.077, 3.610], loss: 1.541990, mean_absolute_error: 21.905171, mean_q: 12.468335, mean_eps: 0.815694
  204947/2000000: episode: 2107, duration: 1.105s, episode steps: 77, steps per second: 70, episode reward: -154.622, mean reward: -2.008 [-100.000, 7.790], mean action: 1.558 [0.000, 3.000], mean observation: 0.112 [-1.347, 5.104], loss: 1.581437, mean_absolute_error: 21.649950, mean_q: 9.825493, mean_eps: 0.815583
  205060/2000000: episode: 2108, duration: 1.653s, episode steps: 113, steps per second: 68, episode reward: -249.287, mean reward: -2.206 [-100.000, 9.298], mean action: 1.416 [0.000, 3.000], mean observation: 0.091 [-1.620, 1.019], loss: 2.089188, mean_absolute_error: 22.589380, mean_q: 11.303753, mean_eps: 0.815498
  205170/2000000: episode: 2109, duration: 1.571s, episode steps: 110, steps per second: 70, episode reward: -163.690, mean reward: -1.488 [-100.000, 3.278], mean action: 1.418 [0.000, 3.000], mean observation: -0.162 [-3.539, 1.000], loss: 1.908088, mean_absolute_error: 22.152411, mean_q: 11.856558, mean_eps: 0.815397
  205273/2000000: episode: 2110, duration: 1.489s, episode steps: 103, steps per second: 69, episode reward: -153.301, mean reward: -1.488 [-100.000, 16.529], mean action: 1.728 [0.000, 3.000], mean observation: -0.094 [-1.220, 1.000], loss: 1.262745, mean_absolute_error: 21.667724, mean_q: 10.675541, mean_eps: 0.815300
  205423/2000000: episode: 2111, duration: 2.078s, episode steps: 150, steps per second: 72, episode reward: -116.858, mean reward: -0.779 [-100.000, 24.919], mean action: 1.620 [0.000, 3.000], mean observation: -0.102 [-1.249, 3.028], loss: 1.371392, mean_absolute_error: 22.303054, mean_q: 11.638049, mean_eps: 0.815187
  205511/2000000: episode: 2112, duration: 1.245s, episode steps: 88, steps per second: 71, episode reward: -183.168, mean reward: -2.081 [-100.000, 14.551], mean action: 1.670 [0.000, 3.000], mean observation: -0.099 [-4.065, 1.000], loss: 2.393177, mean_absolute_error: 22.638814, mean_q: 11.238304, mean_eps: 0.815081
  205579/2000000: episode: 2113, duration: 0.969s, episode steps: 68, steps per second: 70, episode reward: -139.441, mean reward: -2.051 [-100.000, 6.042], mean action: 1.338 [0.000, 3.000], mean observation: 0.073 [-5.186, 1.000], loss: 1.383817, mean_absolute_error: 22.932161, mean_q: 10.616973, mean_eps: 0.815010
  205654/2000000: episode: 2114, duration: 1.073s, episode steps: 75, steps per second: 70, episode reward: -132.159, mean reward: -1.762 [-100.000, 7.461], mean action: 1.733 [0.000, 3.000], mean observation: -0.074 [-1.300, 4.184], loss: 1.380328, mean_absolute_error: 20.745982, mean_q: 12.453029, mean_eps: 0.814946
  205723/2000000: episode: 2115, duration: 0.968s, episode steps: 69, steps per second: 71, episode reward: -77.570, mean reward: -1.124 [-100.000, 12.193], mean action: 1.754 [0.000, 3.000], mean observation: -0.042 [-1.253, 1.000], loss: 2.264836, mean_absolute_error: 22.356759, mean_q: 10.086149, mean_eps: 0.814881
  205792/2000000: episode: 2116, duration: 1.014s, episode steps: 69, steps per second: 68, episode reward: -171.824, mean reward: -2.490 [-100.000, 7.608], mean action: 1.667 [0.000, 3.000], mean observation: -0.084 [-1.590, 1.000], loss: 0.954045, mean_absolute_error: 21.143357, mean_q: 14.278348, mean_eps: 0.814820
  205892/2000000: episode: 2117, duration: 1.448s, episode steps: 100, steps per second: 69, episode reward: -99.818, mean reward: -0.998 [-100.000, 11.901], mean action: 1.790 [0.000, 3.000], mean observation: -0.095 [-3.216, 1.000], loss: 1.505484, mean_absolute_error: 20.936461, mean_q: 13.035136, mean_eps: 0.814744
  205982/2000000: episode: 2118, duration: 1.290s, episode steps: 90, steps per second: 70, episode reward: -95.093, mean reward: -1.057 [-100.000, 6.894], mean action: 1.556 [0.000, 3.000], mean observation: -0.116 [-0.972, 3.681], loss: 1.736962, mean_absolute_error: 22.445379, mean_q: 9.950714, mean_eps: 0.814658
  206060/2000000: episode: 2119, duration: 1.131s, episode steps: 78, steps per second: 69, episode reward: -91.770, mean reward: -1.177 [-100.000, 12.507], mean action: 1.859 [0.000, 3.000], mean observation: -0.108 [-1.073, 1.000], loss: 1.426699, mean_absolute_error: 21.408056, mean_q: 12.070215, mean_eps: 0.814582
  206220/2000000: episode: 2120, duration: 2.317s, episode steps: 160, steps per second: 69, episode reward: -34.438, mean reward: -0.215 [-100.000, 61.187], mean action: 1.594 [0.000, 3.000], mean observation: 0.125 [-2.172, 1.000], loss: 1.623501, mean_absolute_error: 21.833502, mean_q: 10.169600, mean_eps: 0.814476
  206348/2000000: episode: 2121, duration: 1.876s, episode steps: 128, steps per second: 68, episode reward: -215.562, mean reward: -1.684 [-100.000, 7.216], mean action: 1.688 [0.000, 3.000], mean observation: 0.143 [-1.170, 1.242], loss: 2.022637, mean_absolute_error: 22.378776, mean_q: 11.010392, mean_eps: 0.814346
  206434/2000000: episode: 2122, duration: 1.240s, episode steps: 86, steps per second: 69, episode reward: -180.742, mean reward: -2.102 [-100.000, 16.184], mean action: 1.733 [0.000, 3.000], mean observation: -0.188 [-1.341, 3.077], loss: 2.281836, mean_absolute_error: 23.278777, mean_q: 8.989554, mean_eps: 0.814249
  206559/2000000: episode: 2123, duration: 1.754s, episode steps: 125, steps per second: 71, episode reward: -102.909, mean reward: -0.823 [-100.000, 11.063], mean action: 1.488 [0.000, 3.000], mean observation: 0.054 [-3.240, 1.000], loss: 1.902113, mean_absolute_error: 22.332666, mean_q: 11.274929, mean_eps: 0.814154
  206657/2000000: episode: 2124, duration: 1.434s, episode steps: 98, steps per second: 68, episode reward: -127.594, mean reward: -1.302 [-100.000, 15.483], mean action: 1.561 [0.000, 3.000], mean observation: -0.015 [-1.215, 1.000], loss: 1.712066, mean_absolute_error: 22.054234, mean_q: 11.356375, mean_eps: 0.814053
  206801/2000000: episode: 2125, duration: 2.033s, episode steps: 144, steps per second: 71, episode reward: -269.086, mean reward: -1.869 [-100.000, 115.736], mean action: 1.438 [0.000, 3.000], mean observation: 0.064 [-1.567, 2.700], loss: 1.661044, mean_absolute_error: 21.141125, mean_q: 13.318450, mean_eps: 0.813943
  206914/2000000: episode: 2126, duration: 1.588s, episode steps: 113, steps per second: 71, episode reward: -107.855, mean reward: -0.954 [-100.000, 10.204], mean action: 1.513 [0.000, 3.000], mean observation: 0.003 [-1.225, 3.581], loss: 2.043881, mean_absolute_error: 22.549588, mean_q: 10.618387, mean_eps: 0.813828
  207010/2000000: episode: 2127, duration: 1.354s, episode steps: 96, steps per second: 71, episode reward: -147.610, mean reward: -1.538 [-100.000, 10.596], mean action: 1.573 [0.000, 3.000], mean observation: -0.129 [-4.612, 1.000], loss: 1.841416, mean_absolute_error: 22.134391, mean_q: 12.375892, mean_eps: 0.813734
  207096/2000000: episode: 2128, duration: 1.220s, episode steps: 86, steps per second: 70, episode reward: -133.764, mean reward: -1.555 [-100.000, 5.599], mean action: 1.640 [0.000, 3.000], mean observation: -0.090 [-1.239, 1.000], loss: 2.291354, mean_absolute_error: 22.663211, mean_q: 10.850793, mean_eps: 0.813653
  207218/2000000: episode: 2129, duration: 1.742s, episode steps: 122, steps per second: 70, episode reward: -157.533, mean reward: -1.291 [-100.000, 8.050], mean action: 1.557 [0.000, 3.000], mean observation: 0.127 [-3.888, 1.000], loss: 1.511451, mean_absolute_error: 22.404967, mean_q: 10.889262, mean_eps: 0.813560
  207293/2000000: episode: 2130, duration: 1.073s, episode steps: 75, steps per second: 70, episode reward: -195.920, mean reward: -2.612 [-100.000, 5.173], mean action: 1.520 [0.000, 3.000], mean observation: 0.033 [-1.485, 4.375], loss: 1.178674, mean_absolute_error: 20.912033, mean_q: 10.334610, mean_eps: 0.813470
  207383/2000000: episode: 2131, duration: 1.244s, episode steps: 90, steps per second: 72, episode reward: -161.139, mean reward: -1.790 [-100.000, 15.328], mean action: 1.500 [0.000, 3.000], mean observation: 0.063 [-4.581, 1.000], loss: 1.299954, mean_absolute_error: 20.695018, mean_q: 12.767049, mean_eps: 0.813396
  207460/2000000: episode: 2132, duration: 1.131s, episode steps: 77, steps per second: 68, episode reward: -94.990, mean reward: -1.234 [-100.000, 14.609], mean action: 1.416 [0.000, 3.000], mean observation: 0.090 [-3.467, 1.000], loss: 1.507937, mean_absolute_error: 20.768847, mean_q: 13.169811, mean_eps: 0.813322
  207552/2000000: episode: 2133, duration: 1.319s, episode steps: 92, steps per second: 70, episode reward: -81.732, mean reward: -0.888 [-100.000, 15.366], mean action: 1.511 [0.000, 3.000], mean observation: -0.013 [-3.476, 1.000], loss: 1.596084, mean_absolute_error: 21.631509, mean_q: 12.878960, mean_eps: 0.813246
  207627/2000000: episode: 2134, duration: 1.062s, episode steps: 75, steps per second: 71, episode reward: -113.710, mean reward: -1.516 [-100.000, 13.162], mean action: 1.733 [0.000, 3.000], mean observation: -0.065 [-4.269, 1.000], loss: 1.850082, mean_absolute_error: 21.826295, mean_q: 12.348495, mean_eps: 0.813171
  207691/2000000: episode: 2135, duration: 0.901s, episode steps: 64, steps per second: 71, episode reward: -150.838, mean reward: -2.357 [-100.000, 19.139], mean action: 1.719 [0.000, 3.000], mean observation: -0.131 [-1.482, 1.000], loss: 1.639951, mean_absolute_error: 22.880061, mean_q: 7.555420, mean_eps: 0.813108
  207773/2000000: episode: 2136, duration: 1.177s, episode steps: 82, steps per second: 70, episode reward: -148.234, mean reward: -1.808 [-100.000, 12.354], mean action: 1.585 [0.000, 3.000], mean observation: 0.129 [-1.277, 1.000], loss: 1.094408, mean_absolute_error: 21.833505, mean_q: 12.722056, mean_eps: 0.813041
  207891/2000000: episode: 2137, duration: 1.615s, episode steps: 118, steps per second: 73, episode reward: -100.597, mean reward: -0.853 [-100.000, 16.816], mean action: 1.602 [0.000, 3.000], mean observation: 0.002 [-1.148, 1.000], loss: 1.791679, mean_absolute_error: 21.001429, mean_q: 11.894460, mean_eps: 0.812951
  207971/2000000: episode: 2138, duration: 1.138s, episode steps: 80, steps per second: 70, episode reward: -119.950, mean reward: -1.499 [-100.000, 16.669], mean action: 1.600 [0.000, 3.000], mean observation: 0.006 [-4.598, 1.000], loss: 1.855813, mean_absolute_error: 22.574068, mean_q: 8.036797, mean_eps: 0.812863
  208063/2000000: episode: 2139, duration: 1.315s, episode steps: 92, steps per second: 70, episode reward: -243.612, mean reward: -2.648 [-100.000, 16.661], mean action: 1.522 [0.000, 3.000], mean observation: -0.195 [-4.515, 1.000], loss: 2.510585, mean_absolute_error: 23.537740, mean_q: 9.861948, mean_eps: 0.812786
  208136/2000000: episode: 2140, duration: 1.055s, episode steps: 73, steps per second: 69, episode reward: -126.380, mean reward: -1.731 [-100.000, 16.488], mean action: 1.507 [0.000, 3.000], mean observation: -0.045 [-1.399, 4.635], loss: 1.636869, mean_absolute_error: 21.675900, mean_q: 12.247865, mean_eps: 0.812712
  208255/2000000: episode: 2141, duration: 1.685s, episode steps: 119, steps per second: 71, episode reward: -147.107, mean reward: -1.236 [-100.000, 6.118], mean action: 1.748 [0.000, 3.000], mean observation: -0.130 [-1.147, 3.525], loss: 1.570669, mean_absolute_error: 21.949456, mean_q: 11.651836, mean_eps: 0.812625
  208354/2000000: episode: 2142, duration: 1.425s, episode steps: 99, steps per second: 69, episode reward: -142.315, mean reward: -1.438 [-100.000, 10.384], mean action: 1.525 [0.000, 3.000], mean observation: 0.154 [-1.788, 1.000], loss: 1.650275, mean_absolute_error: 21.667133, mean_q: 11.754115, mean_eps: 0.812526
  208417/2000000: episode: 2143, duration: 0.903s, episode steps: 63, steps per second: 70, episode reward: -129.074, mean reward: -2.049 [-100.000, 5.362], mean action: 1.381 [0.000, 3.000], mean observation: 0.010 [-3.430, 1.000], loss: 1.879680, mean_absolute_error: 23.454660, mean_q: 10.349212, mean_eps: 0.812453
  208490/2000000: episode: 2144, duration: 1.023s, episode steps: 73, steps per second: 71, episode reward: -105.205, mean reward: -1.441 [-100.000, 11.842], mean action: 1.630 [0.000, 3.000], mean observation: -0.070 [-3.676, 1.000], loss: 1.909049, mean_absolute_error: 23.092867, mean_q: 11.665186, mean_eps: 0.812391
  208575/2000000: episode: 2145, duration: 1.202s, episode steps: 85, steps per second: 71, episode reward: -140.286, mean reward: -1.650 [-100.000, 7.628], mean action: 1.682 [0.000, 3.000], mean observation: -0.084 [-4.619, 1.000], loss: 2.201790, mean_absolute_error: 23.062133, mean_q: 10.412339, mean_eps: 0.812321
  208659/2000000: episode: 2146, duration: 1.206s, episode steps: 84, steps per second: 70, episode reward: -107.856, mean reward: -1.284 [-100.000, 10.038], mean action: 1.607 [0.000, 3.000], mean observation: -0.085 [-4.077, 1.000], loss: 2.265242, mean_absolute_error: 22.575915, mean_q: 10.587134, mean_eps: 0.812246
  208779/2000000: episode: 2147, duration: 1.712s, episode steps: 120, steps per second: 70, episode reward: -150.064, mean reward: -1.251 [-100.000, 12.954], mean action: 1.683 [0.000, 3.000], mean observation: -0.119 [-1.342, 1.000], loss: 1.633497, mean_absolute_error: 21.883374, mean_q: 12.162240, mean_eps: 0.812154
  208879/2000000: episode: 2148, duration: 1.425s, episode steps: 100, steps per second: 70, episode reward: -186.316, mean reward: -1.863 [-100.000, 8.946], mean action: 1.800 [0.000, 3.000], mean observation: -0.164 [-1.222, 4.212], loss: 1.284821, mean_absolute_error: 21.522978, mean_q: 12.414594, mean_eps: 0.812055
  208993/2000000: episode: 2149, duration: 1.649s, episode steps: 114, steps per second: 69, episode reward: -152.800, mean reward: -1.340 [-100.000, 12.189], mean action: 1.570 [0.000, 3.000], mean observation: -0.077 [-1.160, 2.524], loss: 1.788596, mean_absolute_error: 21.433354, mean_q: 13.202381, mean_eps: 0.811958
  209094/2000000: episode: 2150, duration: 1.430s, episode steps: 101, steps per second: 71, episode reward: -106.640, mean reward: -1.056 [-100.000, 8.441], mean action: 1.475 [0.000, 3.000], mean observation: -0.060 [-3.804, 1.000], loss: 1.525496, mean_absolute_error: 21.367531, mean_q: 11.498463, mean_eps: 0.811860
  209214/2000000: episode: 2151, duration: 1.708s, episode steps: 120, steps per second: 70, episode reward: -165.567, mean reward: -1.380 [-100.000, 12.293], mean action: 1.717 [0.000, 3.000], mean observation: -0.147 [-1.313, 3.221], loss: 1.351041, mean_absolute_error: 22.584637, mean_q: 9.723579, mean_eps: 0.811761
  209276/2000000: episode: 2152, duration: 0.909s, episode steps: 62, steps per second: 68, episode reward: -128.999, mean reward: -2.081 [-100.000, 7.649], mean action: 1.548 [0.000, 3.000], mean observation: 0.062 [-3.850, 1.000], loss: 1.942957, mean_absolute_error: 23.239320, mean_q: 11.566188, mean_eps: 0.811680
  209364/2000000: episode: 2153, duration: 1.293s, episode steps: 88, steps per second: 68, episode reward: -128.825, mean reward: -1.464 [-100.000, 6.478], mean action: 1.580 [0.000, 3.000], mean observation: 0.038 [-3.119, 1.000], loss: 1.655845, mean_absolute_error: 21.864662, mean_q: 12.148874, mean_eps: 0.811614
  209468/2000000: episode: 2154, duration: 1.536s, episode steps: 104, steps per second: 68, episode reward: -150.795, mean reward: -1.450 [-100.000, 6.456], mean action: 1.442 [0.000, 3.000], mean observation: 0.093 [-1.262, 3.966], loss: 1.431607, mean_absolute_error: 20.651466, mean_q: 14.274006, mean_eps: 0.811527
  209564/2000000: episode: 2155, duration: 1.408s, episode steps: 96, steps per second: 68, episode reward: -154.952, mean reward: -1.614 [-100.000, 11.472], mean action: 1.510 [0.000, 3.000], mean observation: 0.106 [-3.022, 1.000], loss: 1.956290, mean_absolute_error: 22.080750, mean_q: 11.006603, mean_eps: 0.811437
  209652/2000000: episode: 2156, duration: 1.297s, episode steps: 88, steps per second: 68, episode reward: -121.689, mean reward: -1.383 [-100.000, 20.745], mean action: 1.534 [0.000, 3.000], mean observation: -0.104 [-1.143, 1.000], loss: 2.405692, mean_absolute_error: 22.563668, mean_q: 11.690948, mean_eps: 0.811355
  209744/2000000: episode: 2157, duration: 1.355s, episode steps: 92, steps per second: 68, episode reward: -84.795, mean reward: -0.922 [-100.000, 11.770], mean action: 1.630 [0.000, 3.000], mean observation: -0.010 [-3.473, 1.000], loss: 1.278754, mean_absolute_error: 20.744763, mean_q: 11.810263, mean_eps: 0.811274
  209850/2000000: episode: 2158, duration: 1.506s, episode steps: 106, steps per second: 70, episode reward: -181.582, mean reward: -1.713 [-100.000, 8.479], mean action: 1.491 [0.000, 3.000], mean observation: 0.002 [-5.639, 1.010], loss: 1.566828, mean_absolute_error: 21.744529, mean_q: 11.398638, mean_eps: 0.811184
  209947/2000000: episode: 2159, duration: 1.374s, episode steps: 97, steps per second: 71, episode reward: -122.036, mean reward: -1.258 [-100.000, 6.671], mean action: 1.608 [0.000, 3.000], mean observation: -0.045 [-1.064, 3.714], loss: 1.364982, mean_absolute_error: 21.051657, mean_q: 13.103330, mean_eps: 0.811092
  210066/2000000: episode: 2160, duration: 1.712s, episode steps: 119, steps per second: 70, episode reward: -342.940, mean reward: -2.882 [-100.000, 2.204], mean action: 1.672 [0.000, 3.000], mean observation: -0.165 [-1.734, 2.956], loss: 2.022971, mean_absolute_error: 23.747903, mean_q: 10.681792, mean_eps: 0.810995
  210149/2000000: episode: 2161, duration: 1.208s, episode steps: 83, steps per second: 69, episode reward: -144.788, mean reward: -1.744 [-100.000, 18.551], mean action: 1.506 [0.000, 3.000], mean observation: 0.030 [-4.305, 1.000], loss: 1.403826, mean_absolute_error: 21.961687, mean_q: 12.627788, mean_eps: 0.810903
  210250/2000000: episode: 2162, duration: 1.443s, episode steps: 101, steps per second: 70, episode reward: -143.107, mean reward: -1.417 [-100.000, 6.593], mean action: 1.663 [0.000, 3.000], mean observation: -0.119 [-1.146, 3.273], loss: 1.378222, mean_absolute_error: 23.681426, mean_q: 11.680178, mean_eps: 0.810820
  210372/2000000: episode: 2163, duration: 1.764s, episode steps: 122, steps per second: 69, episode reward: -155.168, mean reward: -1.272 [-100.000, 11.374], mean action: 1.697 [0.000, 3.000], mean observation: -0.087 [-1.039, 3.033], loss: 1.420283, mean_absolute_error: 21.627351, mean_q: 13.566452, mean_eps: 0.810721
  210503/2000000: episode: 2164, duration: 1.869s, episode steps: 131, steps per second: 70, episode reward: -143.908, mean reward: -1.099 [-100.000, 7.467], mean action: 1.450 [0.000, 3.000], mean observation: 0.059 [-1.304, 4.765], loss: 1.515746, mean_absolute_error: 22.491410, mean_q: 11.832976, mean_eps: 0.810608
  210585/2000000: episode: 2165, duration: 1.180s, episode steps: 82, steps per second: 69, episode reward: -139.063, mean reward: -1.696 [-100.000, 10.133], mean action: 1.683 [0.000, 3.000], mean observation: -0.049 [-1.309, 4.679], loss: 1.624148, mean_absolute_error: 23.161433, mean_q: 12.401068, mean_eps: 0.810510
  210711/2000000: episode: 2166, duration: 1.742s, episode steps: 126, steps per second: 72, episode reward: -173.057, mean reward: -1.373 [-100.000, 6.366], mean action: 1.556 [0.000, 3.000], mean observation: 0.036 [-1.381, 4.991], loss: 1.298687, mean_absolute_error: 21.504450, mean_q: 14.620598, mean_eps: 0.810417
  210802/2000000: episode: 2167, duration: 1.331s, episode steps: 91, steps per second: 68, episode reward: -205.871, mean reward: -2.262 [-100.000, 14.475], mean action: 1.451 [0.000, 3.000], mean observation: 0.098 [-1.443, 5.064], loss: 2.106930, mean_absolute_error: 22.503832, mean_q: 12.004450, mean_eps: 0.810320
  210882/2000000: episode: 2168, duration: 1.141s, episode steps: 80, steps per second: 70, episode reward: -183.981, mean reward: -2.300 [-100.000, 7.853], mean action: 1.625 [0.000, 3.000], mean observation: 0.028 [-1.199, 4.187], loss: 1.528329, mean_absolute_error: 22.587813, mean_q: 14.104991, mean_eps: 0.810242
  210990/2000000: episode: 2169, duration: 1.520s, episode steps: 108, steps per second: 71, episode reward: -139.794, mean reward: -1.294 [-100.000, 9.947], mean action: 1.519 [0.000, 3.000], mean observation: 0.135 [-1.134, 1.000], loss: 1.380597, mean_absolute_error: 22.386189, mean_q: 11.362419, mean_eps: 0.810158
  211103/2000000: episode: 2170, duration: 1.583s, episode steps: 113, steps per second: 71, episode reward: -132.929, mean reward: -1.176 [-100.000, 18.782], mean action: 1.646 [0.000, 3.000], mean observation: -0.038 [-1.065, 1.000], loss: 2.196669, mean_absolute_error: 22.926193, mean_q: 11.658425, mean_eps: 0.810059
  211197/2000000: episode: 2171, duration: 1.366s, episode steps: 94, steps per second: 69, episode reward: -103.119, mean reward: -1.097 [-100.000, 15.397], mean action: 1.766 [0.000, 3.000], mean observation: -0.152 [-1.884, 1.000], loss: 1.341250, mean_absolute_error: 23.243040, mean_q: 12.055629, mean_eps: 0.809965
  211272/2000000: episode: 2172, duration: 1.073s, episode steps: 75, steps per second: 70, episode reward: -132.703, mean reward: -1.769 [-100.000, 10.543], mean action: 1.600 [0.000, 3.000], mean observation: -0.154 [-4.188, 1.000], loss: 1.495882, mean_absolute_error: 22.805900, mean_q: 14.190212, mean_eps: 0.809889
  211331/2000000: episode: 2173, duration: 0.853s, episode steps: 59, steps per second: 69, episode reward: -141.632, mean reward: -2.401 [-100.000, 17.665], mean action: 1.661 [0.000, 3.000], mean observation: -0.148 [-1.699, 1.000], loss: 1.635894, mean_absolute_error: 23.343103, mean_q: 11.895678, mean_eps: 0.809830
  211466/2000000: episode: 2174, duration: 1.907s, episode steps: 135, steps per second: 71, episode reward: -210.835, mean reward: -1.562 [-100.000, 13.969], mean action: 1.467 [0.000, 3.000], mean observation: 0.161 [-2.993, 1.033], loss: 1.476747, mean_absolute_error: 21.828543, mean_q: 13.050382, mean_eps: 0.809742
  211569/2000000: episode: 2175, duration: 1.466s, episode steps: 103, steps per second: 70, episode reward: -191.308, mean reward: -1.857 [-100.000, 11.203], mean action: 1.786 [0.000, 3.000], mean observation: -0.091 [-5.435, 1.016], loss: 1.680288, mean_absolute_error: 23.004523, mean_q: 11.084142, mean_eps: 0.809634
  211653/2000000: episode: 2176, duration: 1.188s, episode steps: 84, steps per second: 71, episode reward: -215.612, mean reward: -2.567 [-100.000, 17.085], mean action: 1.619 [0.000, 3.000], mean observation: 0.075 [-1.549, 5.242], loss: 1.888194, mean_absolute_error: 22.859826, mean_q: 12.176717, mean_eps: 0.809549
  211786/2000000: episode: 2177, duration: 1.857s, episode steps: 133, steps per second: 72, episode reward: -92.264, mean reward: -0.694 [-100.000, 13.400], mean action: 1.504 [0.000, 3.000], mean observation: 0.025 [-1.157, 3.718], loss: 1.654342, mean_absolute_error: 22.975614, mean_q: 11.515384, mean_eps: 0.809452
  211863/2000000: episode: 2178, duration: 1.189s, episode steps: 77, steps per second: 65, episode reward: -125.587, mean reward: -1.631 [-100.000, 5.942], mean action: 1.416 [0.000, 3.000], mean observation: 0.037 [-4.227, 1.000], loss: 2.024789, mean_absolute_error: 24.128143, mean_q: 10.844584, mean_eps: 0.809358
  211981/2000000: episode: 2179, duration: 1.692s, episode steps: 118, steps per second: 70, episode reward: -148.710, mean reward: -1.260 [-100.000, 6.568], mean action: 1.712 [0.000, 3.000], mean observation: -0.001 [-1.303, 4.635], loss: 1.617500, mean_absolute_error: 23.114567, mean_q: 12.635205, mean_eps: 0.809270
  212046/2000000: episode: 2180, duration: 0.916s, episode steps: 65, steps per second: 71, episode reward: -127.704, mean reward: -1.965 [-100.000, 10.869], mean action: 1.769 [0.000, 3.000], mean observation: -0.178 [-1.405, 4.563], loss: 2.017198, mean_absolute_error: 24.725070, mean_q: 9.211596, mean_eps: 0.809187
  212199/2000000: episode: 2181, duration: 2.156s, episode steps: 153, steps per second: 71, episode reward: -75.146, mean reward: -0.491 [-100.000, 44.895], mean action: 1.627 [0.000, 3.000], mean observation: 0.157 [-1.369, 1.510], loss: 2.663019, mean_absolute_error: 23.002530, mean_q: 11.680909, mean_eps: 0.809090
  212300/2000000: episode: 2182, duration: 1.456s, episode steps: 101, steps per second: 69, episode reward: -139.495, mean reward: -1.381 [-100.000, 7.100], mean action: 1.535 [0.000, 3.000], mean observation: 0.003 [-1.190, 4.243], loss: 2.105940, mean_absolute_error: 24.011314, mean_q: 12.046918, mean_eps: 0.808977
  212386/2000000: episode: 2183, duration: 1.329s, episode steps: 86, steps per second: 65, episode reward: -64.301, mean reward: -0.748 [-100.000, 13.533], mean action: 1.570 [0.000, 3.000], mean observation: -0.068 [-0.979, 1.000], loss: 1.728490, mean_absolute_error: 22.842294, mean_q: 12.128958, mean_eps: 0.808892
  212482/2000000: episode: 2184, duration: 1.351s, episode steps: 96, steps per second: 71, episode reward: -136.081, mean reward: -1.418 [-100.000, 19.516], mean action: 1.750 [0.000, 3.000], mean observation: -0.134 [-1.264, 1.000], loss: 1.379505, mean_absolute_error: 22.664935, mean_q: 14.068854, mean_eps: 0.808809
  212585/2000000: episode: 2185, duration: 1.481s, episode steps: 103, steps per second: 70, episode reward: -93.276, mean reward: -0.906 [-100.000, 17.562], mean action: 1.612 [0.000, 3.000], mean observation: -0.015 [-1.050, 1.000], loss: 1.423154, mean_absolute_error: 22.693614, mean_q: 13.045605, mean_eps: 0.808719
  212707/2000000: episode: 2186, duration: 1.690s, episode steps: 122, steps per second: 72, episode reward: -133.681, mean reward: -1.096 [-100.000, 7.692], mean action: 1.516 [0.000, 3.000], mean observation: 0.012 [-3.945, 1.000], loss: 2.074682, mean_absolute_error: 22.610920, mean_q: 12.386319, mean_eps: 0.808619
  212771/2000000: episode: 2187, duration: 0.915s, episode steps: 64, steps per second: 70, episode reward: -146.173, mean reward: -2.284 [-100.000, 12.636], mean action: 1.656 [0.000, 3.000], mean observation: -0.028 [-1.460, 4.673], loss: 1.746609, mean_absolute_error: 21.903437, mean_q: 12.790683, mean_eps: 0.808536
  212859/2000000: episode: 2188, duration: 1.235s, episode steps: 88, steps per second: 71, episode reward: -112.513, mean reward: -1.279 [-100.000, 10.111], mean action: 1.432 [0.000, 3.000], mean observation: 0.096 [-1.140, 4.150], loss: 2.425851, mean_absolute_error: 24.851254, mean_q: 9.154529, mean_eps: 0.808467
  212944/2000000: episode: 2189, duration: 1.250s, episode steps: 85, steps per second: 68, episode reward: -160.653, mean reward: -1.890 [-100.000, 6.413], mean action: 1.659 [0.000, 3.000], mean observation: -0.111 [-1.226, 4.035], loss: 1.662636, mean_absolute_error: 22.419047, mean_q: 12.633721, mean_eps: 0.808390
  213062/2000000: episode: 2190, duration: 1.717s, episode steps: 118, steps per second: 69, episode reward: -124.842, mean reward: -1.058 [-100.000, 8.327], mean action: 1.602 [0.000, 3.000], mean observation: 0.155 [-3.329, 1.000], loss: 1.885971, mean_absolute_error: 22.632829, mean_q: 13.581821, mean_eps: 0.808298
  213183/2000000: episode: 2191, duration: 1.882s, episode steps: 121, steps per second: 64, episode reward: -160.263, mean reward: -1.324 [-100.000, 25.156], mean action: 1.421 [0.000, 3.000], mean observation: 0.140 [-1.428, 1.023], loss: 1.727809, mean_absolute_error: 22.905292, mean_q: 11.654881, mean_eps: 0.808190
  213278/2000000: episode: 2192, duration: 1.396s, episode steps: 95, steps per second: 68, episode reward: -148.150, mean reward: -1.559 [-100.000, 13.362], mean action: 1.537 [0.000, 3.000], mean observation: -0.046 [-1.375, 4.014], loss: 1.927581, mean_absolute_error: 22.552957, mean_q: 12.983257, mean_eps: 0.808093
  213347/2000000: episode: 2193, duration: 0.961s, episode steps: 69, steps per second: 72, episode reward: -95.801, mean reward: -1.388 [-100.000, 6.995], mean action: 1.551 [0.000, 3.000], mean observation: 0.054 [-1.205, 4.375], loss: 1.779687, mean_absolute_error: 23.742519, mean_q: 11.278032, mean_eps: 0.808019
  213495/2000000: episode: 2194, duration: 2.098s, episode steps: 148, steps per second: 71, episode reward: -161.321, mean reward: -1.090 [-100.000, 11.779], mean action: 1.581 [0.000, 3.000], mean observation: -0.035 [-1.242, 3.876], loss: 1.100551, mean_absolute_error: 21.692615, mean_q: 12.869123, mean_eps: 0.807922
  213572/2000000: episode: 2195, duration: 1.162s, episode steps: 77, steps per second: 66, episode reward: -256.332, mean reward: -3.329 [-100.000, 5.592], mean action: 1.597 [0.000, 3.000], mean observation: -0.056 [-1.388, 6.210], loss: 1.382167, mean_absolute_error: 22.003718, mean_q: 11.581631, mean_eps: 0.807821
  213683/2000000: episode: 2196, duration: 1.577s, episode steps: 111, steps per second: 70, episode reward: -144.976, mean reward: -1.306 [-100.000, 16.448], mean action: 1.459 [0.000, 3.000], mean observation: 0.121 [-3.018, 1.000], loss: 1.940482, mean_absolute_error: 22.631996, mean_q: 11.807606, mean_eps: 0.807737
  213802/2000000: episode: 2197, duration: 1.703s, episode steps: 119, steps per second: 70, episode reward: -173.539, mean reward: -1.458 [-100.000, 6.909], mean action: 1.597 [0.000, 3.000], mean observation: -0.108 [-1.233, 1.000], loss: 2.231281, mean_absolute_error: 24.422300, mean_q: 12.433109, mean_eps: 0.807632
  213936/2000000: episode: 2198, duration: 1.915s, episode steps: 134, steps per second: 70, episode reward: -113.947, mean reward: -0.850 [-100.000, 6.834], mean action: 1.619 [0.000, 3.000], mean observation: -0.007 [-1.081, 3.695], loss: 1.752112, mean_absolute_error: 23.555099, mean_q: 10.786977, mean_eps: 0.807519
  214019/2000000: episode: 2199, duration: 1.192s, episode steps: 83, steps per second: 70, episode reward: -74.186, mean reward: -0.894 [-100.000, 9.964], mean action: 1.602 [0.000, 3.000], mean observation: 0.058 [-3.129, 1.000], loss: 1.666919, mean_absolute_error: 24.032796, mean_q: 10.280651, mean_eps: 0.807422
  214123/2000000: episode: 2200, duration: 1.486s, episode steps: 104, steps per second: 70, episode reward: -145.265, mean reward: -1.397 [-100.000, 7.633], mean action: 1.423 [0.000, 3.000], mean observation: 0.024 [-5.048, 1.000], loss: 1.813899, mean_absolute_error: 23.021875, mean_q: 12.167408, mean_eps: 0.807337
  214189/2000000: episode: 2201, duration: 0.956s, episode steps: 66, steps per second: 69, episode reward: -114.657, mean reward: -1.737 [-100.000, 17.270], mean action: 1.515 [0.000, 3.000], mean observation: -0.142 [-1.321, 1.000], loss: 1.216298, mean_absolute_error: 22.412889, mean_q: 13.948596, mean_eps: 0.807260
  214259/2000000: episode: 2202, duration: 0.975s, episode steps: 70, steps per second: 72, episode reward: -126.535, mean reward: -1.808 [-100.000, 11.456], mean action: 1.600 [0.000, 3.000], mean observation: 0.053 [-4.706, 1.000], loss: 2.077765, mean_absolute_error: 23.849829, mean_q: 10.907967, mean_eps: 0.807198
  214348/2000000: episode: 2203, duration: 1.305s, episode steps: 89, steps per second: 68, episode reward: -167.854, mean reward: -1.886 [-100.000, 4.519], mean action: 1.427 [0.000, 3.000], mean observation: 0.064 [-4.131, 1.000], loss: 1.895851, mean_absolute_error: 22.628942, mean_q: 14.902356, mean_eps: 0.807128
  214452/2000000: episode: 2204, duration: 1.511s, episode steps: 104, steps per second: 69, episode reward: -205.159, mean reward: -1.973 [-100.000, 10.147], mean action: 1.721 [0.000, 3.000], mean observation: -0.102 [-2.665, 1.000], loss: 1.368815, mean_absolute_error: 22.635925, mean_q: 14.237650, mean_eps: 0.807042
  214574/2000000: episode: 2205, duration: 1.757s, episode steps: 122, steps per second: 69, episode reward: -191.322, mean reward: -1.568 [-100.000, 5.858], mean action: 1.656 [0.000, 3.000], mean observation: -0.140 [-1.279, 4.192], loss: 1.526914, mean_absolute_error: 22.740464, mean_q: 10.518910, mean_eps: 0.806939
  214659/2000000: episode: 2206, duration: 1.181s, episode steps: 85, steps per second: 72, episode reward: -131.867, mean reward: -1.551 [-100.000, 6.339], mean action: 1.518 [0.000, 3.000], mean observation: 0.097 [-4.025, 1.000], loss: 1.434466, mean_absolute_error: 23.152030, mean_q: 10.645134, mean_eps: 0.806846
  214779/2000000: episode: 2207, duration: 1.678s, episode steps: 120, steps per second: 71, episode reward: -141.031, mean reward: -1.175 [-100.000, 6.345], mean action: 1.650 [0.000, 3.000], mean observation: 0.002 [-3.728, 1.000], loss: 2.008617, mean_absolute_error: 23.847028, mean_q: 11.907669, mean_eps: 0.806754
  214849/2000000: episode: 2208, duration: 1.009s, episode steps: 70, steps per second: 69, episode reward: -132.460, mean reward: -1.892 [-100.000, 9.632], mean action: 1.457 [0.000, 3.000], mean observation: -0.012 [-4.670, 1.000], loss: 1.522449, mean_absolute_error: 22.129021, mean_q: 13.036943, mean_eps: 0.806667
  214959/2000000: episode: 2209, duration: 1.506s, episode steps: 110, steps per second: 73, episode reward: -130.073, mean reward: -1.182 [-100.000, 9.163], mean action: 1.645 [0.000, 3.000], mean observation: 0.097 [-3.598, 1.000], loss: 1.478490, mean_absolute_error: 21.075946, mean_q: 12.935355, mean_eps: 0.806586
  215027/2000000: episode: 2210, duration: 0.972s, episode steps: 68, steps per second: 70, episode reward: -121.380, mean reward: -1.785 [-100.000, 9.348], mean action: 1.618 [0.000, 3.000], mean observation: -0.061 [-1.433, 5.103], loss: 2.466216, mean_absolute_error: 24.721411, mean_q: 11.331493, mean_eps: 0.806507
  215121/2000000: episode: 2211, duration: 1.353s, episode steps: 94, steps per second: 69, episode reward: -214.690, mean reward: -2.284 [-100.000, 12.221], mean action: 1.404 [0.000, 3.000], mean observation: 0.118 [-1.505, 5.928], loss: 1.880843, mean_absolute_error: 23.989120, mean_q: 11.174527, mean_eps: 0.806433
  215213/2000000: episode: 2212, duration: 1.294s, episode steps: 92, steps per second: 71, episode reward: -128.354, mean reward: -1.395 [-100.000, 7.268], mean action: 1.620 [0.000, 3.000], mean observation: -0.017 [-4.002, 1.000], loss: 1.624829, mean_absolute_error: 23.478184, mean_q: 9.671463, mean_eps: 0.806349
  215294/2000000: episode: 2213, duration: 1.134s, episode steps: 81, steps per second: 71, episode reward: -127.152, mean reward: -1.570 [-100.000, 11.623], mean action: 1.568 [0.000, 3.000], mean observation: -0.003 [-4.570, 1.000], loss: 1.303594, mean_absolute_error: 22.991815, mean_q: 10.086911, mean_eps: 0.806271
  215397/2000000: episode: 2214, duration: 1.466s, episode steps: 103, steps per second: 70, episode reward: -173.425, mean reward: -1.684 [-100.000, 8.690], mean action: 1.641 [0.000, 3.000], mean observation: -0.115 [-1.241, 1.000], loss: 1.137861, mean_absolute_error: 22.905041, mean_q: 11.450965, mean_eps: 0.806189
  215530/2000000: episode: 2215, duration: 1.857s, episode steps: 133, steps per second: 72, episode reward: -178.173, mean reward: -1.340 [-100.000, 14.274], mean action: 1.541 [0.000, 3.000], mean observation: -0.017 [-1.541, 6.341], loss: 2.158372, mean_absolute_error: 23.316968, mean_q: 12.084621, mean_eps: 0.806082
  215596/2000000: episode: 2216, duration: 0.952s, episode steps: 66, steps per second: 69, episode reward: -135.639, mean reward: -2.055 [-100.000, 7.987], mean action: 1.697 [0.000, 3.000], mean observation: -0.079 [-1.524, 5.207], loss: 1.171746, mean_absolute_error: 20.866475, mean_q: 15.764164, mean_eps: 0.805994
  215695/2000000: episode: 2217, duration: 1.426s, episode steps: 99, steps per second: 69, episode reward: -148.918, mean reward: -1.504 [-100.000, 8.683], mean action: 1.697 [0.000, 3.000], mean observation: -0.105 [-1.160, 1.000], loss: 2.039078, mean_absolute_error: 23.053721, mean_q: 9.292345, mean_eps: 0.805920
  215770/2000000: episode: 2218, duration: 1.068s, episode steps: 75, steps per second: 70, episode reward: -110.288, mean reward: -1.471 [-100.000, 12.174], mean action: 1.787 [0.000, 3.000], mean observation: -0.140 [-1.206, 3.960], loss: 2.137641, mean_absolute_error: 22.752308, mean_q: 11.473789, mean_eps: 0.805841
  215894/2000000: episode: 2219, duration: 1.743s, episode steps: 124, steps per second: 71, episode reward: -106.163, mean reward: -0.856 [-100.000, 25.013], mean action: 1.460 [0.000, 3.000], mean observation: 0.034 [-1.211, 1.000], loss: 1.875167, mean_absolute_error: 22.464866, mean_q: 11.509437, mean_eps: 0.805751
  215984/2000000: episode: 2220, duration: 1.286s, episode steps: 90, steps per second: 70, episode reward: -139.899, mean reward: -1.554 [-100.000, 11.707], mean action: 1.600 [0.000, 3.000], mean observation: 0.025 [-4.464, 1.000], loss: 1.427552, mean_absolute_error: 22.618826, mean_q: 12.827234, mean_eps: 0.805656
  216075/2000000: episode: 2221, duration: 1.299s, episode steps: 91, steps per second: 70, episode reward: -132.244, mean reward: -1.453 [-100.000, 14.439], mean action: 1.560 [0.000, 3.000], mean observation: -0.080 [-1.171, 4.253], loss: 1.416073, mean_absolute_error: 21.230194, mean_q: 13.576603, mean_eps: 0.805575
  216203/2000000: episode: 2222, duration: 1.801s, episode steps: 128, steps per second: 71, episode reward: -155.603, mean reward: -1.216 [-100.000, 7.759], mean action: 1.664 [0.000, 3.000], mean observation: -0.059 [-1.151, 3.953], loss: 1.214509, mean_absolute_error: 20.929646, mean_q: 14.320884, mean_eps: 0.805476
  216359/2000000: episode: 2223, duration: 2.191s, episode steps: 156, steps per second: 71, episode reward: -111.381, mean reward: -0.714 [-100.000, 21.309], mean action: 1.679 [0.000, 3.000], mean observation: 0.087 [-3.252, 1.000], loss: 1.505278, mean_absolute_error: 22.700389, mean_q: 12.482761, mean_eps: 0.805348
  216466/2000000: episode: 2224, duration: 1.531s, episode steps: 107, steps per second: 70, episode reward: -138.994, mean reward: -1.299 [-100.000, 10.179], mean action: 1.626 [0.000, 3.000], mean observation: -0.072 [-1.356, 1.000], loss: 1.243126, mean_absolute_error: 22.166096, mean_q: 14.301060, mean_eps: 0.805229
  216532/2000000: episode: 2225, duration: 0.965s, episode steps: 66, steps per second: 68, episode reward: -128.377, mean reward: -1.945 [-100.000, 12.563], mean action: 1.742 [0.000, 3.000], mean observation: -0.113 [-4.990, 1.000], loss: 2.160139, mean_absolute_error: 22.368276, mean_q: 13.382117, mean_eps: 0.805152
  216620/2000000: episode: 2226, duration: 1.286s, episode steps: 88, steps per second: 68, episode reward: -91.225, mean reward: -1.037 [-100.000, 7.285], mean action: 1.568 [0.000, 3.000], mean observation: 0.004 [-3.600, 1.000], loss: 1.317054, mean_absolute_error: 22.165453, mean_q: 12.399092, mean_eps: 0.805083
  216715/2000000: episode: 2227, duration: 1.353s, episode steps: 95, steps per second: 70, episode reward: -120.864, mean reward: -1.272 [-100.000, 8.011], mean action: 1.674 [0.000, 3.000], mean observation: -0.068 [-3.701, 1.000], loss: 1.453314, mean_absolute_error: 22.746957, mean_q: 12.245178, mean_eps: 0.805001
  216814/2000000: episode: 2228, duration: 1.407s, episode steps: 99, steps per second: 70, episode reward: -112.463, mean reward: -1.136 [-100.000, 7.388], mean action: 1.687 [0.000, 3.000], mean observation: 0.014 [-3.744, 1.000], loss: 1.567909, mean_absolute_error: 23.223259, mean_q: 11.405683, mean_eps: 0.804912
  216896/2000000: episode: 2229, duration: 1.181s, episode steps: 82, steps per second: 69, episode reward: -98.430, mean reward: -1.200 [-100.000, 16.127], mean action: 1.402 [0.000, 3.000], mean observation: 0.002 [-1.211, 1.000], loss: 1.428088, mean_absolute_error: 22.524766, mean_q: 12.958462, mean_eps: 0.804831
  217047/2000000: episode: 2230, duration: 2.124s, episode steps: 151, steps per second: 71, episode reward: -122.239, mean reward: -0.810 [-100.000, 8.778], mean action: 1.576 [0.000, 3.000], mean observation: -0.022 [-3.794, 1.072], loss: 1.201320, mean_absolute_error: 22.331064, mean_q: 12.951894, mean_eps: 0.804727
  217206/2000000: episode: 2231, duration: 2.278s, episode steps: 159, steps per second: 70, episode reward: -121.026, mean reward: -0.761 [-100.000, 11.776], mean action: 1.604 [0.000, 3.000], mean observation: 0.022 [-2.921, 1.010], loss: 1.505359, mean_absolute_error: 22.451519, mean_q: 12.044277, mean_eps: 0.804587
  217328/2000000: episode: 2232, duration: 1.748s, episode steps: 122, steps per second: 70, episode reward: -152.405, mean reward: -1.249 [-100.000, 71.751], mean action: 1.607 [0.000, 3.000], mean observation: -0.058 [-1.587, 1.000], loss: 1.426884, mean_absolute_error: 22.751114, mean_q: 13.807035, mean_eps: 0.804461
  217412/2000000: episode: 2233, duration: 1.245s, episode steps: 84, steps per second: 67, episode reward: -110.872, mean reward: -1.320 [-100.000, 23.445], mean action: 1.595 [0.000, 3.000], mean observation: 0.105 [-1.099, 1.000], loss: 1.468350, mean_absolute_error: 22.278729, mean_q: 13.750830, mean_eps: 0.804369
  217478/2000000: episode: 2234, duration: 0.956s, episode steps: 66, steps per second: 69, episode reward: -106.973, mean reward: -1.621 [-100.000, 7.692], mean action: 1.485 [0.000, 3.000], mean observation: -0.049 [-4.614, 1.000], loss: 1.298411, mean_absolute_error: 23.243922, mean_q: 13.913710, mean_eps: 0.804300
  217558/2000000: episode: 2235, duration: 1.149s, episode steps: 80, steps per second: 70, episode reward: -127.125, mean reward: -1.589 [-100.000, 17.276], mean action: 1.575 [0.000, 3.000], mean observation: 0.131 [-1.062, 1.000], loss: 1.232049, mean_absolute_error: 23.099924, mean_q: 12.379224, mean_eps: 0.804234
  217685/2000000: episode: 2236, duration: 1.765s, episode steps: 127, steps per second: 72, episode reward: -136.587, mean reward: -1.075 [-100.000, 7.947], mean action: 1.386 [0.000, 3.000], mean observation: 0.022 [-1.305, 4.789], loss: 1.413870, mean_absolute_error: 22.270703, mean_q: 13.736603, mean_eps: 0.804140
  217768/2000000: episode: 2237, duration: 1.245s, episode steps: 83, steps per second: 67, episode reward: -119.062, mean reward: -1.434 [-100.000, 9.025], mean action: 1.602 [0.000, 3.000], mean observation: 0.061 [-3.699, 1.000], loss: 1.643460, mean_absolute_error: 23.171439, mean_q: 13.652992, mean_eps: 0.804047
  217862/2000000: episode: 2238, duration: 1.542s, episode steps: 94, steps per second: 61, episode reward: -103.724, mean reward: -1.103 [-100.000, 17.450], mean action: 1.691 [0.000, 3.000], mean observation: -0.015 [-1.195, 1.000], loss: 3.249542, mean_absolute_error: 23.827540, mean_q: 11.631798, mean_eps: 0.803967
  217955/2000000: episode: 2239, duration: 1.418s, episode steps: 93, steps per second: 66, episode reward: -53.029, mean reward: -0.570 [-100.000, 17.580], mean action: 1.613 [0.000, 3.000], mean observation: -0.015 [-0.927, 1.000], loss: 1.217948, mean_absolute_error: 22.409892, mean_q: 12.265326, mean_eps: 0.803883
  218063/2000000: episode: 2240, duration: 1.646s, episode steps: 108, steps per second: 66, episode reward: -136.390, mean reward: -1.263 [-100.000, 15.927], mean action: 1.648 [0.000, 3.000], mean observation: -0.064 [-1.192, 1.000], loss: 2.031136, mean_absolute_error: 22.744349, mean_q: 10.637748, mean_eps: 0.803793
  218201/2000000: episode: 2241, duration: 2.283s, episode steps: 138, steps per second: 60, episode reward: -111.069, mean reward: -0.805 [-100.000, 8.267], mean action: 1.594 [0.000, 3.000], mean observation: -0.007 [-1.177, 3.772], loss: 1.559459, mean_absolute_error: 22.746553, mean_q: 11.313651, mean_eps: 0.803681
  218260/2000000: episode: 2242, duration: 0.975s, episode steps: 59, steps per second: 61, episode reward: -120.039, mean reward: -2.035 [-100.000, 3.665], mean action: 1.966 [0.000, 3.000], mean observation: -0.153 [-1.423, 2.156], loss: 2.053657, mean_absolute_error: 24.013664, mean_q: 12.495223, mean_eps: 0.803593
  218373/2000000: episode: 2243, duration: 1.747s, episode steps: 113, steps per second: 65, episode reward: -161.938, mean reward: -1.433 [-100.000, 16.242], mean action: 1.558 [0.000, 3.000], mean observation: 0.048 [-4.367, 1.000], loss: 1.937504, mean_absolute_error: 23.326209, mean_q: 12.369941, mean_eps: 0.803516
  218470/2000000: episode: 2244, duration: 1.491s, episode steps: 97, steps per second: 65, episode reward: -136.733, mean reward: -1.410 [-100.000, 11.763], mean action: 1.711 [0.000, 3.000], mean observation: -0.133 [-1.081, 1.000], loss: 1.771441, mean_absolute_error: 21.985115, mean_q: 12.791068, mean_eps: 0.803420
  218562/2000000: episode: 2245, duration: 1.305s, episode steps: 92, steps per second: 70, episode reward: -142.224, mean reward: -1.546 [-100.000, 6.565], mean action: 1.696 [0.000, 3.000], mean observation: -0.111 [-4.265, 1.000], loss: 1.774900, mean_absolute_error: 22.597562, mean_q: 13.899944, mean_eps: 0.803336
  218631/2000000: episode: 2246, duration: 0.956s, episode steps: 69, steps per second: 72, episode reward: -106.729, mean reward: -1.547 [-100.000, 6.359], mean action: 1.594 [0.000, 3.000], mean observation: -0.136 [-1.177, 3.901], loss: 2.023323, mean_absolute_error: 22.753890, mean_q: 12.654477, mean_eps: 0.803264
  218780/2000000: episode: 2247, duration: 2.109s, episode steps: 149, steps per second: 71, episode reward: -200.231, mean reward: -1.344 [-100.000, 10.726], mean action: 1.477 [0.000, 3.000], mean observation: 0.200 [-1.208, 1.182], loss: 1.578588, mean_absolute_error: 23.001275, mean_q: 11.543227, mean_eps: 0.803166
  218898/2000000: episode: 2248, duration: 1.682s, episode steps: 118, steps per second: 70, episode reward: -161.823, mean reward: -1.371 [-100.000, 9.583], mean action: 1.669 [0.000, 3.000], mean observation: 0.045 [-1.275, 3.857], loss: 2.108531, mean_absolute_error: 22.921238, mean_q: 12.703587, mean_eps: 0.803046
  219021/2000000: episode: 2249, duration: 1.719s, episode steps: 123, steps per second: 72, episode reward: -335.743, mean reward: -2.730 [-100.000, 30.656], mean action: 1.561 [0.000, 3.000], mean observation: 0.282 [-1.161, 2.365], loss: 1.337511, mean_absolute_error: 23.753246, mean_q: 11.817050, mean_eps: 0.802936
  219193/2000000: episode: 2250, duration: 2.449s, episode steps: 172, steps per second: 70, episode reward: -138.539, mean reward: -0.805 [-100.000, 33.878], mean action: 1.616 [0.000, 3.000], mean observation: 0.038 [-1.219, 1.141], loss: 1.770651, mean_absolute_error: 22.431219, mean_q: 13.452914, mean_eps: 0.802803
  219306/2000000: episode: 2251, duration: 1.561s, episode steps: 113, steps per second: 72, episode reward: -124.918, mean reward: -1.105 [-100.000, 6.849], mean action: 1.593 [0.000, 3.000], mean observation: -0.054 [-1.200, 3.797], loss: 1.203875, mean_absolute_error: 21.710211, mean_q: 13.700012, mean_eps: 0.802675
  219416/2000000: episode: 2252, duration: 1.552s, episode steps: 110, steps per second: 71, episode reward: -180.321, mean reward: -1.639 [-100.000, 10.184], mean action: 1.664 [0.000, 3.000], mean observation: -0.141 [-1.150, 3.361], loss: 1.550721, mean_absolute_error: 21.911338, mean_q: 13.313542, mean_eps: 0.802576
  219501/2000000: episode: 2253, duration: 1.241s, episode steps: 85, steps per second: 68, episode reward: -129.294, mean reward: -1.521 [-100.000, 11.669], mean action: 1.671 [0.000, 3.000], mean observation: -0.060 [-4.267, 1.000], loss: 1.738981, mean_absolute_error: 23.338198, mean_q: 13.273530, mean_eps: 0.802488
  219601/2000000: episode: 2254, duration: 1.391s, episode steps: 100, steps per second: 72, episode reward: -147.380, mean reward: -1.474 [-100.000, 11.138], mean action: 1.750 [0.000, 3.000], mean observation: -0.088 [-1.372, 3.798], loss: 1.659538, mean_absolute_error: 23.469115, mean_q: 10.421604, mean_eps: 0.802403
  219697/2000000: episode: 2255, duration: 1.337s, episode steps: 96, steps per second: 72, episode reward: -132.553, mean reward: -1.381 [-100.000, 21.410], mean action: 1.417 [0.000, 3.000], mean observation: 0.126 [-3.477, 1.000], loss: 1.537639, mean_absolute_error: 22.941625, mean_q: 10.975967, mean_eps: 0.802315
  219805/2000000: episode: 2256, duration: 1.527s, episode steps: 108, steps per second: 71, episode reward: -198.772, mean reward: -1.840 [-100.000, 4.030], mean action: 1.667 [0.000, 3.000], mean observation: -0.127 [-1.140, 0.982], loss: 1.291011, mean_absolute_error: 21.884378, mean_q: 14.854853, mean_eps: 0.802223
  219894/2000000: episode: 2257, duration: 1.249s, episode steps: 89, steps per second: 71, episode reward: -274.348, mean reward: -3.083 [-100.000, 0.277], mean action: 1.742 [0.000, 3.000], mean observation: -0.116 [-1.644, 0.939], loss: 1.775024, mean_absolute_error: 22.864315, mean_q: 11.735392, mean_eps: 0.802135
  219968/2000000: episode: 2258, duration: 1.067s, episode steps: 74, steps per second: 69, episode reward: -88.421, mean reward: -1.195 [-100.000, 15.160], mean action: 1.419 [0.000, 3.000], mean observation: 0.069 [-1.163, 1.000], loss: 2.071262, mean_absolute_error: 23.493451, mean_q: 11.185558, mean_eps: 0.802063
  220033/2000000: episode: 2259, duration: 0.941s, episode steps: 65, steps per second: 69, episode reward: -152.925, mean reward: -2.353 [-100.000, 7.472], mean action: 1.308 [0.000, 3.000], mean observation: -0.010 [-5.053, 1.000], loss: 2.172539, mean_absolute_error: 23.595743, mean_q: 11.895135, mean_eps: 0.802000
  220118/2000000: episode: 2260, duration: 1.166s, episode steps: 85, steps per second: 73, episode reward: -110.373, mean reward: -1.299 [-100.000, 16.514], mean action: 1.659 [0.000, 3.000], mean observation: -0.071 [-1.184, 3.587], loss: 1.692974, mean_absolute_error: 22.205802, mean_q: 14.785982, mean_eps: 0.801932
  220203/2000000: episode: 2261, duration: 1.301s, episode steps: 85, steps per second: 65, episode reward: -105.630, mean reward: -1.243 [-100.000, 9.086], mean action: 1.671 [0.000, 3.000], mean observation: -0.086 [-3.912, 1.000], loss: 1.688952, mean_absolute_error: 22.987805, mean_q: 11.701241, mean_eps: 0.801856
  220269/2000000: episode: 2262, duration: 0.961s, episode steps: 66, steps per second: 69, episode reward: -94.562, mean reward: -1.433 [-100.000, 14.321], mean action: 1.500 [0.000, 3.000], mean observation: 0.016 [-3.456, 1.000], loss: 1.377863, mean_absolute_error: 22.872710, mean_q: 13.578024, mean_eps: 0.801788
  220431/2000000: episode: 2263, duration: 2.266s, episode steps: 162, steps per second: 71, episode reward: -115.019, mean reward: -0.710 [-100.000, 10.604], mean action: 1.543 [0.000, 3.000], mean observation: -0.002 [-3.311, 1.040], loss: 1.189833, mean_absolute_error: 23.007723, mean_q: 14.033388, mean_eps: 0.801685
  220526/2000000: episode: 2264, duration: 1.366s, episode steps: 95, steps per second: 70, episode reward: -85.143, mean reward: -0.896 [-100.000, 10.948], mean action: 1.779 [0.000, 3.000], mean observation: -0.072 [-0.956, 1.000], loss: 1.871398, mean_absolute_error: 23.090046, mean_q: 10.459082, mean_eps: 0.801570
  220656/2000000: episode: 2265, duration: 1.864s, episode steps: 130, steps per second: 70, episode reward: -386.608, mean reward: -2.974 [-100.000, 40.083], mean action: 1.500 [0.000, 3.000], mean observation: -0.208 [-2.680, 1.000], loss: 2.025152, mean_absolute_error: 22.683540, mean_q: 15.074029, mean_eps: 0.801469
  220769/2000000: episode: 2266, duration: 1.618s, episode steps: 113, steps per second: 70, episode reward: -151.328, mean reward: -1.339 [-100.000, 5.063], mean action: 1.575 [0.000, 3.000], mean observation: 0.063 [-3.633, 1.000], loss: 1.715946, mean_absolute_error: 23.789285, mean_q: 11.306225, mean_eps: 0.801359
  220853/2000000: episode: 2267, duration: 1.175s, episode steps: 84, steps per second: 71, episode reward: -125.771, mean reward: -1.497 [-100.000, 25.799], mean action: 1.619 [0.000, 3.000], mean observation: -0.089 [-1.326, 1.000], loss: 1.738733, mean_absolute_error: 23.004268, mean_q: 16.107789, mean_eps: 0.801269
  220969/2000000: episode: 2268, duration: 1.641s, episode steps: 116, steps per second: 71, episode reward: -68.338, mean reward: -0.589 [-100.000, 43.164], mean action: 1.716 [0.000, 3.000], mean observation: -0.129 [-1.189, 1.838], loss: 1.604018, mean_absolute_error: 23.223730, mean_q: 13.090292, mean_eps: 0.801179
  221085/2000000: episode: 2269, duration: 1.638s, episode steps: 116, steps per second: 71, episode reward: -184.244, mean reward: -1.588 [-100.000, 7.434], mean action: 1.483 [0.000, 3.000], mean observation: 0.051 [-4.569, 1.000], loss: 1.817070, mean_absolute_error: 24.599042, mean_q: 13.136876, mean_eps: 0.801075
  221169/2000000: episode: 2270, duration: 1.181s, episode steps: 84, steps per second: 71, episode reward: -137.796, mean reward: -1.640 [-100.000, 9.566], mean action: 1.500 [0.000, 3.000], mean observation: 0.057 [-3.296, 1.000], loss: 0.948378, mean_absolute_error: 22.944602, mean_q: 15.856048, mean_eps: 0.800985
  221240/2000000: episode: 2271, duration: 1.025s, episode steps: 71, steps per second: 69, episode reward: -173.746, mean reward: -2.447 [-100.000, 9.826], mean action: 1.606 [0.000, 3.000], mean observation: -0.029 [-4.392, 1.000], loss: 2.813368, mean_absolute_error: 26.174807, mean_q: 11.028258, mean_eps: 0.800916
  221314/2000000: episode: 2272, duration: 1.084s, episode steps: 74, steps per second: 68, episode reward: -121.502, mean reward: -1.642 [-100.000, 7.873], mean action: 1.473 [0.000, 3.000], mean observation: 0.058 [-1.211, 4.058], loss: 1.336834, mean_absolute_error: 22.712689, mean_q: 13.391023, mean_eps: 0.800852
  221394/2000000: episode: 2273, duration: 1.143s, episode steps: 80, steps per second: 70, episode reward: -130.487, mean reward: -1.631 [-100.000, 8.349], mean action: 1.712 [0.000, 3.000], mean observation: 0.064 [-1.154, 4.120], loss: 2.005772, mean_absolute_error: 24.354655, mean_q: 12.407057, mean_eps: 0.800781
  221494/2000000: episode: 2274, duration: 1.437s, episode steps: 100, steps per second: 70, episode reward: -89.800, mean reward: -0.898 [-100.000, 61.649], mean action: 1.830 [0.000, 3.000], mean observation: -0.142 [-1.255, 2.244], loss: 2.275961, mean_absolute_error: 24.791155, mean_q: 10.984938, mean_eps: 0.800700
  221578/2000000: episode: 2275, duration: 1.175s, episode steps: 84, steps per second: 72, episode reward: -130.325, mean reward: -1.551 [-100.000, 11.339], mean action: 1.690 [0.000, 3.000], mean observation: -0.026 [-1.366, 1.000], loss: 1.489906, mean_absolute_error: 23.413078, mean_q: 14.346652, mean_eps: 0.800618
  221679/2000000: episode: 2276, duration: 1.399s, episode steps: 101, steps per second: 72, episode reward: -130.533, mean reward: -1.292 [-100.000, 20.037], mean action: 1.634 [0.000, 3.000], mean observation: -0.099 [-3.794, 1.000], loss: 1.744156, mean_absolute_error: 23.514623, mean_q: 14.220249, mean_eps: 0.800535
  221812/2000000: episode: 2277, duration: 1.901s, episode steps: 133, steps per second: 70, episode reward: -102.672, mean reward: -0.772 [-100.000, 7.854], mean action: 1.564 [0.000, 3.000], mean observation: 0.141 [-0.925, 3.089], loss: 1.937354, mean_absolute_error: 23.861235, mean_q: 12.295082, mean_eps: 0.800430
  221943/2000000: episode: 2278, duration: 1.876s, episode steps: 131, steps per second: 70, episode reward: -91.030, mean reward: -0.695 [-100.000, 15.473], mean action: 1.534 [0.000, 3.000], mean observation: -0.068 [-1.006, 1.738], loss: 1.707562, mean_absolute_error: 24.359113, mean_q: 11.298725, mean_eps: 0.800312
  222047/2000000: episode: 2279, duration: 1.502s, episode steps: 104, steps per second: 69, episode reward: -114.399, mean reward: -1.100 [-100.000, 16.848], mean action: 1.625 [0.000, 3.000], mean observation: -0.003 [-4.302, 1.000], loss: 1.734781, mean_absolute_error: 23.986976, mean_q: 12.298545, mean_eps: 0.800205
  222158/2000000: episode: 2280, duration: 1.578s, episode steps: 111, steps per second: 70, episode reward: -169.891, mean reward: -1.531 [-100.000, 12.652], mean action: 1.613 [0.000, 3.000], mean observation: -0.101 [-1.218, 1.000], loss: 1.770538, mean_absolute_error: 22.891704, mean_q: 14.764102, mean_eps: 0.800108
  222247/2000000: episode: 2281, duration: 1.241s, episode steps: 89, steps per second: 72, episode reward: -144.477, mean reward: -1.623 [-100.000, 13.217], mean action: 1.663 [0.000, 3.000], mean observation: -0.109 [-1.150, 3.699], loss: 2.256023, mean_absolute_error: 24.147463, mean_q: 13.015778, mean_eps: 0.800018
  222331/2000000: episode: 2282, duration: 1.182s, episode steps: 84, steps per second: 71, episode reward: -135.892, mean reward: -1.618 [-100.000, 8.581], mean action: 1.417 [0.000, 3.000], mean observation: 0.054 [-3.858, 1.000], loss: 1.784540, mean_absolute_error: 23.464678, mean_q: 12.362412, mean_eps: 0.799941
  222432/2000000: episode: 2283, duration: 1.468s, episode steps: 101, steps per second: 69, episode reward: -138.999, mean reward: -1.376 [-100.000, 10.360], mean action: 1.683 [0.000, 3.000], mean observation: -0.148 [-1.016, 3.572], loss: 2.006177, mean_absolute_error: 22.942315, mean_q: 11.772202, mean_eps: 0.799858
  222518/2000000: episode: 2284, duration: 1.239s, episode steps: 86, steps per second: 69, episode reward: -137.617, mean reward: -1.600 [-100.000, 7.440], mean action: 1.430 [0.000, 3.000], mean observation: -0.022 [-4.586, 1.000], loss: 2.109801, mean_absolute_error: 23.989043, mean_q: 11.729222, mean_eps: 0.799773
  222608/2000000: episode: 2285, duration: 1.316s, episode steps: 90, steps per second: 68, episode reward: -78.338, mean reward: -0.870 [-100.000, 11.658], mean action: 1.722 [0.000, 3.000], mean observation: -0.041 [-1.071, 3.676], loss: 2.147169, mean_absolute_error: 24.731176, mean_q: 9.478695, mean_eps: 0.799694
  222687/2000000: episode: 2286, duration: 1.136s, episode steps: 79, steps per second: 70, episode reward: -131.985, mean reward: -1.671 [-100.000, 11.866], mean action: 1.759 [0.000, 3.000], mean observation: -0.154 [-1.210, 4.108], loss: 1.843518, mean_absolute_error: 24.225753, mean_q: 12.511281, mean_eps: 0.799619
  222778/2000000: episode: 2287, duration: 1.313s, episode steps: 91, steps per second: 69, episode reward: -114.870, mean reward: -1.262 [-100.000, 21.900], mean action: 1.527 [0.000, 3.000], mean observation: -0.085 [-1.129, 1.000], loss: 1.583259, mean_absolute_error: 23.812130, mean_q: 14.189095, mean_eps: 0.799541
  222866/2000000: episode: 2288, duration: 1.267s, episode steps: 88, steps per second: 69, episode reward: -119.748, mean reward: -1.361 [-100.000, 13.142], mean action: 1.466 [0.000, 3.000], mean observation: -0.150 [-1.195, 1.000], loss: 2.327832, mean_absolute_error: 25.750667, mean_q: 11.140855, mean_eps: 0.799460
  222989/2000000: episode: 2289, duration: 1.750s, episode steps: 123, steps per second: 70, episode reward: -115.034, mean reward: -0.935 [-100.000, 6.222], mean action: 1.683 [0.000, 3.000], mean observation: -0.024 [-1.139, 1.027], loss: 1.792762, mean_absolute_error: 24.227233, mean_q: 12.232674, mean_eps: 0.799365
  223071/2000000: episode: 2290, duration: 1.128s, episode steps: 82, steps per second: 73, episode reward: -84.931, mean reward: -1.036 [-100.000, 23.156], mean action: 1.622 [0.000, 3.000], mean observation: -0.118 [-1.293, 1.000], loss: 1.534839, mean_absolute_error: 24.439046, mean_q: 11.632621, mean_eps: 0.799273
  223176/2000000: episode: 2291, duration: 1.525s, episode steps: 105, steps per second: 69, episode reward: -74.757, mean reward: -0.712 [-100.000, 71.443], mean action: 1.743 [0.000, 3.000], mean observation: 0.014 [-1.292, 1.819], loss: 1.935706, mean_absolute_error: 23.703836, mean_q: 11.724237, mean_eps: 0.799190
  223243/2000000: episode: 2292, duration: 0.957s, episode steps: 67, steps per second: 70, episode reward: -132.484, mean reward: -1.977 [-100.000, 16.458], mean action: 1.582 [0.000, 3.000], mean observation: -0.076 [-1.441, 1.000], loss: 2.344291, mean_absolute_error: 24.993667, mean_q: 8.982351, mean_eps: 0.799113
  223329/2000000: episode: 2293, duration: 1.228s, episode steps: 86, steps per second: 70, episode reward: -164.596, mean reward: -1.914 [-100.000, 8.327], mean action: 1.279 [0.000, 3.000], mean observation: 0.072 [-4.157, 1.000], loss: 1.393650, mean_absolute_error: 22.249302, mean_q: 14.251130, mean_eps: 0.799043
  223428/2000000: episode: 2294, duration: 1.398s, episode steps: 99, steps per second: 71, episode reward: -145.879, mean reward: -1.474 [-100.000, 12.372], mean action: 1.485 [0.000, 3.000], mean observation: -0.043 [-1.260, 3.722], loss: 1.508334, mean_absolute_error: 22.907009, mean_q: 13.926002, mean_eps: 0.798960
  223607/2000000: episode: 2295, duration: 2.551s, episode steps: 179, steps per second: 70, episode reward: -100.942, mean reward: -0.564 [-100.000, 18.921], mean action: 1.659 [0.000, 3.000], mean observation: 0.024 [-1.110, 1.000], loss: 1.600839, mean_absolute_error: 23.377934, mean_q: 14.017347, mean_eps: 0.798836
  223687/2000000: episode: 2296, duration: 1.123s, episode steps: 80, steps per second: 71, episode reward: -142.471, mean reward: -1.781 [-100.000, 16.285], mean action: 1.625 [0.000, 3.000], mean observation: -0.099 [-1.252, 1.000], loss: 2.060446, mean_absolute_error: 24.061167, mean_q: 13.805303, mean_eps: 0.798719
  223764/2000000: episode: 2297, duration: 1.115s, episode steps: 77, steps per second: 69, episode reward: -107.838, mean reward: -1.400 [-100.000, 22.508], mean action: 1.675 [0.000, 3.000], mean observation: -0.022 [-1.405, 1.000], loss: 1.721314, mean_absolute_error: 23.813470, mean_q: 12.896094, mean_eps: 0.798648
  223874/2000000: episode: 2298, duration: 1.557s, episode steps: 110, steps per second: 71, episode reward: -264.437, mean reward: -2.404 [-100.000, 21.629], mean action: 1.636 [0.000, 3.000], mean observation: -0.138 [-4.632, 1.000], loss: 1.569918, mean_absolute_error: 23.626745, mean_q: 13.205078, mean_eps: 0.798564
  223996/2000000: episode: 2299, duration: 1.730s, episode steps: 122, steps per second: 71, episode reward: -137.023, mean reward: -1.123 [-100.000, 6.770], mean action: 1.615 [0.000, 3.000], mean observation: 0.108 [-3.438, 1.000], loss: 2.429006, mean_absolute_error: 24.129703, mean_q: 11.797150, mean_eps: 0.798459
  224073/2000000: episode: 2300, duration: 1.171s, episode steps: 77, steps per second: 66, episode reward: -87.116, mean reward: -1.131 [-100.000, 6.457], mean action: 1.468 [0.000, 3.000], mean observation: -0.056 [-3.901, 1.000], loss: 1.645805, mean_absolute_error: 23.079618, mean_q: 12.051945, mean_eps: 0.798369
  224156/2000000: episode: 2301, duration: 1.179s, episode steps: 83, steps per second: 70, episode reward: -76.591, mean reward: -0.923 [-100.000, 14.485], mean action: 1.675 [0.000, 3.000], mean observation: -0.063 [-1.185, 1.000], loss: 1.663503, mean_absolute_error: 24.214305, mean_q: 13.293252, mean_eps: 0.798297
  224236/2000000: episode: 2302, duration: 1.158s, episode steps: 80, steps per second: 69, episode reward: -167.270, mean reward: -2.091 [-100.000, 17.117], mean action: 1.750 [0.000, 3.000], mean observation: -0.060 [-1.425, 1.000], loss: 2.012690, mean_absolute_error: 24.947577, mean_q: 10.824202, mean_eps: 0.798225
  224343/2000000: episode: 2303, duration: 1.497s, episode steps: 107, steps per second: 71, episode reward: -137.984, mean reward: -1.290 [-100.000, 11.587], mean action: 1.701 [0.000, 3.000], mean observation: -0.080 [-0.910, 1.150], loss: 1.855481, mean_absolute_error: 24.152323, mean_q: 14.246674, mean_eps: 0.798141
  224420/2000000: episode: 2304, duration: 1.129s, episode steps: 77, steps per second: 68, episode reward: -131.444, mean reward: -1.707 [-100.000, 5.226], mean action: 1.727 [0.000, 3.000], mean observation: 0.042 [-1.199, 3.700], loss: 1.309260, mean_absolute_error: 23.690193, mean_q: 13.453157, mean_eps: 0.798058
  224518/2000000: episode: 2305, duration: 1.392s, episode steps: 98, steps per second: 70, episode reward: -164.704, mean reward: -1.681 [-100.000, 5.950], mean action: 1.541 [0.000, 3.000], mean observation: 0.085 [-1.130, 3.502], loss: 1.578472, mean_absolute_error: 22.209026, mean_q: 15.585366, mean_eps: 0.797979
  224623/2000000: episode: 2306, duration: 1.468s, episode steps: 105, steps per second: 72, episode reward: -138.581, mean reward: -1.320 [-100.000, 8.802], mean action: 1.676 [0.000, 3.000], mean observation: -0.060 [-3.712, 1.000], loss: 1.971609, mean_absolute_error: 23.773409, mean_q: 11.148872, mean_eps: 0.797887
  224695/2000000: episode: 2307, duration: 1.008s, episode steps: 72, steps per second: 71, episode reward: -75.207, mean reward: -1.045 [-100.000, 12.793], mean action: 1.542 [0.000, 3.000], mean observation: -0.081 [-1.253, 1.000], loss: 1.474311, mean_absolute_error: 23.177676, mean_q: 13.403268, mean_eps: 0.797808
  224817/2000000: episode: 2308, duration: 1.752s, episode steps: 122, steps per second: 70, episode reward: -203.597, mean reward: -1.669 [-100.000, 9.537], mean action: 1.402 [0.000, 3.000], mean observation: 0.183 [-3.678, 1.108], loss: 1.345091, mean_absolute_error: 22.932372, mean_q: 12.672211, mean_eps: 0.797720
  224906/2000000: episode: 2309, duration: 1.250s, episode steps: 89, steps per second: 71, episode reward: -107.327, mean reward: -1.206 [-100.000, 14.846], mean action: 1.494 [0.000, 3.000], mean observation: 0.014 [-1.213, 1.000], loss: 3.107902, mean_absolute_error: 24.113497, mean_q: 11.747606, mean_eps: 0.797624
  224985/2000000: episode: 2310, duration: 1.128s, episode steps: 79, steps per second: 70, episode reward: -159.544, mean reward: -2.020 [-100.000, 7.217], mean action: 1.494 [0.000, 3.000], mean observation: 0.088 [-4.546, 1.000], loss: 2.156238, mean_absolute_error: 24.620609, mean_q: 10.978169, mean_eps: 0.797549
  225059/2000000: episode: 2311, duration: 1.023s, episode steps: 74, steps per second: 72, episode reward: -134.711, mean reward: -1.820 [-100.000, 6.398], mean action: 1.527 [0.000, 3.000], mean observation: -0.012 [-4.360, 1.000], loss: 1.854503, mean_absolute_error: 22.853852, mean_q: 14.421410, mean_eps: 0.797480
  225144/2000000: episode: 2312, duration: 1.225s, episode steps: 85, steps per second: 69, episode reward: -135.516, mean reward: -1.594 [-100.000, 5.847], mean action: 1.353 [0.000, 3.000], mean observation: 0.037 [-3.940, 1.000], loss: 1.801320, mean_absolute_error: 23.739358, mean_q: 11.207726, mean_eps: 0.797410
  225239/2000000: episode: 2313, duration: 1.352s, episode steps: 95, steps per second: 70, episode reward: -120.087, mean reward: -1.264 [-100.000, 10.263], mean action: 1.411 [0.000, 3.000], mean observation: 0.112 [-0.960, 3.709], loss: 1.215427, mean_absolute_error: 24.106984, mean_q: 13.285068, mean_eps: 0.797329
  225312/2000000: episode: 2314, duration: 1.081s, episode steps: 73, steps per second: 68, episode reward: -139.766, mean reward: -1.915 [-100.000, 11.021], mean action: 1.644 [0.000, 3.000], mean observation: -0.041 [-4.462, 1.000], loss: 1.450386, mean_absolute_error: 22.123972, mean_q: 15.482121, mean_eps: 0.797253
  225403/2000000: episode: 2315, duration: 1.307s, episode steps: 91, steps per second: 70, episode reward: -79.998, mean reward: -0.879 [-100.000, 10.661], mean action: 1.714 [0.000, 3.000], mean observation: -0.139 [-0.927, 2.842], loss: 1.632159, mean_absolute_error: 24.753923, mean_q: 12.158036, mean_eps: 0.797180
  225470/2000000: episode: 2316, duration: 0.965s, episode steps: 67, steps per second: 69, episode reward: -197.301, mean reward: -2.945 [-100.000, 8.047], mean action: 1.746 [0.000, 3.000], mean observation: -0.118 [-1.531, 1.000], loss: 2.606420, mean_absolute_error: 25.228657, mean_q: 10.343300, mean_eps: 0.797108
  225575/2000000: episode: 2317, duration: 1.475s, episode steps: 105, steps per second: 71, episode reward: -89.964, mean reward: -0.857 [-100.000, 10.283], mean action: 1.495 [0.000, 3.000], mean observation: 0.108 [-1.066, 3.636], loss: 1.657401, mean_absolute_error: 22.342645, mean_q: 15.224023, mean_eps: 0.797030
  225694/2000000: episode: 2318, duration: 1.683s, episode steps: 119, steps per second: 71, episode reward: -94.442, mean reward: -0.794 [-100.000, 23.468], mean action: 1.529 [0.000, 3.000], mean observation: -0.002 [-1.085, 1.000], loss: 1.530798, mean_absolute_error: 23.045474, mean_q: 14.066826, mean_eps: 0.796929
  225802/2000000: episode: 2319, duration: 1.630s, episode steps: 108, steps per second: 66, episode reward: -98.544, mean reward: -0.912 [-100.000, 7.882], mean action: 1.694 [0.000, 3.000], mean observation: -0.054 [-0.869, 3.150], loss: 1.657660, mean_absolute_error: 24.062512, mean_q: 12.800091, mean_eps: 0.796827
  225866/2000000: episode: 2320, duration: 0.960s, episode steps: 64, steps per second: 67, episode reward: -96.264, mean reward: -1.504 [-100.000, 6.974], mean action: 1.766 [0.000, 3.000], mean observation: -0.143 [-4.558, 1.000], loss: 1.267393, mean_absolute_error: 23.228199, mean_q: 14.166019, mean_eps: 0.796749
  225958/2000000: episode: 2321, duration: 1.449s, episode steps: 92, steps per second: 63, episode reward: -122.261, mean reward: -1.329 [-100.000, 12.788], mean action: 1.337 [0.000, 3.000], mean observation: 0.146 [-1.104, 1.000], loss: 1.873615, mean_absolute_error: 22.479062, mean_q: 13.460115, mean_eps: 0.796679
  226077/2000000: episode: 2322, duration: 1.693s, episode steps: 119, steps per second: 70, episode reward: -201.356, mean reward: -1.692 [-100.000, 5.928], mean action: 1.765 [0.000, 3.000], mean observation: -0.187 [-1.235, 1.000], loss: 1.577566, mean_absolute_error: 23.837110, mean_q: 11.906586, mean_eps: 0.796584
  226149/2000000: episode: 2323, duration: 1.123s, episode steps: 72, steps per second: 64, episode reward: -97.516, mean reward: -1.354 [-100.000, 7.033], mean action: 1.708 [0.000, 3.000], mean observation: -0.103 [-3.866, 1.000], loss: 1.416171, mean_absolute_error: 23.082663, mean_q: 12.035725, mean_eps: 0.796497
  226266/2000000: episode: 2324, duration: 1.703s, episode steps: 117, steps per second: 69, episode reward: -182.956, mean reward: -1.564 [-100.000, 3.100], mean action: 1.256 [0.000, 3.000], mean observation: 0.144 [-1.334, 5.026], loss: 1.692168, mean_absolute_error: 23.108368, mean_q: 13.997494, mean_eps: 0.796413
  226338/2000000: episode: 2325, duration: 1.046s, episode steps: 72, steps per second: 69, episode reward: -107.889, mean reward: -1.498 [-100.000, 15.181], mean action: 1.764 [0.000, 3.000], mean observation: -0.067 [-3.875, 1.000], loss: 1.445905, mean_absolute_error: 22.784845, mean_q: 13.114651, mean_eps: 0.796328
  226448/2000000: episode: 2326, duration: 1.627s, episode steps: 110, steps per second: 68, episode reward: -85.745, mean reward: -0.779 [-100.000, 23.597], mean action: 1.491 [0.000, 3.000], mean observation: -0.125 [-1.546, 1.000], loss: 1.773092, mean_absolute_error: 22.262668, mean_q: 15.213272, mean_eps: 0.796247
  226539/2000000: episode: 2327, duration: 1.306s, episode steps: 91, steps per second: 70, episode reward: -148.155, mean reward: -1.628 [-100.000, 17.516], mean action: 1.396 [0.000, 3.000], mean observation: 0.093 [-1.174, 1.000], loss: 1.838795, mean_absolute_error: 23.907013, mean_q: 12.733488, mean_eps: 0.796157
  226650/2000000: episode: 2328, duration: 1.577s, episode steps: 111, steps per second: 70, episode reward: -238.474, mean reward: -2.148 [-100.000, 21.468], mean action: 1.649 [0.000, 3.000], mean observation: -0.108 [-3.572, 1.000], loss: 2.040226, mean_absolute_error: 24.088788, mean_q: 11.639312, mean_eps: 0.796065
  226803/2000000: episode: 2329, duration: 2.166s, episode steps: 153, steps per second: 71, episode reward: -106.721, mean reward: -0.698 [-100.000, 12.748], mean action: 1.680 [0.000, 3.000], mean observation: 0.004 [-4.188, 1.059], loss: 1.547215, mean_absolute_error: 23.320711, mean_q: 14.030017, mean_eps: 0.795947
  226899/2000000: episode: 2330, duration: 1.380s, episode steps: 96, steps per second: 70, episode reward: -122.073, mean reward: -1.272 [-100.000, 9.386], mean action: 1.740 [0.000, 3.000], mean observation: -0.145 [-3.923, 1.000], loss: 1.604837, mean_absolute_error: 24.014464, mean_q: 11.297972, mean_eps: 0.795835
  226986/2000000: episode: 2331, duration: 1.239s, episode steps: 87, steps per second: 70, episode reward: -147.997, mean reward: -1.701 [-100.000, 18.104], mean action: 1.667 [0.000, 3.000], mean observation: -0.042 [-1.366, 1.000], loss: 1.601142, mean_absolute_error: 22.754591, mean_q: 16.120004, mean_eps: 0.795752
  227112/2000000: episode: 2332, duration: 1.793s, episode steps: 126, steps per second: 70, episode reward: -154.948, mean reward: -1.230 [-100.000, 7.632], mean action: 1.540 [0.000, 3.000], mean observation: 0.048 [-1.271, 4.266], loss: 1.461697, mean_absolute_error: 23.524326, mean_q: 14.568113, mean_eps: 0.795657
  227213/2000000: episode: 2333, duration: 1.513s, episode steps: 101, steps per second: 67, episode reward: -162.074, mean reward: -1.605 [-100.000, 7.155], mean action: 1.604 [0.000, 3.000], mean observation: -0.091 [-1.232, 1.500], loss: 1.350485, mean_absolute_error: 24.060282, mean_q: 11.778675, mean_eps: 0.795554
  227316/2000000: episode: 2334, duration: 1.477s, episode steps: 103, steps per second: 70, episode reward: -102.613, mean reward: -0.996 [-100.000, 12.804], mean action: 1.495 [0.000, 3.000], mean observation: -0.008 [-1.294, 1.000], loss: 1.295877, mean_absolute_error: 23.375516, mean_q: 14.896095, mean_eps: 0.795462
  227388/2000000: episode: 2335, duration: 1.069s, episode steps: 72, steps per second: 67, episode reward: -121.831, mean reward: -1.692 [-100.000, 22.568], mean action: 1.458 [0.000, 3.000], mean observation: -0.074 [-1.476, 1.000], loss: 1.901207, mean_absolute_error: 24.253590, mean_q: 12.089001, mean_eps: 0.795385
  227550/2000000: episode: 2336, duration: 2.478s, episode steps: 162, steps per second: 65, episode reward: -147.897, mean reward: -0.913 [-100.000, 10.755], mean action: 1.568 [0.000, 3.000], mean observation: -0.097 [-2.753, 1.014], loss: 1.630291, mean_absolute_error: 23.527559, mean_q: 13.914456, mean_eps: 0.795279
  227660/2000000: episode: 2337, duration: 1.586s, episode steps: 110, steps per second: 69, episode reward: -144.602, mean reward: -1.315 [-100.000, 7.032], mean action: 1.500 [0.000, 3.000], mean observation: 0.030 [-4.018, 1.000], loss: 1.701223, mean_absolute_error: 23.509369, mean_q: 14.335241, mean_eps: 0.795156
  227751/2000000: episode: 2338, duration: 1.307s, episode steps: 91, steps per second: 70, episode reward: -146.069, mean reward: -1.605 [-100.000, 14.516], mean action: 1.648 [0.000, 3.000], mean observation: 0.019 [-4.465, 1.000], loss: 1.451010, mean_absolute_error: 23.564563, mean_q: 13.453117, mean_eps: 0.795066
  227842/2000000: episode: 2339, duration: 1.298s, episode steps: 91, steps per second: 70, episode reward: -110.070, mean reward: -1.210 [-100.000, 24.483], mean action: 1.626 [0.000, 3.000], mean observation: -0.007 [-1.475, 1.000], loss: 1.492164, mean_absolute_error: 23.545973, mean_q: 12.312350, mean_eps: 0.794984
  227924/2000000: episode: 2340, duration: 1.192s, episode steps: 82, steps per second: 69, episode reward: -95.835, mean reward: -1.169 [-100.000, 7.971], mean action: 1.488 [0.000, 3.000], mean observation: -0.011 [-3.852, 1.000], loss: 1.158440, mean_absolute_error: 22.466780, mean_q: 13.833044, mean_eps: 0.794906
  228013/2000000: episode: 2341, duration: 1.301s, episode steps: 89, steps per second: 68, episode reward: -144.785, mean reward: -1.627 [-100.000, 17.503], mean action: 1.629 [0.000, 3.000], mean observation: 0.011 [-2.561, 1.000], loss: 1.573703, mean_absolute_error: 23.963903, mean_q: 13.752278, mean_eps: 0.794829
  228098/2000000: episode: 2342, duration: 1.187s, episode steps: 85, steps per second: 72, episode reward: -185.000, mean reward: -2.176 [-100.000, 5.307], mean action: 1.565 [0.000, 3.000], mean observation: -0.168 [-1.316, 4.352], loss: 1.534346, mean_absolute_error: 23.367451, mean_q: 13.037000, mean_eps: 0.794750
  228186/2000000: episode: 2343, duration: 1.241s, episode steps: 88, steps per second: 71, episode reward: -85.646, mean reward: -0.973 [-100.000, 20.997], mean action: 1.568 [0.000, 3.000], mean observation: -0.032 [-3.696, 1.000], loss: 1.277175, mean_absolute_error: 22.752317, mean_q: 13.467077, mean_eps: 0.794672
  228301/2000000: episode: 2344, duration: 1.667s, episode steps: 115, steps per second: 69, episode reward: -213.837, mean reward: -1.859 [-100.000, 8.189], mean action: 1.443 [0.000, 3.000], mean observation: 0.106 [-1.573, 1.034], loss: 1.365232, mean_absolute_error: 23.594418, mean_q: 12.550314, mean_eps: 0.794580
  228392/2000000: episode: 2345, duration: 1.316s, episode steps: 91, steps per second: 69, episode reward: -78.664, mean reward: -0.864 [-100.000, 12.680], mean action: 1.747 [0.000, 3.000], mean observation: -0.063 [-1.011, 1.000], loss: 1.137965, mean_absolute_error: 22.535649, mean_q: 13.399145, mean_eps: 0.794489
  228498/2000000: episode: 2346, duration: 1.630s, episode steps: 106, steps per second: 65, episode reward: -133.270, mean reward: -1.257 [-100.000, 7.224], mean action: 1.632 [0.000, 3.000], mean observation: 0.100 [-3.752, 1.000], loss: 1.401500, mean_absolute_error: 23.509050, mean_q: 12.535356, mean_eps: 0.794400
  228602/2000000: episode: 2347, duration: 1.479s, episode steps: 104, steps per second: 70, episode reward: -172.131, mean reward: -1.655 [-100.000, 9.248], mean action: 1.673 [0.000, 3.000], mean observation: -0.135 [-1.329, 4.171], loss: 1.884334, mean_absolute_error: 23.669229, mean_q: 13.025122, mean_eps: 0.794305
  228700/2000000: episode: 2348, duration: 1.421s, episode steps: 98, steps per second: 69, episode reward: -161.531, mean reward: -1.648 [-100.000, 6.371], mean action: 1.582 [0.000, 3.000], mean observation: -0.118 [-4.972, 1.000], loss: 1.682323, mean_absolute_error: 23.153842, mean_q: 13.994318, mean_eps: 0.794215
  228809/2000000: episode: 2349, duration: 1.574s, episode steps: 109, steps per second: 69, episode reward: -91.764, mean reward: -0.842 [-100.000, 9.304], mean action: 1.761 [0.000, 3.000], mean observation: -0.080 [-0.948, 3.049], loss: 1.929801, mean_absolute_error: 23.705898, mean_q: 12.491471, mean_eps: 0.794121
  228908/2000000: episode: 2350, duration: 1.397s, episode steps: 99, steps per second: 71, episode reward: -158.055, mean reward: -1.597 [-100.000, 11.066], mean action: 1.505 [0.000, 3.000], mean observation: -0.171 [-1.260, 3.980], loss: 2.070775, mean_absolute_error: 24.518451, mean_q: 13.190188, mean_eps: 0.794028
  228980/2000000: episode: 2351, duration: 1.073s, episode steps: 72, steps per second: 67, episode reward: -3.933, mean reward: -0.055 [-100.000, 125.271], mean action: 1.667 [0.000, 3.000], mean observation: -0.089 [-1.693, 1.810], loss: 1.385664, mean_absolute_error: 23.312699, mean_q: 13.917333, mean_eps: 0.793952
  229104/2000000: episode: 2352, duration: 1.806s, episode steps: 124, steps per second: 69, episode reward: -333.245, mean reward: -2.687 [-100.000, 2.040], mean action: 1.556 [0.000, 3.000], mean observation: -0.013 [-1.469, 1.075], loss: 2.040719, mean_absolute_error: 24.006142, mean_q: 11.896308, mean_eps: 0.793864
  229229/2000000: episode: 2353, duration: 1.818s, episode steps: 125, steps per second: 69, episode reward: -199.070, mean reward: -1.593 [-100.000, 5.876], mean action: 1.696 [0.000, 3.000], mean observation: -0.155 [-1.060, 1.000], loss: 1.721840, mean_absolute_error: 22.734284, mean_q: 12.557374, mean_eps: 0.793751
  229329/2000000: episode: 2354, duration: 1.437s, episode steps: 100, steps per second: 70, episode reward: -114.309, mean reward: -1.143 [-100.000, 14.481], mean action: 1.620 [0.000, 3.000], mean observation: 0.019 [-3.192, 1.000], loss: 1.938603, mean_absolute_error: 23.688041, mean_q: 14.264437, mean_eps: 0.793648
  229400/2000000: episode: 2355, duration: 1.036s, episode steps: 71, steps per second: 69, episode reward: -133.852, mean reward: -1.885 [-100.000, 5.175], mean action: 1.380 [0.000, 3.000], mean observation: 0.107 [-3.555, 1.000], loss: 1.557483, mean_absolute_error: 23.653401, mean_q: 13.336896, mean_eps: 0.793572
  229507/2000000: episode: 2356, duration: 1.536s, episode steps: 107, steps per second: 70, episode reward: -271.048, mean reward: -2.533 [-100.000, 48.404], mean action: 1.692 [0.000, 3.000], mean observation: -0.171 [-3.140, 1.000], loss: 1.765244, mean_absolute_error: 23.469077, mean_q: 14.406681, mean_eps: 0.793493
  229636/2000000: episode: 2357, duration: 1.888s, episode steps: 129, steps per second: 68, episode reward: -85.161, mean reward: -0.660 [-100.000, 44.131], mean action: 1.674 [0.000, 3.000], mean observation: 0.178 [-1.855, 1.000], loss: 1.761034, mean_absolute_error: 22.773961, mean_q: 14.071773, mean_eps: 0.793387
  229738/2000000: episode: 2358, duration: 1.490s, episode steps: 102, steps per second: 68, episode reward: -144.915, mean reward: -1.421 [-100.000, 8.957], mean action: 1.647 [0.000, 3.000], mean observation: -0.100 [-4.210, 1.000], loss: 1.806016, mean_absolute_error: 23.002006, mean_q: 13.861742, mean_eps: 0.793283
  229848/2000000: episode: 2359, duration: 1.592s, episode steps: 110, steps per second: 69, episode reward: -154.844, mean reward: -1.408 [-100.000, 12.993], mean action: 1.609 [0.000, 3.000], mean observation: -0.105 [-4.199, 1.000], loss: 2.142631, mean_absolute_error: 22.700093, mean_q: 12.992268, mean_eps: 0.793187
  229920/2000000: episode: 2360, duration: 1.094s, episode steps: 72, steps per second: 66, episode reward: -128.773, mean reward: -1.789 [-100.000, 3.982], mean action: 1.556 [0.000, 3.000], mean observation: -0.113 [-1.225, 2.445], loss: 1.599068, mean_absolute_error: 22.974952, mean_q: 13.891600, mean_eps: 0.793106
  229991/2000000: episode: 2361, duration: 1.047s, episode steps: 71, steps per second: 68, episode reward: -118.042, mean reward: -1.663 [-100.000, 6.704], mean action: 1.620 [0.000, 3.000], mean observation: -0.119 [-1.308, 3.461], loss: 1.917057, mean_absolute_error: 25.405522, mean_q: 9.600573, mean_eps: 0.793041
  230076/2000000: episode: 2362, duration: 1.267s, episode steps: 85, steps per second: 67, episode reward: -167.460, mean reward: -1.970 [-100.000, 2.921], mean action: 1.624 [0.000, 3.000], mean observation: -0.132 [-1.221, 4.120], loss: 1.576786, mean_absolute_error: 25.203680, mean_q: 12.592431, mean_eps: 0.792971
  230207/2000000: episode: 2363, duration: 1.923s, episode steps: 131, steps per second: 68, episode reward: -180.817, mean reward: -1.380 [-100.000, 9.530], mean action: 1.527 [0.000, 3.000], mean observation: 0.165 [-2.783, 1.043], loss: 2.186139, mean_absolute_error: 25.566141, mean_q: 12.932828, mean_eps: 0.792874
  230285/2000000: episode: 2364, duration: 1.189s, episode steps: 78, steps per second: 66, episode reward: -135.445, mean reward: -1.736 [-100.000, 6.477], mean action: 1.577 [0.000, 3.000], mean observation: 0.128 [-1.186, 4.625], loss: 1.631255, mean_absolute_error: 25.125695, mean_q: 13.454912, mean_eps: 0.792779
  230378/2000000: episode: 2365, duration: 1.333s, episode steps: 93, steps per second: 70, episode reward: -154.909, mean reward: -1.666 [-100.000, 8.668], mean action: 1.667 [0.000, 3.000], mean observation: 0.031 [-5.002, 1.000], loss: 2.219424, mean_absolute_error: 25.314276, mean_q: 12.647235, mean_eps: 0.792701
  230445/2000000: episode: 2366, duration: 0.961s, episode steps: 67, steps per second: 70, episode reward: -113.263, mean reward: -1.690 [-100.000, 5.591], mean action: 1.567 [0.000, 3.000], mean observation: 0.093 [-1.201, 4.460], loss: 2.088355, mean_absolute_error: 25.158314, mean_q: 11.261359, mean_eps: 0.792629
  230550/2000000: episode: 2367, duration: 1.467s, episode steps: 105, steps per second: 72, episode reward: -120.477, mean reward: -1.147 [-100.000, 23.109], mean action: 1.552 [0.000, 3.000], mean observation: 0.072 [-1.199, 1.000], loss: 1.578873, mean_absolute_error: 23.618234, mean_q: 14.677332, mean_eps: 0.792552
  230647/2000000: episode: 2368, duration: 1.365s, episode steps: 97, steps per second: 71, episode reward: -164.555, mean reward: -1.696 [-100.000, 30.187], mean action: 1.402 [0.000, 3.000], mean observation: -0.132 [-4.234, 1.000], loss: 1.727022, mean_absolute_error: 23.877921, mean_q: 13.841619, mean_eps: 0.792462
  230743/2000000: episode: 2369, duration: 1.370s, episode steps: 96, steps per second: 70, episode reward: -138.386, mean reward: -1.442 [-100.000, 21.145], mean action: 1.719 [0.000, 3.000], mean observation: -0.087 [-1.314, 1.000], loss: 1.424339, mean_absolute_error: 24.448501, mean_q: 16.045261, mean_eps: 0.792375
  230818/2000000: episode: 2370, duration: 1.113s, episode steps: 75, steps per second: 67, episode reward: -117.864, mean reward: -1.572 [-100.000, 8.112], mean action: 1.627 [0.000, 3.000], mean observation: -0.117 [-1.328, 2.473], loss: 1.787765, mean_absolute_error: 24.829759, mean_q: 13.657437, mean_eps: 0.792298
  230914/2000000: episode: 2371, duration: 1.381s, episode steps: 96, steps per second: 70, episode reward: -187.723, mean reward: -1.955 [-100.000, 9.214], mean action: 1.490 [0.000, 3.000], mean observation: 0.051 [-4.624, 1.000], loss: 2.335251, mean_absolute_error: 26.301846, mean_q: 11.491344, mean_eps: 0.792221
  231017/2000000: episode: 2372, duration: 1.469s, episode steps: 103, steps per second: 70, episode reward: -199.005, mean reward: -1.932 [-100.000, 2.943], mean action: 1.660 [0.000, 3.000], mean observation: -0.046 [-1.412, 4.849], loss: 1.670281, mean_absolute_error: 24.719491, mean_q: 14.375759, mean_eps: 0.792131
  231114/2000000: episode: 2373, duration: 1.420s, episode steps: 97, steps per second: 68, episode reward: -88.651, mean reward: -0.914 [-100.000, 10.878], mean action: 1.608 [0.000, 3.000], mean observation: -0.044 [-1.050, 1.000], loss: 1.417074, mean_absolute_error: 24.169682, mean_q: 14.405912, mean_eps: 0.792041
  231201/2000000: episode: 2374, duration: 1.283s, episode steps: 87, steps per second: 68, episode reward: -132.995, mean reward: -1.529 [-100.000, 20.409], mean action: 1.655 [0.000, 3.000], mean observation: -0.100 [-3.912, 1.000], loss: 1.227510, mean_absolute_error: 24.493961, mean_q: 14.271215, mean_eps: 0.791958
  231285/2000000: episode: 2375, duration: 1.195s, episode steps: 84, steps per second: 70, episode reward: -173.941, mean reward: -2.071 [-100.000, 6.620], mean action: 1.571 [0.000, 3.000], mean observation: -0.025 [-1.351, 4.479], loss: 1.708729, mean_absolute_error: 24.593314, mean_q: 14.458289, mean_eps: 0.791880
  231380/2000000: episode: 2376, duration: 1.361s, episode steps: 95, steps per second: 70, episode reward: -146.533, mean reward: -1.542 [-100.000, 10.511], mean action: 1.621 [0.000, 3.000], mean observation: -0.021 [-1.393, 4.404], loss: 1.427540, mean_absolute_error: 24.479747, mean_q: 14.778157, mean_eps: 0.791801
  231492/2000000: episode: 2377, duration: 1.669s, episode steps: 112, steps per second: 67, episode reward: -145.622, mean reward: -1.300 [-100.000, 10.614], mean action: 1.545 [0.000, 3.000], mean observation: -0.118 [-0.938, 3.007], loss: 2.479869, mean_absolute_error: 25.565542, mean_q: 11.074856, mean_eps: 0.791709
  231618/2000000: episode: 2378, duration: 1.830s, episode steps: 126, steps per second: 69, episode reward: -157.509, mean reward: -1.250 [-100.000, 11.834], mean action: 1.492 [0.000, 3.000], mean observation: 0.083 [-4.139, 1.000], loss: 2.129780, mean_absolute_error: 24.829209, mean_q: 12.598416, mean_eps: 0.791601
  231713/2000000: episode: 2379, duration: 1.363s, episode steps: 95, steps per second: 70, episode reward: -96.828, mean reward: -1.019 [-100.000, 13.179], mean action: 1.695 [0.000, 3.000], mean observation: -0.090 [-1.095, 1.000], loss: 1.752672, mean_absolute_error: 23.230702, mean_q: 15.792713, mean_eps: 0.791501
  231780/2000000: episode: 2380, duration: 0.982s, episode steps: 67, steps per second: 68, episode reward: -127.615, mean reward: -1.905 [-100.000, 5.825], mean action: 1.642 [0.000, 3.000], mean observation: -0.140 [-1.250, 4.001], loss: 1.174554, mean_absolute_error: 23.639621, mean_q: 15.305915, mean_eps: 0.791429
  231896/2000000: episode: 2381, duration: 1.731s, episode steps: 116, steps per second: 67, episode reward: -188.631, mean reward: -1.626 [-100.000, 6.913], mean action: 1.664 [0.000, 3.000], mean observation: -0.129 [-1.301, 4.067], loss: 1.346368, mean_absolute_error: 24.743977, mean_q: 14.999109, mean_eps: 0.791348
  232001/2000000: episode: 2382, duration: 1.535s, episode steps: 105, steps per second: 68, episode reward: -149.686, mean reward: -1.426 [-100.000, 9.146], mean action: 1.610 [0.000, 3.000], mean observation: 0.021 [-1.134, 3.430], loss: 1.582917, mean_absolute_error: 24.489219, mean_q: 13.277197, mean_eps: 0.791247
  232094/2000000: episode: 2383, duration: 1.330s, episode steps: 93, steps per second: 70, episode reward: -133.377, mean reward: -1.434 [-100.000, 9.601], mean action: 1.634 [0.000, 3.000], mean observation: -0.094 [-1.036, 1.000], loss: 2.087532, mean_absolute_error: 24.468961, mean_q: 14.705135, mean_eps: 0.791157
  232234/2000000: episode: 2384, duration: 1.984s, episode steps: 140, steps per second: 71, episode reward: -222.863, mean reward: -1.592 [-100.000, 93.108], mean action: 1.650 [0.000, 3.000], mean observation: -0.168 [-1.951, 1.000], loss: 1.437490, mean_absolute_error: 24.450121, mean_q: 15.800446, mean_eps: 0.791052
  232357/2000000: episode: 2385, duration: 1.778s, episode steps: 123, steps per second: 69, episode reward: -401.430, mean reward: -3.264 [-100.000, 107.417], mean action: 1.520 [0.000, 3.000], mean observation: 0.170 [-1.423, 3.435], loss: 2.046419, mean_absolute_error: 25.434516, mean_q: 13.088082, mean_eps: 0.790934
  232455/2000000: episode: 2386, duration: 1.388s, episode steps: 98, steps per second: 71, episode reward: -158.327, mean reward: -1.616 [-100.000, 8.022], mean action: 1.643 [0.000, 3.000], mean observation: -0.105 [-1.255, 4.310], loss: 1.876613, mean_absolute_error: 25.166377, mean_q: 13.185768, mean_eps: 0.790835
  232541/2000000: episode: 2387, duration: 1.247s, episode steps: 86, steps per second: 69, episode reward: -176.221, mean reward: -2.049 [-100.000, 32.292], mean action: 1.570 [0.000, 3.000], mean observation: 0.049 [-1.144, 2.127], loss: 1.158835, mean_absolute_error: 24.081100, mean_q: 13.601437, mean_eps: 0.790752
  232736/2000000: episode: 2388, duration: 2.778s, episode steps: 195, steps per second: 70, episode reward: -153.682, mean reward: -0.788 [-100.000, 11.842], mean action: 1.708 [0.000, 3.000], mean observation: -0.031 [-1.046, 3.453], loss: 1.625956, mean_absolute_error: 25.286239, mean_q: 12.535857, mean_eps: 0.790626
  232832/2000000: episode: 2389, duration: 1.423s, episode steps: 96, steps per second: 67, episode reward: -62.571, mean reward: -0.652 [-100.000, 14.601], mean action: 1.594 [0.000, 3.000], mean observation: -0.023 [-3.240, 1.000], loss: 1.655483, mean_absolute_error: 24.885773, mean_q: 12.349078, mean_eps: 0.790496
  232904/2000000: episode: 2390, duration: 1.052s, episode steps: 72, steps per second: 68, episode reward: -64.925, mean reward: -0.902 [-100.000, 12.524], mean action: 1.611 [0.000, 3.000], mean observation: -0.008 [-3.177, 1.000], loss: 1.544840, mean_absolute_error: 24.310347, mean_q: 14.424929, mean_eps: 0.790421
  232988/2000000: episode: 2391, duration: 1.245s, episode steps: 84, steps per second: 67, episode reward: -109.287, mean reward: -1.301 [-100.000, 6.468], mean action: 1.845 [0.000, 3.000], mean observation: -0.109 [-1.129, 3.593], loss: 1.517820, mean_absolute_error: 26.188940, mean_q: 12.822269, mean_eps: 0.790350
  233054/2000000: episode: 2392, duration: 0.973s, episode steps: 66, steps per second: 68, episode reward: -97.409, mean reward: -1.476 [-100.000, 17.705], mean action: 1.545 [0.000, 3.000], mean observation: -0.002 [-1.314, 4.284], loss: 1.301355, mean_absolute_error: 24.553984, mean_q: 15.213984, mean_eps: 0.790282
  233153/2000000: episode: 2393, duration: 1.413s, episode steps: 99, steps per second: 70, episode reward: -156.147, mean reward: -1.577 [-100.000, 6.820], mean action: 1.606 [0.000, 3.000], mean observation: 0.007 [-1.213, 4.231], loss: 2.040075, mean_absolute_error: 25.975893, mean_q: 11.481218, mean_eps: 0.790206
  233273/2000000: episode: 2394, duration: 1.704s, episode steps: 120, steps per second: 70, episode reward: -127.187, mean reward: -1.060 [-100.000, 15.081], mean action: 1.650 [0.000, 3.000], mean observation: 0.084 [-4.462, 1.002], loss: 1.773735, mean_absolute_error: 23.563641, mean_q: 15.157770, mean_eps: 0.790107
  233370/2000000: episode: 2395, duration: 1.369s, episode steps: 97, steps per second: 71, episode reward: -154.598, mean reward: -1.594 [-100.000, 11.730], mean action: 1.536 [0.000, 3.000], mean observation: -0.017 [-4.196, 1.000], loss: 1.345026, mean_absolute_error: 24.248790, mean_q: 17.506218, mean_eps: 0.790010
  233467/2000000: episode: 2396, duration: 1.358s, episode steps: 97, steps per second: 71, episode reward: -92.092, mean reward: -0.949 [-100.000, 12.461], mean action: 1.660 [0.000, 3.000], mean observation: -0.034 [-1.047, 3.037], loss: 1.594099, mean_absolute_error: 24.236563, mean_q: 16.109504, mean_eps: 0.789924
  233567/2000000: episode: 2397, duration: 1.424s, episode steps: 100, steps per second: 70, episode reward: -149.945, mean reward: -1.499 [-100.000, 8.716], mean action: 1.640 [0.000, 3.000], mean observation: 0.068 [-1.176, 1.000], loss: 1.621333, mean_absolute_error: 24.090629, mean_q: 15.245884, mean_eps: 0.789836
  233651/2000000: episode: 2398, duration: 1.188s, episode steps: 84, steps per second: 71, episode reward: -123.375, mean reward: -1.469 [-100.000, 11.657], mean action: 1.619 [0.000, 3.000], mean observation: 0.083 [-1.188, 4.177], loss: 1.417221, mean_absolute_error: 25.850922, mean_q: 11.447506, mean_eps: 0.789753
  233719/2000000: episode: 2399, duration: 0.980s, episode steps: 68, steps per second: 69, episode reward: -131.963, mean reward: -1.941 [-100.000, 17.216], mean action: 1.691 [0.000, 3.000], mean observation: -0.096 [-4.509, 1.000], loss: 2.613796, mean_absolute_error: 25.277354, mean_q: 10.650859, mean_eps: 0.789684
  233795/2000000: episode: 2400, duration: 1.088s, episode steps: 76, steps per second: 70, episode reward: -165.809, mean reward: -2.182 [-100.000, 16.041], mean action: 1.539 [0.000, 3.000], mean observation: 0.058 [-1.488, 1.153], loss: 1.405647, mean_absolute_error: 24.232588, mean_q: 13.058291, mean_eps: 0.789620
  233884/2000000: episode: 2401, duration: 1.334s, episode steps: 89, steps per second: 67, episode reward: -143.699, mean reward: -1.615 [-100.000, 18.584], mean action: 1.483 [0.000, 3.000], mean observation: 0.088 [-3.488, 1.000], loss: 1.468389, mean_absolute_error: 24.279864, mean_q: 13.565950, mean_eps: 0.789546
  234012/2000000: episode: 2402, duration: 1.851s, episode steps: 128, steps per second: 69, episode reward: -175.154, mean reward: -1.368 [-100.000, 17.263], mean action: 1.516 [0.000, 3.000], mean observation: -0.061 [-1.205, 4.016], loss: 1.449106, mean_absolute_error: 24.865641, mean_q: 14.182235, mean_eps: 0.789449
  234117/2000000: episode: 2403, duration: 1.547s, episode steps: 105, steps per second: 68, episode reward: -120.328, mean reward: -1.146 [-100.000, 12.243], mean action: 1.667 [0.000, 3.000], mean observation: 0.029 [-3.376, 1.000], loss: 1.519014, mean_absolute_error: 24.898486, mean_q: 13.485352, mean_eps: 0.789342
  234220/2000000: episode: 2404, duration: 1.465s, episode steps: 103, steps per second: 70, episode reward: -129.801, mean reward: -1.260 [-100.000, 16.621], mean action: 1.495 [0.000, 3.000], mean observation: 0.093 [-1.234, 1.000], loss: 1.557914, mean_absolute_error: 24.483786, mean_q: 12.877575, mean_eps: 0.789249
  234310/2000000: episode: 2405, duration: 1.317s, episode steps: 90, steps per second: 68, episode reward: -130.666, mean reward: -1.452 [-100.000, 7.764], mean action: 1.456 [0.000, 3.000], mean observation: 0.055 [-1.277, 4.459], loss: 1.778190, mean_absolute_error: 24.565274, mean_q: 12.761490, mean_eps: 0.789162
  234433/2000000: episode: 2406, duration: 1.755s, episode steps: 123, steps per second: 70, episode reward: -210.164, mean reward: -1.709 [-100.000, 3.186], mean action: 1.569 [0.000, 3.000], mean observation: 0.209 [-1.215, 1.053], loss: 1.229851, mean_absolute_error: 25.163291, mean_q: 13.739077, mean_eps: 0.789065
  234499/2000000: episode: 2407, duration: 0.925s, episode steps: 66, steps per second: 71, episode reward: -98.291, mean reward: -1.489 [-100.000, 11.658], mean action: 1.394 [0.000, 3.000], mean observation: 0.082 [-1.244, 4.494], loss: 1.553286, mean_absolute_error: 24.643340, mean_q: 15.829551, mean_eps: 0.788981
  234633/2000000: episode: 2408, duration: 1.894s, episode steps: 134, steps per second: 71, episode reward: -129.289, mean reward: -0.965 [-100.000, 19.981], mean action: 1.530 [0.000, 3.000], mean observation: 0.137 [-1.259, 2.554], loss: 1.692630, mean_absolute_error: 23.597512, mean_q: 14.887203, mean_eps: 0.788891
  234704/2000000: episode: 2409, duration: 1.032s, episode steps: 71, steps per second: 69, episode reward: -108.660, mean reward: -1.530 [-100.000, 7.845], mean action: 1.493 [0.000, 3.000], mean observation: -0.078 [-1.289, 4.548], loss: 1.714972, mean_absolute_error: 25.278861, mean_q: 12.788404, mean_eps: 0.788799
  234768/2000000: episode: 2410, duration: 0.955s, episode steps: 64, steps per second: 67, episode reward: -120.537, mean reward: -1.883 [-100.000, 7.648], mean action: 1.500 [0.000, 3.000], mean observation: -0.070 [-4.942, 1.000], loss: 1.710073, mean_absolute_error: 24.334638, mean_q: 14.386424, mean_eps: 0.788739
  234836/2000000: episode: 2411, duration: 1.019s, episode steps: 68, steps per second: 67, episode reward: -143.747, mean reward: -2.114 [-100.000, 11.505], mean action: 1.500 [0.000, 3.000], mean observation: -0.034 [-5.382, 1.000], loss: 1.267740, mean_absolute_error: 25.623505, mean_q: 13.084717, mean_eps: 0.788680
  234911/2000000: episode: 2412, duration: 1.085s, episode steps: 75, steps per second: 69, episode reward: -175.553, mean reward: -2.341 [-100.000, 15.303], mean action: 1.600 [0.000, 3.000], mean observation: -0.110 [-1.429, 1.000], loss: 1.457682, mean_absolute_error: 26.267935, mean_q: 13.306093, mean_eps: 0.788615
  235024/2000000: episode: 2413, duration: 1.651s, episode steps: 113, steps per second: 68, episode reward: -201.651, mean reward: -1.785 [-100.000, 7.018], mean action: 1.681 [0.000, 3.000], mean observation: -0.130 [-1.355, 4.264], loss: 1.408383, mean_absolute_error: 24.669911, mean_q: 14.569056, mean_eps: 0.788531
  235126/2000000: episode: 2414, duration: 1.467s, episode steps: 102, steps per second: 70, episode reward: -134.621, mean reward: -1.320 [-100.000, 12.212], mean action: 1.392 [0.000, 3.000], mean observation: -0.076 [-4.437, 1.015], loss: 1.322769, mean_absolute_error: 23.447327, mean_q: 15.269145, mean_eps: 0.788433
  235227/2000000: episode: 2415, duration: 1.426s, episode steps: 101, steps per second: 71, episode reward: -142.327, mean reward: -1.409 [-100.000, 12.574], mean action: 1.634 [0.000, 3.000], mean observation: -0.044 [-1.406, 4.723], loss: 1.635373, mean_absolute_error: 25.056415, mean_q: 12.945285, mean_eps: 0.788342
  235324/2000000: episode: 2416, duration: 1.399s, episode steps: 97, steps per second: 69, episode reward: -137.711, mean reward: -1.420 [-100.000, 10.580], mean action: 1.588 [0.000, 3.000], mean observation: 0.132 [-2.612, 1.028], loss: 2.212163, mean_absolute_error: 24.204805, mean_q: 15.153374, mean_eps: 0.788253
  235396/2000000: episode: 2417, duration: 1.089s, episode steps: 72, steps per second: 66, episode reward: -98.688, mean reward: -1.371 [-100.000, 10.952], mean action: 1.486 [0.000, 3.000], mean observation: 0.045 [-3.918, 1.000], loss: 1.378183, mean_absolute_error: 23.096240, mean_q: 13.811031, mean_eps: 0.788178
  235484/2000000: episode: 2418, duration: 1.296s, episode steps: 88, steps per second: 68, episode reward: -115.682, mean reward: -1.315 [-100.000, 20.235], mean action: 1.239 [0.000, 3.000], mean observation: -0.014 [-1.207, 1.000], loss: 1.683127, mean_absolute_error: 25.031141, mean_q: 14.105235, mean_eps: 0.788106
  235601/2000000: episode: 2419, duration: 1.690s, episode steps: 117, steps per second: 69, episode reward: -164.456, mean reward: -1.406 [-100.000, 13.762], mean action: 1.692 [0.000, 3.000], mean observation: 0.107 [-2.864, 1.000], loss: 1.723820, mean_absolute_error: 25.167934, mean_q: 12.345807, mean_eps: 0.788012
  235685/2000000: episode: 2420, duration: 1.187s, episode steps: 84, steps per second: 71, episode reward: -105.826, mean reward: -1.260 [-100.000, 11.834], mean action: 1.643 [0.000, 3.000], mean observation: -0.055 [-1.139, 3.599], loss: 1.851128, mean_absolute_error: 23.994296, mean_q: 13.253775, mean_eps: 0.787920
  235800/2000000: episode: 2421, duration: 1.626s, episode steps: 115, steps per second: 71, episode reward: -158.434, mean reward: -1.378 [-100.000, 16.041], mean action: 1.574 [0.000, 3.000], mean observation: 0.086 [-3.766, 1.000], loss: 1.458518, mean_absolute_error: 24.606528, mean_q: 14.088953, mean_eps: 0.787832
  235870/2000000: episode: 2422, duration: 1.042s, episode steps: 70, steps per second: 67, episode reward: -98.869, mean reward: -1.412 [-100.000, 7.723], mean action: 1.743 [0.000, 3.000], mean observation: -0.105 [-1.231, 3.605], loss: 2.121938, mean_absolute_error: 25.578074, mean_q: 11.279964, mean_eps: 0.787749
  235946/2000000: episode: 2423, duration: 1.092s, episode steps: 76, steps per second: 70, episode reward: -149.871, mean reward: -1.972 [-100.000, 7.532], mean action: 1.711 [0.000, 3.000], mean observation: -0.046 [-1.351, 4.176], loss: 1.164876, mean_absolute_error: 25.304632, mean_q: 13.986874, mean_eps: 0.787683
  236024/2000000: episode: 2424, duration: 1.139s, episode steps: 78, steps per second: 69, episode reward: -113.691, mean reward: -1.458 [-100.000, 6.926], mean action: 1.692 [0.000, 3.000], mean observation: -0.116 [-1.107, 3.861], loss: 1.622869, mean_absolute_error: 23.759779, mean_q: 13.984657, mean_eps: 0.787614
  236134/2000000: episode: 2425, duration: 1.585s, episode steps: 110, steps per second: 69, episode reward: -104.546, mean reward: -0.950 [-100.000, 13.569], mean action: 1.500 [0.000, 3.000], mean observation: -0.003 [-2.896, 1.000], loss: 1.933434, mean_absolute_error: 25.859853, mean_q: 13.784541, mean_eps: 0.787530
  236214/2000000: episode: 2426, duration: 1.135s, episode steps: 80, steps per second: 70, episode reward: -118.861, mean reward: -1.486 [-100.000, 12.617], mean action: 1.613 [0.000, 3.000], mean observation: 0.081 [-1.211, 1.000], loss: 1.802967, mean_absolute_error: 24.985754, mean_q: 12.845932, mean_eps: 0.787443
  236334/2000000: episode: 2427, duration: 1.696s, episode steps: 120, steps per second: 71, episode reward: -165.728, mean reward: -1.381 [-100.000, 12.810], mean action: 1.708 [0.000, 3.000], mean observation: -0.113 [-1.143, 3.863], loss: 1.268138, mean_absolute_error: 23.654506, mean_q: 15.404557, mean_eps: 0.787353
  236425/2000000: episode: 2428, duration: 1.304s, episode steps: 91, steps per second: 70, episode reward: -104.823, mean reward: -1.152 [-100.000, 10.775], mean action: 1.846 [0.000, 3.000], mean observation: -0.093 [-0.932, 3.195], loss: 1.830192, mean_absolute_error: 24.882446, mean_q: 13.867631, mean_eps: 0.787258
  236591/2000000: episode: 2429, duration: 2.312s, episode steps: 166, steps per second: 72, episode reward: -158.561, mean reward: -0.955 [-100.000, 12.019], mean action: 1.506 [0.000, 3.000], mean observation: 0.118 [-3.874, 1.090], loss: 1.656243, mean_absolute_error: 24.604317, mean_q: 14.887194, mean_eps: 0.787143
  236732/2000000: episode: 2430, duration: 2.019s, episode steps: 141, steps per second: 70, episode reward: -138.483, mean reward: -0.982 [-100.000, 5.574], mean action: 1.631 [0.000, 3.000], mean observation: 0.010 [-1.149, 3.598], loss: 1.762970, mean_absolute_error: 24.896321, mean_q: 12.597713, mean_eps: 0.787006
  236802/2000000: episode: 2431, duration: 1.226s, episode steps: 70, steps per second: 57, episode reward: -138.810, mean reward: -1.983 [-100.000, 9.864], mean action: 1.500 [0.000, 3.000], mean observation: 0.093 [-1.458, 5.386], loss: 1.733645, mean_absolute_error: 25.179465, mean_q: 15.044460, mean_eps: 0.786911
  236874/2000000: episode: 2432, duration: 1.030s, episode steps: 72, steps per second: 70, episode reward: -128.977, mean reward: -1.791 [-100.000, 6.430], mean action: 1.444 [0.000, 3.000], mean observation: 0.014 [-1.418, 1.000], loss: 1.507284, mean_absolute_error: 25.718912, mean_q: 13.717845, mean_eps: 0.786846
  236993/2000000: episode: 2433, duration: 1.708s, episode steps: 119, steps per second: 70, episode reward: -141.427, mean reward: -1.188 [-100.000, 7.567], mean action: 1.630 [0.000, 3.000], mean observation: 0.175 [-3.287, 1.010], loss: 1.701401, mean_absolute_error: 24.979041, mean_q: 15.993953, mean_eps: 0.786759
  237083/2000000: episode: 2434, duration: 1.264s, episode steps: 90, steps per second: 71, episode reward: -154.634, mean reward: -1.718 [-100.000, 4.096], mean action: 1.522 [0.000, 3.000], mean observation: 0.064 [-3.508, 0.943], loss: 1.890234, mean_absolute_error: 25.647379, mean_q: 15.082857, mean_eps: 0.786666
  237184/2000000: episode: 2435, duration: 1.485s, episode steps: 101, steps per second: 68, episode reward: -107.835, mean reward: -1.068 [-100.000, 11.118], mean action: 1.545 [0.000, 3.000], mean observation: -0.034 [-1.111, 1.000], loss: 2.230977, mean_absolute_error: 26.618043, mean_q: 10.450119, mean_eps: 0.786581
  237253/2000000: episode: 2436, duration: 1.018s, episode steps: 69, steps per second: 68, episode reward: -138.855, mean reward: -2.012 [-100.000, 6.105], mean action: 1.551 [0.000, 3.000], mean observation: -0.006 [-1.556, 3.635], loss: 1.586767, mean_absolute_error: 24.574942, mean_q: 13.789840, mean_eps: 0.786504
  237370/2000000: episode: 2437, duration: 1.645s, episode steps: 117, steps per second: 71, episode reward: -116.958, mean reward: -1.000 [-100.000, 14.702], mean action: 1.547 [0.000, 3.000], mean observation: -0.061 [-0.954, 3.261], loss: 1.759596, mean_absolute_error: 23.984287, mean_q: 16.662452, mean_eps: 0.786419
  237443/2000000: episode: 2438, duration: 1.028s, episode steps: 73, steps per second: 71, episode reward: -96.412, mean reward: -1.321 [-100.000, 20.474], mean action: 1.630 [0.000, 3.000], mean observation: 0.011 [-3.770, 1.000], loss: 1.860805, mean_absolute_error: 25.437272, mean_q: 14.224504, mean_eps: 0.786335
  237561/2000000: episode: 2439, duration: 1.701s, episode steps: 118, steps per second: 69, episode reward: -151.235, mean reward: -1.282 [-100.000, 13.533], mean action: 1.610 [0.000, 3.000], mean observation: -0.112 [-1.142, 1.000], loss: 2.113522, mean_absolute_error: 25.441130, mean_q: 15.939676, mean_eps: 0.786248
  237686/2000000: episode: 2440, duration: 1.730s, episode steps: 125, steps per second: 72, episode reward: -124.887, mean reward: -0.999 [-100.000, 14.135], mean action: 1.568 [0.000, 3.000], mean observation: -0.083 [-1.167, 3.607], loss: 1.407750, mean_absolute_error: 24.047118, mean_q: 14.796603, mean_eps: 0.786138
  237803/2000000: episode: 2441, duration: 1.677s, episode steps: 117, steps per second: 70, episode reward: -200.647, mean reward: -1.715 [-100.000, 12.861], mean action: 1.573 [0.000, 3.000], mean observation: 0.229 [-2.653, 1.000], loss: 1.854760, mean_absolute_error: 25.848236, mean_q: 12.481725, mean_eps: 0.786030
  237892/2000000: episode: 2442, duration: 1.312s, episode steps: 89, steps per second: 68, episode reward: -174.513, mean reward: -1.961 [-100.000, 8.412], mean action: 1.596 [0.000, 3.000], mean observation: -0.011 [-1.446, 4.459], loss: 1.668388, mean_absolute_error: 24.738227, mean_q: 15.143487, mean_eps: 0.785939
  237988/2000000: episode: 2443, duration: 1.453s, episode steps: 96, steps per second: 66, episode reward: -144.932, mean reward: -1.510 [-100.000, 7.518], mean action: 1.312 [0.000, 3.000], mean observation: 0.061 [-3.452, 1.000], loss: 1.330888, mean_absolute_error: 23.904129, mean_q: 15.931425, mean_eps: 0.785856
  238109/2000000: episode: 2444, duration: 1.753s, episode steps: 121, steps per second: 69, episode reward: -93.719, mean reward: -0.775 [-100.000, 36.521], mean action: 1.595 [0.000, 3.000], mean observation: -0.141 [-1.212, 3.878], loss: 1.575657, mean_absolute_error: 24.723071, mean_q: 14.181441, mean_eps: 0.785757
  238197/2000000: episode: 2445, duration: 1.266s, episode steps: 88, steps per second: 70, episode reward: -161.468, mean reward: -1.835 [-100.000, 17.967], mean action: 1.602 [0.000, 3.000], mean observation: 0.067 [-3.033, 1.000], loss: 2.353705, mean_absolute_error: 24.835677, mean_q: 13.040491, mean_eps: 0.785661
  238268/2000000: episode: 2446, duration: 1.034s, episode steps: 71, steps per second: 69, episode reward: -133.081, mean reward: -1.874 [-100.000, 12.700], mean action: 1.606 [0.000, 3.000], mean observation: 0.042 [-4.021, 1.000], loss: 1.787217, mean_absolute_error: 24.141698, mean_q: 14.568093, mean_eps: 0.785591
  238356/2000000: episode: 2447, duration: 1.292s, episode steps: 88, steps per second: 68, episode reward: -213.152, mean reward: -2.422 [-100.000, 10.948], mean action: 1.705 [0.000, 3.000], mean observation: -0.068 [-4.941, 1.000], loss: 2.089371, mean_absolute_error: 24.895809, mean_q: 14.524307, mean_eps: 0.785521
  238452/2000000: episode: 2448, duration: 1.420s, episode steps: 96, steps per second: 68, episode reward: -84.951, mean reward: -0.885 [-100.000, 11.863], mean action: 1.688 [0.000, 3.000], mean observation: -0.054 [-0.964, 3.001], loss: 1.950164, mean_absolute_error: 25.468621, mean_q: 13.203973, mean_eps: 0.785438
  238521/2000000: episode: 2449, duration: 1.118s, episode steps: 69, steps per second: 62, episode reward: -129.755, mean reward: -1.881 [-100.000, 11.909], mean action: 1.739 [0.000, 3.000], mean observation: -0.129 [-1.431, 4.423], loss: 2.336744, mean_absolute_error: 26.117807, mean_q: 12.833773, mean_eps: 0.785363
  238636/2000000: episode: 2450, duration: 1.876s, episode steps: 115, steps per second: 61, episode reward: -137.722, mean reward: -1.198 [-100.000, 11.270], mean action: 1.600 [0.000, 3.000], mean observation: -0.106 [-1.002, 1.000], loss: 1.859801, mean_absolute_error: 24.877005, mean_q: 15.286976, mean_eps: 0.785280
  238732/2000000: episode: 2451, duration: 1.433s, episode steps: 96, steps per second: 67, episode reward: -88.438, mean reward: -0.921 [-100.000, 17.795], mean action: 1.719 [0.000, 3.000], mean observation: -0.044 [-1.085, 1.000], loss: 1.792423, mean_absolute_error: 24.462028, mean_q: 14.724839, mean_eps: 0.785186
  238835/2000000: episode: 2452, duration: 1.461s, episode steps: 103, steps per second: 71, episode reward: -154.364, mean reward: -1.499 [-100.000, 11.040], mean action: 1.689 [0.000, 3.000], mean observation: -0.146 [-2.089, 1.000], loss: 1.604573, mean_absolute_error: 25.037294, mean_q: 14.360278, mean_eps: 0.785096
  238920/2000000: episode: 2453, duration: 1.242s, episode steps: 85, steps per second: 68, episode reward: -78.078, mean reward: -0.919 [-100.000, 23.641], mean action: 1.600 [0.000, 3.000], mean observation: -0.028 [-1.179, 1.000], loss: 1.489980, mean_absolute_error: 25.007754, mean_q: 13.813224, mean_eps: 0.785012
  239032/2000000: episode: 2454, duration: 1.616s, episode steps: 112, steps per second: 69, episode reward: -120.641, mean reward: -1.077 [-100.000, 9.758], mean action: 1.545 [0.000, 3.000], mean observation: 0.040 [-3.395, 1.000], loss: 1.614751, mean_absolute_error: 25.137954, mean_q: 14.298521, mean_eps: 0.784923
  239134/2000000: episode: 2455, duration: 1.452s, episode steps: 102, steps per second: 70, episode reward: -180.028, mean reward: -1.765 [-100.000, 11.497], mean action: 1.480 [0.000, 3.000], mean observation: 0.094 [-4.424, 1.000], loss: 1.546522, mean_absolute_error: 25.181106, mean_q: 15.467432, mean_eps: 0.784826
  239273/2000000: episode: 2456, duration: 1.990s, episode steps: 139, steps per second: 70, episode reward: -66.679, mean reward: -0.480 [-100.000, 18.675], mean action: 1.604 [0.000, 3.000], mean observation: 0.134 [-3.002, 1.044], loss: 1.626965, mean_absolute_error: 23.859858, mean_q: 15.732991, mean_eps: 0.784716
  239360/2000000: episode: 2457, duration: 1.245s, episode steps: 87, steps per second: 70, episode reward: -124.974, mean reward: -1.436 [-100.000, 6.853], mean action: 1.609 [0.000, 3.000], mean observation: -0.063 [-3.864, 1.000], loss: 1.734912, mean_absolute_error: 25.197622, mean_q: 15.672135, mean_eps: 0.784616
  239506/2000000: episode: 2458, duration: 2.088s, episode steps: 146, steps per second: 70, episode reward: -138.071, mean reward: -0.946 [-100.000, 52.679], mean action: 1.603 [0.000, 3.000], mean observation: -0.027 [-1.462, 1.748], loss: 1.699647, mean_absolute_error: 25.413553, mean_q: 13.716302, mean_eps: 0.784511
  239585/2000000: episode: 2459, duration: 1.154s, episode steps: 79, steps per second: 68, episode reward: -83.928, mean reward: -1.062 [-100.000, 15.242], mean action: 1.646 [0.000, 3.000], mean observation: -0.014 [-1.138, 1.000], loss: 1.910704, mean_absolute_error: 25.538213, mean_q: 12.824592, mean_eps: 0.784409
  239655/2000000: episode: 2460, duration: 0.978s, episode steps: 70, steps per second: 72, episode reward: -109.954, mean reward: -1.571 [-100.000, 16.597], mean action: 1.629 [0.000, 3.000], mean observation: 0.004 [-1.422, 1.000], loss: 1.289942, mean_absolute_error: 24.867367, mean_q: 13.674183, mean_eps: 0.784342
  239756/2000000: episode: 2461, duration: 1.482s, episode steps: 101, steps per second: 68, episode reward: -110.359, mean reward: -1.093 [-100.000, 10.419], mean action: 1.644 [0.000, 3.000], mean observation: -0.084 [-0.987, 3.101], loss: 1.769552, mean_absolute_error: 25.421260, mean_q: 15.037918, mean_eps: 0.784266
  239837/2000000: episode: 2462, duration: 1.183s, episode steps: 81, steps per second: 68, episode reward: -117.764, mean reward: -1.454 [-100.000, 16.277], mean action: 1.691 [0.000, 3.000], mean observation: -0.062 [-1.286, 1.000], loss: 2.718027, mean_absolute_error: 25.973969, mean_q: 13.981499, mean_eps: 0.784184
  239923/2000000: episode: 2463, duration: 1.214s, episode steps: 86, steps per second: 71, episode reward: -147.320, mean reward: -1.713 [-100.000, 11.043], mean action: 1.663 [0.000, 3.000], mean observation: -0.116 [-1.304, 3.985], loss: 1.405356, mean_absolute_error: 23.544224, mean_q: 16.579128, mean_eps: 0.784108
  240045/2000000: episode: 2464, duration: 1.777s, episode steps: 122, steps per second: 69, episode reward: -264.498, mean reward: -2.168 [-100.000, 6.570], mean action: 1.582 [0.000, 3.000], mean observation: -0.198 [-3.766, 1.155], loss: 2.158331, mean_absolute_error: 24.098750, mean_q: 13.462499, mean_eps: 0.784014
  240147/2000000: episode: 2465, duration: 1.425s, episode steps: 102, steps per second: 72, episode reward: -154.232, mean reward: -1.512 [-100.000, 5.138], mean action: 1.676 [0.000, 3.000], mean observation: -0.092 [-1.065, 3.393], loss: 1.738076, mean_absolute_error: 26.803832, mean_q: 13.238065, mean_eps: 0.783914
  240267/2000000: episode: 2466, duration: 1.710s, episode steps: 120, steps per second: 70, episode reward: -161.418, mean reward: -1.345 [-100.000, 5.939], mean action: 1.517 [0.000, 3.000], mean observation: 0.079 [-4.071, 1.000], loss: 2.150529, mean_absolute_error: 26.069885, mean_q: 12.368516, mean_eps: 0.783815
  240342/2000000: episode: 2467, duration: 1.084s, episode steps: 75, steps per second: 69, episode reward: -107.351, mean reward: -1.431 [-100.000, 8.046], mean action: 1.613 [0.000, 3.000], mean observation: 0.094 [-1.084, 3.907], loss: 1.105227, mean_absolute_error: 23.538046, mean_q: 15.150960, mean_eps: 0.783726
  240448/2000000: episode: 2468, duration: 1.521s, episode steps: 106, steps per second: 70, episode reward: -111.004, mean reward: -1.047 [-100.000, 14.125], mean action: 1.585 [0.000, 3.000], mean observation: 0.119 [-1.083, 3.028], loss: 1.677246, mean_absolute_error: 24.759187, mean_q: 13.921230, mean_eps: 0.783645
  240536/2000000: episode: 2469, duration: 1.302s, episode steps: 88, steps per second: 68, episode reward: -131.318, mean reward: -1.492 [-100.000, 13.360], mean action: 1.648 [0.000, 3.000], mean observation: -0.172 [-4.381, 1.000], loss: 1.879087, mean_absolute_error: 26.147440, mean_q: 12.710976, mean_eps: 0.783559
  240636/2000000: episode: 2470, duration: 1.489s, episode steps: 100, steps per second: 67, episode reward: -129.765, mean reward: -1.298 [-100.000, 13.466], mean action: 1.620 [0.000, 3.000], mean observation: 0.093 [-2.837, 1.000], loss: 1.638817, mean_absolute_error: 25.582399, mean_q: 13.437481, mean_eps: 0.783474
  240728/2000000: episode: 2471, duration: 1.379s, episode steps: 92, steps per second: 67, episode reward: -145.409, mean reward: -1.581 [-100.000, 7.122], mean action: 1.609 [0.000, 3.000], mean observation: -0.054 [-1.190, 3.890], loss: 1.337913, mean_absolute_error: 25.170604, mean_q: 15.076277, mean_eps: 0.783388
  240840/2000000: episode: 2472, duration: 1.650s, episode steps: 112, steps per second: 68, episode reward: -128.800, mean reward: -1.150 [-100.000, 15.253], mean action: 1.625 [0.000, 3.000], mean observation: -0.118 [-0.964, 1.756], loss: 1.446285, mean_absolute_error: 25.152108, mean_q: 13.940139, mean_eps: 0.783296
  240962/2000000: episode: 2473, duration: 1.759s, episode steps: 122, steps per second: 69, episode reward: -92.773, mean reward: -0.760 [-100.000, 12.509], mean action: 1.516 [0.000, 3.000], mean observation: 0.043 [-1.296, 1.010], loss: 2.244021, mean_absolute_error: 25.860997, mean_q: 13.910124, mean_eps: 0.783190
  241025/2000000: episode: 2474, duration: 0.932s, episode steps: 63, steps per second: 68, episode reward: -73.770, mean reward: -1.171 [-100.000, 15.451], mean action: 1.556 [0.000, 3.000], mean observation: -0.061 [-1.282, 3.963], loss: 1.328634, mean_absolute_error: 25.429661, mean_q: 13.636778, mean_eps: 0.783105
  241103/2000000: episode: 2475, duration: 1.075s, episode steps: 78, steps per second: 73, episode reward: -118.185, mean reward: -1.515 [-100.000, 8.925], mean action: 1.692 [0.000, 3.000], mean observation: -0.071 [-1.232, 1.700], loss: 1.627936, mean_absolute_error: 24.733790, mean_q: 13.710714, mean_eps: 0.783042
  241208/2000000: episode: 2476, duration: 1.552s, episode steps: 105, steps per second: 68, episode reward: -135.002, mean reward: -1.286 [-100.000, 13.315], mean action: 1.571 [0.000, 3.000], mean observation: -0.001 [-4.564, 1.000], loss: 1.291421, mean_absolute_error: 24.749116, mean_q: 14.034973, mean_eps: 0.782961
  241318/2000000: episode: 2477, duration: 1.565s, episode steps: 110, steps per second: 70, episode reward: -216.355, mean reward: -1.967 [-100.000, 14.631], mean action: 1.736 [0.000, 3.000], mean observation: -0.064 [-4.870, 1.007], loss: 1.329664, mean_absolute_error: 25.586257, mean_q: 13.322017, mean_eps: 0.782864
  241412/2000000: episode: 2478, duration: 1.390s, episode steps: 94, steps per second: 68, episode reward: -109.243, mean reward: -1.162 [-100.000, 19.420], mean action: 1.702 [0.000, 3.000], mean observation: -0.107 [-1.088, 3.283], loss: 2.642071, mean_absolute_error: 25.096041, mean_q: 13.818035, mean_eps: 0.782772
  241488/2000000: episode: 2479, duration: 1.125s, episode steps: 76, steps per second: 68, episode reward: -100.457, mean reward: -1.322 [-100.000, 6.943], mean action: 1.605 [0.000, 3.000], mean observation: -0.144 [-3.864, 1.000], loss: 1.568698, mean_absolute_error: 25.375082, mean_q: 15.772887, mean_eps: 0.782697
  241622/2000000: episode: 2480, duration: 1.953s, episode steps: 134, steps per second: 69, episode reward: -129.834, mean reward: -0.969 [-100.000, 15.718], mean action: 1.582 [0.000, 3.000], mean observation: -0.059 [-1.126, 3.907], loss: 2.048164, mean_absolute_error: 25.808062, mean_q: 15.877064, mean_eps: 0.782601
  241691/2000000: episode: 2481, duration: 0.988s, episode steps: 69, steps per second: 70, episode reward: -131.453, mean reward: -1.905 [-100.000, 11.439], mean action: 1.623 [0.000, 3.000], mean observation: -0.133 [-1.496, 1.000], loss: 3.398249, mean_absolute_error: 26.959405, mean_q: 11.532681, mean_eps: 0.782510
  241798/2000000: episode: 2482, duration: 1.552s, episode steps: 107, steps per second: 69, episode reward: -177.049, mean reward: -1.655 [-100.000, 6.880], mean action: 1.533 [0.000, 3.000], mean observation: 0.168 [-3.697, 1.000], loss: 1.375255, mean_absolute_error: 24.674651, mean_q: 14.844833, mean_eps: 0.782430
  241914/2000000: episode: 2483, duration: 1.701s, episode steps: 116, steps per second: 68, episode reward: -139.348, mean reward: -1.201 [-100.000, 9.338], mean action: 1.440 [0.000, 3.000], mean observation: 0.058 [-3.393, 1.000], loss: 1.527374, mean_absolute_error: 25.127421, mean_q: 14.685253, mean_eps: 0.782330
  241989/2000000: episode: 2484, duration: 1.094s, episode steps: 75, steps per second: 69, episode reward: -140.182, mean reward: -1.869 [-100.000, 10.130], mean action: 1.400 [0.000, 3.000], mean observation: -0.095 [-1.198, 1.000], loss: 1.430457, mean_absolute_error: 25.564690, mean_q: 14.037583, mean_eps: 0.782243
  242054/2000000: episode: 2485, duration: 0.914s, episode steps: 65, steps per second: 71, episode reward: -118.450, mean reward: -1.822 [-100.000, 9.372], mean action: 1.569 [0.000, 3.000], mean observation: -0.127 [-4.714, 1.000], loss: 2.122734, mean_absolute_error: 25.817238, mean_q: 12.382100, mean_eps: 0.782180
  242119/2000000: episode: 2486, duration: 0.928s, episode steps: 65, steps per second: 70, episode reward: -99.855, mean reward: -1.536 [-100.000, 16.723], mean action: 1.369 [0.000, 3.000], mean observation: 0.052 [-1.347, 1.000], loss: 1.243977, mean_absolute_error: 24.166637, mean_q: 16.784088, mean_eps: 0.782123
  242204/2000000: episode: 2487, duration: 1.257s, episode steps: 85, steps per second: 68, episode reward: -193.467, mean reward: -2.276 [-100.000, 8.892], mean action: 1.824 [0.000, 3.000], mean observation: -0.200 [-3.511, 1.000], loss: 1.440325, mean_absolute_error: 24.801352, mean_q: 16.225055, mean_eps: 0.782056
  242272/2000000: episode: 2488, duration: 1.022s, episode steps: 68, steps per second: 67, episode reward: -92.173, mean reward: -1.355 [-100.000, 11.331], mean action: 1.706 [0.000, 3.000], mean observation: -0.127 [-3.706, 1.000], loss: 1.411349, mean_absolute_error: 24.877381, mean_q: 13.439933, mean_eps: 0.781988
  242352/2000000: episode: 2489, duration: 1.194s, episode steps: 80, steps per second: 67, episode reward: -120.065, mean reward: -1.501 [-100.000, 17.050], mean action: 1.637 [0.000, 3.000], mean observation: -0.137 [-1.221, 4.060], loss: 1.587748, mean_absolute_error: 24.454454, mean_q: 15.007633, mean_eps: 0.781921
  242459/2000000: episode: 2490, duration: 1.559s, episode steps: 107, steps per second: 69, episode reward: -140.087, mean reward: -1.309 [-100.000, 15.684], mean action: 1.570 [0.000, 3.000], mean observation: 0.172 [-2.547, 1.000], loss: 1.224026, mean_absolute_error: 24.199089, mean_q: 17.333488, mean_eps: 0.781836
  242569/2000000: episode: 2491, duration: 1.607s, episode steps: 110, steps per second: 68, episode reward: -152.172, mean reward: -1.383 [-100.000, 8.913], mean action: 1.618 [0.000, 3.000], mean observation: -0.007 [-3.897, 1.000], loss: 1.857484, mean_absolute_error: 24.553479, mean_q: 15.108940, mean_eps: 0.781737
  242649/2000000: episode: 2492, duration: 1.481s, episode steps: 80, steps per second: 54, episode reward: -98.229, mean reward: -1.228 [-100.000, 14.184], mean action: 1.650 [0.000, 3.000], mean observation: -0.034 [-1.326, 1.000], loss: 2.139031, mean_absolute_error: 27.104318, mean_q: 11.585798, mean_eps: 0.781651
  242800/2000000: episode: 2493, duration: 2.234s, episode steps: 151, steps per second: 68, episode reward: -94.287, mean reward: -0.624 [-100.000, 17.947], mean action: 1.616 [0.000, 3.000], mean observation: 0.010 [-0.870, 2.520], loss: 1.496041, mean_absolute_error: 25.760527, mean_q: 15.917856, mean_eps: 0.781548
  242871/2000000: episode: 2494, duration: 1.032s, episode steps: 71, steps per second: 69, episode reward: -121.046, mean reward: -1.705 [-100.000, 21.197], mean action: 1.465 [0.000, 3.000], mean observation: 0.002 [-4.557, 1.000], loss: 1.123837, mean_absolute_error: 25.400387, mean_q: 14.122054, mean_eps: 0.781449
  242999/2000000: episode: 2495, duration: 1.821s, episode steps: 128, steps per second: 70, episode reward: -85.132, mean reward: -0.665 [-100.000, 15.028], mean action: 1.492 [0.000, 3.000], mean observation: 0.015 [-1.041, 1.000], loss: 1.909602, mean_absolute_error: 25.995988, mean_q: 13.352863, mean_eps: 0.781359
  243066/2000000: episode: 2496, duration: 0.975s, episode steps: 67, steps per second: 69, episode reward: -116.745, mean reward: -1.742 [-100.000, 9.467], mean action: 1.522 [0.000, 3.000], mean observation: -0.056 [-4.693, 1.000], loss: 1.799284, mean_absolute_error: 25.396722, mean_q: 14.733758, mean_eps: 0.781271
  243169/2000000: episode: 2497, duration: 1.463s, episode steps: 103, steps per second: 70, episode reward: -121.717, mean reward: -1.182 [-100.000, 8.061], mean action: 1.699 [0.000, 3.000], mean observation: 0.026 [-4.209, 1.000], loss: 1.614846, mean_absolute_error: 25.048126, mean_q: 17.148587, mean_eps: 0.781194
  243251/2000000: episode: 2498, duration: 1.146s, episode steps: 82, steps per second: 72, episode reward: -80.521, mean reward: -0.982 [-100.000, 16.274], mean action: 1.598 [0.000, 3.000], mean observation: -0.092 [-1.011, 1.000], loss: 1.736680, mean_absolute_error: 26.114260, mean_q: 13.981494, mean_eps: 0.781111
  243346/2000000: episode: 2499, duration: 1.375s, episode steps: 95, steps per second: 69, episode reward: -104.786, mean reward: -1.103 [-100.000, 7.042], mean action: 1.505 [0.000, 3.000], mean observation: 0.113 [-0.934, 2.013], loss: 1.492578, mean_absolute_error: 25.182705, mean_q: 14.004210, mean_eps: 0.781032
  243456/2000000: episode: 2500, duration: 1.579s, episode steps: 110, steps per second: 70, episode reward: -110.701, mean reward: -1.006 [-100.000, 17.865], mean action: 1.564 [0.000, 3.000], mean observation: -0.076 [-1.247, 1.000], loss: 1.955739, mean_absolute_error: 26.248897, mean_q: 12.831498, mean_eps: 0.780940
  243546/2000000: episode: 2501, duration: 1.303s, episode steps: 90, steps per second: 69, episode reward: -143.003, mean reward: -1.589 [-100.000, 9.362], mean action: 1.644 [0.000, 3.000], mean observation: -0.105 [-1.284, 3.991], loss: 1.625841, mean_absolute_error: 25.246827, mean_q: 16.407232, mean_eps: 0.780850
  243651/2000000: episode: 2502, duration: 1.475s, episode steps: 105, steps per second: 71, episode reward: -207.194, mean reward: -1.973 [-100.000, 10.187], mean action: 1.657 [0.000, 3.000], mean observation: -0.124 [-1.395, 4.103], loss: 1.708039, mean_absolute_error: 25.612374, mean_q: 15.037050, mean_eps: 0.780762
  243783/2000000: episode: 2503, duration: 1.878s, episode steps: 132, steps per second: 70, episode reward: -107.152, mean reward: -0.812 [-100.000, 10.516], mean action: 1.485 [0.000, 3.000], mean observation: 0.091 [-1.251, 4.482], loss: 1.268279, mean_absolute_error: 25.048919, mean_q: 15.905664, mean_eps: 0.780656
  243866/2000000: episode: 2504, duration: 1.227s, episode steps: 83, steps per second: 68, episode reward: -106.819, mean reward: -1.287 [-100.000, 6.706], mean action: 1.747 [0.000, 3.000], mean observation: -0.077 [-4.551, 1.000], loss: 1.210334, mean_absolute_error: 24.657197, mean_q: 14.718025, mean_eps: 0.780558
  243968/2000000: episode: 2505, duration: 1.451s, episode steps: 102, steps per second: 70, episode reward: -165.604, mean reward: -1.624 [-100.000, 17.104], mean action: 1.539 [0.000, 3.000], mean observation: 0.121 [-2.710, 1.147], loss: 2.188691, mean_absolute_error: 25.041669, mean_q: 15.120573, mean_eps: 0.780476
  244057/2000000: episode: 2506, duration: 1.295s, episode steps: 89, steps per second: 69, episode reward: -145.347, mean reward: -1.633 [-100.000, 10.938], mean action: 1.551 [0.000, 3.000], mean observation: -0.142 [-1.282, 1.000], loss: 1.646255, mean_absolute_error: 25.716761, mean_q: 14.166520, mean_eps: 0.780389
  244144/2000000: episode: 2507, duration: 1.269s, episode steps: 87, steps per second: 69, episode reward: -120.969, mean reward: -1.390 [-100.000, 16.512], mean action: 1.644 [0.000, 3.000], mean observation: 0.013 [-1.317, 1.000], loss: 2.038951, mean_absolute_error: 27.552519, mean_q: 12.493240, mean_eps: 0.780310
  244248/2000000: episode: 2508, duration: 1.531s, episode steps: 104, steps per second: 68, episode reward: -132.547, mean reward: -1.274 [-100.000, 7.327], mean action: 1.654 [0.000, 3.000], mean observation: 0.142 [-3.681, 1.000], loss: 1.964232, mean_absolute_error: 26.095035, mean_q: 14.261813, mean_eps: 0.780225
  244365/2000000: episode: 2509, duration: 1.720s, episode steps: 117, steps per second: 68, episode reward: -165.706, mean reward: -1.416 [-100.000, 7.763], mean action: 1.709 [0.000, 3.000], mean observation: -0.051 [-1.182, 4.021], loss: 1.925596, mean_absolute_error: 26.312965, mean_q: 11.344958, mean_eps: 0.780125
  244472/2000000: episode: 2510, duration: 1.530s, episode steps: 107, steps per second: 70, episode reward: -224.835, mean reward: -2.101 [-100.000, 2.375], mean action: 1.598 [0.000, 3.000], mean observation: -0.100 [-1.539, 1.039], loss: 2.279150, mean_absolute_error: 26.713529, mean_q: 11.231100, mean_eps: 0.780024
  244572/2000000: episode: 2511, duration: 1.463s, episode steps: 100, steps per second: 68, episode reward: -156.979, mean reward: -1.570 [-100.000, 14.447], mean action: 1.490 [0.000, 3.000], mean observation: 0.157 [-1.206, 1.000], loss: 1.813533, mean_absolute_error: 23.976663, mean_q: 15.010478, mean_eps: 0.779932
  244673/2000000: episode: 2512, duration: 1.471s, episode steps: 101, steps per second: 69, episode reward: -104.110, mean reward: -1.031 [-100.000, 11.449], mean action: 1.624 [0.000, 3.000], mean observation: -0.006 [-1.131, 1.000], loss: 1.252770, mean_absolute_error: 23.614240, mean_q: 17.141386, mean_eps: 0.779840
  244768/2000000: episode: 2513, duration: 1.375s, episode steps: 95, steps per second: 69, episode reward: -136.730, mean reward: -1.439 [-100.000, 6.401], mean action: 1.589 [0.000, 3.000], mean observation: -0.022 [-1.199, 4.082], loss: 1.384589, mean_absolute_error: 25.363761, mean_q: 14.262552, mean_eps: 0.779752
  244877/2000000: episode: 2514, duration: 1.584s, episode steps: 109, steps per second: 69, episode reward: -127.403, mean reward: -1.169 [-100.000, 20.081], mean action: 1.624 [0.000, 3.000], mean observation: -0.029 [-1.302, 1.000], loss: 1.475172, mean_absolute_error: 25.784106, mean_q: 13.558360, mean_eps: 0.779660
  244992/2000000: episode: 2515, duration: 1.662s, episode steps: 115, steps per second: 69, episode reward: -131.737, mean reward: -1.146 [-100.000, 11.232], mean action: 1.617 [0.000, 3.000], mean observation: 0.002 [-1.046, 3.626], loss: 1.665504, mean_absolute_error: 24.710313, mean_q: 14.350085, mean_eps: 0.779559
  245087/2000000: episode: 2516, duration: 1.441s, episode steps: 95, steps per second: 66, episode reward: -142.173, mean reward: -1.497 [-100.000, 11.209], mean action: 1.779 [0.000, 3.000], mean observation: -0.083 [-1.098, 3.810], loss: 1.255929, mean_absolute_error: 25.844029, mean_q: 14.051460, mean_eps: 0.779466
  245206/2000000: episode: 2517, duration: 1.705s, episode steps: 119, steps per second: 70, episode reward: -118.242, mean reward: -0.994 [-100.000, 6.488], mean action: 1.681 [0.000, 3.000], mean observation: -0.038 [-1.010, 1.000], loss: 1.029886, mean_absolute_error: 24.014067, mean_q: 17.227492, mean_eps: 0.779369
  245326/2000000: episode: 2518, duration: 1.699s, episode steps: 120, steps per second: 71, episode reward: -144.585, mean reward: -1.205 [-100.000, 12.005], mean action: 1.342 [0.000, 3.000], mean observation: 0.120 [-1.122, 1.000], loss: 2.077108, mean_absolute_error: 25.313713, mean_q: 15.421877, mean_eps: 0.779261
  245448/2000000: episode: 2519, duration: 1.762s, episode steps: 122, steps per second: 69, episode reward: -125.022, mean reward: -1.025 [-100.000, 12.244], mean action: 1.369 [0.000, 3.000], mean observation: 0.097 [-1.296, 1.000], loss: 1.853295, mean_absolute_error: 25.906010, mean_q: 14.460380, mean_eps: 0.779153
  245605/2000000: episode: 2520, duration: 2.285s, episode steps: 157, steps per second: 69, episode reward: -35.116, mean reward: -0.224 [-100.000, 92.713], mean action: 1.694 [0.000, 3.000], mean observation: -0.045 [-1.404, 1.090], loss: 1.349421, mean_absolute_error: 25.253311, mean_q: 15.006134, mean_eps: 0.779027
  245759/2000000: episode: 2521, duration: 2.153s, episode steps: 154, steps per second: 72, episode reward: -104.893, mean reward: -0.681 [-100.000, 9.829], mean action: 1.448 [0.000, 3.000], mean observation: 0.140 [-0.959, 3.227], loss: 2.025159, mean_absolute_error: 26.437374, mean_q: 12.103873, mean_eps: 0.778886
  245873/2000000: episode: 2522, duration: 1.654s, episode steps: 114, steps per second: 69, episode reward: -187.679, mean reward: -1.646 [-100.000, 12.861], mean action: 1.561 [0.000, 3.000], mean observation: -0.121 [-1.219, 4.161], loss: 1.690583, mean_absolute_error: 26.157499, mean_q: 12.751921, mean_eps: 0.778766
  245979/2000000: episode: 2523, duration: 1.450s, episode steps: 106, steps per second: 73, episode reward: -135.514, mean reward: -1.278 [-100.000, 11.012], mean action: 1.500 [0.000, 3.000], mean observation: 0.051 [-1.273, 1.000], loss: 1.426053, mean_absolute_error: 25.366159, mean_q: 15.793024, mean_eps: 0.778667
  246048/2000000: episode: 2524, duration: 1.029s, episode steps: 69, steps per second: 67, episode reward: -145.607, mean reward: -2.110 [-100.000, 4.113], mean action: 1.391 [0.000, 3.000], mean observation: 0.060 [-4.803, 1.000], loss: 1.576610, mean_absolute_error: 25.566631, mean_q: 14.012029, mean_eps: 0.778589
  246151/2000000: episode: 2525, duration: 1.501s, episode steps: 103, steps per second: 69, episode reward: -152.983, mean reward: -1.485 [-100.000, 17.157], mean action: 1.641 [0.000, 3.000], mean observation: -0.120 [-1.268, 3.862], loss: 1.149197, mean_absolute_error: 24.954307, mean_q: 15.312030, mean_eps: 0.778512
  246226/2000000: episode: 2526, duration: 1.078s, episode steps: 75, steps per second: 70, episode reward: -74.673, mean reward: -0.996 [-100.000, 11.465], mean action: 1.680 [0.000, 3.000], mean observation: -0.015 [-1.179, 1.000], loss: 1.227062, mean_absolute_error: 24.628567, mean_q: 15.226584, mean_eps: 0.778431
  246290/2000000: episode: 2527, duration: 0.907s, episode steps: 64, steps per second: 71, episode reward: -108.811, mean reward: -1.700 [-100.000, 15.977], mean action: 1.500 [0.000, 3.000], mean observation: 0.047 [-1.405, 1.000], loss: 1.910111, mean_absolute_error: 24.866462, mean_q: 13.475458, mean_eps: 0.778368
  246392/2000000: episode: 2528, duration: 1.463s, episode steps: 102, steps per second: 70, episode reward: -290.373, mean reward: -2.847 [-100.000, 8.626], mean action: 1.647 [0.000, 3.000], mean observation: -0.005 [-1.603, 4.335], loss: 1.499690, mean_absolute_error: 24.689579, mean_q: 13.912265, mean_eps: 0.778294
  246525/2000000: episode: 2529, duration: 1.936s, episode steps: 133, steps per second: 69, episode reward: -101.089, mean reward: -0.760 [-100.000, 26.337], mean action: 1.541 [0.000, 3.000], mean observation: 0.199 [-1.498, 1.536], loss: 2.389133, mean_absolute_error: 26.743078, mean_q: 11.300726, mean_eps: 0.778188
  246672/2000000: episode: 2530, duration: 2.114s, episode steps: 147, steps per second: 70, episode reward: -226.082, mean reward: -1.538 [-100.000, 92.229], mean action: 1.612 [0.000, 3.000], mean observation: -0.100 [-2.392, 1.015], loss: 1.188592, mean_absolute_error: 25.321374, mean_q: 14.600095, mean_eps: 0.778062
  246777/2000000: episode: 2531, duration: 1.558s, episode steps: 105, steps per second: 67, episode reward: -164.986, mean reward: -1.571 [-100.000, 6.158], mean action: 1.619 [0.000, 3.000], mean observation: -0.067 [-1.215, 3.526], loss: 2.072168, mean_absolute_error: 25.820389, mean_q: 13.970057, mean_eps: 0.777948
  246886/2000000: episode: 2532, duration: 1.526s, episode steps: 109, steps per second: 71, episode reward: -99.013, mean reward: -0.908 [-100.000, 15.125], mean action: 1.725 [0.000, 3.000], mean observation: -0.025 [-1.051, 3.193], loss: 1.234271, mean_absolute_error: 25.839576, mean_q: 12.623365, mean_eps: 0.777851
  246965/2000000: episode: 2533, duration: 1.146s, episode steps: 79, steps per second: 69, episode reward: -115.044, mean reward: -1.456 [-100.000, 7.328], mean action: 1.519 [0.000, 3.000], mean observation: 0.061 [-1.208, 4.213], loss: 2.288485, mean_absolute_error: 25.523314, mean_q: 15.063787, mean_eps: 0.777767
  247098/2000000: episode: 2534, duration: 1.880s, episode steps: 133, steps per second: 71, episode reward: -89.945, mean reward: -0.676 [-100.000, 23.122], mean action: 1.639 [0.000, 3.000], mean observation: 0.048 [-1.141, 1.000], loss: 1.639968, mean_absolute_error: 25.466339, mean_q: 14.011117, mean_eps: 0.777671
  247192/2000000: episode: 2535, duration: 1.389s, episode steps: 94, steps per second: 68, episode reward: -125.649, mean reward: -1.337 [-100.000, 11.263], mean action: 1.415 [0.000, 3.000], mean observation: 0.105 [-0.991, 1.000], loss: 1.076458, mean_absolute_error: 24.990403, mean_q: 15.512009, mean_eps: 0.777570
  247330/2000000: episode: 2536, duration: 1.987s, episode steps: 138, steps per second: 69, episode reward: -95.108, mean reward: -0.689 [-100.000, 19.529], mean action: 1.616 [0.000, 3.000], mean observation: 0.084 [-0.713, 2.181], loss: 1.929926, mean_absolute_error: 25.548839, mean_q: 14.555172, mean_eps: 0.777466
  247403/2000000: episode: 2537, duration: 1.035s, episode steps: 73, steps per second: 70, episode reward: -154.437, mean reward: -2.116 [-100.000, 9.517], mean action: 1.534 [0.000, 3.000], mean observation: 0.112 [-1.369, 5.065], loss: 1.207605, mean_absolute_error: 25.028723, mean_q: 15.021757, mean_eps: 0.777371
  247520/2000000: episode: 2538, duration: 1.696s, episode steps: 117, steps per second: 69, episode reward: -188.246, mean reward: -1.609 [-100.000, 2.358], mean action: 1.735 [0.000, 3.000], mean observation: -0.114 [-4.386, 1.000], loss: 1.560844, mean_absolute_error: 25.566727, mean_q: 15.359452, mean_eps: 0.777286
  247591/2000000: episode: 2539, duration: 1.054s, episode steps: 71, steps per second: 67, episode reward: -159.005, mean reward: -2.240 [-100.000, 19.480], mean action: 1.789 [0.000, 3.000], mean observation: -0.117 [-4.699, 1.000], loss: 1.034589, mean_absolute_error: 25.257705, mean_q: 14.341592, mean_eps: 0.777201
  247704/2000000: episode: 2540, duration: 1.895s, episode steps: 113, steps per second: 60, episode reward: -98.693, mean reward: -0.873 [-100.000, 17.909], mean action: 1.717 [0.000, 3.000], mean observation: -0.167 [-1.701, 1.000], loss: 1.322831, mean_absolute_error: 25.745001, mean_q: 14.317141, mean_eps: 0.777119
  247807/2000000: episode: 2541, duration: 1.767s, episode steps: 103, steps per second: 58, episode reward: -94.492, mean reward: -0.917 [-100.000, 19.058], mean action: 1.524 [0.000, 3.000], mean observation: -0.028 [-3.677, 1.000], loss: 1.371068, mean_absolute_error: 25.247900, mean_q: 12.785599, mean_eps: 0.777021
  247875/2000000: episode: 2542, duration: 1.039s, episode steps: 68, steps per second: 65, episode reward: -113.326, mean reward: -1.667 [-100.000, 9.683], mean action: 1.485 [0.000, 3.000], mean observation: -0.048 [-4.520, 1.000], loss: 1.483440, mean_absolute_error: 25.318822, mean_q: 13.579755, mean_eps: 0.776944
  247958/2000000: episode: 2543, duration: 1.296s, episode steps: 83, steps per second: 64, episode reward: -158.342, mean reward: -1.908 [-100.000, 18.464], mean action: 1.639 [0.000, 3.000], mean observation: -0.093 [-1.409, 1.000], loss: 1.780457, mean_absolute_error: 25.062568, mean_q: 13.732149, mean_eps: 0.776876
  248034/2000000: episode: 2544, duration: 1.162s, episode steps: 76, steps per second: 65, episode reward: -138.951, mean reward: -1.828 [-100.000, 7.291], mean action: 1.566 [0.000, 3.000], mean observation: -0.000 [-5.077, 1.000], loss: 1.280206, mean_absolute_error: 25.910754, mean_q: 14.819339, mean_eps: 0.776804
  248174/2000000: episode: 2545, duration: 2.120s, episode steps: 140, steps per second: 66, episode reward: -152.749, mean reward: -1.091 [-100.000, 7.694], mean action: 1.486 [0.000, 3.000], mean observation: 0.197 [-0.911, 3.196], loss: 1.252669, mean_absolute_error: 25.274281, mean_q: 14.091832, mean_eps: 0.776706
  248261/2000000: episode: 2546, duration: 1.283s, episode steps: 87, steps per second: 68, episode reward: -81.780, mean reward: -0.940 [-100.000, 6.925], mean action: 1.586 [0.000, 3.000], mean observation: -0.058 [-1.054, 1.627], loss: 1.586573, mean_absolute_error: 25.693918, mean_q: 13.274602, mean_eps: 0.776604
  248335/2000000: episode: 2547, duration: 1.065s, episode steps: 74, steps per second: 69, episode reward: -136.240, mean reward: -1.841 [-100.000, 6.119], mean action: 1.689 [0.000, 3.000], mean observation: -0.165 [-1.293, 1.000], loss: 1.496622, mean_absolute_error: 24.285577, mean_q: 13.721085, mean_eps: 0.776532
  248441/2000000: episode: 2548, duration: 1.579s, episode steps: 106, steps per second: 67, episode reward: -157.485, mean reward: -1.486 [-100.000, 20.482], mean action: 1.585 [0.000, 3.000], mean observation: -0.084 [-4.815, 1.009], loss: 1.752032, mean_absolute_error: 26.480710, mean_q: 13.420510, mean_eps: 0.776451
  248604/2000000: episode: 2549, duration: 2.411s, episode steps: 163, steps per second: 68, episode reward: -68.917, mean reward: -0.423 [-100.000, 15.597], mean action: 1.485 [0.000, 3.000], mean observation: 0.137 [-2.738, 1.000], loss: 1.457105, mean_absolute_error: 25.490163, mean_q: 14.051799, mean_eps: 0.776330
  248726/2000000: episode: 2550, duration: 1.917s, episode steps: 122, steps per second: 64, episode reward: -197.579, mean reward: -1.620 [-100.000, 3.185], mean action: 1.549 [0.000, 3.000], mean observation: -0.137 [-1.009, 0.946], loss: 1.327370, mean_absolute_error: 24.793618, mean_q: 16.645895, mean_eps: 0.776202
  248792/2000000: episode: 2551, duration: 1.157s, episode steps: 66, steps per second: 57, episode reward: -133.783, mean reward: -2.027 [-100.000, 7.260], mean action: 1.697 [0.000, 3.000], mean observation: -0.064 [-4.668, 1.000], loss: 1.941519, mean_absolute_error: 26.584229, mean_q: 14.199622, mean_eps: 0.776118
  248879/2000000: episode: 2552, duration: 1.529s, episode steps: 87, steps per second: 57, episode reward: -149.365, mean reward: -1.717 [-100.000, 8.175], mean action: 1.701 [0.000, 3.000], mean observation: -0.067 [-1.370, 4.860], loss: 1.072851, mean_absolute_error: 24.219620, mean_q: 14.295812, mean_eps: 0.776049
  248952/2000000: episode: 2553, duration: 1.252s, episode steps: 73, steps per second: 58, episode reward: -173.674, mean reward: -2.379 [-100.000, 8.292], mean action: 1.548 [0.000, 3.000], mean observation: -0.087 [-1.407, 2.830], loss: 1.496468, mean_absolute_error: 24.173382, mean_q: 15.601185, mean_eps: 0.775977
  249019/2000000: episode: 2554, duration: 1.136s, episode steps: 67, steps per second: 59, episode reward: -159.891, mean reward: -2.386 [-100.000, 6.605], mean action: 1.836 [0.000, 3.000], mean observation: -0.163 [-1.426, 4.331], loss: 1.468254, mean_absolute_error: 25.280390, mean_q: 14.329505, mean_eps: 0.775914
  249136/2000000: episode: 2555, duration: 1.931s, episode steps: 117, steps per second: 61, episode reward: -150.265, mean reward: -1.284 [-100.000, 13.384], mean action: 1.624 [0.000, 3.000], mean observation: -0.008 [-1.226, 4.399], loss: 1.235206, mean_absolute_error: 25.006503, mean_q: 15.868951, mean_eps: 0.775832
  249213/2000000: episode: 2556, duration: 1.180s, episode steps: 77, steps per second: 65, episode reward: -141.459, mean reward: -1.837 [-100.000, 9.032], mean action: 1.481 [0.000, 3.000], mean observation: 0.074 [-1.221, 1.000], loss: 2.123817, mean_absolute_error: 25.216988, mean_q: 15.710612, mean_eps: 0.775743
  249296/2000000: episode: 2557, duration: 1.350s, episode steps: 83, steps per second: 62, episode reward: -136.319, mean reward: -1.642 [-100.000, 10.400], mean action: 1.470 [0.000, 3.000], mean observation: -0.025 [-4.852, 1.000], loss: 1.751526, mean_absolute_error: 26.117378, mean_q: 14.984248, mean_eps: 0.775671
  249409/2000000: episode: 2558, duration: 1.823s, episode steps: 113, steps per second: 62, episode reward: -125.327, mean reward: -1.109 [-100.000, 10.015], mean action: 1.611 [0.000, 3.000], mean observation: -0.055 [-1.096, 3.378], loss: 1.641363, mean_absolute_error: 25.263306, mean_q: 14.074958, mean_eps: 0.775583
  249495/2000000: episode: 2559, duration: 1.292s, episode steps: 86, steps per second: 67, episode reward: -136.129, mean reward: -1.583 [-100.000, 15.099], mean action: 1.558 [0.000, 3.000], mean observation: -0.060 [-1.368, 4.238], loss: 1.611154, mean_absolute_error: 24.662908, mean_q: 14.870627, mean_eps: 0.775493
  249584/2000000: episode: 2560, duration: 1.425s, episode steps: 89, steps per second: 62, episode reward: -105.668, mean reward: -1.187 [-100.000, 10.973], mean action: 1.607 [0.000, 3.000], mean observation: 0.084 [-1.018, 1.000], loss: 2.009325, mean_absolute_error: 25.645957, mean_q: 11.214509, mean_eps: 0.775416
  249654/2000000: episode: 2561, duration: 1.179s, episode steps: 70, steps per second: 59, episode reward: -69.369, mean reward: -0.991 [-100.000, 22.023], mean action: 1.814 [0.000, 3.000], mean observation: -0.032 [-3.724, 1.000], loss: 1.145372, mean_absolute_error: 25.581271, mean_q: 14.710075, mean_eps: 0.775344
  249725/2000000: episode: 2562, duration: 1.223s, episode steps: 71, steps per second: 58, episode reward: -116.793, mean reward: -1.645 [-100.000, 6.591], mean action: 1.563 [0.000, 3.000], mean observation: -0.082 [-4.614, 1.000], loss: 1.439707, mean_absolute_error: 24.999189, mean_q: 14.650299, mean_eps: 0.775279
  249787/2000000: episode: 2563, duration: 0.993s, episode steps: 62, steps per second: 62, episode reward: -157.550, mean reward: -2.541 [-100.000, 5.748], mean action: 1.661 [0.000, 3.000], mean observation: -0.180 [-5.761, 1.000], loss: 2.080743, mean_absolute_error: 26.813713, mean_q: 16.944480, mean_eps: 0.775220
  249872/2000000: episode: 2564, duration: 1.368s, episode steps: 85, steps per second: 62, episode reward: -134.568, mean reward: -1.583 [-100.000, 4.351], mean action: 1.576 [0.000, 3.000], mean observation: 0.069 [-1.349, 1.000], loss: 1.685795, mean_absolute_error: 26.079978, mean_q: 15.855775, mean_eps: 0.775155
  249964/2000000: episode: 2565, duration: 1.427s, episode steps: 92, steps per second: 64, episode reward: -135.397, mean reward: -1.472 [-100.000, 12.245], mean action: 1.478 [0.000, 3.000], mean observation: 0.044 [-3.937, 1.000], loss: 1.548992, mean_absolute_error: 24.754096, mean_q: 15.666410, mean_eps: 0.775076
  250085/2000000: episode: 2566, duration: 2.158s, episode steps: 121, steps per second: 56, episode reward: -224.462, mean reward: -1.855 [-100.000, 72.930], mean action: 1.661 [0.000, 3.000], mean observation: -0.173 [-1.800, 1.000], loss: 1.105711, mean_absolute_error: 24.538464, mean_q: 18.313307, mean_eps: 0.774978
  250209/2000000: episode: 2567, duration: 2.098s, episode steps: 124, steps per second: 59, episode reward: -152.569, mean reward: -1.230 [-100.000, 6.193], mean action: 1.621 [0.000, 3.000], mean observation: 0.183 [-2.913, 1.000], loss: 1.736477, mean_absolute_error: 26.846673, mean_q: 14.858079, mean_eps: 0.774867
  250320/2000000: episode: 2568, duration: 1.836s, episode steps: 111, steps per second: 60, episode reward: -175.086, mean reward: -1.577 [-100.000, 7.186], mean action: 1.703 [0.000, 3.000], mean observation: -0.021 [-1.371, 4.589], loss: 1.418929, mean_absolute_error: 26.289289, mean_q: 16.068309, mean_eps: 0.774762
  250425/2000000: episode: 2569, duration: 1.784s, episode steps: 105, steps per second: 59, episode reward: -66.759, mean reward: -0.636 [-100.000, 7.271], mean action: 1.590 [0.000, 3.000], mean observation: 0.004 [-3.037, 1.000], loss: 1.206593, mean_absolute_error: 25.779229, mean_q: 16.003476, mean_eps: 0.774665
  250533/2000000: episode: 2570, duration: 1.661s, episode steps: 108, steps per second: 65, episode reward: -149.601, mean reward: -1.385 [-100.000, 6.958], mean action: 1.611 [0.000, 3.000], mean observation: -0.102 [-1.189, 4.195], loss: 1.658749, mean_absolute_error: 27.333204, mean_q: 15.971832, mean_eps: 0.774568
  250657/2000000: episode: 2571, duration: 1.855s, episode steps: 124, steps per second: 67, episode reward: -218.131, mean reward: -1.759 [-100.000, 3.655], mean action: 1.524 [0.000, 3.000], mean observation: 0.228 [-1.231, 1.012], loss: 1.790866, mean_absolute_error: 26.514301, mean_q: 14.029777, mean_eps: 0.774464
  250752/2000000: episode: 2572, duration: 1.421s, episode steps: 95, steps per second: 67, episode reward: -111.763, mean reward: -1.176 [-100.000, 7.132], mean action: 1.589 [0.000, 3.000], mean observation: -0.001 [-3.984, 1.000], loss: 1.168619, mean_absolute_error: 27.432697, mean_q: 14.671839, mean_eps: 0.774366
  250845/2000000: episode: 2573, duration: 1.420s, episode steps: 93, steps per second: 66, episode reward: -157.955, mean reward: -1.698 [-100.000, 7.108], mean action: 1.699 [0.000, 3.000], mean observation: -0.061 [-1.375, 4.776], loss: 1.722022, mean_absolute_error: 25.304979, mean_q: 15.562507, mean_eps: 0.774282
  250972/2000000: episode: 2574, duration: 1.865s, episode steps: 127, steps per second: 68, episode reward: -143.820, mean reward: -1.132 [-100.000, 8.746], mean action: 1.591 [0.000, 3.000], mean observation: 0.094 [-3.676, 1.000], loss: 1.567115, mean_absolute_error: 27.047750, mean_q: 17.200173, mean_eps: 0.774183
  251070/2000000: episode: 2575, duration: 1.472s, episode steps: 98, steps per second: 67, episode reward: -119.689, mean reward: -1.221 [-100.000, 9.160], mean action: 1.694 [0.000, 3.000], mean observation: -0.050 [-3.939, 1.000], loss: 1.545915, mean_absolute_error: 25.522372, mean_q: 16.816220, mean_eps: 0.774082
  251190/2000000: episode: 2576, duration: 1.738s, episode steps: 120, steps per second: 69, episode reward: -140.346, mean reward: -1.170 [-100.000, 7.418], mean action: 1.567 [0.000, 3.000], mean observation: 0.146 [-1.208, 4.406], loss: 1.589896, mean_absolute_error: 26.888079, mean_q: 16.311883, mean_eps: 0.773983
  251269/2000000: episode: 2577, duration: 1.306s, episode steps: 79, steps per second: 60, episode reward: -83.793, mean reward: -1.061 [-100.000, 9.535], mean action: 1.696 [0.000, 3.000], mean observation: -0.078 [-1.085, 3.116], loss: 2.094155, mean_absolute_error: 26.838085, mean_q: 16.336491, mean_eps: 0.773893
  251384/2000000: episode: 2578, duration: 1.731s, episode steps: 115, steps per second: 66, episode reward: -237.641, mean reward: -2.066 [-100.000, 18.958], mean action: 1.417 [0.000, 3.000], mean observation: 0.190 [-3.260, 1.000], loss: 1.521077, mean_absolute_error: 26.326873, mean_q: 15.627711, mean_eps: 0.773807
  251496/2000000: episode: 2579, duration: 1.745s, episode steps: 112, steps per second: 64, episode reward: -136.068, mean reward: -1.215 [-100.000, 21.166], mean action: 1.473 [0.000, 3.000], mean observation: 0.112 [-1.175, 3.516], loss: 2.068863, mean_absolute_error: 26.250070, mean_q: 15.651502, mean_eps: 0.773706
  251607/2000000: episode: 2580, duration: 1.690s, episode steps: 111, steps per second: 66, episode reward: -180.052, mean reward: -1.622 [-100.000, 2.742], mean action: 1.432 [0.000, 3.000], mean observation: 0.162 [-1.120, 1.114], loss: 1.455122, mean_absolute_error: 25.858496, mean_q: 17.682796, mean_eps: 0.773605
  251729/2000000: episode: 2581, duration: 1.852s, episode steps: 122, steps per second: 66, episode reward: -140.777, mean reward: -1.154 [-100.000, 9.830], mean action: 1.639 [0.000, 3.000], mean observation: -0.023 [-0.996, 3.372], loss: 1.749261, mean_absolute_error: 26.523887, mean_q: 16.342455, mean_eps: 0.773499
  251820/2000000: episode: 2582, duration: 1.347s, episode steps: 91, steps per second: 68, episode reward: -103.200, mean reward: -1.134 [-100.000, 11.774], mean action: 1.451 [0.000, 3.000], mean observation: 0.145 [-1.094, 3.334], loss: 1.740120, mean_absolute_error: 26.920132, mean_q: 14.332979, mean_eps: 0.773403
  251907/2000000: episode: 2583, duration: 1.288s, episode steps: 87, steps per second: 68, episode reward: -66.418, mean reward: -0.763 [-100.000, 19.201], mean action: 1.678 [0.000, 3.000], mean observation: -0.060 [-1.025, 1.000], loss: 1.373433, mean_absolute_error: 26.555685, mean_q: 17.802079, mean_eps: 0.773324
  252022/2000000: episode: 2584, duration: 1.680s, episode steps: 115, steps per second: 68, episode reward: -121.028, mean reward: -1.052 [-100.000, 7.173], mean action: 1.704 [0.000, 3.000], mean observation: -0.020 [-1.034, 3.865], loss: 1.779277, mean_absolute_error: 27.332903, mean_q: 15.487027, mean_eps: 0.773232
  252122/2000000: episode: 2585, duration: 1.452s, episode steps: 100, steps per second: 69, episode reward: -181.098, mean reward: -1.811 [-100.000, 14.229], mean action: 1.460 [0.000, 3.000], mean observation: 0.114 [-4.271, 1.000], loss: 1.741771, mean_absolute_error: 26.865181, mean_q: 14.775656, mean_eps: 0.773135
  252239/2000000: episode: 2586, duration: 1.711s, episode steps: 117, steps per second: 68, episode reward: -150.506, mean reward: -1.286 [-100.000, 11.266], mean action: 1.376 [0.000, 3.000], mean observation: 0.039 [-1.586, 5.448], loss: 1.762357, mean_absolute_error: 26.404402, mean_q: 16.349233, mean_eps: 0.773038
  252338/2000000: episode: 2587, duration: 1.468s, episode steps: 99, steps per second: 67, episode reward: -146.470, mean reward: -1.479 [-100.000, 6.459], mean action: 1.465 [0.000, 3.000], mean observation: -0.040 [-3.939, 1.000], loss: 1.394594, mean_absolute_error: 26.704648, mean_q: 16.599982, mean_eps: 0.772941
  252444/2000000: episode: 2588, duration: 1.596s, episode steps: 106, steps per second: 66, episode reward: -148.109, mean reward: -1.397 [-100.000, 8.325], mean action: 1.604 [0.000, 3.000], mean observation: -0.103 [-1.187, 4.412], loss: 1.014346, mean_absolute_error: 26.390050, mean_q: 14.976885, mean_eps: 0.772849
  252560/2000000: episode: 2589, duration: 1.755s, episode steps: 116, steps per second: 66, episode reward: -129.671, mean reward: -1.118 [-100.000, 7.361], mean action: 1.612 [0.000, 3.000], mean observation: 0.150 [-0.732, 1.677], loss: 1.298508, mean_absolute_error: 25.611241, mean_q: 14.539476, mean_eps: 0.772750
  252618/2000000: episode: 2590, duration: 0.899s, episode steps: 58, steps per second: 64, episode reward: -135.058, mean reward: -2.329 [-100.000, 8.620], mean action: 1.517 [0.000, 3.000], mean observation: 0.030 [-4.966, 1.000], loss: 1.596569, mean_absolute_error: 25.724972, mean_q: 15.734390, mean_eps: 0.772671
  252718/2000000: episode: 2591, duration: 1.478s, episode steps: 100, steps per second: 68, episode reward: -121.193, mean reward: -1.212 [-100.000, 22.083], mean action: 1.320 [0.000, 3.000], mean observation: -0.020 [-3.656, 1.000], loss: 2.204847, mean_absolute_error: 26.252224, mean_q: 16.380589, mean_eps: 0.772599
  252791/2000000: episode: 2592, duration: 1.096s, episode steps: 73, steps per second: 67, episode reward: -114.024, mean reward: -1.562 [-100.000, 7.561], mean action: 1.781 [0.000, 3.000], mean observation: -0.140 [-1.235, 3.605], loss: 1.642041, mean_absolute_error: 27.354530, mean_q: 15.945617, mean_eps: 0.772521
  252861/2000000: episode: 2593, duration: 1.071s, episode steps: 70, steps per second: 65, episode reward: -167.660, mean reward: -2.395 [-100.000, 6.838], mean action: 1.557 [0.000, 3.000], mean observation: -0.071 [-1.537, 1.000], loss: 1.851278, mean_absolute_error: 29.129711, mean_q: 14.380918, mean_eps: 0.772457
  253003/2000000: episode: 2594, duration: 2.166s, episode steps: 142, steps per second: 66, episode reward: -67.486, mean reward: -0.475 [-100.000, 19.825], mean action: 1.718 [0.000, 3.000], mean observation: 0.058 [-1.144, 1.015], loss: 2.224929, mean_absolute_error: 25.487039, mean_q: 16.740603, mean_eps: 0.772361
  253060/2000000: episode: 2595, duration: 0.882s, episode steps: 57, steps per second: 65, episode reward: -174.236, mean reward: -3.057 [-100.000, 5.948], mean action: 1.632 [0.000, 3.000], mean observation: -0.045 [-1.702, 5.643], loss: 1.420898, mean_absolute_error: 27.808627, mean_q: 14.030017, mean_eps: 0.772273
  253162/2000000: episode: 2596, duration: 1.562s, episode steps: 102, steps per second: 65, episode reward: -127.079, mean reward: -1.246 [-100.000, 15.619], mean action: 1.598 [0.000, 3.000], mean observation: 0.067 [-1.098, 1.000], loss: 2.143754, mean_absolute_error: 27.085311, mean_q: 15.366660, mean_eps: 0.772201
  253231/2000000: episode: 2597, duration: 1.007s, episode steps: 69, steps per second: 69, episode reward: -104.055, mean reward: -1.508 [-100.000, 7.255], mean action: 1.536 [0.000, 3.000], mean observation: -0.144 [-4.307, 1.000], loss: 2.019160, mean_absolute_error: 26.391468, mean_q: 16.862483, mean_eps: 0.772124
  253323/2000000: episode: 2598, duration: 1.366s, episode steps: 92, steps per second: 67, episode reward: -80.729, mean reward: -0.877 [-100.000, 13.060], mean action: 1.630 [0.000, 3.000], mean observation: -0.031 [-3.360, 1.000], loss: 2.210105, mean_absolute_error: 27.639026, mean_q: 13.942488, mean_eps: 0.772052
  253409/2000000: episode: 2599, duration: 1.294s, episode steps: 86, steps per second: 66, episode reward: -131.222, mean reward: -1.526 [-100.000, 11.978], mean action: 1.616 [0.000, 3.000], mean observation: 0.021 [-1.314, 4.549], loss: 1.985567, mean_absolute_error: 27.353222, mean_q: 14.280042, mean_eps: 0.771971
  253521/2000000: episode: 2600, duration: 1.635s, episode steps: 112, steps per second: 69, episode reward: -116.283, mean reward: -1.038 [-100.000, 13.149], mean action: 1.562 [0.000, 3.000], mean observation: 0.108 [-3.456, 1.000], loss: 1.823061, mean_absolute_error: 26.656648, mean_q: 17.086816, mean_eps: 0.771881
  253669/2000000: episode: 2601, duration: 2.250s, episode steps: 148, steps per second: 66, episode reward: -110.525, mean reward: -0.747 [-100.000, 8.856], mean action: 1.635 [0.000, 3.000], mean observation: 0.047 [-4.225, 1.073], loss: 1.444060, mean_absolute_error: 26.708152, mean_q: 16.446094, mean_eps: 0.771764
  253732/2000000: episode: 2602, duration: 0.980s, episode steps: 63, steps per second: 64, episode reward: -90.410, mean reward: -1.435 [-100.000, 7.791], mean action: 1.635 [0.000, 3.000], mean observation: -0.162 [-4.300, 1.000], loss: 1.086444, mean_absolute_error: 24.782662, mean_q: 17.184401, mean_eps: 0.771670
  253824/2000000: episode: 2603, duration: 1.413s, episode steps: 92, steps per second: 65, episode reward: -137.879, mean reward: -1.499 [-100.000, 11.830], mean action: 1.717 [0.000, 3.000], mean observation: 0.049 [-1.180, 4.173], loss: 1.595359, mean_absolute_error: 25.837947, mean_q: 17.357978, mean_eps: 0.771602
  253904/2000000: episode: 2604, duration: 1.279s, episode steps: 80, steps per second: 63, episode reward: -139.349, mean reward: -1.742 [-100.000, 6.187], mean action: 1.550 [0.000, 3.000], mean observation: 0.112 [-1.288, 4.903], loss: 1.590877, mean_absolute_error: 26.022436, mean_q: 15.491023, mean_eps: 0.771524
  253977/2000000: episode: 2605, duration: 1.165s, episode steps: 73, steps per second: 63, episode reward: -103.228, mean reward: -1.414 [-100.000, 17.956], mean action: 1.589 [0.000, 3.000], mean observation: -0.058 [-1.307, 1.000], loss: 1.382781, mean_absolute_error: 25.998454, mean_q: 18.575131, mean_eps: 0.771454
  254073/2000000: episode: 2606, duration: 1.428s, episode steps: 96, steps per second: 67, episode reward: -79.200, mean reward: -0.825 [-100.000, 7.490], mean action: 1.479 [0.000, 3.000], mean observation: 0.049 [-3.456, 1.000], loss: 1.807786, mean_absolute_error: 25.828906, mean_q: 17.561569, mean_eps: 0.771377
  254195/2000000: episode: 2607, duration: 1.803s, episode steps: 122, steps per second: 68, episode reward: -134.834, mean reward: -1.105 [-100.000, 11.222], mean action: 1.500 [0.000, 3.000], mean observation: 0.108 [-3.676, 1.043], loss: 1.568179, mean_absolute_error: 26.851801, mean_q: 13.913701, mean_eps: 0.771279
  254281/2000000: episode: 2608, duration: 1.469s, episode steps: 86, steps per second: 59, episode reward: -100.454, mean reward: -1.168 [-100.000, 11.471], mean action: 1.744 [0.000, 3.000], mean observation: -0.138 [-1.108, 1.908], loss: 1.834330, mean_absolute_error: 27.291128, mean_q: 16.045720, mean_eps: 0.771186
  254389/2000000: episode: 2609, duration: 1.659s, episode steps: 108, steps per second: 65, episode reward: -173.604, mean reward: -1.607 [-100.000, 14.425], mean action: 1.454 [0.000, 3.000], mean observation: 0.157 [-4.130, 1.000], loss: 1.620225, mean_absolute_error: 27.503064, mean_q: 15.762126, mean_eps: 0.771098
  254509/2000000: episode: 2610, duration: 1.738s, episode steps: 120, steps per second: 69, episode reward: -151.480, mean reward: -1.262 [-100.000, 7.419], mean action: 1.492 [0.000, 3.000], mean observation: 0.105 [-3.692, 1.000], loss: 1.263718, mean_absolute_error: 25.586697, mean_q: 16.949183, mean_eps: 0.770995
  254655/2000000: episode: 2611, duration: 2.101s, episode steps: 146, steps per second: 69, episode reward: -192.076, mean reward: -1.316 [-100.000, 10.714], mean action: 1.651 [0.000, 3.000], mean observation: -0.020 [-1.631, 4.862], loss: 1.124466, mean_absolute_error: 25.283321, mean_q: 16.409946, mean_eps: 0.770876
  254792/2000000: episode: 2612, duration: 2.023s, episode steps: 137, steps per second: 68, episode reward: -131.621, mean reward: -0.961 [-100.000, 8.815], mean action: 1.620 [0.000, 3.000], mean observation: 0.090 [-4.176, 1.047], loss: 1.440119, mean_absolute_error: 26.915675, mean_q: 14.449595, mean_eps: 0.770750
  254857/2000000: episode: 2613, duration: 1.001s, episode steps: 65, steps per second: 65, episode reward: -133.106, mean reward: -2.048 [-100.000, 6.513], mean action: 1.615 [0.000, 3.000], mean observation: 0.073 [-1.313, 3.902], loss: 1.852884, mean_absolute_error: 25.972911, mean_q: 17.215274, mean_eps: 0.770658
  254958/2000000: episode: 2614, duration: 1.527s, episode steps: 101, steps per second: 66, episode reward: -143.128, mean reward: -1.417 [-100.000, 7.654], mean action: 1.515 [0.000, 3.000], mean observation: 0.020 [-1.188, 1.000], loss: 1.755825, mean_absolute_error: 27.937816, mean_q: 17.734127, mean_eps: 0.770583
  255058/2000000: episode: 2615, duration: 1.507s, episode steps: 100, steps per second: 66, episode reward: -115.025, mean reward: -1.150 [-100.000, 8.218], mean action: 1.710 [0.000, 3.000], mean observation: 0.102 [-1.025, 2.750], loss: 1.421512, mean_absolute_error: 25.721326, mean_q: 16.951161, mean_eps: 0.770493
  255144/2000000: episode: 2616, duration: 1.276s, episode steps: 86, steps per second: 67, episode reward: -104.502, mean reward: -1.215 [-100.000, 11.557], mean action: 1.558 [0.000, 3.000], mean observation: 0.090 [-3.161, 1.000], loss: 2.657028, mean_absolute_error: 26.675611, mean_q: 16.095420, mean_eps: 0.770410
  255219/2000000: episode: 2617, duration: 1.104s, episode steps: 75, steps per second: 68, episode reward: -109.677, mean reward: -1.462 [-100.000, 13.989], mean action: 1.573 [0.000, 3.000], mean observation: 0.030 [-4.479, 1.000], loss: 1.975709, mean_absolute_error: 25.958660, mean_q: 16.446707, mean_eps: 0.770338
  255338/2000000: episode: 2618, duration: 1.739s, episode steps: 119, steps per second: 68, episode reward: -111.542, mean reward: -0.937 [-100.000, 11.641], mean action: 1.571 [0.000, 3.000], mean observation: -0.032 [-1.309, 1.001], loss: 1.651790, mean_absolute_error: 26.334323, mean_q: 15.686897, mean_eps: 0.770250
  255440/2000000: episode: 2619, duration: 1.601s, episode steps: 102, steps per second: 64, episode reward: -133.185, mean reward: -1.306 [-100.000, 12.114], mean action: 1.520 [0.000, 3.000], mean observation: -0.050 [-1.257, 1.000], loss: 1.392924, mean_absolute_error: 26.205623, mean_q: 15.465099, mean_eps: 0.770151
  255542/2000000: episode: 2620, duration: 1.668s, episode steps: 102, steps per second: 61, episode reward: -75.372, mean reward: -0.739 [-100.000, 8.180], mean action: 1.686 [0.000, 3.000], mean observation: 0.059 [-0.846, 2.779], loss: 1.324286, mean_absolute_error: 26.704057, mean_q: 15.947212, mean_eps: 0.770059
  255672/2000000: episode: 2621, duration: 2.090s, episode steps: 130, steps per second: 62, episode reward: -126.398, mean reward: -0.972 [-100.000, 7.506], mean action: 1.546 [0.000, 3.000], mean observation: -0.010 [-3.609, 1.000], loss: 1.536352, mean_absolute_error: 26.754852, mean_q: 16.215268, mean_eps: 0.769955
  255771/2000000: episode: 2622, duration: 1.615s, episode steps: 99, steps per second: 61, episode reward: -120.755, mean reward: -1.220 [-100.000, 16.239], mean action: 1.808 [0.000, 3.000], mean observation: -0.098 [-3.760, 1.000], loss: 2.145552, mean_absolute_error: 27.551651, mean_q: 15.499627, mean_eps: 0.769852
  255869/2000000: episode: 2623, duration: 1.544s, episode steps: 98, steps per second: 63, episode reward: -113.950, mean reward: -1.163 [-100.000, 6.797], mean action: 1.449 [0.000, 3.000], mean observation: -0.104 [-0.988, 3.047], loss: 1.751729, mean_absolute_error: 28.021400, mean_q: 16.249775, mean_eps: 0.769762
  255941/2000000: episode: 2624, duration: 1.140s, episode steps: 72, steps per second: 63, episode reward: -137.593, mean reward: -1.911 [-100.000, 5.613], mean action: 1.764 [0.000, 3.000], mean observation: -0.123 [-1.163, 1.000], loss: 1.296663, mean_absolute_error: 27.278026, mean_q: 13.375195, mean_eps: 0.769685
  256042/2000000: episode: 2625, duration: 1.517s, episode steps: 101, steps per second: 67, episode reward: -151.498, mean reward: -1.500 [-100.000, 10.820], mean action: 1.574 [0.000, 3.000], mean observation: -0.066 [-1.226, 4.037], loss: 1.316919, mean_absolute_error: 25.070303, mean_q: 18.170680, mean_eps: 0.769607
  256116/2000000: episode: 2626, duration: 1.152s, episode steps: 74, steps per second: 64, episode reward: -116.397, mean reward: -1.573 [-100.000, 10.980], mean action: 1.459 [0.000, 3.000], mean observation: 0.025 [-4.321, 1.000], loss: 2.434536, mean_absolute_error: 26.682732, mean_q: 16.675375, mean_eps: 0.769530
  256223/2000000: episode: 2627, duration: 1.644s, episode steps: 107, steps per second: 65, episode reward: -85.666, mean reward: -0.801 [-100.000, 17.231], mean action: 1.477 [0.000, 3.000], mean observation: 0.028 [-1.059, 1.000], loss: 1.561383, mean_absolute_error: 26.479053, mean_q: 16.318995, mean_eps: 0.769449
  256335/2000000: episode: 2628, duration: 1.676s, episode steps: 112, steps per second: 67, episode reward: -135.883, mean reward: -1.213 [-100.000, 10.836], mean action: 1.527 [0.000, 3.000], mean observation: 0.089 [-1.156, 1.000], loss: 1.133765, mean_absolute_error: 26.679138, mean_q: 15.415943, mean_eps: 0.769350
  256412/2000000: episode: 2629, duration: 1.191s, episode steps: 77, steps per second: 65, episode reward: -95.042, mean reward: -1.234 [-100.000, 11.082], mean action: 1.766 [0.000, 3.000], mean observation: 0.066 [-1.085, 3.600], loss: 2.821969, mean_absolute_error: 27.781663, mean_q: 13.895626, mean_eps: 0.769265
  256522/2000000: episode: 2630, duration: 1.729s, episode steps: 110, steps per second: 64, episode reward: -178.802, mean reward: -1.625 [-100.000, 9.882], mean action: 1.655 [0.000, 3.000], mean observation: -0.116 [-1.345, 4.864], loss: 1.801743, mean_absolute_error: 27.089949, mean_q: 17.357924, mean_eps: 0.769181
  256622/2000000: episode: 2631, duration: 1.575s, episode steps: 100, steps per second: 63, episode reward: -137.215, mean reward: -1.372 [-100.000, 6.375], mean action: 1.500 [0.000, 3.000], mean observation: 0.112 [-3.307, 1.000], loss: 1.341126, mean_absolute_error: 25.147982, mean_q: 19.121771, mean_eps: 0.769085
  256731/2000000: episode: 2632, duration: 1.656s, episode steps: 109, steps per second: 66, episode reward: -82.110, mean reward: -0.753 [-100.000, 46.460], mean action: 1.349 [0.000, 3.000], mean observation: -0.055 [-2.694, 1.000], loss: 1.301530, mean_absolute_error: 26.211417, mean_q: 16.321882, mean_eps: 0.768992
  256802/2000000: episode: 2633, duration: 1.119s, episode steps: 71, steps per second: 63, episode reward: -95.533, mean reward: -1.346 [-100.000, 15.963], mean action: 1.718 [0.000, 3.000], mean observation: -0.154 [-1.216, 1.000], loss: 2.019786, mean_absolute_error: 25.206981, mean_q: 16.431797, mean_eps: 0.768911
  256938/2000000: episode: 2634, duration: 1.948s, episode steps: 136, steps per second: 70, episode reward: -127.697, mean reward: -0.939 [-100.000, 9.784], mean action: 1.551 [0.000, 3.000], mean observation: 0.089 [-3.856, 1.043], loss: 0.965397, mean_absolute_error: 25.490040, mean_q: 18.374110, mean_eps: 0.768817
  257022/2000000: episode: 2635, duration: 1.196s, episode steps: 84, steps per second: 70, episode reward: -112.990, mean reward: -1.345 [-100.000, 8.003], mean action: 1.548 [0.000, 3.000], mean observation: 0.098 [-1.186, 4.177], loss: 1.822485, mean_absolute_error: 25.914883, mean_q: 16.989696, mean_eps: 0.768718
  257099/2000000: episode: 2636, duration: 1.087s, episode steps: 77, steps per second: 71, episode reward: -100.509, mean reward: -1.305 [-100.000, 19.454], mean action: 1.753 [0.000, 3.000], mean observation: -0.084 [-1.176, 1.000], loss: 1.431234, mean_absolute_error: 26.411323, mean_q: 15.301772, mean_eps: 0.768646
  257174/2000000: episode: 2637, duration: 1.093s, episode steps: 75, steps per second: 69, episode reward: -106.700, mean reward: -1.423 [-100.000, 6.854], mean action: 1.707 [0.000, 3.000], mean observation: -0.115 [-3.805, 1.000], loss: 1.426514, mean_absolute_error: 26.344237, mean_q: 18.767016, mean_eps: 0.768578
  257303/2000000: episode: 2638, duration: 1.834s, episode steps: 129, steps per second: 70, episode reward: -140.240, mean reward: -1.087 [-100.000, 24.805], mean action: 1.550 [0.000, 3.000], mean observation: 0.138 [-3.939, 1.000], loss: 1.379951, mean_absolute_error: 25.157699, mean_q: 17.872386, mean_eps: 0.768486
  257400/2000000: episode: 2639, duration: 1.457s, episode steps: 97, steps per second: 67, episode reward: -110.903, mean reward: -1.143 [-100.000, 8.155], mean action: 1.608 [0.000, 3.000], mean observation: -0.070 [-3.769, 1.000], loss: 1.730863, mean_absolute_error: 25.708935, mean_q: 17.852711, mean_eps: 0.768385
  257495/2000000: episode: 2640, duration: 1.372s, episode steps: 95, steps per second: 69, episode reward: -103.715, mean reward: -1.092 [-100.000, 13.640], mean action: 1.442 [0.000, 3.000], mean observation: 0.046 [-1.150, 3.262], loss: 1.219363, mean_absolute_error: 25.934013, mean_q: 18.751458, mean_eps: 0.768299
  257609/2000000: episode: 2641, duration: 1.651s, episode steps: 114, steps per second: 69, episode reward: -130.689, mean reward: -1.146 [-100.000, 18.455], mean action: 1.711 [0.000, 3.000], mean observation: 0.118 [-1.101, 1.000], loss: 1.955045, mean_absolute_error: 26.570989, mean_q: 17.044986, mean_eps: 0.768203
  257747/2000000: episode: 2642, duration: 1.958s, episode steps: 138, steps per second: 70, episode reward: -83.329, mean reward: -0.604 [-100.000, 13.604], mean action: 1.638 [0.000, 3.000], mean observation: 0.119 [-0.967, 2.946], loss: 1.341606, mean_absolute_error: 25.703689, mean_q: 17.640172, mean_eps: 0.768090
  257914/2000000: episode: 2643, duration: 2.411s, episode steps: 167, steps per second: 69, episode reward: -103.919, mean reward: -0.622 [-100.000, 7.640], mean action: 1.695 [0.000, 3.000], mean observation: 0.085 [-0.869, 3.017], loss: 1.802387, mean_absolute_error: 27.609965, mean_q: 15.361463, mean_eps: 0.767953
  257990/2000000: episode: 2644, duration: 1.090s, episode steps: 76, steps per second: 70, episode reward: -92.914, mean reward: -1.223 [-100.000, 18.689], mean action: 1.276 [0.000, 3.000], mean observation: 0.095 [-1.018, 1.000], loss: 2.063947, mean_absolute_error: 27.609046, mean_q: 14.674027, mean_eps: 0.767843
  258144/2000000: episode: 2645, duration: 2.230s, episode steps: 154, steps per second: 69, episode reward: -59.009, mean reward: -0.383 [-100.000, 21.072], mean action: 1.604 [0.000, 3.000], mean observation: 0.024 [-0.909, 1.033], loss: 1.753832, mean_absolute_error: 26.467147, mean_q: 17.125063, mean_eps: 0.767741
  258257/2000000: episode: 2646, duration: 1.682s, episode steps: 113, steps per second: 67, episode reward: -193.395, mean reward: -1.711 [-100.000, 6.822], mean action: 1.779 [0.000, 3.000], mean observation: -0.108 [-1.197, 1.039], loss: 1.667305, mean_absolute_error: 26.088330, mean_q: 16.541973, mean_eps: 0.767620
  258355/2000000: episode: 2647, duration: 1.376s, episode steps: 98, steps per second: 71, episode reward: -160.765, mean reward: -1.640 [-100.000, 20.314], mean action: 1.429 [0.000, 3.000], mean observation: 0.069 [-1.439, 5.507], loss: 1.643289, mean_absolute_error: 25.762089, mean_q: 16.411021, mean_eps: 0.767525
  258474/2000000: episode: 2648, duration: 1.726s, episode steps: 119, steps per second: 69, episode reward: -109.887, mean reward: -0.923 [-100.000, 17.147], mean action: 1.681 [0.000, 3.000], mean observation: -0.004 [-1.278, 1.000], loss: 1.567687, mean_absolute_error: 27.090288, mean_q: 15.658293, mean_eps: 0.767427
  258541/2000000: episode: 2649, duration: 0.985s, episode steps: 67, steps per second: 68, episode reward: -86.481, mean reward: -1.291 [-100.000, 21.952], mean action: 1.582 [0.000, 3.000], mean observation: 0.034 [-1.333, 1.000], loss: 2.157789, mean_absolute_error: 27.486563, mean_q: 15.871654, mean_eps: 0.767343
  258654/2000000: episode: 2650, duration: 1.646s, episode steps: 113, steps per second: 69, episode reward: -132.691, mean reward: -1.174 [-100.000, 16.075], mean action: 1.628 [0.000, 3.000], mean observation: 0.144 [-1.137, 1.007], loss: 2.505445, mean_absolute_error: 27.785879, mean_q: 15.232002, mean_eps: 0.767262
  258762/2000000: episode: 2651, duration: 1.543s, episode steps: 108, steps per second: 70, episode reward: -111.933, mean reward: -1.036 [-100.000, 18.046], mean action: 1.519 [0.000, 3.000], mean observation: -0.109 [-1.078, 1.000], loss: 1.556668, mean_absolute_error: 27.133131, mean_q: 14.829462, mean_eps: 0.767163
  258866/2000000: episode: 2652, duration: 1.496s, episode steps: 104, steps per second: 70, episode reward: -136.048, mean reward: -1.308 [-100.000, 22.858], mean action: 1.433 [0.000, 3.000], mean observation: 0.142 [-1.171, 1.000], loss: 1.845282, mean_absolute_error: 25.646940, mean_q: 15.912029, mean_eps: 0.767067
  258972/2000000: episode: 2653, duration: 1.534s, episode steps: 106, steps per second: 69, episode reward: -76.206, mean reward: -0.719 [-100.000, 12.857], mean action: 1.660 [0.000, 3.000], mean observation: 0.048 [-0.936, 1.000], loss: 1.676906, mean_absolute_error: 26.021716, mean_q: 16.350546, mean_eps: 0.766974
  259063/2000000: episode: 2654, duration: 1.310s, episode steps: 91, steps per second: 69, episode reward: -108.679, mean reward: -1.194 [-100.000, 11.253], mean action: 1.626 [0.000, 3.000], mean observation: 0.032 [-1.081, 1.000], loss: 1.551058, mean_absolute_error: 25.779984, mean_q: 15.875954, mean_eps: 0.766886
  259156/2000000: episode: 2655, duration: 1.370s, episode steps: 93, steps per second: 68, episode reward: -143.851, mean reward: -1.547 [-100.000, 6.318], mean action: 1.462 [0.000, 3.000], mean observation: 0.075 [-3.943, 1.000], loss: 2.027121, mean_absolute_error: 25.743801, mean_q: 16.989990, mean_eps: 0.766803
  259228/2000000: episode: 2656, duration: 1.084s, episode steps: 72, steps per second: 66, episode reward: -194.687, mean reward: -2.704 [-100.000, 7.749], mean action: 1.556 [0.000, 3.000], mean observation: 0.036 [-1.429, 3.959], loss: 1.481385, mean_absolute_error: 26.073066, mean_q: 18.863734, mean_eps: 0.766729
  259316/2000000: episode: 2657, duration: 1.303s, episode steps: 88, steps per second: 68, episode reward: -156.611, mean reward: -1.780 [-100.000, 14.074], mean action: 1.591 [0.000, 3.000], mean observation: 0.101 [-1.443, 4.685], loss: 1.939689, mean_absolute_error: 25.514488, mean_q: 17.649803, mean_eps: 0.766657
  259391/2000000: episode: 2658, duration: 1.096s, episode steps: 75, steps per second: 68, episode reward: -111.224, mean reward: -1.483 [-100.000, 11.196], mean action: 1.787 [0.000, 3.000], mean observation: -0.145 [-1.174, 3.679], loss: 1.571101, mean_absolute_error: 27.027247, mean_q: 14.709881, mean_eps: 0.766583
  259525/2000000: episode: 2659, duration: 1.913s, episode steps: 134, steps per second: 70, episode reward: -122.413, mean reward: -0.914 [-100.000, 7.793], mean action: 1.567 [0.000, 3.000], mean observation: 0.080 [-1.261, 4.572], loss: 2.182334, mean_absolute_error: 26.231314, mean_q: 19.920047, mean_eps: 0.766488
  259606/2000000: episode: 2660, duration: 1.158s, episode steps: 81, steps per second: 70, episode reward: -135.724, mean reward: -1.676 [-100.000, 10.869], mean action: 1.605 [0.000, 3.000], mean observation: 0.088 [-1.270, 4.727], loss: 1.895386, mean_absolute_error: 25.996758, mean_q: 16.785966, mean_eps: 0.766391
  259744/2000000: episode: 2661, duration: 1.975s, episode steps: 138, steps per second: 70, episode reward: -176.718, mean reward: -1.281 [-100.000, 6.070], mean action: 1.500 [0.000, 3.000], mean observation: 0.137 [-3.820, 1.000], loss: 1.183036, mean_absolute_error: 25.004155, mean_q: 18.203108, mean_eps: 0.766293
  259814/2000000: episode: 2662, duration: 1.028s, episode steps: 70, steps per second: 68, episode reward: -109.271, mean reward: -1.561 [-100.000, 11.858], mean action: 1.357 [0.000, 3.000], mean observation: 0.062 [-4.161, 1.000], loss: 1.524621, mean_absolute_error: 26.383163, mean_q: 17.233313, mean_eps: 0.766200
  259877/2000000: episode: 2663, duration: 0.908s, episode steps: 63, steps per second: 69, episode reward: -94.547, mean reward: -1.501 [-100.000, 16.437], mean action: 1.524 [0.000, 3.000], mean observation: 0.058 [-3.726, 1.000], loss: 1.630402, mean_absolute_error: 27.046867, mean_q: 15.153068, mean_eps: 0.766139
  260046/2000000: episode: 2664, duration: 2.424s, episode steps: 169, steps per second: 70, episode reward: -88.685, mean reward: -0.525 [-100.000, 7.416], mean action: 1.692 [0.000, 3.000], mean observation: 0.043 [-0.831, 3.163], loss: 1.737363, mean_absolute_error: 26.331775, mean_q: 18.683678, mean_eps: 0.766034
  260174/2000000: episode: 2665, duration: 1.835s, episode steps: 128, steps per second: 70, episode reward: -153.427, mean reward: -1.199 [-100.000, 20.318], mean action: 1.594 [0.000, 3.000], mean observation: -0.066 [-1.270, 3.670], loss: 2.242357, mean_absolute_error: 26.796710, mean_q: 16.284658, mean_eps: 0.765901
  260270/2000000: episode: 2666, duration: 1.371s, episode steps: 96, steps per second: 70, episode reward: -149.464, mean reward: -1.557 [-100.000, 9.293], mean action: 1.646 [0.000, 3.000], mean observation: 0.121 [-3.110, 1.000], loss: 1.472812, mean_absolute_error: 26.507905, mean_q: 17.987553, mean_eps: 0.765800
  260338/2000000: episode: 2667, duration: 0.984s, episode steps: 68, steps per second: 69, episode reward: -139.663, mean reward: -2.054 [-100.000, 12.902], mean action: 1.471 [0.000, 3.000], mean observation: -0.001 [-1.431, 5.041], loss: 1.169442, mean_absolute_error: 25.803525, mean_q: 18.412074, mean_eps: 0.765726
  260444/2000000: episode: 2668, duration: 1.511s, episode steps: 106, steps per second: 70, episode reward: -158.747, mean reward: -1.498 [-100.000, 9.019], mean action: 1.519 [0.000, 3.000], mean observation: 0.162 [-3.308, 1.000], loss: 2.173078, mean_absolute_error: 27.477286, mean_q: 18.010804, mean_eps: 0.765649
  260542/2000000: episode: 2669, duration: 1.449s, episode steps: 98, steps per second: 68, episode reward: -113.401, mean reward: -1.157 [-100.000, 6.798], mean action: 1.653 [0.000, 3.000], mean observation: 0.033 [-3.008, 1.000], loss: 1.453236, mean_absolute_error: 26.756342, mean_q: 17.226377, mean_eps: 0.765557
  260627/2000000: episode: 2670, duration: 1.199s, episode steps: 85, steps per second: 71, episode reward: -118.918, mean reward: -1.399 [-100.000, 8.318], mean action: 1.447 [0.000, 3.000], mean observation: 0.082 [-1.113, 3.749], loss: 1.571576, mean_absolute_error: 27.001614, mean_q: 16.114565, mean_eps: 0.765474
  260732/2000000: episode: 2671, duration: 1.535s, episode steps: 105, steps per second: 68, episode reward: -164.840, mean reward: -1.570 [-100.000, 10.384], mean action: 1.771 [0.000, 3.000], mean observation: 0.023 [-1.260, 4.474], loss: 1.956094, mean_absolute_error: 27.000153, mean_q: 19.480722, mean_eps: 0.765390
  260828/2000000: episode: 2672, duration: 1.436s, episode steps: 96, steps per second: 67, episode reward: -163.518, mean reward: -1.703 [-100.000, 11.639], mean action: 1.719 [0.000, 3.000], mean observation: -0.121 [-1.234, 2.737], loss: 1.737723, mean_absolute_error: 27.276941, mean_q: 16.617503, mean_eps: 0.765300
  260971/2000000: episode: 2673, duration: 2.186s, episode steps: 143, steps per second: 65, episode reward: -230.912, mean reward: -1.615 [-100.000, 19.717], mean action: 1.608 [0.000, 3.000], mean observation: 0.198 [-1.118, 4.452], loss: 1.483096, mean_absolute_error: 27.475220, mean_q: 16.058374, mean_eps: 0.765192
  261061/2000000: episode: 2674, duration: 1.292s, episode steps: 90, steps per second: 70, episode reward: -117.032, mean reward: -1.300 [-100.000, 24.995], mean action: 1.489 [0.000, 3.000], mean observation: 0.123 [-1.051, 1.000], loss: 1.750021, mean_absolute_error: 26.683729, mean_q: 18.342564, mean_eps: 0.765086
  261162/2000000: episode: 2675, duration: 1.439s, episode steps: 101, steps per second: 70, episode reward: -130.973, mean reward: -1.297 [-100.000, 15.011], mean action: 1.752 [0.000, 3.000], mean observation: -0.148 [-1.159, 1.000], loss: 1.556448, mean_absolute_error: 26.382151, mean_q: 15.152610, mean_eps: 0.764999
  261244/2000000: episode: 2676, duration: 1.193s, episode steps: 82, steps per second: 69, episode reward: -164.177, mean reward: -2.002 [-100.000, 10.713], mean action: 1.598 [0.000, 3.000], mean observation: -0.151 [-6.531, 1.000], loss: 1.424233, mean_absolute_error: 27.323176, mean_q: 16.430659, mean_eps: 0.764918
  261326/2000000: episode: 2677, duration: 1.192s, episode steps: 82, steps per second: 69, episode reward: -124.013, mean reward: -1.512 [-100.000, 6.932], mean action: 1.610 [0.000, 3.000], mean observation: -0.134 [-4.194, 1.000], loss: 2.117673, mean_absolute_error: 28.510453, mean_q: 13.911312, mean_eps: 0.764844
  261442/2000000: episode: 2678, duration: 1.654s, episode steps: 116, steps per second: 70, episode reward: -102.365, mean reward: -0.882 [-100.000, 12.257], mean action: 1.612 [0.000, 3.000], mean observation: 0.097 [-0.927, 1.000], loss: 1.286511, mean_absolute_error: 27.257611, mean_q: 15.985715, mean_eps: 0.764754
  261524/2000000: episode: 2679, duration: 1.195s, episode steps: 82, steps per second: 69, episode reward: -95.526, mean reward: -1.165 [-100.000, 16.095], mean action: 1.720 [0.000, 3.000], mean observation: -0.121 [-1.011, 1.000], loss: 2.366120, mean_absolute_error: 29.294470, mean_q: 15.952368, mean_eps: 0.764666
  261615/2000000: episode: 2680, duration: 1.323s, episode steps: 91, steps per second: 69, episode reward: -156.331, mean reward: -1.718 [-100.000, 6.432], mean action: 1.725 [0.000, 3.000], mean observation: -0.116 [-1.187, 4.095], loss: 1.230648, mean_absolute_error: 26.158100, mean_q: 17.491859, mean_eps: 0.764589
  261721/2000000: episode: 2681, duration: 1.526s, episode steps: 106, steps per second: 69, episode reward: -174.116, mean reward: -1.643 [-100.000, 12.683], mean action: 1.557 [0.000, 3.000], mean observation: 0.154 [-1.118, 4.060], loss: 1.050907, mean_absolute_error: 25.692028, mean_q: 17.556402, mean_eps: 0.764499
  261842/2000000: episode: 2682, duration: 1.722s, episode steps: 121, steps per second: 70, episode reward: -266.649, mean reward: -2.204 [-100.000, 9.516], mean action: 1.752 [0.000, 3.000], mean observation: -0.166 [-1.589, 1.000], loss: 2.078813, mean_absolute_error: 28.014666, mean_q: 16.439586, mean_eps: 0.764396
  261979/2000000: episode: 2683, duration: 1.943s, episode steps: 137, steps per second: 71, episode reward: -138.303, mean reward: -1.010 [-100.000, 6.383], mean action: 1.664 [0.000, 3.000], mean observation: 0.053 [-3.111, 1.000], loss: 2.176335, mean_absolute_error: 28.992912, mean_q: 13.400253, mean_eps: 0.764281
  262047/2000000: episode: 2684, duration: 1.005s, episode steps: 68, steps per second: 68, episode reward: -83.690, mean reward: -1.231 [-100.000, 11.130], mean action: 1.632 [0.000, 3.000], mean observation: -0.058 [-1.268, 1.000], loss: 1.835250, mean_absolute_error: 26.598988, mean_q: 15.998502, mean_eps: 0.764189
  262158/2000000: episode: 2685, duration: 1.593s, episode steps: 111, steps per second: 70, episode reward: -106.873, mean reward: -0.963 [-100.000, 19.731], mean action: 1.532 [0.000, 3.000], mean observation: 0.093 [-1.899, 1.000], loss: 0.975255, mean_absolute_error: 25.433926, mean_q: 19.602171, mean_eps: 0.764108
  262289/2000000: episode: 2686, duration: 1.862s, episode steps: 131, steps per second: 70, episode reward: -85.143, mean reward: -0.650 [-100.000, 13.653], mean action: 1.573 [0.000, 3.000], mean observation: -0.002 [-0.952, 1.000], loss: 1.672423, mean_absolute_error: 26.527119, mean_q: 15.050752, mean_eps: 0.763998
  262415/2000000: episode: 2687, duration: 1.795s, episode steps: 126, steps per second: 70, episode reward: -165.023, mean reward: -1.310 [-100.000, 7.501], mean action: 1.619 [0.000, 3.000], mean observation: 0.021 [-1.271, 4.505], loss: 1.506127, mean_absolute_error: 27.098789, mean_q: 16.758299, mean_eps: 0.763883
  262510/2000000: episode: 2688, duration: 1.342s, episode steps: 95, steps per second: 71, episode reward: -187.480, mean reward: -1.973 [-100.000, 8.158], mean action: 1.579 [0.000, 3.000], mean observation: 0.007 [-1.418, 4.330], loss: 1.577930, mean_absolute_error: 26.620789, mean_q: 17.918504, mean_eps: 0.763784
  262594/2000000: episode: 2689, duration: 1.308s, episode steps: 84, steps per second: 64, episode reward: -156.593, mean reward: -1.864 [-100.000, 16.930], mean action: 1.810 [0.000, 3.000], mean observation: -0.121 [-1.464, 4.850], loss: 1.380405, mean_absolute_error: 27.706087, mean_q: 17.636003, mean_eps: 0.763703
  262670/2000000: episode: 2690, duration: 1.126s, episode steps: 76, steps per second: 68, episode reward: -149.081, mean reward: -1.962 [-100.000, 18.817], mean action: 1.434 [0.000, 3.000], mean observation: 0.064 [-1.305, 4.107], loss: 1.465690, mean_absolute_error: 27.590546, mean_q: 15.087273, mean_eps: 0.763631
  262744/2000000: episode: 2691, duration: 1.365s, episode steps: 74, steps per second: 54, episode reward: -130.106, mean reward: -1.758 [-100.000, 8.525], mean action: 1.784 [0.000, 3.000], mean observation: -0.099 [-1.277, 4.411], loss: 1.964112, mean_absolute_error: 27.591400, mean_q: 13.614044, mean_eps: 0.763565
  262810/2000000: episode: 2692, duration: 1.291s, episode steps: 66, steps per second: 51, episode reward: -81.925, mean reward: -1.241 [-100.000, 16.954], mean action: 1.652 [0.000, 3.000], mean observation: -0.058 [-4.387, 1.000], loss: 1.562174, mean_absolute_error: 27.837866, mean_q: 16.623321, mean_eps: 0.763502
  262926/2000000: episode: 2693, duration: 1.790s, episode steps: 116, steps per second: 65, episode reward: -173.983, mean reward: -1.500 [-100.000, 7.466], mean action: 1.621 [0.000, 3.000], mean observation: -0.091 [-3.964, 1.000], loss: 2.183422, mean_absolute_error: 28.323171, mean_q: 14.717611, mean_eps: 0.763419
  263041/2000000: episode: 2694, duration: 1.718s, episode steps: 115, steps per second: 67, episode reward: -110.047, mean reward: -0.957 [-100.000, 13.318], mean action: 1.609 [0.000, 3.000], mean observation: 0.020 [-1.016, 3.198], loss: 1.131178, mean_absolute_error: 25.821044, mean_q: 18.018538, mean_eps: 0.763314
  263167/2000000: episode: 2695, duration: 1.802s, episode steps: 126, steps per second: 70, episode reward: -226.413, mean reward: -1.797 [-100.000, 56.751], mean action: 1.437 [0.000, 3.000], mean observation: 0.158 [-1.535, 4.084], loss: 2.055906, mean_absolute_error: 28.480576, mean_q: 14.386198, mean_eps: 0.763206
  263255/2000000: episode: 2696, duration: 1.268s, episode steps: 88, steps per second: 69, episode reward: -129.384, mean reward: -1.470 [-100.000, 17.839], mean action: 1.636 [0.000, 3.000], mean observation: 0.117 [-3.436, 1.000], loss: 2.281599, mean_absolute_error: 28.930255, mean_q: 13.357787, mean_eps: 0.763111
  263351/2000000: episode: 2697, duration: 1.381s, episode steps: 96, steps per second: 70, episode reward: -167.811, mean reward: -1.748 [-100.000, 7.323], mean action: 1.552 [0.000, 3.000], mean observation: -0.084 [-1.266, 3.753], loss: 2.025866, mean_absolute_error: 28.012975, mean_q: 17.686268, mean_eps: 0.763028
  263429/2000000: episode: 2698, duration: 1.155s, episode steps: 78, steps per second: 68, episode reward: -124.253, mean reward: -1.593 [-100.000, 16.634], mean action: 1.641 [0.000, 3.000], mean observation: -0.039 [-1.227, 4.187], loss: 2.034964, mean_absolute_error: 28.158306, mean_q: 15.453485, mean_eps: 0.762949
  263557/2000000: episode: 2699, duration: 1.827s, episode steps: 128, steps per second: 70, episode reward: -226.864, mean reward: -1.772 [-100.000, 63.171], mean action: 1.656 [0.000, 3.000], mean observation: -0.107 [-3.141, 1.000], loss: 1.868102, mean_absolute_error: 27.785062, mean_q: 16.639068, mean_eps: 0.762855
  263685/2000000: episode: 2700, duration: 1.835s, episode steps: 128, steps per second: 70, episode reward: -141.754, mean reward: -1.107 [-100.000, 11.313], mean action: 1.641 [0.000, 3.000], mean observation: -0.045 [-0.922, 3.220], loss: 1.702908, mean_absolute_error: 27.536441, mean_q: 13.947996, mean_eps: 0.762740
  263788/2000000: episode: 2701, duration: 1.541s, episode steps: 103, steps per second: 67, episode reward: -124.020, mean reward: -1.204 [-100.000, 8.833], mean action: 1.650 [0.000, 3.000], mean observation: -0.057 [-1.068, 3.508], loss: 1.618636, mean_absolute_error: 27.364332, mean_q: 16.268601, mean_eps: 0.762638
  263891/2000000: episode: 2702, duration: 1.540s, episode steps: 103, steps per second: 67, episode reward: -133.858, mean reward: -1.300 [-100.000, 18.612], mean action: 1.641 [0.000, 3.000], mean observation: -0.080 [-1.211, 1.000], loss: 1.575857, mean_absolute_error: 26.351656, mean_q: 16.222903, mean_eps: 0.762546
  264001/2000000: episode: 2703, duration: 1.603s, episode steps: 110, steps per second: 69, episode reward: -221.364, mean reward: -2.012 [-100.000, 2.733], mean action: 1.564 [0.000, 3.000], mean observation: 0.230 [-1.193, 1.055], loss: 1.461703, mean_absolute_error: 26.651031, mean_q: 14.831177, mean_eps: 0.762449
  264081/2000000: episode: 2704, duration: 1.170s, episode steps: 80, steps per second: 68, episode reward: -113.933, mean reward: -1.424 [-100.000, 7.389], mean action: 1.663 [0.000, 3.000], mean observation: -0.119 [-3.422, 1.000], loss: 1.782893, mean_absolute_error: 27.069288, mean_q: 15.836220, mean_eps: 0.762362
  264183/2000000: episode: 2705, duration: 1.441s, episode steps: 102, steps per second: 71, episode reward: -166.995, mean reward: -1.637 [-100.000, 19.138], mean action: 1.637 [0.000, 3.000], mean observation: 0.139 [-1.198, 1.000], loss: 2.174753, mean_absolute_error: 26.703653, mean_q: 14.936689, mean_eps: 0.762281
  264269/2000000: episode: 2706, duration: 1.244s, episode steps: 86, steps per second: 69, episode reward: -109.793, mean reward: -1.277 [-100.000, 19.455], mean action: 1.674 [0.000, 3.000], mean observation: -0.118 [-3.870, 1.000], loss: 1.455170, mean_absolute_error: 27.215136, mean_q: 16.158570, mean_eps: 0.762197
  264383/2000000: episode: 2707, duration: 1.585s, episode steps: 114, steps per second: 72, episode reward: -131.791, mean reward: -1.156 [-100.000, 6.005], mean action: 1.719 [0.000, 3.000], mean observation: 0.099 [-2.644, 1.000], loss: 1.687625, mean_absolute_error: 27.627735, mean_q: 15.687633, mean_eps: 0.762107
  264454/2000000: episode: 2708, duration: 1.026s, episode steps: 71, steps per second: 69, episode reward: -107.111, mean reward: -1.509 [-100.000, 5.760], mean action: 1.282 [0.000, 3.000], mean observation: -0.025 [-4.170, 1.000], loss: 1.527586, mean_absolute_error: 26.786975, mean_q: 15.060765, mean_eps: 0.762024
  264566/2000000: episode: 2709, duration: 1.595s, episode steps: 112, steps per second: 70, episode reward: -86.932, mean reward: -0.776 [-100.000, 17.229], mean action: 1.714 [0.000, 3.000], mean observation: 0.116 [-2.728, 1.000], loss: 1.885243, mean_absolute_error: 27.847948, mean_q: 17.179186, mean_eps: 0.761941
  264706/2000000: episode: 2710, duration: 2.005s, episode steps: 140, steps per second: 70, episode reward: -75.103, mean reward: -0.536 [-100.000, 16.303], mean action: 1.586 [0.000, 3.000], mean observation: 0.090 [-0.823, 1.878], loss: 2.129109, mean_absolute_error: 28.689965, mean_q: 11.288637, mean_eps: 0.761828
  264795/2000000: episode: 2711, duration: 1.254s, episode steps: 89, steps per second: 71, episode reward: -130.750, mean reward: -1.469 [-100.000, 12.942], mean action: 1.652 [0.000, 3.000], mean observation: -0.052 [-1.232, 1.000], loss: 1.339672, mean_absolute_error: 27.771581, mean_q: 16.879480, mean_eps: 0.761725
  264890/2000000: episode: 2712, duration: 1.403s, episode steps: 95, steps per second: 68, episode reward: -93.411, mean reward: -0.983 [-100.000, 16.039], mean action: 1.695 [0.000, 3.000], mean observation: -0.077 [-1.026, 1.000], loss: 2.151721, mean_absolute_error: 26.004184, mean_q: 17.070887, mean_eps: 0.761642
  264957/2000000: episode: 2713, duration: 0.967s, episode steps: 67, steps per second: 69, episode reward: -109.519, mean reward: -1.635 [-100.000, 8.554], mean action: 1.552 [0.000, 3.000], mean observation: 0.091 [-1.318, 4.747], loss: 1.760257, mean_absolute_error: 27.607038, mean_q: 16.411951, mean_eps: 0.761568
  265061/2000000: episode: 2714, duration: 1.510s, episode steps: 104, steps per second: 69, episode reward: -141.427, mean reward: -1.360 [-100.000, 18.865], mean action: 1.567 [0.000, 3.000], mean observation: 0.157 [-2.017, 1.072], loss: 2.897761, mean_absolute_error: 28.522256, mean_q: 14.903715, mean_eps: 0.761491
  265183/2000000: episode: 2715, duration: 1.734s, episode steps: 122, steps per second: 70, episode reward: -125.347, mean reward: -1.027 [-100.000, 11.310], mean action: 1.434 [0.000, 3.000], mean observation: -0.004 [-1.151, 1.000], loss: 1.554531, mean_absolute_error: 26.110640, mean_q: 15.713044, mean_eps: 0.761390
  265263/2000000: episode: 2716, duration: 1.150s, episode steps: 80, steps per second: 70, episode reward: -123.501, mean reward: -1.544 [-100.000, 7.039], mean action: 1.663 [0.000, 3.000], mean observation: -0.109 [-6.095, 1.000], loss: 1.014764, mean_absolute_error: 26.762323, mean_q: 14.297897, mean_eps: 0.761300
  265353/2000000: episode: 2717, duration: 1.308s, episode steps: 90, steps per second: 69, episode reward: -114.387, mean reward: -1.271 [-100.000, 7.427], mean action: 1.544 [0.000, 3.000], mean observation: 0.059 [-3.283, 1.000], loss: 1.169194, mean_absolute_error: 26.184223, mean_q: 17.435374, mean_eps: 0.761223
  265472/2000000: episode: 2718, duration: 1.744s, episode steps: 119, steps per second: 68, episode reward: -125.480, mean reward: -1.054 [-100.000, 13.377], mean action: 1.529 [0.000, 3.000], mean observation: 0.051 [-1.175, 3.194], loss: 1.701156, mean_absolute_error: 27.320120, mean_q: 17.823338, mean_eps: 0.761129
  265579/2000000: episode: 2719, duration: 1.559s, episode steps: 107, steps per second: 69, episode reward: -89.888, mean reward: -0.840 [-100.000, 13.803], mean action: 1.561 [0.000, 3.000], mean observation: -0.031 [-2.836, 1.000], loss: 1.430610, mean_absolute_error: 26.717716, mean_q: 16.082978, mean_eps: 0.761028
  265661/2000000: episode: 2720, duration: 1.209s, episode steps: 82, steps per second: 68, episode reward: -77.449, mean reward: -0.945 [-100.000, 21.256], mean action: 1.732 [0.000, 3.000], mean observation: -0.103 [-1.064, 3.922], loss: 1.500283, mean_absolute_error: 25.902728, mean_q: 16.923878, mean_eps: 0.760942
  265750/2000000: episode: 2721, duration: 1.276s, episode steps: 89, steps per second: 70, episode reward: -29.772, mean reward: -0.335 [-100.000, 21.802], mean action: 1.663 [0.000, 3.000], mean observation: -0.049 [-0.895, 1.000], loss: 1.662024, mean_absolute_error: 26.758761, mean_q: 15.628216, mean_eps: 0.760865
  265870/2000000: episode: 2722, duration: 1.713s, episode steps: 120, steps per second: 70, episode reward: -121.186, mean reward: -1.010 [-100.000, 6.646], mean action: 1.517 [0.000, 3.000], mean observation: -0.006 [-1.172, 4.123], loss: 1.394059, mean_absolute_error: 26.105665, mean_q: 17.346787, mean_eps: 0.760771
  265977/2000000: episode: 2723, duration: 1.569s, episode steps: 107, steps per second: 68, episode reward: -105.426, mean reward: -0.985 [-100.000, 10.765], mean action: 1.645 [0.000, 3.000], mean observation: 0.018 [-3.731, 1.000], loss: 1.811711, mean_absolute_error: 27.154168, mean_q: 17.299567, mean_eps: 0.760668
  266121/2000000: episode: 2724, duration: 2.056s, episode steps: 144, steps per second: 70, episode reward: -203.521, mean reward: -1.413 [-100.000, 6.479], mean action: 1.674 [0.000, 3.000], mean observation: -0.099 [-1.294, 1.027], loss: 1.728949, mean_absolute_error: 26.723153, mean_q: 14.193924, mean_eps: 0.760555
  266226/2000000: episode: 2725, duration: 1.489s, episode steps: 105, steps per second: 71, episode reward: -65.266, mean reward: -0.622 [-100.000, 16.851], mean action: 1.771 [0.000, 3.000], mean observation: 0.002 [-0.903, 1.000], loss: 1.612382, mean_absolute_error: 27.662242, mean_q: 14.178623, mean_eps: 0.760443
  266325/2000000: episode: 2726, duration: 1.414s, episode steps: 99, steps per second: 70, episode reward: -67.157, mean reward: -0.678 [-100.000, 9.963], mean action: 1.626 [0.000, 3.000], mean observation: -0.017 [-1.004, 3.007], loss: 1.949303, mean_absolute_error: 26.651246, mean_q: 15.398760, mean_eps: 0.760352
  266446/2000000: episode: 2727, duration: 1.705s, episode steps: 121, steps per second: 71, episode reward: -94.416, mean reward: -0.780 [-100.000, 8.597], mean action: 1.603 [0.000, 3.000], mean observation: -0.001 [-3.497, 1.000], loss: 1.917888, mean_absolute_error: 27.001953, mean_q: 19.032989, mean_eps: 0.760253
  266557/2000000: episode: 2728, duration: 1.611s, episode steps: 111, steps per second: 69, episode reward: -122.776, mean reward: -1.106 [-100.000, 12.533], mean action: 1.387 [0.000, 3.000], mean observation: 0.062 [-2.936, 1.000], loss: 1.421343, mean_absolute_error: 26.938998, mean_q: 16.273941, mean_eps: 0.760148
  266628/2000000: episode: 2729, duration: 1.008s, episode steps: 71, steps per second: 70, episode reward: -138.088, mean reward: -1.945 [-100.000, 10.313], mean action: 1.704 [0.000, 3.000], mean observation: -0.108 [-1.295, 1.000], loss: 1.676241, mean_absolute_error: 25.676664, mean_q: 19.334661, mean_eps: 0.760067
  266737/2000000: episode: 2730, duration: 1.581s, episode steps: 109, steps per second: 69, episode reward: -140.856, mean reward: -1.292 [-100.000, 6.287], mean action: 1.771 [0.000, 3.000], mean observation: -0.013 [-3.978, 1.000], loss: 1.831784, mean_absolute_error: 27.696502, mean_q: 15.625669, mean_eps: 0.759986
  266844/2000000: episode: 2731, duration: 1.561s, episode steps: 107, steps per second: 69, episode reward: -107.147, mean reward: -1.001 [-100.000, 19.400], mean action: 1.654 [0.000, 3.000], mean observation: -0.042 [-1.099, 1.000], loss: 1.583438, mean_absolute_error: 27.285342, mean_q: 16.972906, mean_eps: 0.759889
  266955/2000000: episode: 2732, duration: 1.593s, episode steps: 111, steps per second: 70, episode reward: -142.910, mean reward: -1.287 [-100.000, 7.979], mean action: 1.568 [0.000, 3.000], mean observation: -0.095 [-3.516, 1.000], loss: 2.058228, mean_absolute_error: 27.209593, mean_q: 15.892866, mean_eps: 0.759792
  267043/2000000: episode: 2733, duration: 1.268s, episode steps: 88, steps per second: 69, episode reward: -98.864, mean reward: -1.123 [-100.000, 7.522], mean action: 1.636 [0.000, 3.000], mean observation: 0.055 [-1.005, 3.418], loss: 1.252617, mean_absolute_error: 26.961200, mean_q: 17.328980, mean_eps: 0.759702
  267110/2000000: episode: 2734, duration: 0.973s, episode steps: 67, steps per second: 69, episode reward: -94.235, mean reward: -1.406 [-100.000, 6.904], mean action: 1.537 [0.000, 3.000], mean observation: 0.052 [-1.082, 3.810], loss: 1.036677, mean_absolute_error: 25.993360, mean_q: 18.156978, mean_eps: 0.759632
  267170/2000000: episode: 2735, duration: 0.846s, episode steps: 60, steps per second: 71, episode reward: -118.626, mean reward: -1.977 [-100.000, 19.423], mean action: 1.367 [0.000, 3.000], mean observation: 0.069 [-1.415, 1.000], loss: 1.133189, mean_absolute_error: 25.060714, mean_q: 19.958585, mean_eps: 0.759574
  267279/2000000: episode: 2736, duration: 1.544s, episode steps: 109, steps per second: 71, episode reward: -88.265, mean reward: -0.810 [-100.000, 7.479], mean action: 1.514 [0.000, 3.000], mean observation: 0.088 [-0.974, 3.214], loss: 1.911632, mean_absolute_error: 28.785291, mean_q: 14.520764, mean_eps: 0.759498
  267362/2000000: episode: 2737, duration: 1.191s, episode steps: 83, steps per second: 70, episode reward: -97.396, mean reward: -1.173 [-100.000, 13.738], mean action: 1.458 [0.000, 3.000], mean observation: 0.029 [-3.663, 1.000], loss: 1.025763, mean_absolute_error: 25.522135, mean_q: 18.131724, mean_eps: 0.759412
  267449/2000000: episode: 2738, duration: 1.241s, episode steps: 87, steps per second: 70, episode reward: -172.296, mean reward: -1.980 [-100.000, 10.028], mean action: 1.379 [0.000, 3.000], mean observation: 0.127 [-2.517, 1.000], loss: 1.431075, mean_absolute_error: 26.864937, mean_q: 18.527484, mean_eps: 0.759335
  267545/2000000: episode: 2739, duration: 1.394s, episode steps: 96, steps per second: 69, episode reward: -138.705, mean reward: -1.445 [-100.000, 6.283], mean action: 1.531 [0.000, 3.000], mean observation: 0.032 [-4.043, 1.000], loss: 1.124988, mean_absolute_error: 26.727232, mean_q: 18.770425, mean_eps: 0.759252
  267623/2000000: episode: 2740, duration: 1.084s, episode steps: 78, steps per second: 72, episode reward: -120.459, mean reward: -1.544 [-100.000, 15.739], mean action: 1.679 [0.000, 3.000], mean observation: -0.173 [-4.276, 1.000], loss: 2.288408, mean_absolute_error: 27.811207, mean_q: 14.635547, mean_eps: 0.759174
  267747/2000000: episode: 2741, duration: 1.776s, episode steps: 124, steps per second: 70, episode reward: -127.888, mean reward: -1.031 [-100.000, 19.713], mean action: 1.621 [0.000, 3.000], mean observation: 0.048 [-1.265, 1.000], loss: 1.409078, mean_absolute_error: 26.949366, mean_q: 16.167782, mean_eps: 0.759084
  267866/2000000: episode: 2742, duration: 1.691s, episode steps: 119, steps per second: 70, episode reward: -174.856, mean reward: -1.469 [-100.000, 6.273], mean action: 1.681 [0.000, 3.000], mean observation: -0.017 [-1.396, 4.216], loss: 1.978314, mean_absolute_error: 27.320561, mean_q: 18.030994, mean_eps: 0.758975
  267972/2000000: episode: 2743, duration: 1.542s, episode steps: 106, steps per second: 69, episode reward: -131.401, mean reward: -1.240 [-100.000, 7.316], mean action: 1.623 [0.000, 3.000], mean observation: -0.055 [-3.366, 1.000], loss: 1.655606, mean_absolute_error: 26.303289, mean_q: 16.766939, mean_eps: 0.758874
  268071/2000000: episode: 2744, duration: 1.402s, episode steps: 99, steps per second: 71, episode reward: -113.710, mean reward: -1.149 [-100.000, 7.284], mean action: 1.515 [0.000, 3.000], mean observation: 0.054 [-3.628, 1.000], loss: 1.459579, mean_absolute_error: 26.318711, mean_q: 17.060417, mean_eps: 0.758782
  268207/2000000: episode: 2745, duration: 1.936s, episode steps: 136, steps per second: 70, episode reward: -117.909, mean reward: -0.867 [-100.000, 13.179], mean action: 1.581 [0.000, 3.000], mean observation: -0.032 [-0.916, 3.436], loss: 1.543139, mean_absolute_error: 25.859128, mean_q: 19.284363, mean_eps: 0.758676
  268340/2000000: episode: 2746, duration: 1.923s, episode steps: 133, steps per second: 69, episode reward: -151.663, mean reward: -1.140 [-100.000, 10.215], mean action: 1.534 [0.000, 3.000], mean observation: 0.031 [-3.730, 1.072], loss: 1.408896, mean_absolute_error: 26.454395, mean_q: 18.654600, mean_eps: 0.758555
  268436/2000000: episode: 2747, duration: 1.405s, episode steps: 96, steps per second: 68, episode reward: -175.496, mean reward: -1.828 [-100.000, 21.544], mean action: 1.500 [0.000, 3.000], mean observation: 0.114 [-1.336, 1.000], loss: 1.949771, mean_absolute_error: 27.652071, mean_q: 17.732154, mean_eps: 0.758453
  268578/2000000: episode: 2748, duration: 2.029s, episode steps: 142, steps per second: 70, episode reward: -159.712, mean reward: -1.125 [-100.000, 12.601], mean action: 1.655 [0.000, 3.000], mean observation: -0.102 [-1.324, 4.036], loss: 1.578432, mean_absolute_error: 26.765623, mean_q: 17.048848, mean_eps: 0.758345
  268687/2000000: episode: 2749, duration: 1.546s, episode steps: 109, steps per second: 71, episode reward: -133.888, mean reward: -1.228 [-100.000, 7.690], mean action: 1.807 [0.000, 3.000], mean observation: -0.085 [-1.040, 3.658], loss: 1.476484, mean_absolute_error: 27.125876, mean_q: 15.411691, mean_eps: 0.758231
  268767/2000000: episode: 2750, duration: 1.114s, episode steps: 80, steps per second: 72, episode reward: -94.790, mean reward: -1.185 [-100.000, 12.538], mean action: 1.550 [0.000, 3.000], mean observation: 0.018 [-1.221, 1.000], loss: 1.806576, mean_absolute_error: 26.702160, mean_q: 17.836935, mean_eps: 0.758147
  268835/2000000: episode: 2751, duration: 0.966s, episode steps: 68, steps per second: 70, episode reward: -108.473, mean reward: -1.595 [-100.000, 12.461], mean action: 1.676 [0.000, 3.000], mean observation: -0.115 [-1.339, 4.061], loss: 1.012536, mean_absolute_error: 27.193717, mean_q: 16.925110, mean_eps: 0.758080
  268899/2000000: episode: 2752, duration: 0.922s, episode steps: 64, steps per second: 69, episode reward: -121.540, mean reward: -1.899 [-100.000, 12.847], mean action: 1.547 [0.000, 3.000], mean observation: -0.145 [-5.185, 1.000], loss: 2.613736, mean_absolute_error: 30.394179, mean_q: 12.636747, mean_eps: 0.758021
  269088/2000000: episode: 2753, duration: 2.697s, episode steps: 189, steps per second: 70, episode reward: -65.217, mean reward: -0.345 [-100.000, 13.517], mean action: 1.709 [0.000, 3.000], mean observation: 0.008 [-0.780, 1.000], loss: 1.328327, mean_absolute_error: 26.586348, mean_q: 16.411559, mean_eps: 0.757907
  269165/2000000: episode: 2754, duration: 1.130s, episode steps: 77, steps per second: 68, episode reward: -109.521, mean reward: -1.422 [-100.000, 9.703], mean action: 1.740 [0.000, 3.000], mean observation: -0.153 [-2.094, 1.000], loss: 1.520778, mean_absolute_error: 26.465281, mean_q: 16.992995, mean_eps: 0.757787
  269318/2000000: episode: 2755, duration: 2.278s, episode steps: 153, steps per second: 67, episode reward: -314.561, mean reward: -2.056 [-100.000, 42.816], mean action: 1.458 [0.000, 3.000], mean observation: 0.228 [-0.955, 2.382], loss: 1.520763, mean_absolute_error: 26.742660, mean_q: 17.476930, mean_eps: 0.757682
  269436/2000000: episode: 2756, duration: 1.719s, episode steps: 118, steps per second: 69, episode reward: -146.862, mean reward: -1.245 [-100.000, 6.428], mean action: 1.500 [0.000, 3.000], mean observation: 0.030 [-1.398, 4.860], loss: 1.891932, mean_absolute_error: 27.127473, mean_q: 16.799230, mean_eps: 0.757562
  269505/2000000: episode: 2757, duration: 1.024s, episode steps: 69, steps per second: 67, episode reward: -123.245, mean reward: -1.786 [-100.000, 9.301], mean action: 1.609 [0.000, 3.000], mean observation: -0.169 [-4.513, 1.000], loss: 1.605796, mean_absolute_error: 28.200807, mean_q: 14.195952, mean_eps: 0.757477
  269587/2000000: episode: 2758, duration: 1.137s, episode steps: 82, steps per second: 72, episode reward: -63.527, mean reward: -0.775 [-100.000, 13.137], mean action: 1.610 [0.000, 3.000], mean observation: 0.016 [-0.981, 3.223], loss: 1.809817, mean_absolute_error: 27.015606, mean_q: 16.128482, mean_eps: 0.757409
  269691/2000000: episode: 2759, duration: 1.496s, episode steps: 104, steps per second: 70, episode reward: -316.443, mean reward: -3.043 [-100.000, 81.298], mean action: 1.721 [0.000, 3.000], mean observation: -0.198 [-3.339, 1.000], loss: 1.994392, mean_absolute_error: 27.570245, mean_q: 13.846429, mean_eps: 0.757326
  269793/2000000: episode: 2760, duration: 1.478s, episode steps: 102, steps per second: 69, episode reward: -137.197, mean reward: -1.345 [-100.000, 6.594], mean action: 1.431 [0.000, 3.000], mean observation: 0.127 [-2.785, 1.000], loss: 1.538649, mean_absolute_error: 26.268185, mean_q: 16.793851, mean_eps: 0.757232
  269911/2000000: episode: 2761, duration: 1.649s, episode steps: 118, steps per second: 72, episode reward: -94.655, mean reward: -0.802 [-100.000, 12.233], mean action: 1.576 [0.000, 3.000], mean observation: 0.083 [-2.651, 1.000], loss: 1.672800, mean_absolute_error: 27.438142, mean_q: 14.838595, mean_eps: 0.757133
  270018/2000000: episode: 2762, duration: 1.521s, episode steps: 107, steps per second: 70, episode reward: -172.531, mean reward: -1.612 [-100.000, 11.339], mean action: 1.607 [0.000, 3.000], mean observation: 0.042 [-4.417, 1.000], loss: 1.898219, mean_absolute_error: 26.748535, mean_q: 16.769520, mean_eps: 0.757032
  270097/2000000: episode: 2763, duration: 1.128s, episode steps: 79, steps per second: 70, episode reward: -88.751, mean reward: -1.123 [-100.000, 8.520], mean action: 1.633 [0.000, 3.000], mean observation: 0.120 [-0.965, 3.149], loss: 1.475042, mean_absolute_error: 28.705462, mean_q: 18.358787, mean_eps: 0.756948
  270176/2000000: episode: 2764, duration: 1.130s, episode steps: 79, steps per second: 70, episode reward: -120.293, mean reward: -1.523 [-100.000, 6.976], mean action: 1.747 [0.000, 3.000], mean observation: -0.087 [-1.172, 3.820], loss: 2.203450, mean_absolute_error: 28.757014, mean_q: 15.551404, mean_eps: 0.756878
  270255/2000000: episode: 2765, duration: 1.129s, episode steps: 79, steps per second: 70, episode reward: -136.495, mean reward: -1.728 [-100.000, 14.640], mean action: 1.608 [0.000, 3.000], mean observation: 0.158 [-1.131, 4.050], loss: 1.207211, mean_absolute_error: 26.925088, mean_q: 18.490750, mean_eps: 0.756807
  270343/2000000: episode: 2766, duration: 1.246s, episode steps: 88, steps per second: 71, episode reward: -96.283, mean reward: -1.094 [-100.000, 16.201], mean action: 1.364 [0.000, 3.000], mean observation: 0.018 [-1.136, 1.000], loss: 2.207786, mean_absolute_error: 27.663207, mean_q: 16.898600, mean_eps: 0.756732
  270445/2000000: episode: 2767, duration: 1.456s, episode steps: 102, steps per second: 70, episode reward: -118.644, mean reward: -1.163 [-100.000, 12.427], mean action: 1.716 [0.000, 3.000], mean observation: 0.011 [-1.291, 1.000], loss: 1.712099, mean_absolute_error: 28.306670, mean_q: 16.449684, mean_eps: 0.756645
  270548/2000000: episode: 2768, duration: 1.484s, episode steps: 103, steps per second: 69, episode reward: -87.955, mean reward: -0.854 [-100.000, 19.732], mean action: 1.515 [0.000, 3.000], mean observation: 0.056 [-1.146, 1.000], loss: 2.073882, mean_absolute_error: 29.413648, mean_q: 14.695541, mean_eps: 0.756554
  270639/2000000: episode: 2769, duration: 1.315s, episode steps: 91, steps per second: 69, episode reward: -114.287, mean reward: -1.256 [-100.000, 15.533], mean action: 1.604 [0.000, 3.000], mean observation: -0.019 [-1.261, 4.079], loss: 1.597777, mean_absolute_error: 28.333798, mean_q: 17.315676, mean_eps: 0.756467
  270772/2000000: episode: 2770, duration: 1.955s, episode steps: 133, steps per second: 68, episode reward: -372.230, mean reward: -2.799 [-100.000, 3.723], mean action: 1.759 [0.000, 3.000], mean observation: -0.217 [-1.975, 1.014], loss: 1.939495, mean_absolute_error: 29.213408, mean_q: 16.941101, mean_eps: 0.756366
  270834/2000000: episode: 2771, duration: 0.900s, episode steps: 62, steps per second: 69, episode reward: -123.056, mean reward: -1.985 [-100.000, 6.879], mean action: 1.581 [0.000, 3.000], mean observation: -0.008 [-4.511, 1.000], loss: 1.821186, mean_absolute_error: 27.696681, mean_q: 18.719549, mean_eps: 0.756278
  270929/2000000: episode: 2772, duration: 1.374s, episode steps: 95, steps per second: 69, episode reward: -144.188, mean reward: -1.518 [-100.000, 15.807], mean action: 1.621 [0.000, 3.000], mean observation: -0.081 [-1.237, 3.266], loss: 2.217309, mean_absolute_error: 29.369855, mean_q: 14.790461, mean_eps: 0.756206
  271000/2000000: episode: 2773, duration: 1.043s, episode steps: 71, steps per second: 68, episode reward: -104.259, mean reward: -1.468 [-100.000, 6.410], mean action: 1.634 [0.000, 3.000], mean observation: -0.097 [-1.302, 3.608], loss: 1.455342, mean_absolute_error: 27.613722, mean_q: 21.210646, mean_eps: 0.756132
  271092/2000000: episode: 2774, duration: 1.358s, episode steps: 92, steps per second: 68, episode reward: -203.920, mean reward: -2.217 [-100.000, 13.098], mean action: 1.489 [0.000, 3.000], mean observation: -0.050 [-5.095, 1.000], loss: 2.371559, mean_absolute_error: 29.680783, mean_q: 16.491847, mean_eps: 0.756060
  271164/2000000: episode: 2775, duration: 1.078s, episode steps: 72, steps per second: 67, episode reward: -132.393, mean reward: -1.839 [-100.000, 10.196], mean action: 1.889 [0.000, 3.000], mean observation: -0.111 [-1.346, 4.543], loss: 2.376359, mean_absolute_error: 30.353488, mean_q: 13.598238, mean_eps: 0.755987
  271274/2000000: episode: 2776, duration: 1.579s, episode steps: 110, steps per second: 70, episode reward: -234.268, mean reward: -2.130 [-100.000, 7.170], mean action: 1.455 [0.000, 3.000], mean observation: 0.178 [-2.950, 1.050], loss: 1.452903, mean_absolute_error: 27.573650, mean_q: 16.555232, mean_eps: 0.755904
  271383/2000000: episode: 2777, duration: 1.548s, episode steps: 109, steps per second: 70, episode reward: -134.392, mean reward: -1.233 [-100.000, 13.899], mean action: 1.651 [0.000, 3.000], mean observation: 0.029 [-1.313, 1.000], loss: 2.145202, mean_absolute_error: 29.177843, mean_q: 15.572918, mean_eps: 0.755805
  271472/2000000: episode: 2778, duration: 1.294s, episode steps: 89, steps per second: 69, episode reward: -164.724, mean reward: -1.851 [-100.000, 9.649], mean action: 1.427 [0.000, 3.000], mean observation: 0.136 [-3.314, 1.000], loss: 1.707281, mean_absolute_error: 28.397622, mean_q: 16.425949, mean_eps: 0.755717
  271548/2000000: episode: 2779, duration: 1.132s, episode steps: 76, steps per second: 67, episode reward: -117.640, mean reward: -1.548 [-100.000, 7.026], mean action: 1.645 [0.000, 3.000], mean observation: -0.010 [-1.344, 4.468], loss: 1.974249, mean_absolute_error: 28.791604, mean_q: 14.918008, mean_eps: 0.755643
  271688/2000000: episode: 2780, duration: 2.040s, episode steps: 140, steps per second: 69, episode reward: -120.206, mean reward: -0.859 [-100.000, 9.834], mean action: 1.529 [0.000, 3.000], mean observation: 0.156 [-0.962, 3.511], loss: 2.136679, mean_absolute_error: 28.230312, mean_q: 17.720329, mean_eps: 0.755546
  271852/2000000: episode: 2781, duration: 2.421s, episode steps: 164, steps per second: 68, episode reward: -108.591, mean reward: -0.662 [-100.000, 33.285], mean action: 1.537 [0.000, 3.000], mean observation: 0.183 [-1.031, 1.314], loss: 1.575016, mean_absolute_error: 26.615265, mean_q: 17.701780, mean_eps: 0.755409
  271985/2000000: episode: 2782, duration: 1.930s, episode steps: 133, steps per second: 69, episode reward: -127.727, mean reward: -0.960 [-100.000, 17.993], mean action: 1.617 [0.000, 3.000], mean observation: 0.084 [-1.172, 1.009], loss: 2.053208, mean_absolute_error: 28.169183, mean_q: 16.655842, mean_eps: 0.755274
  272072/2000000: episode: 2783, duration: 1.270s, episode steps: 87, steps per second: 69, episode reward: -105.407, mean reward: -1.212 [-100.000, 9.036], mean action: 1.632 [0.000, 3.000], mean observation: -0.061 [-1.082, 3.475], loss: 1.554685, mean_absolute_error: 27.667553, mean_q: 16.649415, mean_eps: 0.755175
  272202/2000000: episode: 2784, duration: 1.844s, episode steps: 130, steps per second: 71, episode reward: -111.215, mean reward: -0.856 [-100.000, 13.158], mean action: 1.623 [0.000, 3.000], mean observation: -0.095 [-2.104, 1.053], loss: 1.302420, mean_absolute_error: 29.419693, mean_q: 15.888149, mean_eps: 0.755078
  272340/2000000: episode: 2785, duration: 1.963s, episode steps: 138, steps per second: 70, episode reward: -117.306, mean reward: -0.850 [-100.000, 8.578], mean action: 1.543 [0.000, 3.000], mean observation: -0.066 [-0.858, 2.682], loss: 1.841181, mean_absolute_error: 27.829842, mean_q: 15.197765, mean_eps: 0.754957
  272411/2000000: episode: 2786, duration: 1.026s, episode steps: 71, steps per second: 69, episode reward: -85.022, mean reward: -1.197 [-100.000, 9.344], mean action: 1.817 [0.000, 3.000], mean observation: -0.156 [-4.028, 1.000], loss: 1.332318, mean_absolute_error: 28.134084, mean_q: 19.775948, mean_eps: 0.754863
  272547/2000000: episode: 2787, duration: 1.932s, episode steps: 136, steps per second: 70, episode reward: -65.217, mean reward: -0.480 [-100.000, 14.738], mean action: 1.735 [0.000, 3.000], mean observation: 0.107 [-2.509, 1.000], loss: 1.826379, mean_absolute_error: 28.746663, mean_q: 17.531794, mean_eps: 0.754770
  272628/2000000: episode: 2788, duration: 1.170s, episode steps: 81, steps per second: 69, episode reward: -130.622, mean reward: -1.613 [-100.000, 17.700], mean action: 1.506 [0.000, 3.000], mean observation: 0.148 [-1.209, 4.136], loss: 1.742524, mean_absolute_error: 28.230117, mean_q: 18.065082, mean_eps: 0.754673
  272753/2000000: episode: 2789, duration: 1.819s, episode steps: 125, steps per second: 69, episode reward: -110.898, mean reward: -0.887 [-100.000, 16.866], mean action: 1.416 [0.000, 3.000], mean observation: 0.032 [-1.213, 1.000], loss: 2.243150, mean_absolute_error: 28.110021, mean_q: 16.332665, mean_eps: 0.754579
  272853/2000000: episode: 2790, duration: 1.390s, episode steps: 100, steps per second: 72, episode reward: -129.784, mean reward: -1.298 [-100.000, 11.646], mean action: 1.750 [0.000, 3.000], mean observation: -0.104 [-1.073, 3.423], loss: 1.793668, mean_absolute_error: 27.281600, mean_q: 16.971158, mean_eps: 0.754476
  272977/2000000: episode: 2791, duration: 1.759s, episode steps: 124, steps per second: 70, episode reward: -108.990, mean reward: -0.879 [-100.000, 14.870], mean action: 1.653 [0.000, 3.000], mean observation: 0.067 [-1.323, 1.000], loss: 1.579300, mean_absolute_error: 28.306740, mean_q: 15.246703, mean_eps: 0.754376
  273088/2000000: episode: 2792, duration: 1.596s, episode steps: 111, steps per second: 70, episode reward: -127.670, mean reward: -1.150 [-100.000, 10.784], mean action: 1.468 [0.000, 3.000], mean observation: 0.057 [-1.199, 4.300], loss: 1.208380, mean_absolute_error: 28.225975, mean_q: 16.798746, mean_eps: 0.754271
  273233/2000000: episode: 2793, duration: 2.097s, episode steps: 145, steps per second: 69, episode reward: -343.120, mean reward: -2.366 [-100.000, 96.413], mean action: 1.676 [0.000, 3.000], mean observation: 0.060 [-2.648, 1.030], loss: 1.710357, mean_absolute_error: 27.612326, mean_q: 17.630978, mean_eps: 0.754156
  273317/2000000: episode: 2794, duration: 1.200s, episode steps: 84, steps per second: 70, episode reward: -105.139, mean reward: -1.252 [-100.000, 9.487], mean action: 1.667 [0.000, 3.000], mean observation: -0.016 [-1.233, 4.279], loss: 1.523789, mean_absolute_error: 28.490071, mean_q: 18.429837, mean_eps: 0.754052
  273401/2000000: episode: 2795, duration: 1.181s, episode steps: 84, steps per second: 71, episode reward: -107.290, mean reward: -1.277 [-100.000, 11.233], mean action: 1.524 [0.000, 3.000], mean observation: 0.062 [-1.127, 4.247], loss: 0.976633, mean_absolute_error: 27.693458, mean_q: 18.267359, mean_eps: 0.753976
  273523/2000000: episode: 2796, duration: 1.692s, episode steps: 122, steps per second: 72, episode reward: -116.120, mean reward: -0.952 [-100.000, 17.604], mean action: 1.590 [0.000, 3.000], mean observation: 0.187 [-0.988, 1.000], loss: 2.278133, mean_absolute_error: 29.149987, mean_q: 14.734380, mean_eps: 0.753884
  273646/2000000: episode: 2797, duration: 1.753s, episode steps: 123, steps per second: 70, episode reward: -114.948, mean reward: -0.935 [-100.000, 9.847], mean action: 1.585 [0.000, 3.000], mean observation: -0.012 [-1.241, 4.344], loss: 1.511402, mean_absolute_error: 27.798331, mean_q: 16.556294, mean_eps: 0.753774
  273748/2000000: episode: 2798, duration: 1.494s, episode steps: 102, steps per second: 68, episode reward: -115.998, mean reward: -1.137 [-100.000, 6.986], mean action: 1.520 [0.000, 3.000], mean observation: -0.049 [-1.138, 2.503], loss: 1.777792, mean_absolute_error: 30.101499, mean_q: 14.507390, mean_eps: 0.753674
  273843/2000000: episode: 2799, duration: 1.377s, episode steps: 95, steps per second: 69, episode reward: -134.001, mean reward: -1.411 [-100.000, 20.229], mean action: 1.568 [0.000, 3.000], mean observation: 0.031 [-1.240, 1.000], loss: 1.627088, mean_absolute_error: 28.786022, mean_q: 16.400495, mean_eps: 0.753585
  273965/2000000: episode: 2800, duration: 1.847s, episode steps: 122, steps per second: 66, episode reward: -103.869, mean reward: -0.851 [-100.000, 14.831], mean action: 1.648 [0.000, 3.000], mean observation: -0.007 [-1.126, 3.479], loss: 1.240089, mean_absolute_error: 27.686403, mean_q: 17.120427, mean_eps: 0.753486
  274104/2000000: episode: 2801, duration: 1.994s, episode steps: 139, steps per second: 70, episode reward: -115.858, mean reward: -0.834 [-100.000, 7.143], mean action: 1.525 [0.000, 3.000], mean observation: 0.072 [-3.265, 1.000], loss: 2.077423, mean_absolute_error: 29.030769, mean_q: 16.572152, mean_eps: 0.753369
  274221/2000000: episode: 2802, duration: 1.686s, episode steps: 117, steps per second: 69, episode reward: -321.575, mean reward: -2.749 [-100.000, 86.058], mean action: 1.624 [0.000, 3.000], mean observation: -0.217 [-2.824, 1.000], loss: 1.370225, mean_absolute_error: 28.284934, mean_q: 18.503373, mean_eps: 0.753254
  274301/2000000: episode: 2803, duration: 1.145s, episode steps: 80, steps per second: 70, episode reward: -129.527, mean reward: -1.619 [-100.000, 10.004], mean action: 1.625 [0.000, 3.000], mean observation: 0.089 [-3.764, 1.000], loss: 1.275322, mean_absolute_error: 26.965383, mean_q: 20.275287, mean_eps: 0.753164
  274435/2000000: episode: 2804, duration: 1.888s, episode steps: 134, steps per second: 71, episode reward: -156.864, mean reward: -1.171 [-100.000, 13.044], mean action: 1.590 [0.000, 3.000], mean observation: -0.031 [-1.834, 1.000], loss: 2.395051, mean_absolute_error: 28.427958, mean_q: 16.520275, mean_eps: 0.753069
  274576/2000000: episode: 2805, duration: 2.050s, episode steps: 141, steps per second: 69, episode reward: -88.900, mean reward: -0.630 [-100.000, 17.566], mean action: 1.624 [0.000, 3.000], mean observation: 0.034 [-0.886, 1.000], loss: 1.850492, mean_absolute_error: 28.568760, mean_q: 16.527402, mean_eps: 0.752946
  274664/2000000: episode: 2806, duration: 1.310s, episode steps: 88, steps per second: 67, episode reward: -116.352, mean reward: -1.322 [-100.000, 17.126], mean action: 1.477 [0.000, 3.000], mean observation: -0.012 [-4.301, 1.000], loss: 2.566594, mean_absolute_error: 29.447488, mean_q: 14.062562, mean_eps: 0.752844
  274738/2000000: episode: 2807, duration: 1.089s, episode steps: 74, steps per second: 68, episode reward: -80.440, mean reward: -1.087 [-100.000, 11.290], mean action: 1.514 [0.000, 3.000], mean observation: -0.061 [-1.264, 1.000], loss: 1.879406, mean_absolute_error: 28.660926, mean_q: 15.675868, mean_eps: 0.752770
  274835/2000000: episode: 2808, duration: 1.370s, episode steps: 97, steps per second: 71, episode reward: -202.778, mean reward: -2.090 [-100.000, 6.993], mean action: 1.464 [0.000, 3.000], mean observation: 0.105 [-1.567, 1.000], loss: 1.547721, mean_absolute_error: 28.391909, mean_q: 18.903159, mean_eps: 0.752693
  274950/2000000: episode: 2809, duration: 1.633s, episode steps: 115, steps per second: 70, episode reward: -104.298, mean reward: -0.907 [-100.000, 12.352], mean action: 1.800 [0.000, 3.000], mean observation: 0.005 [-0.939, 1.000], loss: 2.507996, mean_absolute_error: 28.898542, mean_q: 16.365519, mean_eps: 0.752597
  275043/2000000: episode: 2810, duration: 1.333s, episode steps: 93, steps per second: 70, episode reward: -178.777, mean reward: -1.922 [-100.000, 14.256], mean action: 1.688 [0.000, 3.000], mean observation: 0.120 [-3.135, 1.000], loss: 1.205829, mean_absolute_error: 27.106887, mean_q: 18.484934, mean_eps: 0.752504
  275120/2000000: episode: 2811, duration: 1.142s, episode steps: 77, steps per second: 67, episode reward: -195.784, mean reward: -2.543 [-100.000, 16.542], mean action: 1.468 [0.000, 3.000], mean observation: -0.155 [-4.593, 1.000], loss: 1.654486, mean_absolute_error: 27.636771, mean_q: 14.804866, mean_eps: 0.752428
  275208/2000000: episode: 2812, duration: 1.289s, episode steps: 88, steps per second: 68, episode reward: -89.371, mean reward: -1.016 [-100.000, 15.365], mean action: 1.602 [0.000, 3.000], mean observation: 0.072 [-0.964, 3.053], loss: 1.265060, mean_absolute_error: 28.063165, mean_q: 19.735326, mean_eps: 0.752354
  275290/2000000: episode: 2813, duration: 1.193s, episode steps: 82, steps per second: 69, episode reward: -97.542, mean reward: -1.190 [-100.000, 6.933], mean action: 1.512 [0.000, 3.000], mean observation: -0.091 [-1.187, 3.765], loss: 1.409001, mean_absolute_error: 27.377404, mean_q: 17.030558, mean_eps: 0.752277
  275394/2000000: episode: 2814, duration: 1.549s, episode steps: 104, steps per second: 67, episode reward: -151.138, mean reward: -1.453 [-100.000, 6.823], mean action: 1.721 [0.000, 3.000], mean observation: -0.087 [-1.130, 4.067], loss: 1.668524, mean_absolute_error: 28.822947, mean_q: 15.551137, mean_eps: 0.752192
  275512/2000000: episode: 2815, duration: 1.697s, episode steps: 118, steps per second: 70, episode reward: -128.826, mean reward: -1.092 [-100.000, 14.092], mean action: 1.466 [0.000, 3.000], mean observation: -0.106 [-3.523, 1.000], loss: 1.515724, mean_absolute_error: 27.973589, mean_q: 16.692953, mean_eps: 0.752093
  275636/2000000: episode: 2816, duration: 1.794s, episode steps: 124, steps per second: 69, episode reward: -200.081, mean reward: -1.614 [-100.000, 16.680], mean action: 1.677 [0.000, 3.000], mean observation: -0.129 [-1.213, 4.186], loss: 1.592245, mean_absolute_error: 29.003475, mean_q: 15.535348, mean_eps: 0.751985
  275725/2000000: episode: 2817, duration: 1.317s, episode steps: 89, steps per second: 68, episode reward: -66.657, mean reward: -0.749 [-100.000, 18.692], mean action: 1.719 [0.000, 3.000], mean observation: 0.073 [-0.913, 3.030], loss: 1.826755, mean_absolute_error: 27.574011, mean_q: 17.470877, mean_eps: 0.751888
  275798/2000000: episode: 2818, duration: 1.124s, episode steps: 73, steps per second: 65, episode reward: -113.092, mean reward: -1.549 [-100.000, 8.577], mean action: 1.658 [0.000, 3.000], mean observation: -0.096 [-3.559, 1.000], loss: 1.292211, mean_absolute_error: 26.716272, mean_q: 19.066503, mean_eps: 0.751814
  275878/2000000: episode: 2819, duration: 1.134s, episode steps: 80, steps per second: 71, episode reward: -159.608, mean reward: -1.995 [-100.000, 7.074], mean action: 1.625 [0.000, 3.000], mean observation: -0.082 [-1.324, 4.237], loss: 1.184755, mean_absolute_error: 25.912700, mean_q: 20.917805, mean_eps: 0.751746
  275957/2000000: episode: 2820, duration: 1.124s, episode steps: 79, steps per second: 70, episode reward: -136.408, mean reward: -1.727 [-100.000, 9.647], mean action: 1.759 [0.000, 3.000], mean observation: -0.026 [-1.400, 4.724], loss: 3.012224, mean_absolute_error: 30.938111, mean_q: 13.513809, mean_eps: 0.751674
  276077/2000000: episode: 2821, duration: 1.710s, episode steps: 120, steps per second: 70, episode reward: -78.489, mean reward: -0.654 [-100.000, 21.890], mean action: 1.700 [0.000, 3.000], mean observation: -0.116 [-1.027, 1.193], loss: 1.418679, mean_absolute_error: 27.986453, mean_q: 19.967851, mean_eps: 0.751584
  276199/2000000: episode: 2822, duration: 1.708s, episode steps: 122, steps per second: 71, episode reward: -96.993, mean reward: -0.795 [-100.000, 14.457], mean action: 1.639 [0.000, 3.000], mean observation: 0.065 [-1.045, 1.000], loss: 1.450364, mean_absolute_error: 27.684862, mean_q: 16.542040, mean_eps: 0.751476
  276306/2000000: episode: 2823, duration: 1.515s, episode steps: 107, steps per second: 71, episode reward: -136.009, mean reward: -1.271 [-100.000, 11.969], mean action: 1.617 [0.000, 3.000], mean observation: 0.084 [-1.371, 4.792], loss: 1.806399, mean_absolute_error: 29.632630, mean_q: 16.682585, mean_eps: 0.751373
  276420/2000000: episode: 2824, duration: 1.654s, episode steps: 114, steps per second: 69, episode reward: -108.432, mean reward: -0.951 [-100.000, 30.208], mean action: 1.658 [0.000, 3.000], mean observation: -0.110 [-1.088, 2.757], loss: 1.682584, mean_absolute_error: 27.920228, mean_q: 17.157783, mean_eps: 0.751274
  276513/2000000: episode: 2825, duration: 1.354s, episode steps: 93, steps per second: 69, episode reward: -110.482, mean reward: -1.188 [-100.000, 6.688], mean action: 1.785 [0.000, 3.000], mean observation: -0.076 [-1.026, 3.630], loss: 1.892027, mean_absolute_error: 27.436718, mean_q: 18.366893, mean_eps: 0.751181
  276644/2000000: episode: 2826, duration: 1.867s, episode steps: 131, steps per second: 70, episode reward: -111.943, mean reward: -0.855 [-100.000, 7.026], mean action: 1.687 [0.000, 3.000], mean observation: 0.019 [-1.110, 3.886], loss: 1.226055, mean_absolute_error: 28.371755, mean_q: 17.823503, mean_eps: 0.751080
  276757/2000000: episode: 2827, duration: 1.660s, episode steps: 113, steps per second: 68, episode reward: -122.232, mean reward: -1.082 [-100.000, 9.391], mean action: 1.611 [0.000, 3.000], mean observation: 0.101 [-0.858, 4.244], loss: 1.217556, mean_absolute_error: 27.377548, mean_q: 15.415584, mean_eps: 0.750970
  276830/2000000: episode: 2828, duration: 1.024s, episode steps: 73, steps per second: 71, episode reward: -142.995, mean reward: -1.959 [-100.000, 8.239], mean action: 1.452 [0.000, 3.000], mean observation: 0.076 [-1.334, 4.822], loss: 1.151603, mean_absolute_error: 27.663548, mean_q: 17.445416, mean_eps: 0.750885
  276919/2000000: episode: 2829, duration: 1.246s, episode steps: 89, steps per second: 71, episode reward: -96.127, mean reward: -1.080 [-100.000, 10.065], mean action: 1.404 [0.000, 3.000], mean observation: 0.131 [-0.875, 2.910], loss: 2.166266, mean_absolute_error: 30.687577, mean_q: 15.162468, mean_eps: 0.750813
  277015/2000000: episode: 2830, duration: 1.365s, episode steps: 96, steps per second: 70, episode reward: -108.950, mean reward: -1.135 [-100.000, 68.158], mean action: 1.583 [0.000, 3.000], mean observation: -0.037 [-3.760, 1.000], loss: 1.498756, mean_absolute_error: 28.027369, mean_q: 18.480236, mean_eps: 0.750731
  277077/2000000: episode: 2831, duration: 0.906s, episode steps: 62, steps per second: 68, episode reward: -147.771, mean reward: -2.383 [-100.000, 21.510], mean action: 1.532 [0.000, 3.000], mean observation: -0.044 [-5.262, 1.000], loss: 1.178430, mean_absolute_error: 27.479700, mean_q: 20.720814, mean_eps: 0.750659
  277155/2000000: episode: 2832, duration: 1.084s, episode steps: 78, steps per second: 72, episode reward: -93.158, mean reward: -1.194 [-100.000, 17.712], mean action: 1.744 [0.000, 3.000], mean observation: -0.126 [-1.104, 1.000], loss: 2.365729, mean_absolute_error: 30.948347, mean_q: 12.884544, mean_eps: 0.750596
  277289/2000000: episode: 2833, duration: 1.925s, episode steps: 134, steps per second: 70, episode reward: -71.368, mean reward: -0.533 [-100.000, 23.206], mean action: 1.679 [0.000, 3.000], mean observation: 0.203 [-1.272, 1.367], loss: 2.059590, mean_absolute_error: 28.669546, mean_q: 15.578323, mean_eps: 0.750500
  277414/2000000: episode: 2834, duration: 1.786s, episode steps: 125, steps per second: 70, episode reward: -170.650, mean reward: -1.365 [-100.000, 23.377], mean action: 1.520 [0.000, 3.000], mean observation: 0.143 [-2.320, 1.016], loss: 1.558615, mean_absolute_error: 28.101332, mean_q: 18.358708, mean_eps: 0.750383
  277526/2000000: episode: 2835, duration: 1.637s, episode steps: 112, steps per second: 68, episode reward: -212.732, mean reward: -1.899 [-100.000, 30.429], mean action: 1.652 [0.000, 3.000], mean observation: 0.197 [-1.195, 3.201], loss: 1.782925, mean_absolute_error: 28.917254, mean_q: 16.473822, mean_eps: 0.750277
  277621/2000000: episode: 2836, duration: 1.405s, episode steps: 95, steps per second: 68, episode reward: -127.761, mean reward: -1.345 [-100.000, 12.921], mean action: 1.632 [0.000, 3.000], mean observation: 0.021 [-3.751, 1.000], loss: 1.533126, mean_absolute_error: 27.698399, mean_q: 16.932945, mean_eps: 0.750183
  277739/2000000: episode: 2837, duration: 1.664s, episode steps: 118, steps per second: 71, episode reward: -119.105, mean reward: -1.009 [-100.000, 8.163], mean action: 1.568 [0.000, 3.000], mean observation: 0.046 [-4.006, 1.021], loss: 1.569009, mean_absolute_error: 27.349142, mean_q: 17.349607, mean_eps: 0.750088
  277859/2000000: episode: 2838, duration: 1.662s, episode steps: 120, steps per second: 72, episode reward: -97.616, mean reward: -0.813 [-100.000, 10.760], mean action: 1.583 [0.000, 3.000], mean observation: 0.122 [-2.457, 1.000], loss: 1.671811, mean_absolute_error: 27.236996, mean_q: 15.904240, mean_eps: 0.749982
  277959/2000000: episode: 2839, duration: 1.449s, episode steps: 100, steps per second: 69, episode reward: -171.597, mean reward: -1.716 [-100.000, 7.823], mean action: 1.520 [0.000, 3.000], mean observation: 0.033 [-3.471, 1.000], loss: 1.416943, mean_absolute_error: 28.559355, mean_q: 15.334548, mean_eps: 0.749883
  278042/2000000: episode: 2840, duration: 1.184s, episode steps: 83, steps per second: 70, episode reward: -129.768, mean reward: -1.563 [-100.000, 11.126], mean action: 1.590 [0.000, 3.000], mean observation: -0.133 [-4.240, 1.000], loss: 2.509939, mean_absolute_error: 28.921413, mean_q: 16.788113, mean_eps: 0.749800
  278150/2000000: episode: 2841, duration: 1.519s, episode steps: 108, steps per second: 71, episode reward: -146.227, mean reward: -1.354 [-100.000, 11.700], mean action: 1.565 [0.000, 3.000], mean observation: -0.057 [-1.397, 4.805], loss: 1.536053, mean_absolute_error: 28.523751, mean_q: 15.693785, mean_eps: 0.749714
  278287/2000000: episode: 2842, duration: 1.941s, episode steps: 137, steps per second: 71, episode reward: -138.488, mean reward: -1.011 [-100.000, 8.551], mean action: 1.577 [0.000, 3.000], mean observation: 0.079 [-2.537, 1.020], loss: 1.779672, mean_absolute_error: 27.460791, mean_q: 16.357573, mean_eps: 0.749604
  278399/2000000: episode: 2843, duration: 1.589s, episode steps: 112, steps per second: 71, episode reward: -121.786, mean reward: -1.087 [-100.000, 12.085], mean action: 1.732 [0.000, 3.000], mean observation: -0.060 [-3.998, 1.000], loss: 1.975077, mean_absolute_error: 29.608703, mean_q: 15.178815, mean_eps: 0.749492
  278463/2000000: episode: 2844, duration: 0.901s, episode steps: 64, steps per second: 71, episode reward: -125.932, mean reward: -1.968 [-100.000, 6.669], mean action: 1.594 [0.000, 3.000], mean observation: -0.179 [-1.445, 4.710], loss: 2.213859, mean_absolute_error: 28.998733, mean_q: 16.355405, mean_eps: 0.749413
  278544/2000000: episode: 2845, duration: 1.182s, episode steps: 81, steps per second: 69, episode reward: -126.965, mean reward: -1.567 [-100.000, 12.377], mean action: 1.506 [0.000, 3.000], mean observation: -0.087 [-4.227, 1.000], loss: 1.740845, mean_absolute_error: 27.233380, mean_q: 18.293430, mean_eps: 0.749348
  278721/2000000: episode: 2846, duration: 2.545s, episode steps: 177, steps per second: 70, episode reward: -139.814, mean reward: -0.790 [-100.000, 21.603], mean action: 1.757 [0.000, 3.000], mean observation: -0.065 [-0.912, 3.372], loss: 1.851664, mean_absolute_error: 27.863596, mean_q: 17.515816, mean_eps: 0.749231
  278825/2000000: episode: 2847, duration: 1.464s, episode steps: 104, steps per second: 71, episode reward: -181.228, mean reward: -1.743 [-100.000, 6.446], mean action: 1.558 [0.000, 3.000], mean observation: -0.080 [-1.340, 4.757], loss: 1.866873, mean_absolute_error: 28.861414, mean_q: 16.887920, mean_eps: 0.749103
  278954/2000000: episode: 2848, duration: 1.810s, episode steps: 129, steps per second: 71, episode reward: -87.981, mean reward: -0.682 [-100.000, 14.385], mean action: 1.620 [0.000, 3.000], mean observation: 0.094 [-1.160, 1.052], loss: 2.029871, mean_absolute_error: 27.935439, mean_q: 19.205757, mean_eps: 0.748999
  279034/2000000: episode: 2849, duration: 1.138s, episode steps: 80, steps per second: 70, episode reward: -145.767, mean reward: -1.822 [-100.000, 15.983], mean action: 1.413 [0.000, 3.000], mean observation: -0.119 [-4.152, 1.000], loss: 1.557328, mean_absolute_error: 27.859067, mean_q: 17.041714, mean_eps: 0.748905
  279165/2000000: episode: 2850, duration: 1.875s, episode steps: 131, steps per second: 70, episode reward: -84.194, mean reward: -0.643 [-100.000, 19.508], mean action: 1.580 [0.000, 3.000], mean observation: 0.067 [-3.482, 1.051], loss: 1.266129, mean_absolute_error: 27.416921, mean_q: 20.163819, mean_eps: 0.748810
  279282/2000000: episode: 2851, duration: 1.966s, episode steps: 117, steps per second: 59, episode reward: -76.982, mean reward: -0.658 [-100.000, 20.687], mean action: 1.521 [0.000, 3.000], mean observation: 0.108 [-1.374, 1.200], loss: 1.517749, mean_absolute_error: 27.257192, mean_q: 18.994209, mean_eps: 0.748698
  279384/2000000: episode: 2852, duration: 1.552s, episode steps: 102, steps per second: 66, episode reward: -126.758, mean reward: -1.243 [-100.000, 9.196], mean action: 1.608 [0.000, 3.000], mean observation: -0.106 [-0.934, 1.773], loss: 1.898050, mean_absolute_error: 29.464730, mean_q: 13.808658, mean_eps: 0.748601
  279487/2000000: episode: 2853, duration: 1.679s, episode steps: 103, steps per second: 61, episode reward: -103.371, mean reward: -1.004 [-100.000, 9.955], mean action: 1.612 [0.000, 3.000], mean observation: 0.027 [-2.800, 1.000], loss: 1.170104, mean_absolute_error: 28.945954, mean_q: 17.869933, mean_eps: 0.748509
  279606/2000000: episode: 2854, duration: 1.721s, episode steps: 119, steps per second: 69, episode reward: -135.434, mean reward: -1.138 [-100.000, 6.438], mean action: 1.723 [0.000, 3.000], mean observation: -0.069 [-0.924, 3.077], loss: 1.724536, mean_absolute_error: 28.398441, mean_q: 17.959662, mean_eps: 0.748409
  279693/2000000: episode: 2855, duration: 1.278s, episode steps: 87, steps per second: 68, episode reward: -146.459, mean reward: -1.683 [-100.000, 6.192], mean action: 1.632 [0.000, 3.000], mean observation: -0.184 [-4.981, 1.000], loss: 1.026682, mean_absolute_error: 27.433202, mean_q: 19.001830, mean_eps: 0.748315
  279749/2000000: episode: 2856, duration: 0.987s, episode steps: 56, steps per second: 57, episode reward: -122.854, mean reward: -2.194 [-100.000, 28.884], mean action: 1.500 [0.000, 3.000], mean observation: -0.015 [-1.624, 6.196], loss: 1.805348, mean_absolute_error: 28.755146, mean_q: 15.976355, mean_eps: 0.748250
  279817/2000000: episode: 2857, duration: 1.235s, episode steps: 68, steps per second: 55, episode reward: -137.227, mean reward: -2.018 [-100.000, 6.489], mean action: 1.618 [0.000, 3.000], mean observation: 0.121 [-1.185, 1.000], loss: 1.027557, mean_absolute_error: 28.628169, mean_q: 18.515948, mean_eps: 0.748194
  279886/2000000: episode: 2858, duration: 1.035s, episode steps: 69, steps per second: 67, episode reward: -128.106, mean reward: -1.857 [-100.000, 13.066], mean action: 1.580 [0.000, 3.000], mean observation: -0.091 [-1.293, 4.018], loss: 1.274558, mean_absolute_error: 28.385957, mean_q: 18.455076, mean_eps: 0.748133
  280020/2000000: episode: 2859, duration: 2.002s, episode steps: 134, steps per second: 67, episode reward: -82.150, mean reward: -0.613 [-100.000, 21.328], mean action: 1.701 [0.000, 3.000], mean observation: -0.087 [-2.196, 1.000], loss: 1.744087, mean_absolute_error: 28.075121, mean_q: 18.875622, mean_eps: 0.748043
  280096/2000000: episode: 2860, duration: 1.238s, episode steps: 76, steps per second: 61, episode reward: -113.621, mean reward: -1.495 [-100.000, 14.224], mean action: 1.605 [0.000, 3.000], mean observation: 0.048 [-1.219, 4.163], loss: 1.349828, mean_absolute_error: 26.870781, mean_q: 19.511608, mean_eps: 0.747950
  280190/2000000: episode: 2861, duration: 1.681s, episode steps: 94, steps per second: 56, episode reward: -127.401, mean reward: -1.355 [-100.000, 13.655], mean action: 1.660 [0.000, 3.000], mean observation: -0.155 [-1.124, 1.000], loss: 1.809249, mean_absolute_error: 28.950886, mean_q: 14.213845, mean_eps: 0.747872
  280281/2000000: episode: 2862, duration: 1.658s, episode steps: 91, steps per second: 55, episode reward: -182.627, mean reward: -2.007 [-100.000, 8.669], mean action: 1.538 [0.000, 3.000], mean observation: -0.153 [-1.308, 4.471], loss: 1.434585, mean_absolute_error: 28.715857, mean_q: 16.779702, mean_eps: 0.747788
  280357/2000000: episode: 2863, duration: 1.274s, episode steps: 76, steps per second: 60, episode reward: -133.372, mean reward: -1.755 [-100.000, 10.327], mean action: 1.632 [0.000, 3.000], mean observation: -0.086 [-1.325, 4.433], loss: 1.338447, mean_absolute_error: 27.929310, mean_q: 16.989622, mean_eps: 0.747712
  280434/2000000: episode: 2864, duration: 1.156s, episode steps: 77, steps per second: 67, episode reward: -133.261, mean reward: -1.731 [-100.000, 5.822], mean action: 1.740 [0.000, 3.000], mean observation: -0.158 [-1.128, 2.771], loss: 1.462490, mean_absolute_error: 28.767619, mean_q: 17.480531, mean_eps: 0.747644
  280549/2000000: episode: 2865, duration: 1.700s, episode steps: 115, steps per second: 68, episode reward: -177.022, mean reward: -1.539 [-100.000, 14.452], mean action: 1.426 [0.000, 3.000], mean observation: 0.111 [-1.467, 1.038], loss: 1.327166, mean_absolute_error: 28.579533, mean_q: 17.497717, mean_eps: 0.747557
  280632/2000000: episode: 2866, duration: 1.209s, episode steps: 83, steps per second: 69, episode reward: -80.036, mean reward: -0.964 [-100.000, 30.943], mean action: 1.759 [0.000, 3.000], mean observation: -0.144 [-1.101, 1.921], loss: 1.727932, mean_absolute_error: 29.176162, mean_q: 13.761039, mean_eps: 0.747469
  280710/2000000: episode: 2867, duration: 1.172s, episode steps: 78, steps per second: 67, episode reward: -95.969, mean reward: -1.230 [-100.000, 7.423], mean action: 1.500 [0.000, 3.000], mean observation: 0.016 [-3.531, 1.000], loss: 2.173049, mean_absolute_error: 28.050022, mean_q: 17.153223, mean_eps: 0.747397
  280778/2000000: episode: 2868, duration: 1.016s, episode steps: 68, steps per second: 67, episode reward: -134.903, mean reward: -1.984 [-100.000, 7.615], mean action: 1.662 [0.000, 3.000], mean observation: -0.050 [-1.456, 5.141], loss: 1.357914, mean_absolute_error: 29.005369, mean_q: 17.444349, mean_eps: 0.747330
  280848/2000000: episode: 2869, duration: 1.041s, episode steps: 70, steps per second: 67, episode reward: -94.565, mean reward: -1.351 [-100.000, 14.365], mean action: 1.529 [0.000, 3.000], mean observation: 0.038 [-1.316, 1.000], loss: 1.321192, mean_absolute_error: 27.094423, mean_q: 18.185513, mean_eps: 0.747269
  280947/2000000: episode: 2870, duration: 1.477s, episode steps: 99, steps per second: 67, episode reward: -91.845, mean reward: -0.928 [-100.000, 6.408], mean action: 1.455 [0.000, 3.000], mean observation: 0.077 [-2.132, 1.000], loss: 1.986223, mean_absolute_error: 28.782683, mean_q: 17.973084, mean_eps: 0.747194
  281088/2000000: episode: 2871, duration: 2.137s, episode steps: 141, steps per second: 66, episode reward: -129.533, mean reward: -0.919 [-100.000, 9.658], mean action: 1.468 [0.000, 3.000], mean observation: 0.082 [-0.962, 1.000], loss: 1.926660, mean_absolute_error: 29.059463, mean_q: 16.414436, mean_eps: 0.747086
  281246/2000000: episode: 2872, duration: 2.290s, episode steps: 158, steps per second: 69, episode reward: -316.701, mean reward: -2.004 [-100.000, 47.529], mean action: 1.563 [0.000, 3.000], mean observation: 0.240 [-1.337, 2.732], loss: 2.217230, mean_absolute_error: 29.319959, mean_q: 16.452977, mean_eps: 0.746951
  281372/2000000: episode: 2873, duration: 1.835s, episode steps: 126, steps per second: 69, episode reward: -111.763, mean reward: -0.887 [-100.000, 30.290], mean action: 1.714 [0.000, 3.000], mean observation: 0.150 [-1.632, 1.495], loss: 1.546813, mean_absolute_error: 28.966127, mean_q: 17.423402, mean_eps: 0.746823
  281438/2000000: episode: 2874, duration: 0.965s, episode steps: 66, steps per second: 68, episode reward: -101.201, mean reward: -1.533 [-100.000, 21.753], mean action: 1.833 [0.000, 3.000], mean observation: -0.154 [-1.142, 1.000], loss: 1.407644, mean_absolute_error: 28.695836, mean_q: 16.213350, mean_eps: 0.746736
  281560/2000000: episode: 2875, duration: 1.761s, episode steps: 122, steps per second: 69, episode reward: -74.534, mean reward: -0.611 [-100.000, 13.443], mean action: 1.590 [0.000, 3.000], mean observation: -0.051 [-1.578, 1.000], loss: 1.683134, mean_absolute_error: 29.324116, mean_q: 13.639538, mean_eps: 0.746652
  281630/2000000: episode: 2876, duration: 1.025s, episode steps: 70, steps per second: 68, episode reward: -132.426, mean reward: -1.892 [-100.000, 6.180], mean action: 1.671 [0.000, 3.000], mean observation: -0.136 [-1.296, 4.018], loss: 1.556741, mean_absolute_error: 29.187445, mean_q: 16.762489, mean_eps: 0.746565
  281716/2000000: episode: 2877, duration: 1.241s, episode steps: 86, steps per second: 69, episode reward: -92.946, mean reward: -1.081 [-100.000, 15.817], mean action: 1.570 [0.000, 3.000], mean observation: -0.061 [-1.065, 3.651], loss: 1.410035, mean_absolute_error: 28.074631, mean_q: 16.828247, mean_eps: 0.746495
  281866/2000000: episode: 2878, duration: 2.128s, episode steps: 150, steps per second: 70, episode reward: -88.035, mean reward: -0.587 [-100.000, 18.351], mean action: 1.620 [0.000, 3.000], mean observation: 0.117 [-1.111, 1.057], loss: 1.559090, mean_absolute_error: 28.535727, mean_q: 17.502315, mean_eps: 0.746389
  281998/2000000: episode: 2879, duration: 1.891s, episode steps: 132, steps per second: 70, episode reward: -119.143, mean reward: -0.903 [-100.000, 5.845], mean action: 1.477 [0.000, 3.000], mean observation: 0.038 [-4.146, 1.016], loss: 1.462090, mean_absolute_error: 28.189438, mean_q: 15.617888, mean_eps: 0.746261
  282205/2000000: episode: 2880, duration: 2.965s, episode steps: 207, steps per second: 70, episode reward: -90.243, mean reward: -0.436 [-100.000, 14.379], mean action: 1.700 [0.000, 3.000], mean observation: -0.012 [-0.988, 1.003], loss: 1.856958, mean_absolute_error: 28.391056, mean_q: 15.489561, mean_eps: 0.746108
  282315/2000000: episode: 2881, duration: 1.565s, episode steps: 110, steps per second: 70, episode reward: -94.332, mean reward: -0.858 [-100.000, 15.963], mean action: 1.664 [0.000, 3.000], mean observation: 0.078 [-0.976, 3.391], loss: 1.819127, mean_absolute_error: 28.996928, mean_q: 17.252823, mean_eps: 0.745966
  282384/2000000: episode: 2882, duration: 1.027s, episode steps: 69, steps per second: 67, episode reward: -119.177, mean reward: -1.727 [-100.000, 16.275], mean action: 1.304 [0.000, 3.000], mean observation: 0.041 [-3.472, 1.000], loss: 2.272944, mean_absolute_error: 28.286610, mean_q: 16.495153, mean_eps: 0.745887
  282474/2000000: episode: 2883, duration: 1.300s, episode steps: 90, steps per second: 69, episode reward: -50.438, mean reward: -0.560 [-100.000, 17.175], mean action: 1.656 [0.000, 3.000], mean observation: -0.040 [-1.026, 1.000], loss: 1.666934, mean_absolute_error: 27.576802, mean_q: 15.737458, mean_eps: 0.745815
  282571/2000000: episode: 2884, duration: 1.356s, episode steps: 97, steps per second: 72, episode reward: -122.805, mean reward: -1.266 [-100.000, 7.476], mean action: 1.423 [0.000, 3.000], mean observation: 0.080 [-3.944, 1.000], loss: 1.996410, mean_absolute_error: 29.043516, mean_q: 17.964866, mean_eps: 0.745730
  282699/2000000: episode: 2885, duration: 1.834s, episode steps: 128, steps per second: 70, episode reward: -62.922, mean reward: -0.492 [-100.000, 21.834], mean action: 1.727 [0.000, 3.000], mean observation: 0.033 [-0.836, 2.097], loss: 1.310290, mean_absolute_error: 28.804495, mean_q: 15.993894, mean_eps: 0.745629
  282822/2000000: episode: 2886, duration: 1.778s, episode steps: 123, steps per second: 69, episode reward: -125.972, mean reward: -1.024 [-100.000, 18.456], mean action: 1.626 [0.000, 3.000], mean observation: 0.066 [-1.154, 1.000], loss: 1.697698, mean_absolute_error: 27.658731, mean_q: 18.731835, mean_eps: 0.745516
  282969/2000000: episode: 2887, duration: 2.093s, episode steps: 147, steps per second: 70, episode reward: -237.752, mean reward: -1.617 [-100.000, 35.919], mean action: 1.558 [0.000, 3.000], mean observation: 0.137 [-1.395, 3.462], loss: 1.792148, mean_absolute_error: 28.850396, mean_q: 16.443533, mean_eps: 0.745394
  283068/2000000: episode: 2888, duration: 1.415s, episode steps: 99, steps per second: 70, episode reward: -125.738, mean reward: -1.270 [-100.000, 20.204], mean action: 1.414 [0.000, 3.000], mean observation: 0.009 [-1.451, 1.000], loss: 1.733262, mean_absolute_error: 26.947159, mean_q: 19.519026, mean_eps: 0.745284
  283146/2000000: episode: 2889, duration: 1.129s, episode steps: 78, steps per second: 69, episode reward: -114.692, mean reward: -1.470 [-100.000, 11.331], mean action: 1.564 [0.000, 3.000], mean observation: 0.039 [-1.273, 1.000], loss: 1.072615, mean_absolute_error: 27.051923, mean_q: 21.057407, mean_eps: 0.745205
  283224/2000000: episode: 2890, duration: 1.142s, episode steps: 78, steps per second: 68, episode reward: -132.902, mean reward: -1.704 [-100.000, 12.804], mean action: 1.654 [0.000, 3.000], mean observation: 0.119 [-1.259, 1.000], loss: 1.839906, mean_absolute_error: 29.732844, mean_q: 14.856167, mean_eps: 0.745134
  283309/2000000: episode: 2891, duration: 1.258s, episode steps: 85, steps per second: 68, episode reward: -167.188, mean reward: -1.967 [-100.000, 7.923], mean action: 1.776 [0.000, 3.000], mean observation: -0.114 [-1.197, 3.213], loss: 1.375160, mean_absolute_error: 29.451387, mean_q: 13.413144, mean_eps: 0.745061
  283457/2000000: episode: 2892, duration: 2.131s, episode steps: 148, steps per second: 69, episode reward: -68.598, mean reward: -0.463 [-100.000, 10.373], mean action: 1.419 [0.000, 3.000], mean observation: 0.004 [-2.043, 1.000], loss: 1.799392, mean_absolute_error: 28.380936, mean_q: 16.649682, mean_eps: 0.744954
  283548/2000000: episode: 2893, duration: 1.297s, episode steps: 91, steps per second: 70, episode reward: -150.189, mean reward: -1.650 [-100.000, 5.646], mean action: 1.659 [0.000, 3.000], mean observation: 0.018 [-3.783, 1.000], loss: 1.544099, mean_absolute_error: 27.468699, mean_q: 18.850306, mean_eps: 0.744848
  283730/2000000: episode: 2894, duration: 2.589s, episode steps: 182, steps per second: 70, episode reward: -90.901, mean reward: -0.499 [-100.000, 12.567], mean action: 1.659 [0.000, 3.000], mean observation: -0.016 [-3.247, 1.000], loss: 1.716983, mean_absolute_error: 28.432521, mean_q: 16.494382, mean_eps: 0.744726
  283842/2000000: episode: 2895, duration: 1.604s, episode steps: 112, steps per second: 70, episode reward: -146.917, mean reward: -1.312 [-100.000, 7.550], mean action: 1.580 [0.000, 3.000], mean observation: -0.057 [-1.134, 3.624], loss: 2.053652, mean_absolute_error: 28.432771, mean_q: 16.963518, mean_eps: 0.744593
  283981/2000000: episode: 2896, duration: 2.002s, episode steps: 139, steps per second: 69, episode reward: -139.994, mean reward: -1.007 [-100.000, 17.397], mean action: 1.626 [0.000, 3.000], mean observation: -0.122 [-3.267, 1.000], loss: 1.548938, mean_absolute_error: 28.287231, mean_q: 18.044421, mean_eps: 0.744479
  284064/2000000: episode: 2897, duration: 1.194s, episode steps: 83, steps per second: 70, episode reward: -156.297, mean reward: -1.883 [-100.000, 9.781], mean action: 1.422 [0.000, 3.000], mean observation: 0.087 [-4.414, 1.000], loss: 2.943856, mean_absolute_error: 29.028754, mean_q: 17.934795, mean_eps: 0.744380
  284165/2000000: episode: 2898, duration: 1.474s, episode steps: 101, steps per second: 69, episode reward: -102.524, mean reward: -1.015 [-100.000, 7.003], mean action: 1.782 [0.000, 3.000], mean observation: -0.078 [-0.987, 3.174], loss: 1.715222, mean_absolute_error: 27.488266, mean_q: 16.865981, mean_eps: 0.744297
  284296/2000000: episode: 2899, duration: 1.846s, episode steps: 131, steps per second: 71, episode reward: -99.115, mean reward: -0.757 [-100.000, 11.697], mean action: 1.649 [0.000, 3.000], mean observation: -0.112 [-0.892, 2.831], loss: 1.841775, mean_absolute_error: 28.034840, mean_q: 17.573143, mean_eps: 0.744193
  284380/2000000: episode: 2900, duration: 1.249s, episode steps: 84, steps per second: 67, episode reward: -116.840, mean reward: -1.391 [-100.000, 19.704], mean action: 1.595 [0.000, 3.000], mean observation: 0.069 [-3.567, 1.000], loss: 1.779874, mean_absolute_error: 29.735584, mean_q: 14.729103, mean_eps: 0.744098
  284464/2000000: episode: 2901, duration: 1.249s, episode steps: 84, steps per second: 67, episode reward: -102.353, mean reward: -1.218 [-100.000, 17.947], mean action: 1.571 [0.000, 3.000], mean observation: 0.044 [-1.208, 1.000], loss: 1.261613, mean_absolute_error: 27.810742, mean_q: 16.820724, mean_eps: 0.744022
  284550/2000000: episode: 2902, duration: 1.240s, episode steps: 86, steps per second: 69, episode reward: -106.320, mean reward: -1.236 [-100.000, 8.288], mean action: 1.605 [0.000, 3.000], mean observation: 0.101 [-2.682, 1.000], loss: 1.324605, mean_absolute_error: 27.727342, mean_q: 18.101512, mean_eps: 0.743945
  284646/2000000: episode: 2903, duration: 1.346s, episode steps: 96, steps per second: 71, episode reward: -141.031, mean reward: -1.469 [-100.000, 10.546], mean action: 1.583 [0.000, 3.000], mean observation: 0.023 [-4.552, 1.000], loss: 1.098754, mean_absolute_error: 26.669525, mean_q: 17.104364, mean_eps: 0.743862
  284761/2000000: episode: 2904, duration: 1.651s, episode steps: 115, steps per second: 70, episode reward: -135.520, mean reward: -1.178 [-100.000, 6.911], mean action: 1.626 [0.000, 3.000], mean observation: 0.003 [-1.088, 3.904], loss: 1.706453, mean_absolute_error: 28.445778, mean_q: 16.006322, mean_eps: 0.743766
  284925/2000000: episode: 2905, duration: 2.334s, episode steps: 164, steps per second: 70, episode reward: -109.958, mean reward: -0.670 [-100.000, 6.128], mean action: 1.598 [0.000, 3.000], mean observation: 0.142 [-0.960, 1.596], loss: 1.537138, mean_absolute_error: 27.700973, mean_q: 18.131856, mean_eps: 0.743640
  285031/2000000: episode: 2906, duration: 1.493s, episode steps: 106, steps per second: 71, episode reward: -92.940, mean reward: -0.877 [-100.000, 14.247], mean action: 1.642 [0.000, 3.000], mean observation: -0.001 [-3.483, 1.000], loss: 1.631200, mean_absolute_error: 28.808784, mean_q: 16.079074, mean_eps: 0.743520
  285109/2000000: episode: 2907, duration: 1.130s, episode steps: 78, steps per second: 69, episode reward: -115.180, mean reward: -1.477 [-100.000, 16.824], mean action: 1.641 [0.000, 3.000], mean observation: 0.114 [-1.087, 1.000], loss: 1.354833, mean_absolute_error: 27.407843, mean_q: 16.931209, mean_eps: 0.743437
  285190/2000000: episode: 2908, duration: 1.129s, episode steps: 81, steps per second: 72, episode reward: -66.244, mean reward: -0.818 [-100.000, 11.903], mean action: 1.679 [0.000, 3.000], mean observation: -0.082 [-0.999, 1.000], loss: 1.828581, mean_absolute_error: 28.268863, mean_q: 16.795876, mean_eps: 0.743365
  285273/2000000: episode: 2909, duration: 1.195s, episode steps: 83, steps per second: 69, episode reward: -110.438, mean reward: -1.331 [-100.000, 6.888], mean action: 1.578 [0.000, 3.000], mean observation: 0.024 [-3.915, 1.000], loss: 1.908970, mean_absolute_error: 29.068708, mean_q: 14.272176, mean_eps: 0.743291
  285403/2000000: episode: 2910, duration: 1.849s, episode steps: 130, steps per second: 70, episode reward: -298.693, mean reward: -2.298 [-100.000, 43.985], mean action: 1.531 [0.000, 3.000], mean observation: 0.242 [-1.571, 3.708], loss: 1.556888, mean_absolute_error: 29.387432, mean_q: 17.180833, mean_eps: 0.743196
  285493/2000000: episode: 2911, duration: 1.305s, episode steps: 90, steps per second: 69, episode reward: -37.104, mean reward: -0.412 [-100.000, 13.470], mean action: 1.556 [0.000, 3.000], mean observation: -0.013 [-0.935, 1.000], loss: 1.616020, mean_absolute_error: 28.680115, mean_q: 15.972048, mean_eps: 0.743097
  285597/2000000: episode: 2912, duration: 1.486s, episode steps: 104, steps per second: 70, episode reward: -89.841, mean reward: -0.864 [-100.000, 9.172], mean action: 1.587 [0.000, 3.000], mean observation: 0.001 [-1.092, 3.670], loss: 2.119006, mean_absolute_error: 29.177726, mean_q: 17.803385, mean_eps: 0.743009
  285709/2000000: episode: 2913, duration: 1.603s, episode steps: 112, steps per second: 70, episode reward: -131.712, mean reward: -1.176 [-100.000, 12.655], mean action: 1.616 [0.000, 3.000], mean observation: -0.102 [-3.457, 1.000], loss: 1.796300, mean_absolute_error: 30.074952, mean_q: 15.847612, mean_eps: 0.742911
  285837/2000000: episode: 2914, duration: 1.866s, episode steps: 128, steps per second: 69, episode reward: -129.923, mean reward: -1.015 [-100.000, 9.913], mean action: 1.570 [0.000, 3.000], mean observation: 0.001 [-1.112, 2.755], loss: 1.299303, mean_absolute_error: 27.156863, mean_q: 20.101287, mean_eps: 0.742803
  286045/2000000: episode: 2915, duration: 2.956s, episode steps: 208, steps per second: 70, episode reward: -208.251, mean reward: -1.001 [-100.000, 77.539], mean action: 1.635 [0.000, 3.000], mean observation: 0.219 [-1.272, 2.047], loss: 2.341134, mean_absolute_error: 28.951077, mean_q: 15.827544, mean_eps: 0.742652
  286162/2000000: episode: 2916, duration: 1.674s, episode steps: 117, steps per second: 70, episode reward: -95.859, mean reward: -0.819 [-100.000, 6.607], mean action: 1.658 [0.000, 3.000], mean observation: 0.109 [-0.871, 3.062], loss: 1.302843, mean_absolute_error: 28.049511, mean_q: 15.040808, mean_eps: 0.742506
  286267/2000000: episode: 2917, duration: 1.473s, episode steps: 105, steps per second: 71, episode reward: -128.153, mean reward: -1.221 [-100.000, 13.480], mean action: 1.562 [0.000, 3.000], mean observation: 0.100 [-1.141, 1.000], loss: 2.223148, mean_absolute_error: 28.036660, mean_q: 16.346227, mean_eps: 0.742407
  286393/2000000: episode: 2918, duration: 1.809s, episode steps: 126, steps per second: 70, episode reward: -93.010, mean reward: -0.738 [-100.000, 17.822], mean action: 1.460 [0.000, 3.000], mean observation: 0.007 [-1.092, 1.000], loss: 1.774551, mean_absolute_error: 28.076760, mean_q: 17.302162, mean_eps: 0.742303
  286487/2000000: episode: 2919, duration: 1.321s, episode steps: 94, steps per second: 71, episode reward: -230.931, mean reward: -2.457 [-100.000, 123.733], mean action: 1.681 [0.000, 3.000], mean observation: -0.032 [-1.589, 2.179], loss: 1.638940, mean_absolute_error: 28.760175, mean_q: 14.342273, mean_eps: 0.742204
  286569/2000000: episode: 2920, duration: 1.171s, episode steps: 82, steps per second: 70, episode reward: -102.937, mean reward: -1.255 [-100.000, 13.282], mean action: 1.390 [0.000, 3.000], mean observation: 0.104 [-1.242, 4.376], loss: 1.606348, mean_absolute_error: 27.378648, mean_q: 19.431011, mean_eps: 0.742125
  286677/2000000: episode: 2921, duration: 1.497s, episode steps: 108, steps per second: 72, episode reward: -157.589, mean reward: -1.459 [-100.000, 10.933], mean action: 1.667 [0.000, 3.000], mean observation: 0.019 [-1.353, 4.383], loss: 1.283878, mean_absolute_error: 27.891175, mean_q: 17.917493, mean_eps: 0.742038
  286747/2000000: episode: 2922, duration: 0.957s, episode steps: 70, steps per second: 73, episode reward: -105.302, mean reward: -1.504 [-100.000, 9.310], mean action: 1.629 [0.000, 3.000], mean observation: 0.051 [-1.181, 4.148], loss: 2.798338, mean_absolute_error: 30.300633, mean_q: 15.961649, mean_eps: 0.741959
  286854/2000000: episode: 2923, duration: 1.526s, episode steps: 107, steps per second: 70, episode reward: -119.151, mean reward: -1.114 [-100.000, 16.075], mean action: 1.551 [0.000, 3.000], mean observation: 0.004 [-1.192, 1.000], loss: 1.849482, mean_absolute_error: 28.358419, mean_q: 17.028646, mean_eps: 0.741880
  286943/2000000: episode: 2924, duration: 1.244s, episode steps: 89, steps per second: 72, episode reward: -136.500, mean reward: -1.534 [-100.000, 10.567], mean action: 1.416 [0.000, 3.000], mean observation: 0.011 [-2.181, 1.000], loss: 1.332672, mean_absolute_error: 27.720074, mean_q: 20.148998, mean_eps: 0.741792
  287099/2000000: episode: 2925, duration: 2.281s, episode steps: 156, steps per second: 68, episode reward: -78.837, mean reward: -0.505 [-100.000, 16.843], mean action: 1.500 [0.000, 3.000], mean observation: 0.096 [-1.037, 1.055], loss: 1.564491, mean_absolute_error: 28.710420, mean_q: 18.618276, mean_eps: 0.741682
  287175/2000000: episode: 2926, duration: 1.113s, episode steps: 76, steps per second: 68, episode reward: -108.786, mean reward: -1.431 [-100.000, 5.460], mean action: 1.750 [0.000, 3.000], mean observation: 0.061 [-1.101, 2.680], loss: 3.017916, mean_absolute_error: 30.867493, mean_q: 12.714939, mean_eps: 0.741578
  287293/2000000: episode: 2927, duration: 1.685s, episode steps: 118, steps per second: 70, episode reward: -153.870, mean reward: -1.304 [-100.000, 6.635], mean action: 1.780 [0.000, 3.000], mean observation: -0.130 [-0.986, 3.458], loss: 1.505999, mean_absolute_error: 28.645258, mean_q: 17.181597, mean_eps: 0.741489
  287383/2000000: episode: 2928, duration: 1.228s, episode steps: 90, steps per second: 73, episode reward: -146.036, mean reward: -1.623 [-100.000, 7.012], mean action: 1.433 [0.000, 3.000], mean observation: -0.016 [-1.309, 4.552], loss: 1.956606, mean_absolute_error: 29.341501, mean_q: 17.756370, mean_eps: 0.741396
  287488/2000000: episode: 2929, duration: 1.554s, episode steps: 105, steps per second: 68, episode reward: -134.972, mean reward: -1.285 [-100.000, 10.659], mean action: 1.629 [0.000, 3.000], mean observation: 0.104 [-1.175, 3.861], loss: 1.981557, mean_absolute_error: 28.619662, mean_q: 15.970968, mean_eps: 0.741309
  287559/2000000: episode: 2930, duration: 1.022s, episode steps: 71, steps per second: 69, episode reward: -113.134, mean reward: -1.593 [-100.000, 6.427], mean action: 1.746 [0.000, 3.000], mean observation: -0.129 [-1.225, 3.902], loss: 1.968743, mean_absolute_error: 28.521561, mean_q: 16.060642, mean_eps: 0.741230
  287729/2000000: episode: 2931, duration: 2.410s, episode steps: 170, steps per second: 71, episode reward: -70.947, mean reward: -0.417 [-100.000, 90.716], mean action: 1.541 [0.000, 3.000], mean observation: -0.087 [-1.282, 1.621], loss: 1.570778, mean_absolute_error: 27.693757, mean_q: 18.098418, mean_eps: 0.741120
  287846/2000000: episode: 2932, duration: 1.651s, episode steps: 117, steps per second: 71, episode reward: -94.176, mean reward: -0.805 [-100.000, 7.810], mean action: 1.692 [0.000, 3.000], mean observation: -0.061 [-1.028, 3.039], loss: 1.738428, mean_absolute_error: 28.430658, mean_q: 18.002565, mean_eps: 0.740991
  287930/2000000: episode: 2933, duration: 1.185s, episode steps: 84, steps per second: 71, episode reward: -119.828, mean reward: -1.427 [-100.000, 6.752], mean action: 1.571 [0.000, 3.000], mean observation: -0.140 [-1.023, 1.000], loss: 1.317095, mean_absolute_error: 28.605615, mean_q: 16.826005, mean_eps: 0.740901
  288004/2000000: episode: 2934, duration: 1.072s, episode steps: 74, steps per second: 69, episode reward: -65.209, mean reward: -0.881 [-100.000, 17.763], mean action: 1.649 [0.000, 3.000], mean observation: 0.027 [-1.069, 1.000], loss: 1.945923, mean_absolute_error: 28.024065, mean_q: 15.328033, mean_eps: 0.740831
  288121/2000000: episode: 2935, duration: 1.668s, episode steps: 117, steps per second: 70, episode reward: -100.099, mean reward: -0.856 [-100.000, 12.923], mean action: 1.556 [0.000, 3.000], mean observation: 0.010 [-3.665, 1.000], loss: 2.288326, mean_absolute_error: 28.702453, mean_q: 15.696819, mean_eps: 0.740744
  288210/2000000: episode: 2936, duration: 1.254s, episode steps: 89, steps per second: 71, episode reward: -115.261, mean reward: -1.295 [-100.000, 10.476], mean action: 1.618 [0.000, 3.000], mean observation: -0.064 [-1.156, 4.217], loss: 2.167675, mean_absolute_error: 29.820116, mean_q: 13.929005, mean_eps: 0.740651
  288292/2000000: episode: 2937, duration: 1.183s, episode steps: 82, steps per second: 69, episode reward: -90.496, mean reward: -1.104 [-100.000, 13.522], mean action: 1.476 [0.000, 3.000], mean observation: -0.009 [-1.237, 1.000], loss: 2.285277, mean_absolute_error: 29.698253, mean_q: 16.648893, mean_eps: 0.740575
  288429/2000000: episode: 2938, duration: 1.962s, episode steps: 137, steps per second: 70, episode reward: -107.054, mean reward: -0.781 [-100.000, 14.178], mean action: 1.518 [0.000, 3.000], mean observation: -0.053 [-3.036, 1.008], loss: 1.965150, mean_absolute_error: 28.763762, mean_q: 16.695607, mean_eps: 0.740476
  288537/2000000: episode: 2939, duration: 1.525s, episode steps: 108, steps per second: 71, episode reward: -103.748, mean reward: -0.961 [-100.000, 12.443], mean action: 1.713 [0.000, 3.000], mean observation: 0.153 [-2.704, 1.000], loss: 2.285088, mean_absolute_error: 28.788100, mean_q: 16.047127, mean_eps: 0.740364
  288619/2000000: episode: 2940, duration: 1.123s, episode steps: 82, steps per second: 73, episode reward: -107.544, mean reward: -1.312 [-100.000, 10.610], mean action: 1.768 [0.000, 3.000], mean observation: -0.152 [-1.263, 4.115], loss: 1.337026, mean_absolute_error: 27.444113, mean_q: 19.394208, mean_eps: 0.740280
  288688/2000000: episode: 2941, duration: 1.007s, episode steps: 69, steps per second: 69, episode reward: -135.413, mean reward: -1.963 [-100.000, 8.743], mean action: 1.290 [0.000, 3.000], mean observation: 0.099 [-1.330, 1.000], loss: 2.512627, mean_absolute_error: 30.232764, mean_q: 17.198704, mean_eps: 0.740213
  288819/2000000: episode: 2942, duration: 1.838s, episode steps: 131, steps per second: 71, episode reward: -64.466, mean reward: -0.492 [-100.000, 14.809], mean action: 1.679 [0.000, 3.000], mean observation: 0.151 [-3.033, 1.009], loss: 1.649534, mean_absolute_error: 28.642944, mean_q: 17.329143, mean_eps: 0.740123
  288934/2000000: episode: 2943, duration: 1.632s, episode steps: 115, steps per second: 70, episode reward: -152.880, mean reward: -1.329 [-100.000, 12.468], mean action: 1.565 [0.000, 3.000], mean observation: 0.157 [-3.438, 1.000], loss: 2.404438, mean_absolute_error: 30.297480, mean_q: 15.228057, mean_eps: 0.740012
  289043/2000000: episode: 2944, duration: 1.539s, episode steps: 109, steps per second: 71, episode reward: -139.937, mean reward: -1.284 [-100.000, 7.463], mean action: 1.615 [0.000, 3.000], mean observation: -0.043 [-1.389, 1.029], loss: 2.963825, mean_absolute_error: 30.097213, mean_q: 14.655835, mean_eps: 0.739911
  289138/2000000: episode: 2945, duration: 1.343s, episode steps: 95, steps per second: 71, episode reward: -91.860, mean reward: -0.967 [-100.000, 16.691], mean action: 1.684 [0.000, 3.000], mean observation: -0.027 [-1.123, 1.000], loss: 1.823442, mean_absolute_error: 28.443202, mean_q: 17.093026, mean_eps: 0.739819
  289247/2000000: episode: 2946, duration: 1.515s, episode steps: 109, steps per second: 72, episode reward: -63.043, mean reward: -0.578 [-100.000, 86.292], mean action: 1.661 [0.000, 3.000], mean observation: -0.028 [-1.353, 1.748], loss: 1.653334, mean_absolute_error: 28.715477, mean_q: 16.461448, mean_eps: 0.739727
  289335/2000000: episode: 2947, duration: 1.228s, episode steps: 88, steps per second: 72, episode reward: -133.816, mean reward: -1.521 [-100.000, 11.880], mean action: 1.375 [0.000, 3.000], mean observation: 0.112 [-3.439, 1.000], loss: 1.601237, mean_absolute_error: 27.952516, mean_q: 18.122263, mean_eps: 0.739639
  289428/2000000: episode: 2948, duration: 1.338s, episode steps: 93, steps per second: 69, episode reward: -72.206, mean reward: -0.776 [-100.000, 10.912], mean action: 1.742 [0.000, 3.000], mean observation: 0.069 [-0.973, 1.000], loss: 1.537472, mean_absolute_error: 28.431916, mean_q: 18.452685, mean_eps: 0.739558
  289530/2000000: episode: 2949, duration: 1.457s, episode steps: 102, steps per second: 70, episode reward: -246.310, mean reward: -2.415 [-100.000, 8.914], mean action: 1.824 [0.000, 3.000], mean observation: -0.199 [-3.683, 1.000], loss: 1.208292, mean_absolute_error: 29.499867, mean_q: 14.573275, mean_eps: 0.739470
  289639/2000000: episode: 2950, duration: 1.530s, episode steps: 109, steps per second: 71, episode reward: -173.314, mean reward: -1.590 [-100.000, 9.376], mean action: 1.596 [0.000, 3.000], mean observation: 0.150 [-1.561, 1.231], loss: 1.565661, mean_absolute_error: 28.026726, mean_q: 17.129429, mean_eps: 0.739374
  289764/2000000: episode: 2951, duration: 1.798s, episode steps: 125, steps per second: 70, episode reward: -151.667, mean reward: -1.213 [-100.000, 7.627], mean action: 1.600 [0.000, 3.000], mean observation: 0.146 [-2.226, 1.000], loss: 1.609904, mean_absolute_error: 27.646274, mean_q: 18.528060, mean_eps: 0.739270
  289908/2000000: episode: 2952, duration: 2.059s, episode steps: 144, steps per second: 70, episode reward: -126.771, mean reward: -0.880 [-100.000, 15.450], mean action: 1.778 [0.000, 3.000], mean observation: -0.058 [-0.948, 3.388], loss: 1.942004, mean_absolute_error: 28.867963, mean_q: 16.221646, mean_eps: 0.739149
  289994/2000000: episode: 2953, duration: 1.240s, episode steps: 86, steps per second: 69, episode reward: -142.934, mean reward: -1.662 [-100.000, 11.321], mean action: 1.547 [0.000, 3.000], mean observation: 0.100 [-1.284, 4.573], loss: 2.253255, mean_absolute_error: 30.089963, mean_q: 13.535422, mean_eps: 0.739045
  290107/2000000: episode: 2954, duration: 1.573s, episode steps: 113, steps per second: 72, episode reward: -132.476, mean reward: -1.172 [-100.000, 13.296], mean action: 1.522 [0.000, 3.000], mean observation: -0.115 [-3.806, 1.000], loss: 1.657660, mean_absolute_error: 29.449986, mean_q: 17.443072, mean_eps: 0.738955
  290204/2000000: episode: 2955, duration: 1.397s, episode steps: 97, steps per second: 69, episode reward: -97.736, mean reward: -1.008 [-100.000, 22.481], mean action: 1.680 [0.000, 3.000], mean observation: -0.154 [-1.450, 1.386], loss: 1.088577, mean_absolute_error: 28.633587, mean_q: 18.404320, mean_eps: 0.738861
  290280/2000000: episode: 2956, duration: 1.125s, episode steps: 76, steps per second: 68, episode reward: -118.487, mean reward: -1.559 [-100.000, 6.324], mean action: 1.395 [0.000, 3.000], mean observation: 0.028 [-4.047, 1.000], loss: 1.770655, mean_absolute_error: 30.444730, mean_q: 18.102383, mean_eps: 0.738784
  290419/2000000: episode: 2957, duration: 1.978s, episode steps: 139, steps per second: 70, episode reward: -97.623, mean reward: -0.702 [-100.000, 14.525], mean action: 1.590 [0.000, 3.000], mean observation: 0.038 [-1.186, 3.950], loss: 2.005301, mean_absolute_error: 29.664276, mean_q: 14.632493, mean_eps: 0.738687
  290505/2000000: episode: 2958, duration: 1.265s, episode steps: 86, steps per second: 68, episode reward: -97.658, mean reward: -1.136 [-100.000, 6.293], mean action: 1.535 [0.000, 3.000], mean observation: 0.053 [-0.980, 3.398], loss: 2.172366, mean_absolute_error: 30.031105, mean_q: 17.681178, mean_eps: 0.738584
  290681/2000000: episode: 2959, duration: 2.471s, episode steps: 176, steps per second: 71, episode reward: -123.744, mean reward: -0.703 [-100.000, 6.480], mean action: 1.642 [0.000, 3.000], mean observation: -0.010 [-0.912, 3.226], loss: 2.197707, mean_absolute_error: 30.409059, mean_q: 14.583616, mean_eps: 0.738465
  290810/2000000: episode: 2960, duration: 1.799s, episode steps: 129, steps per second: 72, episode reward: -192.678, mean reward: -1.494 [-100.000, 5.484], mean action: 1.705 [0.000, 3.000], mean observation: -0.117 [-1.222, 3.071], loss: 1.570992, mean_absolute_error: 28.446776, mean_q: 18.879836, mean_eps: 0.738329
  290949/2000000: episode: 2961, duration: 1.953s, episode steps: 139, steps per second: 71, episode reward: -106.943, mean reward: -0.769 [-100.000, 47.618], mean action: 1.576 [0.000, 3.000], mean observation: -0.098 [-1.922, 1.265], loss: 1.475240, mean_absolute_error: 29.078031, mean_q: 16.623496, mean_eps: 0.738208
  291018/2000000: episode: 2962, duration: 0.966s, episode steps: 69, steps per second: 71, episode reward: -149.905, mean reward: -2.173 [-100.000, 23.476], mean action: 1.362 [0.000, 3.000], mean observation: -0.034 [-1.474, 5.125], loss: 2.668132, mean_absolute_error: 30.312402, mean_q: 16.499838, mean_eps: 0.738114
  291127/2000000: episode: 2963, duration: 1.524s, episode steps: 109, steps per second: 72, episode reward: -123.442, mean reward: -1.132 [-100.000, 16.850], mean action: 1.752 [0.000, 3.000], mean observation: -0.143 [-0.993, 1.000], loss: 1.574999, mean_absolute_error: 29.116358, mean_q: 19.004201, mean_eps: 0.738035
  291264/2000000: episode: 2964, duration: 1.989s, episode steps: 137, steps per second: 69, episode reward: -126.061, mean reward: -0.920 [-100.000, 6.982], mean action: 1.569 [0.000, 3.000], mean observation: 0.088 [-3.728, 1.013], loss: 1.805237, mean_absolute_error: 29.854482, mean_q: 16.164332, mean_eps: 0.737925
  291355/2000000: episode: 2965, duration: 1.284s, episode steps: 91, steps per second: 71, episode reward: -112.220, mean reward: -1.233 [-100.000, 17.052], mean action: 1.582 [0.000, 3.000], mean observation: 0.009 [-1.164, 1.000], loss: 1.797248, mean_absolute_error: 29.041166, mean_q: 19.259074, mean_eps: 0.737823
  291446/2000000: episode: 2966, duration: 1.279s, episode steps: 91, steps per second: 71, episode reward: -175.114, mean reward: -1.924 [-100.000, 11.982], mean action: 1.703 [0.000, 3.000], mean observation: 0.015 [-4.365, 1.000], loss: 1.874663, mean_absolute_error: 29.546875, mean_q: 15.699778, mean_eps: 0.737740
  291545/2000000: episode: 2967, duration: 1.392s, episode steps: 99, steps per second: 71, episode reward: -132.505, mean reward: -1.338 [-100.000, 10.790], mean action: 1.677 [0.000, 3.000], mean observation: -0.076 [-1.238, 1.000], loss: 1.564067, mean_absolute_error: 29.624253, mean_q: 17.718640, mean_eps: 0.737654
  291620/2000000: episode: 2968, duration: 1.075s, episode steps: 75, steps per second: 70, episode reward: -79.765, mean reward: -1.064 [-100.000, 13.111], mean action: 1.667 [0.000, 3.000], mean observation: 0.020 [-1.145, 1.000], loss: 2.101492, mean_absolute_error: 29.613626, mean_q: 17.480310, mean_eps: 0.737576
  291717/2000000: episode: 2969, duration: 1.416s, episode steps: 97, steps per second: 68, episode reward: -136.293, mean reward: -1.405 [-100.000, 8.480], mean action: 1.732 [0.000, 3.000], mean observation: -0.046 [-4.467, 1.000], loss: 1.661431, mean_absolute_error: 29.649027, mean_q: 16.401885, mean_eps: 0.737499
  291815/2000000: episode: 2970, duration: 1.355s, episode steps: 98, steps per second: 72, episode reward: -99.013, mean reward: -1.010 [-100.000, 18.609], mean action: 1.643 [0.000, 3.000], mean observation: -0.164 [-1.095, 3.072], loss: 1.571365, mean_absolute_error: 28.327343, mean_q: 17.433033, mean_eps: 0.737411
  291936/2000000: episode: 2971, duration: 1.750s, episode steps: 121, steps per second: 69, episode reward: -111.512, mean reward: -0.922 [-100.000, 11.900], mean action: 1.579 [0.000, 3.000], mean observation: -0.032 [-1.218, 1.000], loss: 1.486617, mean_absolute_error: 29.911818, mean_q: 18.405589, mean_eps: 0.737313
  292014/2000000: episode: 2972, duration: 1.130s, episode steps: 78, steps per second: 69, episode reward: -99.168, mean reward: -1.271 [-100.000, 6.384], mean action: 1.744 [0.000, 3.000], mean observation: -0.087 [-1.087, 3.951], loss: 2.124361, mean_absolute_error: 29.239214, mean_q: 18.528028, mean_eps: 0.737223
  292135/2000000: episode: 2973, duration: 1.673s, episode steps: 121, steps per second: 72, episode reward: -103.047, mean reward: -0.852 [-100.000, 9.942], mean action: 1.694 [0.000, 3.000], mean observation: -0.052 [-1.033, 3.708], loss: 1.325254, mean_absolute_error: 29.272174, mean_q: 17.343914, mean_eps: 0.737133
  292233/2000000: episode: 2974, duration: 1.392s, episode steps: 98, steps per second: 70, episode reward: -167.300, mean reward: -1.707 [-100.000, 14.590], mean action: 1.500 [0.000, 3.000], mean observation: 0.099 [-2.563, 1.000], loss: 1.417210, mean_absolute_error: 30.408257, mean_q: 15.968274, mean_eps: 0.737034
  292298/2000000: episode: 2975, duration: 0.919s, episode steps: 65, steps per second: 71, episode reward: -107.436, mean reward: -1.653 [-100.000, 9.021], mean action: 1.738 [0.000, 3.000], mean observation: -0.009 [-4.440, 1.000], loss: 1.047505, mean_absolute_error: 28.327627, mean_q: 19.644390, mean_eps: 0.736961
  292390/2000000: episode: 2976, duration: 1.295s, episode steps: 92, steps per second: 71, episode reward: -186.113, mean reward: -2.023 [-100.000, 26.876], mean action: 1.761 [0.000, 3.000], mean observation: -0.126 [-1.298, 1.467], loss: 1.682998, mean_absolute_error: 29.435527, mean_q: 18.472233, mean_eps: 0.736890
  292554/2000000: episode: 2977, duration: 2.324s, episode steps: 164, steps per second: 71, episode reward: -130.989, mean reward: -0.799 [-100.000, 9.681], mean action: 1.695 [0.000, 3.000], mean observation: 0.135 [-2.703, 1.000], loss: 1.237338, mean_absolute_error: 29.013239, mean_q: 18.942883, mean_eps: 0.736775
  292623/2000000: episode: 2978, duration: 0.983s, episode steps: 69, steps per second: 70, episode reward: -130.449, mean reward: -1.891 [-100.000, 9.215], mean action: 1.623 [0.000, 3.000], mean observation: -0.154 [-1.366, 4.329], loss: 1.291339, mean_absolute_error: 30.357051, mean_q: 16.483302, mean_eps: 0.736671
  292728/2000000: episode: 2979, duration: 1.498s, episode steps: 105, steps per second: 70, episode reward: -131.958, mean reward: -1.257 [-100.000, 15.928], mean action: 1.514 [0.000, 3.000], mean observation: 0.141 [-1.155, 1.000], loss: 1.302830, mean_absolute_error: 29.029883, mean_q: 17.059543, mean_eps: 0.736593
  292855/2000000: episode: 2980, duration: 1.799s, episode steps: 127, steps per second: 71, episode reward: -84.502, mean reward: -0.665 [-100.000, 11.850], mean action: 1.606 [0.000, 3.000], mean observation: 0.096 [-0.838, 3.082], loss: 1.603374, mean_absolute_error: 28.592207, mean_q: 17.774284, mean_eps: 0.736489
  292961/2000000: episode: 2981, duration: 1.524s, episode steps: 106, steps per second: 70, episode reward: -91.279, mean reward: -0.861 [-100.000, 16.952], mean action: 1.491 [0.000, 3.000], mean observation: -0.027 [-1.156, 1.000], loss: 1.419794, mean_absolute_error: 28.929903, mean_q: 17.159951, mean_eps: 0.736383
  293045/2000000: episode: 2982, duration: 1.185s, episode steps: 84, steps per second: 71, episode reward: -119.115, mean reward: -1.418 [-100.000, 20.305], mean action: 1.619 [0.000, 3.000], mean observation: 0.044 [-1.345, 1.000], loss: 2.822914, mean_absolute_error: 31.124495, mean_q: 15.055991, mean_eps: 0.736296
  293153/2000000: episode: 2983, duration: 1.519s, episode steps: 108, steps per second: 71, episode reward: -121.095, mean reward: -1.121 [-100.000, 12.940], mean action: 1.481 [0.000, 3.000], mean observation: 0.035 [-1.203, 1.000], loss: 1.842084, mean_absolute_error: 29.519856, mean_q: 14.638540, mean_eps: 0.736210
  293249/2000000: episode: 2984, duration: 1.381s, episode steps: 96, steps per second: 70, episode reward: -99.298, mean reward: -1.034 [-100.000, 11.397], mean action: 1.688 [0.000, 3.000], mean observation: -0.094 [-0.945, 3.347], loss: 1.503905, mean_absolute_error: 29.540147, mean_q: 16.549673, mean_eps: 0.736118
  293410/2000000: episode: 2985, duration: 2.269s, episode steps: 161, steps per second: 71, episode reward: -144.193, mean reward: -0.896 [-100.000, 12.968], mean action: 1.491 [0.000, 3.000], mean observation: 0.190 [-2.942, 1.000], loss: 1.199867, mean_absolute_error: 28.738753, mean_q: 18.868997, mean_eps: 0.736003
  293559/2000000: episode: 2986, duration: 2.110s, episode steps: 149, steps per second: 71, episode reward: -111.568, mean reward: -0.749 [-100.000, 10.447], mean action: 1.497 [0.000, 3.000], mean observation: 0.189 [-2.821, 1.009], loss: 1.428237, mean_absolute_error: 29.817602, mean_q: 14.805820, mean_eps: 0.735864
  293638/2000000: episode: 2987, duration: 1.134s, episode steps: 79, steps per second: 70, episode reward: -153.040, mean reward: -1.937 [-100.000, 7.122], mean action: 1.835 [0.000, 3.000], mean observation: -0.186 [-1.237, 4.012], loss: 1.332502, mean_absolute_error: 28.057497, mean_q: 19.072632, mean_eps: 0.735762
  293755/2000000: episode: 2988, duration: 1.663s, episode steps: 117, steps per second: 70, episode reward: -112.016, mean reward: -0.957 [-100.000, 17.422], mean action: 1.624 [0.000, 3.000], mean observation: 0.057 [-1.046, 1.000], loss: 2.104337, mean_absolute_error: 30.768921, mean_q: 16.538671, mean_eps: 0.735674
  293882/2000000: episode: 2989, duration: 1.811s, episode steps: 127, steps per second: 70, episode reward: -198.039, mean reward: -1.559 [-100.000, 5.496], mean action: 1.583 [0.000, 3.000], mean observation: -0.108 [-1.434, 4.431], loss: 1.931722, mean_absolute_error: 30.481376, mean_q: 16.359806, mean_eps: 0.735564
  293994/2000000: episode: 2990, duration: 1.610s, episode steps: 112, steps per second: 70, episode reward: -107.377, mean reward: -0.959 [-100.000, 6.639], mean action: 1.545 [0.000, 3.000], mean observation: 0.012 [-3.497, 1.000], loss: 1.781390, mean_absolute_error: 30.142096, mean_q: 17.351323, mean_eps: 0.735456
  294089/2000000: episode: 2991, duration: 1.376s, episode steps: 95, steps per second: 69, episode reward: -140.262, mean reward: -1.476 [-100.000, 5.571], mean action: 1.789 [0.000, 3.000], mean observation: -0.088 [-1.185, 3.827], loss: 2.568201, mean_absolute_error: 30.113374, mean_q: 16.603192, mean_eps: 0.735362
  294158/2000000: episode: 2992, duration: 1.116s, episode steps: 69, steps per second: 62, episode reward: -112.226, mean reward: -1.626 [-100.000, 9.213], mean action: 1.565 [0.000, 3.000], mean observation: -0.140 [-1.296, 2.524], loss: 1.826521, mean_absolute_error: 29.138191, mean_q: 16.017053, mean_eps: 0.735288
  294245/2000000: episode: 2993, duration: 1.243s, episode steps: 87, steps per second: 70, episode reward: -145.574, mean reward: -1.673 [-100.000, 16.649], mean action: 1.655 [0.000, 3.000], mean observation: 0.032 [-4.020, 1.000], loss: 1.943342, mean_absolute_error: 30.497792, mean_q: 15.469466, mean_eps: 0.735218
  294386/2000000: episode: 2994, duration: 2.014s, episode steps: 141, steps per second: 70, episode reward: -114.639, mean reward: -0.813 [-100.000, 33.284], mean action: 1.823 [0.000, 3.000], mean observation: -0.083 [-1.258, 3.001], loss: 1.327318, mean_absolute_error: 29.976499, mean_q: 14.220451, mean_eps: 0.735116
  294506/2000000: episode: 2995, duration: 1.725s, episode steps: 120, steps per second: 70, episode reward: -187.423, mean reward: -1.562 [-100.000, 8.158], mean action: 1.667 [0.000, 3.000], mean observation: 0.003 [-4.274, 1.000], loss: 1.939059, mean_absolute_error: 30.867475, mean_q: 15.273975, mean_eps: 0.734999
  294620/2000000: episode: 2996, duration: 1.663s, episode steps: 114, steps per second: 69, episode reward: -126.448, mean reward: -1.109 [-100.000, 6.794], mean action: 1.684 [0.000, 3.000], mean observation: -0.008 [-4.076, 1.000], loss: 1.204385, mean_absolute_error: 28.269619, mean_q: 19.019116, mean_eps: 0.734894
  294722/2000000: episode: 2997, duration: 1.483s, episode steps: 102, steps per second: 69, episode reward: -128.994, mean reward: -1.265 [-100.000, 8.417], mean action: 1.549 [0.000, 3.000], mean observation: 0.027 [-3.740, 1.000], loss: 1.263584, mean_absolute_error: 30.376085, mean_q: 18.120366, mean_eps: 0.734797
  294828/2000000: episode: 2998, duration: 1.515s, episode steps: 106, steps per second: 70, episode reward: -74.872, mean reward: -0.706 [-100.000, 14.316], mean action: 1.698 [0.000, 3.000], mean observation: -0.031 [-0.897, 2.701], loss: 1.745319, mean_absolute_error: 29.204401, mean_q: 19.373878, mean_eps: 0.734703
  294959/2000000: episode: 2999, duration: 1.877s, episode steps: 131, steps per second: 70, episode reward: -91.100, mean reward: -0.695 [-100.000, 7.190], mean action: 1.603 [0.000, 3.000], mean observation: 0.011 [-2.568, 1.000], loss: 2.049636, mean_absolute_error: 30.021007, mean_q: 17.284102, mean_eps: 0.734597
  295095/2000000: episode: 3000, duration: 1.933s, episode steps: 136, steps per second: 70, episode reward: -75.645, mean reward: -0.556 [-100.000, 14.699], mean action: 1.581 [0.000, 3.000], mean observation: 0.037 [-0.906, 1.000], loss: 1.193626, mean_absolute_error: 29.592293, mean_q: 17.016833, mean_eps: 0.734477
  295185/2000000: episode: 3001, duration: 1.307s, episode steps: 90, steps per second: 69, episode reward: -133.859, mean reward: -1.487 [-100.000, 15.052], mean action: 1.522 [0.000, 3.000], mean observation: 0.119 [-1.093, 1.000], loss: 2.121536, mean_absolute_error: 29.213689, mean_q: 16.118541, mean_eps: 0.734374
  295283/2000000: episode: 3002, duration: 1.430s, episode steps: 98, steps per second: 69, episode reward: -119.737, mean reward: -1.222 [-100.000, 9.439], mean action: 1.653 [0.000, 3.000], mean observation: -0.109 [-3.839, 1.000], loss: 1.135955, mean_absolute_error: 28.376936, mean_q: 16.628981, mean_eps: 0.734289
  295378/2000000: episode: 3003, duration: 1.376s, episode steps: 95, steps per second: 69, episode reward: -124.347, mean reward: -1.309 [-100.000, 21.748], mean action: 1.611 [0.000, 3.000], mean observation: 0.119 [-3.928, 1.000], loss: 1.453619, mean_absolute_error: 28.577590, mean_q: 17.563389, mean_eps: 0.734203
  295509/2000000: episode: 3004, duration: 1.872s, episode steps: 131, steps per second: 70, episode reward: -110.457, mean reward: -0.843 [-100.000, 22.104], mean action: 1.702 [0.000, 3.000], mean observation: 0.103 [-1.022, 1.000], loss: 2.159556, mean_absolute_error: 29.489029, mean_q: 15.474101, mean_eps: 0.734100
  295595/2000000: episode: 3005, duration: 1.216s, episode steps: 86, steps per second: 71, episode reward: -151.542, mean reward: -1.762 [-100.000, 11.296], mean action: 1.488 [0.000, 3.000], mean observation: 0.100 [-3.321, 1.000], loss: 1.417647, mean_absolute_error: 28.432672, mean_q: 19.705686, mean_eps: 0.734003
  295686/2000000: episode: 3006, duration: 1.391s, episode steps: 91, steps per second: 65, episode reward: -159.868, mean reward: -1.757 [-100.000, 17.417], mean action: 1.824 [0.000, 3.000], mean observation: -0.134 [-1.108, 3.615], loss: 1.374170, mean_absolute_error: 28.683451, mean_q: 16.328165, mean_eps: 0.733924
  295763/2000000: episode: 3007, duration: 1.260s, episode steps: 77, steps per second: 61, episode reward: -142.536, mean reward: -1.851 [-100.000, 9.806], mean action: 1.481 [0.000, 3.000], mean observation: 0.058 [-4.051, 1.000], loss: 1.738881, mean_absolute_error: 30.242610, mean_q: 13.768971, mean_eps: 0.733848
  295846/2000000: episode: 3008, duration: 1.309s, episode steps: 83, steps per second: 63, episode reward: -88.399, mean reward: -1.065 [-100.000, 11.540], mean action: 1.578 [0.000, 3.000], mean observation: 0.019 [-3.385, 1.000], loss: 1.889172, mean_absolute_error: 29.807608, mean_q: 14.848877, mean_eps: 0.733776
  295950/2000000: episode: 3009, duration: 1.521s, episode steps: 104, steps per second: 68, episode reward: -120.011, mean reward: -1.154 [-100.000, 9.478], mean action: 1.635 [0.000, 3.000], mean observation: 0.002 [-1.033, 3.509], loss: 1.961436, mean_absolute_error: 29.780911, mean_q: 18.338498, mean_eps: 0.733692
  296040/2000000: episode: 3010, duration: 1.303s, episode steps: 90, steps per second: 69, episode reward: -127.569, mean reward: -1.417 [-100.000, 7.903], mean action: 1.500 [0.000, 3.000], mean observation: -0.069 [-1.284, 4.642], loss: 1.528046, mean_absolute_error: 29.176546, mean_q: 19.584709, mean_eps: 0.733605
  296109/2000000: episode: 3011, duration: 1.023s, episode steps: 69, steps per second: 67, episode reward: -134.652, mean reward: -1.951 [-100.000, 9.829], mean action: 1.609 [0.000, 3.000], mean observation: -0.135 [-1.346, 1.000], loss: 1.511041, mean_absolute_error: 29.488017, mean_q: 18.044085, mean_eps: 0.733533
  296219/2000000: episode: 3012, duration: 1.554s, episode steps: 110, steps per second: 71, episode reward: -108.202, mean reward: -0.984 [-100.000, 8.494], mean action: 1.555 [0.000, 3.000], mean observation: -0.024 [-1.109, 4.060], loss: 1.769400, mean_absolute_error: 29.031860, mean_q: 17.765461, mean_eps: 0.733452
  296305/2000000: episode: 3013, duration: 1.265s, episode steps: 86, steps per second: 68, episode reward: -123.726, mean reward: -1.439 [-100.000, 11.742], mean action: 1.709 [0.000, 3.000], mean observation: -0.048 [-1.339, 1.000], loss: 1.711955, mean_absolute_error: 29.517129, mean_q: 17.154208, mean_eps: 0.733364
  296401/2000000: episode: 3014, duration: 1.365s, episode steps: 96, steps per second: 70, episode reward: -138.190, mean reward: -1.439 [-100.000, 9.278], mean action: 1.573 [0.000, 3.000], mean observation: 0.002 [-4.081, 1.000], loss: 1.844313, mean_absolute_error: 29.088438, mean_q: 19.132380, mean_eps: 0.733281
  296519/2000000: episode: 3015, duration: 1.726s, episode steps: 118, steps per second: 68, episode reward: -110.384, mean reward: -0.935 [-100.000, 10.282], mean action: 1.432 [0.000, 3.000], mean observation: 0.158 [-3.130, 1.009], loss: 1.743073, mean_absolute_error: 30.333082, mean_q: 15.834912, mean_eps: 0.733186
  296592/2000000: episode: 3016, duration: 1.101s, episode steps: 73, steps per second: 66, episode reward: -105.600, mean reward: -1.447 [-100.000, 14.180], mean action: 1.753 [0.000, 3.000], mean observation: -0.138 [-1.159, 1.000], loss: 1.944866, mean_absolute_error: 29.876795, mean_q: 15.707361, mean_eps: 0.733101
  296699/2000000: episode: 3017, duration: 1.550s, episode steps: 107, steps per second: 69, episode reward: -112.295, mean reward: -1.049 [-100.000, 16.034], mean action: 1.542 [0.000, 3.000], mean observation: -0.010 [-1.102, 1.000], loss: 1.211394, mean_absolute_error: 29.829127, mean_q: 16.127746, mean_eps: 0.733020
  296798/2000000: episode: 3018, duration: 1.424s, episode steps: 99, steps per second: 70, episode reward: -119.614, mean reward: -1.208 [-100.000, 11.973], mean action: 1.657 [0.000, 3.000], mean observation: -0.062 [-1.190, 3.666], loss: 1.696620, mean_absolute_error: 29.079310, mean_q: 20.591379, mean_eps: 0.732927
  296892/2000000: episode: 3019, duration: 1.371s, episode steps: 94, steps per second: 69, episode reward: -105.742, mean reward: -1.125 [-100.000, 8.962], mean action: 1.734 [0.000, 3.000], mean observation: -0.127 [-3.129, 1.000], loss: 1.734966, mean_absolute_error: 29.687358, mean_q: 16.111277, mean_eps: 0.732840
  296995/2000000: episode: 3020, duration: 1.482s, episode steps: 103, steps per second: 69, episode reward: -108.839, mean reward: -1.057 [-100.000, 5.960], mean action: 1.641 [0.000, 3.000], mean observation: -0.089 [-0.877, 1.749], loss: 2.241721, mean_absolute_error: 30.556257, mean_q: 17.707210, mean_eps: 0.732752
  297068/2000000: episode: 3021, duration: 1.084s, episode steps: 73, steps per second: 67, episode reward: -127.124, mean reward: -1.741 [-100.000, 9.421], mean action: 1.616 [0.000, 3.000], mean observation: 0.043 [-4.756, 1.000], loss: 1.316008, mean_absolute_error: 28.666869, mean_q: 17.783243, mean_eps: 0.732673
  297131/2000000: episode: 3022, duration: 0.903s, episode steps: 63, steps per second: 70, episode reward: -101.869, mean reward: -1.617 [-100.000, 15.715], mean action: 1.571 [0.000, 3.000], mean observation: -0.163 [-4.524, 1.000], loss: 1.267921, mean_absolute_error: 29.879470, mean_q: 15.779385, mean_eps: 0.732612
  297239/2000000: episode: 3023, duration: 1.518s, episode steps: 108, steps per second: 71, episode reward: -95.834, mean reward: -0.887 [-100.000, 6.959], mean action: 1.676 [0.000, 3.000], mean observation: 0.082 [-0.900, 3.520], loss: 1.645480, mean_absolute_error: 30.300491, mean_q: 15.568602, mean_eps: 0.732534
  297351/2000000: episode: 3024, duration: 1.618s, episode steps: 112, steps per second: 69, episode reward: -159.789, mean reward: -1.427 [-100.000, 10.615], mean action: 1.589 [0.000, 3.000], mean observation: 0.152 [-3.123, 1.000], loss: 1.614760, mean_absolute_error: 29.518455, mean_q: 17.092846, mean_eps: 0.732435
  297431/2000000: episode: 3025, duration: 1.153s, episode steps: 80, steps per second: 69, episode reward: -116.276, mean reward: -1.453 [-100.000, 8.290], mean action: 1.650 [0.000, 3.000], mean observation: 0.078 [-1.226, 4.448], loss: 1.472858, mean_absolute_error: 29.205218, mean_q: 19.126818, mean_eps: 0.732349
  297514/2000000: episode: 3026, duration: 1.187s, episode steps: 83, steps per second: 70, episode reward: -122.584, mean reward: -1.477 [-100.000, 11.917], mean action: 1.410 [0.000, 3.000], mean observation: 0.083 [-3.718, 1.000], loss: 1.779675, mean_absolute_error: 30.227852, mean_q: 14.501162, mean_eps: 0.732275
  297626/2000000: episode: 3027, duration: 1.581s, episode steps: 112, steps per second: 71, episode reward: -348.907, mean reward: -3.115 [-100.000, 93.127], mean action: 1.705 [0.000, 3.000], mean observation: -0.248 [-3.330, 1.000], loss: 1.894964, mean_absolute_error: 30.374524, mean_q: 16.223519, mean_eps: 0.732187
  297724/2000000: episode: 3028, duration: 1.425s, episode steps: 98, steps per second: 69, episode reward: -113.769, mean reward: -1.161 [-100.000, 13.006], mean action: 1.724 [0.000, 3.000], mean observation: -0.098 [-1.059, 2.875], loss: 2.353003, mean_absolute_error: 31.349511, mean_q: 16.416383, mean_eps: 0.732093
  297905/2000000: episode: 3029, duration: 2.568s, episode steps: 181, steps per second: 70, episode reward: -164.463, mean reward: -0.909 [-100.000, 7.287], mean action: 1.713 [0.000, 3.000], mean observation: 0.063 [-1.117, 4.253], loss: 1.718280, mean_absolute_error: 29.191355, mean_q: 18.065460, mean_eps: 0.731967
  298015/2000000: episode: 3030, duration: 1.564s, episode steps: 110, steps per second: 70, episode reward: -85.932, mean reward: -0.781 [-100.000, 11.897], mean action: 1.673 [0.000, 3.000], mean observation: 0.047 [-2.434, 1.000], loss: 1.222448, mean_absolute_error: 29.665894, mean_q: 16.129726, mean_eps: 0.731836
  298108/2000000: episode: 3031, duration: 1.367s, episode steps: 93, steps per second: 68, episode reward: -133.912, mean reward: -1.440 [-100.000, 10.163], mean action: 1.602 [0.000, 3.000], mean observation: -0.107 [-0.998, 1.000], loss: 1.579528, mean_absolute_error: 30.133872, mean_q: 17.282186, mean_eps: 0.731746
  298207/2000000: episode: 3032, duration: 1.414s, episode steps: 99, steps per second: 70, episode reward: -140.064, mean reward: -1.415 [-100.000, 6.727], mean action: 1.606 [0.000, 3.000], mean observation: 0.088 [-1.215, 4.509], loss: 1.160958, mean_absolute_error: 29.249216, mean_q: 17.800505, mean_eps: 0.731660
  298313/2000000: episode: 3033, duration: 1.544s, episode steps: 106, steps per second: 69, episode reward: -113.088, mean reward: -1.067 [-100.000, 10.766], mean action: 1.462 [0.000, 3.000], mean observation: 0.058 [-1.066, 1.000], loss: 1.895542, mean_absolute_error: 28.095631, mean_q: 19.229161, mean_eps: 0.731566
  298398/2000000: episode: 3034, duration: 1.186s, episode steps: 85, steps per second: 72, episode reward: -64.054, mean reward: -0.754 [-100.000, 18.811], mean action: 1.588 [0.000, 3.000], mean observation: 0.001 [-1.009, 2.975], loss: 1.382188, mean_absolute_error: 29.989535, mean_q: 16.025141, mean_eps: 0.731480
  298505/2000000: episode: 3035, duration: 1.542s, episode steps: 107, steps per second: 69, episode reward: -208.067, mean reward: -1.945 [-100.000, 4.099], mean action: 1.598 [0.000, 3.000], mean observation: 0.171 [-1.101, 1.056], loss: 1.671145, mean_absolute_error: 28.548316, mean_q: 17.937839, mean_eps: 0.731393
  298586/2000000: episode: 3036, duration: 1.149s, episode steps: 81, steps per second: 70, episode reward: -125.909, mean reward: -1.554 [-100.000, 6.437], mean action: 1.679 [0.000, 3.000], mean observation: -0.157 [-1.331, 4.342], loss: 2.320428, mean_absolute_error: 31.323677, mean_q: 16.096278, mean_eps: 0.731309
  298685/2000000: episode: 3037, duration: 1.434s, episode steps: 99, steps per second: 69, episode reward: -128.887, mean reward: -1.302 [-100.000, 6.805], mean action: 1.424 [0.000, 3.000], mean observation: 0.085 [-1.219, 4.173], loss: 1.929709, mean_absolute_error: 29.695795, mean_q: 15.170514, mean_eps: 0.731228
  298760/2000000: episode: 3038, duration: 1.082s, episode steps: 75, steps per second: 69, episode reward: -136.876, mean reward: -1.825 [-100.000, 9.729], mean action: 1.440 [0.000, 3.000], mean observation: 0.092 [-4.000, 1.000], loss: 2.511232, mean_absolute_error: 31.780728, mean_q: 15.814516, mean_eps: 0.731150
  298871/2000000: episode: 3039, duration: 1.611s, episode steps: 111, steps per second: 69, episode reward: -130.860, mean reward: -1.179 [-100.000, 6.305], mean action: 1.577 [0.000, 3.000], mean observation: -0.078 [-4.115, 1.000], loss: 1.478387, mean_absolute_error: 28.998695, mean_q: 17.872678, mean_eps: 0.731067
  298969/2000000: episode: 3040, duration: 1.432s, episode steps: 98, steps per second: 68, episode reward: -128.843, mean reward: -1.315 [-100.000, 10.736], mean action: 1.449 [0.000, 3.000], mean observation: 0.108 [-1.012, 1.000], loss: 2.243326, mean_absolute_error: 30.195574, mean_q: 15.716485, mean_eps: 0.730972
  299079/2000000: episode: 3041, duration: 1.512s, episode steps: 110, steps per second: 73, episode reward: -123.915, mean reward: -1.127 [-100.000, 10.426], mean action: 1.618 [0.000, 3.000], mean observation: -0.004 [-1.291, 4.272], loss: 1.590465, mean_absolute_error: 28.534084, mean_q: 18.862082, mean_eps: 0.730878
  299190/2000000: episode: 3042, duration: 1.618s, episode steps: 111, steps per second: 69, episode reward: -111.687, mean reward: -1.006 [-100.000, 11.677], mean action: 1.622 [0.000, 3.000], mean observation: 0.038 [-3.741, 1.000], loss: 1.944376, mean_absolute_error: 28.991240, mean_q: 16.445467, mean_eps: 0.730779
  299317/2000000: episode: 3043, duration: 1.810s, episode steps: 127, steps per second: 70, episode reward: -221.604, mean reward: -1.745 [-100.000, 30.133], mean action: 1.654 [0.000, 3.000], mean observation: 0.215 [-0.913, 2.741], loss: 1.524650, mean_absolute_error: 30.540357, mean_q: 16.630207, mean_eps: 0.730671
  299404/2000000: episode: 3044, duration: 1.252s, episode steps: 87, steps per second: 69, episode reward: -130.897, mean reward: -1.505 [-100.000, 6.655], mean action: 1.655 [0.000, 3.000], mean observation: 0.100 [-3.382, 1.000], loss: 1.356224, mean_absolute_error: 28.406731, mean_q: 18.350907, mean_eps: 0.730576
  299519/2000000: episode: 3045, duration: 1.660s, episode steps: 115, steps per second: 69, episode reward: -127.552, mean reward: -1.109 [-100.000, 13.520], mean action: 1.539 [0.000, 3.000], mean observation: 0.017 [-3.913, 1.000], loss: 1.787533, mean_absolute_error: 28.798120, mean_q: 17.372170, mean_eps: 0.730486
  299586/2000000: episode: 3046, duration: 0.972s, episode steps: 67, steps per second: 69, episode reward: -108.531, mean reward: -1.620 [-100.000, 7.565], mean action: 1.627 [0.000, 3.000], mean observation: -0.144 [-1.325, 4.017], loss: 2.582469, mean_absolute_error: 31.811059, mean_q: 14.267708, mean_eps: 0.730403
  299655/2000000: episode: 3047, duration: 0.981s, episode steps: 69, steps per second: 70, episode reward: -138.809, mean reward: -2.012 [-100.000, 7.604], mean action: 1.565 [0.000, 3.000], mean observation: -0.070 [-1.462, 1.000], loss: 1.695631, mean_absolute_error: 28.415280, mean_q: 16.232999, mean_eps: 0.730342
  299805/2000000: episode: 3048, duration: 2.135s, episode steps: 150, steps per second: 70, episode reward: -68.318, mean reward: -0.455 [-100.000, 13.613], mean action: 1.507 [0.000, 3.000], mean observation: 0.046 [-0.746, 1.000], loss: 1.090238, mean_absolute_error: 28.635722, mean_q: 17.322005, mean_eps: 0.730243
  299918/2000000: episode: 3049, duration: 1.617s, episode steps: 113, steps per second: 70, episode reward: -162.509, mean reward: -1.438 [-100.000, 12.412], mean action: 1.708 [0.000, 3.000], mean observation: -0.078 [-1.239, 4.188], loss: 1.877958, mean_absolute_error: 29.615026, mean_q: 14.522533, mean_eps: 0.730124
  300031/2000000: episode: 3050, duration: 1.668s, episode steps: 113, steps per second: 68, episode reward: -120.126, mean reward: -1.063 [-100.000, 8.318], mean action: 1.779 [0.000, 3.000], mean observation: -0.108 [-0.985, 1.000], loss: 2.114710, mean_absolute_error: 30.659260, mean_q: 15.973972, mean_eps: 0.730023
  300137/2000000: episode: 3051, duration: 1.655s, episode steps: 106, steps per second: 64, episode reward: -140.147, mean reward: -1.322 [-100.000, 11.729], mean action: 1.557 [0.000, 3.000], mean observation: 0.072 [-3.717, 1.000], loss: 1.691421, mean_absolute_error: 29.790051, mean_q: 19.084375, mean_eps: 0.729924
  300272/2000000: episode: 3052, duration: 2.011s, episode steps: 135, steps per second: 67, episode reward: -94.980, mean reward: -0.704 [-100.000, 13.383], mean action: 1.696 [0.000, 3.000], mean observation: -0.076 [-1.100, 2.110], loss: 1.298596, mean_absolute_error: 28.250024, mean_q: 20.357670, mean_eps: 0.729816
  300355/2000000: episode: 3053, duration: 1.283s, episode steps: 83, steps per second: 65, episode reward: -116.610, mean reward: -1.405 [-100.000, 20.324], mean action: 1.723 [0.000, 3.000], mean observation: -0.140 [-1.238, 1.000], loss: 1.641614, mean_absolute_error: 31.386785, mean_q: 14.291395, mean_eps: 0.729719
  300456/2000000: episode: 3054, duration: 1.480s, episode steps: 101, steps per second: 68, episode reward: -139.280, mean reward: -1.379 [-100.000, 10.957], mean action: 1.564 [0.000, 3.000], mean observation: -0.063 [-1.298, 2.679], loss: 1.622318, mean_absolute_error: 30.228957, mean_q: 16.471725, mean_eps: 0.729636
  300568/2000000: episode: 3055, duration: 1.627s, episode steps: 112, steps per second: 69, episode reward: -138.267, mean reward: -1.235 [-100.000, 6.276], mean action: 1.562 [0.000, 3.000], mean observation: 0.101 [-3.376, 1.000], loss: 1.151137, mean_absolute_error: 29.906776, mean_q: 18.135056, mean_eps: 0.729541
  300658/2000000: episode: 3056, duration: 1.301s, episode steps: 90, steps per second: 69, episode reward: -132.442, mean reward: -1.472 [-100.000, 10.377], mean action: 1.400 [0.000, 3.000], mean observation: 0.138 [-3.796, 1.000], loss: 1.647402, mean_absolute_error: 30.727632, mean_q: 18.851709, mean_eps: 0.729449
  300769/2000000: episode: 3057, duration: 1.600s, episode steps: 111, steps per second: 69, episode reward: -90.363, mean reward: -0.814 [-100.000, 22.212], mean action: 1.721 [0.000, 3.000], mean observation: 0.038 [-2.607, 1.000], loss: 2.017625, mean_absolute_error: 29.974340, mean_q: 16.694153, mean_eps: 0.729357
  300876/2000000: episode: 3058, duration: 1.538s, episode steps: 107, steps per second: 70, episode reward: -114.625, mean reward: -1.071 [-100.000, 19.265], mean action: 1.551 [0.000, 3.000], mean observation: 0.002 [-1.286, 1.000], loss: 1.846475, mean_absolute_error: 30.948576, mean_q: 16.959320, mean_eps: 0.729260
  300954/2000000: episode: 3059, duration: 1.131s, episode steps: 78, steps per second: 69, episode reward: -107.537, mean reward: -1.379 [-100.000, 17.567], mean action: 1.590 [0.000, 3.000], mean observation: -0.145 [-3.821, 1.000], loss: 1.400131, mean_absolute_error: 28.593963, mean_q: 17.581227, mean_eps: 0.729177
  301060/2000000: episode: 3060, duration: 1.531s, episode steps: 106, steps per second: 69, episode reward: -121.795, mean reward: -1.149 [-100.000, 11.803], mean action: 1.472 [0.000, 3.000], mean observation: -0.032 [-1.378, 1.000], loss: 1.196465, mean_absolute_error: 30.503449, mean_q: 19.313831, mean_eps: 0.729095
  301224/2000000: episode: 3061, duration: 2.494s, episode steps: 164, steps per second: 66, episode reward: -62.311, mean reward: -0.380 [-100.000, 12.313], mean action: 1.524 [0.000, 3.000], mean observation: 0.059 [-0.950, 1.018], loss: 1.838282, mean_absolute_error: 31.163372, mean_q: 16.176514, mean_eps: 0.728974
  301329/2000000: episode: 3062, duration: 1.557s, episode steps: 105, steps per second: 67, episode reward: -132.809, mean reward: -1.265 [-100.000, 6.448], mean action: 1.571 [0.000, 3.000], mean observation: 0.024 [-1.253, 2.753], loss: 1.487670, mean_absolute_error: 29.159694, mean_q: 19.255797, mean_eps: 0.728852
  301418/2000000: episode: 3063, duration: 1.252s, episode steps: 89, steps per second: 71, episode reward: -114.278, mean reward: -1.284 [-100.000, 6.536], mean action: 1.921 [0.000, 3.000], mean observation: -0.094 [-1.024, 1.000], loss: 1.281599, mean_absolute_error: 29.671262, mean_q: 19.464192, mean_eps: 0.728763
  301515/2000000: episode: 3064, duration: 1.376s, episode steps: 97, steps per second: 70, episode reward: -62.355, mean reward: -0.643 [-100.000, 11.065], mean action: 1.526 [0.000, 3.000], mean observation: 0.016 [-0.887, 2.920], loss: 2.573211, mean_absolute_error: 30.122164, mean_q: 18.592302, mean_eps: 0.728681
  301627/2000000: episode: 3065, duration: 1.604s, episode steps: 112, steps per second: 70, episode reward: -130.165, mean reward: -1.162 [-100.000, 9.911], mean action: 1.571 [0.000, 3.000], mean observation: 0.017 [-3.524, 1.000], loss: 1.170707, mean_absolute_error: 29.407534, mean_q: 20.404812, mean_eps: 0.728587
  301741/2000000: episode: 3066, duration: 1.631s, episode steps: 114, steps per second: 70, episode reward: -92.193, mean reward: -0.809 [-100.000, 13.499], mean action: 1.632 [0.000, 3.000], mean observation: 0.017 [-1.152, 1.022], loss: 1.897513, mean_absolute_error: 30.589134, mean_q: 17.856505, mean_eps: 0.728484
  301841/2000000: episode: 3067, duration: 1.432s, episode steps: 100, steps per second: 70, episode reward: -93.087, mean reward: -0.931 [-100.000, 21.691], mean action: 1.750 [0.000, 3.000], mean observation: -0.028 [-1.079, 1.000], loss: 2.179507, mean_absolute_error: 29.880507, mean_q: 17.751115, mean_eps: 0.728387
  301926/2000000: episode: 3068, duration: 1.169s, episode steps: 85, steps per second: 73, episode reward: -121.484, mean reward: -1.429 [-100.000, 10.747], mean action: 1.576 [0.000, 3.000], mean observation: 0.090 [-3.131, 1.000], loss: 1.157261, mean_absolute_error: 30.013649, mean_q: 17.044247, mean_eps: 0.728304
  302072/2000000: episode: 3069, duration: 2.081s, episode steps: 146, steps per second: 70, episode reward: -124.310, mean reward: -0.851 [-100.000, 10.292], mean action: 1.630 [0.000, 3.000], mean observation: -0.064 [-0.812, 1.028], loss: 1.910908, mean_absolute_error: 30.159455, mean_q: 16.095099, mean_eps: 0.728202
  302204/2000000: episode: 3070, duration: 1.940s, episode steps: 132, steps per second: 68, episode reward: -89.224, mean reward: -0.676 [-100.000, 28.214], mean action: 1.674 [0.000, 3.000], mean observation: -0.039 [-1.313, 1.028], loss: 1.562868, mean_absolute_error: 30.203920, mean_q: 18.590418, mean_eps: 0.728078
  302292/2000000: episode: 3071, duration: 1.295s, episode steps: 88, steps per second: 68, episode reward: -128.726, mean reward: -1.463 [-100.000, 14.774], mean action: 1.648 [0.000, 3.000], mean observation: -0.022 [-1.247, 1.000], loss: 1.760587, mean_absolute_error: 29.583353, mean_q: 19.395330, mean_eps: 0.727979
  302421/2000000: episode: 3072, duration: 1.964s, episode steps: 129, steps per second: 66, episode reward: -71.211, mean reward: -0.552 [-100.000, 17.279], mean action: 1.736 [0.000, 3.000], mean observation: -0.013 [-0.826, 1.000], loss: 1.519922, mean_absolute_error: 30.699620, mean_q: 17.000528, mean_eps: 0.727880
  302545/2000000: episode: 3073, duration: 1.749s, episode steps: 124, steps per second: 71, episode reward: -99.648, mean reward: -0.804 [-100.000, 13.255], mean action: 1.476 [0.000, 3.000], mean observation: 0.037 [-1.240, 4.384], loss: 1.302706, mean_absolute_error: 28.337805, mean_q: 20.205430, mean_eps: 0.727764
  302670/2000000: episode: 3074, duration: 1.782s, episode steps: 125, steps per second: 70, episode reward: -129.637, mean reward: -1.037 [-100.000, 12.092], mean action: 1.592 [0.000, 3.000], mean observation: -0.097 [-1.013, 3.595], loss: 1.246813, mean_absolute_error: 29.848455, mean_q: 16.968673, mean_eps: 0.727653
  302765/2000000: episode: 3075, duration: 1.351s, episode steps: 95, steps per second: 70, episode reward: -64.687, mean reward: -0.681 [-100.000, 7.378], mean action: 1.568 [0.000, 3.000], mean observation: -0.050 [-3.025, 1.000], loss: 1.766324, mean_absolute_error: 30.036786, mean_q: 18.373537, mean_eps: 0.727554
  302843/2000000: episode: 3076, duration: 1.103s, episode steps: 78, steps per second: 71, episode reward: -132.754, mean reward: -1.702 [-100.000, 8.533], mean action: 1.436 [0.000, 3.000], mean observation: 0.112 [-1.836, 1.000], loss: 1.380961, mean_absolute_error: 30.083765, mean_q: 16.886135, mean_eps: 0.727476
  302926/2000000: episode: 3077, duration: 1.197s, episode steps: 83, steps per second: 69, episode reward: -83.276, mean reward: -1.003 [-100.000, 17.578], mean action: 1.494 [0.000, 3.000], mean observation: -0.049 [-1.232, 1.000], loss: 2.236581, mean_absolute_error: 32.004650, mean_q: 14.849853, mean_eps: 0.727404
  303013/2000000: episode: 3078, duration: 1.238s, episode steps: 87, steps per second: 70, episode reward: -109.481, mean reward: -1.258 [-100.000, 14.994], mean action: 1.483 [0.000, 3.000], mean observation: -0.050 [-1.255, 1.000], loss: 1.673807, mean_absolute_error: 30.590760, mean_q: 17.992318, mean_eps: 0.727327
  303132/2000000: episode: 3079, duration: 1.681s, episode steps: 119, steps per second: 71, episode reward: -84.823, mean reward: -0.713 [-100.000, 11.963], mean action: 1.563 [0.000, 3.000], mean observation: 0.046 [-0.930, 1.000], loss: 1.452229, mean_absolute_error: 29.496555, mean_q: 18.660275, mean_eps: 0.727235
  303215/2000000: episode: 3080, duration: 1.197s, episode steps: 83, steps per second: 69, episode reward: -94.978, mean reward: -1.144 [-100.000, 7.611], mean action: 1.627 [0.000, 3.000], mean observation: -0.146 [-3.587, 1.000], loss: 1.771242, mean_absolute_error: 28.873933, mean_q: 17.836596, mean_eps: 0.727145
  303297/2000000: episode: 3081, duration: 1.177s, episode steps: 82, steps per second: 70, episode reward: -128.443, mean reward: -1.566 [-100.000, 15.916], mean action: 1.500 [0.000, 3.000], mean observation: -0.075 [-1.122, 3.215], loss: 1.480822, mean_absolute_error: 30.212367, mean_q: 17.510608, mean_eps: 0.727070
  303362/2000000: episode: 3082, duration: 0.906s, episode steps: 65, steps per second: 72, episode reward: -117.386, mean reward: -1.806 [-100.000, 13.174], mean action: 1.415 [0.000, 3.000], mean observation: -0.005 [-1.446, 4.888], loss: 1.454371, mean_absolute_error: 29.923388, mean_q: 16.473463, mean_eps: 0.727003
  303472/2000000: episode: 3083, duration: 1.579s, episode steps: 110, steps per second: 70, episode reward: -124.856, mean reward: -1.135 [-100.000, 10.553], mean action: 1.618 [0.000, 3.000], mean observation: 0.077 [-3.286, 1.000], loss: 1.491116, mean_absolute_error: 30.402163, mean_q: 17.407665, mean_eps: 0.726926
  303571/2000000: episode: 3084, duration: 1.424s, episode steps: 99, steps per second: 70, episode reward: -152.136, mean reward: -1.537 [-100.000, 11.001], mean action: 1.515 [0.000, 3.000], mean observation: 0.019 [-4.126, 1.000], loss: 1.288593, mean_absolute_error: 28.836236, mean_q: 18.869792, mean_eps: 0.726832
  303656/2000000: episode: 3085, duration: 1.273s, episode steps: 85, steps per second: 67, episode reward: -109.116, mean reward: -1.284 [-100.000, 14.397], mean action: 1.718 [0.000, 3.000], mean observation: -0.135 [-3.346, 1.000], loss: 1.441981, mean_absolute_error: 29.498766, mean_q: 18.123677, mean_eps: 0.726749
  303737/2000000: episode: 3086, duration: 1.206s, episode steps: 81, steps per second: 67, episode reward: -99.025, mean reward: -1.223 [-100.000, 9.174], mean action: 1.617 [0.000, 3.000], mean observation: 0.049 [-3.798, 1.000], loss: 1.631816, mean_absolute_error: 28.509492, mean_q: 18.693316, mean_eps: 0.726674
  303812/2000000: episode: 3087, duration: 1.089s, episode steps: 75, steps per second: 69, episode reward: -150.206, mean reward: -2.003 [-100.000, 4.308], mean action: 1.653 [0.000, 3.000], mean observation: 0.106 [-1.802, 1.000], loss: 1.564414, mean_absolute_error: 29.476601, mean_q: 18.359077, mean_eps: 0.726603
  303886/2000000: episode: 3088, duration: 1.091s, episode steps: 74, steps per second: 68, episode reward: -115.806, mean reward: -1.565 [-100.000, 8.057], mean action: 1.811 [0.000, 3.000], mean observation: -0.171 [-1.227, 3.412], loss: 1.169542, mean_absolute_error: 29.026949, mean_q: 18.467513, mean_eps: 0.726537
  303992/2000000: episode: 3089, duration: 1.533s, episode steps: 106, steps per second: 69, episode reward: -106.339, mean reward: -1.003 [-100.000, 11.360], mean action: 1.538 [0.000, 3.000], mean observation: 0.088 [-0.888, 2.646], loss: 2.028597, mean_absolute_error: 30.970542, mean_q: 18.670601, mean_eps: 0.726456
  304071/2000000: episode: 3090, duration: 1.165s, episode steps: 79, steps per second: 68, episode reward: -69.774, mean reward: -0.883 [-100.000, 20.271], mean action: 1.646 [0.000, 3.000], mean observation: -0.004 [-0.951, 3.068], loss: 2.019767, mean_absolute_error: 30.699326, mean_q: 15.132274, mean_eps: 0.726373
  304165/2000000: episode: 3091, duration: 1.399s, episode steps: 94, steps per second: 67, episode reward: -117.875, mean reward: -1.254 [-100.000, 11.237], mean action: 1.574 [0.000, 3.000], mean observation: 0.108 [-3.893, 1.000], loss: 1.247099, mean_absolute_error: 29.640236, mean_q: 19.347928, mean_eps: 0.726294
  304254/2000000: episode: 3092, duration: 1.280s, episode steps: 89, steps per second: 70, episode reward: -107.834, mean reward: -1.212 [-100.000, 26.051], mean action: 1.888 [0.000, 3.000], mean observation: -0.129 [-1.215, 2.099], loss: 1.663549, mean_absolute_error: 29.561241, mean_q: 19.373868, mean_eps: 0.726211
  304331/2000000: episode: 3093, duration: 1.096s, episode steps: 77, steps per second: 70, episode reward: -126.723, mean reward: -1.646 [-100.000, 8.186], mean action: 1.545 [0.000, 3.000], mean observation: 0.055 [-2.652, 1.000], loss: 1.070331, mean_absolute_error: 28.808728, mean_q: 21.291241, mean_eps: 0.726137
  304415/2000000: episode: 3094, duration: 1.207s, episode steps: 84, steps per second: 70, episode reward: -120.126, mean reward: -1.430 [-100.000, 15.710], mean action: 1.833 [0.000, 3.000], mean observation: -0.081 [-1.276, 3.895], loss: 1.553273, mean_absolute_error: 30.890713, mean_q: 16.489726, mean_eps: 0.726065
  304543/2000000: episode: 3095, duration: 1.854s, episode steps: 128, steps per second: 69, episode reward: -115.245, mean reward: -0.900 [-100.000, 10.018], mean action: 1.547 [0.000, 3.000], mean observation: 0.043 [-3.112, 1.005], loss: 1.731726, mean_absolute_error: 30.882797, mean_q: 18.016524, mean_eps: 0.725970
  304650/2000000: episode: 3096, duration: 1.532s, episode steps: 107, steps per second: 70, episode reward: -77.284, mean reward: -0.722 [-100.000, 13.643], mean action: 1.729 [0.000, 3.000], mean observation: 0.042 [-1.084, 1.000], loss: 1.701636, mean_absolute_error: 31.155755, mean_q: 14.660642, mean_eps: 0.725864
  304784/2000000: episode: 3097, duration: 1.913s, episode steps: 134, steps per second: 70, episode reward: -104.606, mean reward: -0.781 [-100.000, 11.162], mean action: 1.716 [0.000, 3.000], mean observation: 0.083 [-3.145, 1.000], loss: 1.767643, mean_absolute_error: 29.394126, mean_q: 18.656265, mean_eps: 0.725756
  304870/2000000: episode: 3098, duration: 1.272s, episode steps: 86, steps per second: 68, episode reward: -87.406, mean reward: -1.016 [-100.000, 6.937], mean action: 1.779 [0.000, 3.000], mean observation: -0.120 [-1.021, 3.771], loss: 1.601514, mean_absolute_error: 29.800051, mean_q: 18.973089, mean_eps: 0.725657
  304936/2000000: episode: 3099, duration: 0.988s, episode steps: 66, steps per second: 67, episode reward: -95.109, mean reward: -1.441 [-100.000, 6.977], mean action: 1.576 [0.000, 3.000], mean observation: -0.004 [-4.732, 1.000], loss: 1.430436, mean_absolute_error: 31.483527, mean_q: 19.075111, mean_eps: 0.725588
  305005/2000000: episode: 3100, duration: 1.024s, episode steps: 69, steps per second: 67, episode reward: -122.302, mean reward: -1.772 [-100.000, 7.169], mean action: 1.536 [0.000, 3.000], mean observation: -0.174 [-1.287, 3.932], loss: 1.130748, mean_absolute_error: 28.571431, mean_q: 22.237853, mean_eps: 0.725527
  305118/2000000: episode: 3101, duration: 1.691s, episode steps: 113, steps per second: 67, episode reward: -74.219, mean reward: -0.657 [-100.000, 14.784], mean action: 1.779 [0.000, 3.000], mean observation: -0.066 [-2.783, 1.000], loss: 1.181905, mean_absolute_error: 29.474181, mean_q: 19.522441, mean_eps: 0.725444
  305244/2000000: episode: 3102, duration: 1.821s, episode steps: 126, steps per second: 69, episode reward: -109.324, mean reward: -0.868 [-100.000, 18.570], mean action: 1.611 [0.000, 3.000], mean observation: 0.079 [-1.293, 3.707], loss: 1.566413, mean_absolute_error: 29.679562, mean_q: 18.461614, mean_eps: 0.725338
  305316/2000000: episode: 3103, duration: 1.064s, episode steps: 72, steps per second: 68, episode reward: -80.552, mean reward: -1.119 [-100.000, 11.120], mean action: 1.514 [0.000, 3.000], mean observation: -0.005 [-1.319, 1.000], loss: 1.209927, mean_absolute_error: 30.700992, mean_q: 20.948471, mean_eps: 0.725250
  305400/2000000: episode: 3104, duration: 1.235s, episode steps: 84, steps per second: 68, episode reward: -122.383, mean reward: -1.457 [-100.000, 6.324], mean action: 1.548 [0.000, 3.000], mean observation: 0.077 [-3.441, 1.000], loss: 1.699267, mean_absolute_error: 30.926453, mean_q: 17.158884, mean_eps: 0.725180
  305529/2000000: episode: 3105, duration: 1.872s, episode steps: 129, steps per second: 69, episode reward: -124.436, mean reward: -0.965 [-100.000, 6.168], mean action: 1.612 [0.000, 3.000], mean observation: 0.059 [-3.130, 1.000], loss: 2.326534, mean_absolute_error: 31.687847, mean_q: 14.992048, mean_eps: 0.725082
  305619/2000000: episode: 3106, duration: 1.264s, episode steps: 90, steps per second: 71, episode reward: -127.925, mean reward: -1.421 [-100.000, 11.325], mean action: 1.767 [0.000, 3.000], mean observation: -0.137 [-1.172, 1.000], loss: 2.243407, mean_absolute_error: 31.275704, mean_q: 14.954173, mean_eps: 0.724983
  305700/2000000: episode: 3107, duration: 1.205s, episode steps: 81, steps per second: 67, episode reward: -132.255, mean reward: -1.633 [-100.000, 10.168], mean action: 1.802 [0.000, 3.000], mean observation: -0.075 [-1.285, 1.000], loss: 1.751311, mean_absolute_error: 30.020885, mean_q: 16.968309, mean_eps: 0.724908
  305809/2000000: episode: 3108, duration: 1.598s, episode steps: 109, steps per second: 68, episode reward: -154.716, mean reward: -1.419 [-100.000, 9.094], mean action: 1.771 [0.000, 3.000], mean observation: -0.161 [-0.974, 2.734], loss: 1.606809, mean_absolute_error: 30.484554, mean_q: 17.717238, mean_eps: 0.724821
  305919/2000000: episode: 3109, duration: 1.548s, episode steps: 110, steps per second: 71, episode reward: -115.351, mean reward: -1.049 [-100.000, 5.744], mean action: 1.482 [0.000, 3.000], mean observation: 0.106 [-1.035, 3.335], loss: 1.904479, mean_absolute_error: 30.606954, mean_q: 16.518428, mean_eps: 0.724722
  306026/2000000: episode: 3110, duration: 1.554s, episode steps: 107, steps per second: 69, episode reward: -84.452, mean reward: -0.789 [-100.000, 13.385], mean action: 1.804 [0.000, 3.000], mean observation: -0.086 [-0.954, 3.009], loss: 1.903633, mean_absolute_error: 30.998659, mean_q: 18.506387, mean_eps: 0.724625
  306143/2000000: episode: 3111, duration: 1.656s, episode steps: 117, steps per second: 71, episode reward: -123.135, mean reward: -1.052 [-100.000, 16.294], mean action: 1.667 [0.000, 3.000], mean observation: -0.101 [-1.164, 1.000], loss: 2.006680, mean_absolute_error: 30.298175, mean_q: 15.244992, mean_eps: 0.724524
  306238/2000000: episode: 3112, duration: 1.399s, episode steps: 95, steps per second: 68, episode reward: -102.606, mean reward: -1.080 [-100.000, 11.883], mean action: 1.484 [0.000, 3.000], mean observation: 0.037 [-1.145, 3.669], loss: 1.337431, mean_absolute_error: 29.466333, mean_q: 16.852458, mean_eps: 0.724429
  306338/2000000: episode: 3113, duration: 1.438s, episode steps: 100, steps per second: 70, episode reward: -134.499, mean reward: -1.345 [-100.000, 18.675], mean action: 1.620 [0.000, 3.000], mean observation: 0.056 [-1.202, 1.000], loss: 1.191072, mean_absolute_error: 29.011373, mean_q: 16.626038, mean_eps: 0.724341
  306404/2000000: episode: 3114, duration: 0.976s, episode steps: 66, steps per second: 68, episode reward: -105.303, mean reward: -1.596 [-100.000, 16.051], mean action: 1.606 [0.000, 3.000], mean observation: -0.144 [-1.398, 1.000], loss: 1.034293, mean_absolute_error: 28.724531, mean_q: 18.682930, mean_eps: 0.724267
  306481/2000000: episode: 3115, duration: 1.139s, episode steps: 77, steps per second: 68, episode reward: -107.093, mean reward: -1.391 [-100.000, 6.620], mean action: 1.623 [0.000, 3.000], mean observation: -0.074 [-1.127, 4.071], loss: 1.536818, mean_absolute_error: 30.328065, mean_q: 18.148812, mean_eps: 0.724202
  306614/2000000: episode: 3116, duration: 1.902s, episode steps: 133, steps per second: 70, episode reward: -127.478, mean reward: -0.958 [-100.000, 6.065], mean action: 1.526 [0.000, 3.000], mean observation: 0.057 [-1.190, 4.197], loss: 1.757799, mean_absolute_error: 29.764155, mean_q: 18.064909, mean_eps: 0.724107
  306745/2000000: episode: 3117, duration: 1.879s, episode steps: 131, steps per second: 70, episode reward: -104.221, mean reward: -0.796 [-100.000, 13.807], mean action: 1.603 [0.000, 3.000], mean observation: 0.140 [-1.629, 1.000], loss: 1.558161, mean_absolute_error: 29.837604, mean_q: 19.491925, mean_eps: 0.723988
  306868/2000000: episode: 3118, duration: 1.774s, episode steps: 123, steps per second: 69, episode reward: -111.656, mean reward: -0.908 [-100.000, 5.788], mean action: 1.618 [0.000, 3.000], mean observation: -0.011 [-2.958, 1.000], loss: 1.665983, mean_absolute_error: 30.199900, mean_q: 17.067653, mean_eps: 0.723875
  306959/2000000: episode: 3119, duration: 1.339s, episode steps: 91, steps per second: 68, episode reward: -72.213, mean reward: -0.794 [-100.000, 10.016], mean action: 1.703 [0.000, 3.000], mean observation: 0.059 [-2.399, 1.000], loss: 2.024551, mean_absolute_error: 30.706353, mean_q: 15.867555, mean_eps: 0.723779
  307044/2000000: episode: 3120, duration: 1.257s, episode steps: 85, steps per second: 68, episode reward: -151.291, mean reward: -1.780 [-100.000, 9.206], mean action: 1.376 [0.000, 3.000], mean observation: 0.098 [-4.191, 1.000], loss: 1.333308, mean_absolute_error: 31.203577, mean_q: 14.527540, mean_eps: 0.723700
  307134/2000000: episode: 3121, duration: 1.331s, episode steps: 90, steps per second: 68, episode reward: -91.385, mean reward: -1.015 [-100.000, 13.351], mean action: 1.600 [0.000, 3.000], mean observation: -0.086 [-0.965, 2.875], loss: 2.034019, mean_absolute_error: 30.195267, mean_q: 13.876112, mean_eps: 0.723621
  307246/2000000: episode: 3122, duration: 1.625s, episode steps: 112, steps per second: 69, episode reward: -128.751, mean reward: -1.150 [-100.000, 15.972], mean action: 1.670 [0.000, 3.000], mean observation: -0.065 [-1.061, 3.566], loss: 1.563232, mean_absolute_error: 28.719112, mean_q: 20.468096, mean_eps: 0.723529
  307372/2000000: episode: 3123, duration: 1.854s, episode steps: 126, steps per second: 68, episode reward: -97.300, mean reward: -0.772 [-100.000, 13.470], mean action: 1.667 [0.000, 3.000], mean observation: 0.101 [-1.007, 3.188], loss: 1.899394, mean_absolute_error: 29.593982, mean_q: 16.533028, mean_eps: 0.723423
  307455/2000000: episode: 3124, duration: 1.288s, episode steps: 83, steps per second: 64, episode reward: -149.591, mean reward: -1.802 [-100.000, 7.838], mean action: 1.373 [0.000, 3.000], mean observation: 0.120 [-4.257, 1.000], loss: 1.571304, mean_absolute_error: 29.944288, mean_q: 17.740528, mean_eps: 0.723329
  307532/2000000: episode: 3125, duration: 1.154s, episode steps: 77, steps per second: 67, episode reward: -117.260, mean reward: -1.523 [-100.000, 21.640], mean action: 1.597 [0.000, 3.000], mean observation: -0.045 [-1.436, 1.000], loss: 1.160528, mean_absolute_error: 30.628138, mean_q: 16.300496, mean_eps: 0.723257
  307646/2000000: episode: 3126, duration: 1.641s, episode steps: 114, steps per second: 69, episode reward: -102.088, mean reward: -0.896 [-100.000, 5.991], mean action: 1.649 [0.000, 3.000], mean observation: -0.026 [-0.970, 3.195], loss: 1.699909, mean_absolute_error: 29.756799, mean_q: 18.182636, mean_eps: 0.723171
  307762/2000000: episode: 3127, duration: 1.687s, episode steps: 116, steps per second: 69, episode reward: -125.017, mean reward: -1.078 [-100.000, 22.285], mean action: 1.638 [0.000, 3.000], mean observation: -0.031 [-1.228, 1.000], loss: 2.186819, mean_absolute_error: 29.701861, mean_q: 16.119689, mean_eps: 0.723066
  307856/2000000: episode: 3128, duration: 1.377s, episode steps: 94, steps per second: 68, episode reward: -99.298, mean reward: -1.056 [-100.000, 6.858], mean action: 1.606 [0.000, 3.000], mean observation: 0.010 [-2.647, 1.000], loss: 1.226412, mean_absolute_error: 28.570894, mean_q: 22.115487, mean_eps: 0.722973
  308018/2000000: episode: 3129, duration: 2.368s, episode steps: 162, steps per second: 68, episode reward: -108.575, mean reward: -0.670 [-100.000, 7.081], mean action: 1.654 [0.000, 3.000], mean observation: 0.124 [-0.885, 3.427], loss: 1.259248, mean_absolute_error: 30.068813, mean_q: 19.232031, mean_eps: 0.722858
  308109/2000000: episode: 3130, duration: 1.318s, episode steps: 91, steps per second: 69, episode reward: -131.728, mean reward: -1.448 [-100.000, 12.506], mean action: 1.604 [0.000, 3.000], mean observation: 0.019 [-1.279, 4.539], loss: 1.513968, mean_absolute_error: 30.062446, mean_q: 17.962915, mean_eps: 0.722742
  308261/2000000: episode: 3131, duration: 2.195s, episode steps: 152, steps per second: 69, episode reward: -122.134, mean reward: -0.804 [-100.000, 16.466], mean action: 1.711 [0.000, 3.000], mean observation: 0.088 [-1.044, 3.874], loss: 1.396566, mean_absolute_error: 29.864937, mean_q: 19.563874, mean_eps: 0.722633
  308411/2000000: episode: 3132, duration: 2.125s, episode steps: 150, steps per second: 71, episode reward: -115.937, mean reward: -0.773 [-100.000, 4.603], mean action: 1.600 [0.000, 3.000], mean observation: 0.098 [-0.976, 1.446], loss: 1.492788, mean_absolute_error: 31.675231, mean_q: 16.685143, mean_eps: 0.722498
  308499/2000000: episode: 3133, duration: 1.293s, episode steps: 88, steps per second: 68, episode reward: -151.013, mean reward: -1.716 [-100.000, 6.116], mean action: 1.534 [0.000, 3.000], mean observation: 0.044 [-4.345, 1.000], loss: 2.072062, mean_absolute_error: 30.319190, mean_q: 19.115520, mean_eps: 0.722391
  308616/2000000: episode: 3134, duration: 1.701s, episode steps: 117, steps per second: 69, episode reward: -147.542, mean reward: -1.261 [-100.000, 10.272], mean action: 1.564 [0.000, 3.000], mean observation: 0.002 [-3.977, 1.000], loss: 1.346892, mean_absolute_error: 29.669490, mean_q: 16.837192, mean_eps: 0.722300
  308722/2000000: episode: 3135, duration: 1.537s, episode steps: 106, steps per second: 69, episode reward: -125.288, mean reward: -1.182 [-100.000, 10.735], mean action: 1.708 [0.000, 3.000], mean observation: -0.115 [-0.936, 2.820], loss: 1.282421, mean_absolute_error: 28.252878, mean_q: 19.910568, mean_eps: 0.722199
  308832/2000000: episode: 3136, duration: 1.616s, episode steps: 110, steps per second: 68, episode reward: -123.072, mean reward: -1.119 [-100.000, 7.010], mean action: 1.545 [0.000, 3.000], mean observation: 0.002 [-3.361, 1.000], loss: 2.240682, mean_absolute_error: 31.395861, mean_q: 16.017000, mean_eps: 0.722102
  308949/2000000: episode: 3137, duration: 1.735s, episode steps: 117, steps per second: 67, episode reward: -124.967, mean reward: -1.068 [-100.000, 6.769], mean action: 1.658 [0.000, 3.000], mean observation: -0.091 [-0.960, 3.322], loss: 1.860357, mean_absolute_error: 31.245459, mean_q: 17.807054, mean_eps: 0.721999
  309024/2000000: episode: 3138, duration: 1.113s, episode steps: 75, steps per second: 67, episode reward: -90.690, mean reward: -1.209 [-100.000, 10.642], mean action: 1.667 [0.000, 3.000], mean observation: 0.032 [-3.443, 1.000], loss: 1.892316, mean_absolute_error: 30.590722, mean_q: 17.135474, mean_eps: 0.721913
  309161/2000000: episode: 3139, duration: 1.989s, episode steps: 137, steps per second: 69, episode reward: -166.641, mean reward: -1.216 [-100.000, 7.031], mean action: 1.606 [0.000, 3.000], mean observation: 0.119 [-3.997, 1.000], loss: 1.925941, mean_absolute_error: 31.273098, mean_q: 16.895021, mean_eps: 0.721817
  309259/2000000: episode: 3140, duration: 1.404s, episode steps: 98, steps per second: 70, episode reward: -130.235, mean reward: -1.329 [-100.000, 22.433], mean action: 1.653 [0.000, 3.000], mean observation: 0.026 [-1.238, 1.000], loss: 1.102650, mean_absolute_error: 29.211379, mean_q: 20.033133, mean_eps: 0.721711
  309390/2000000: episode: 3141, duration: 1.901s, episode steps: 131, steps per second: 69, episode reward: -112.786, mean reward: -0.861 [-100.000, 16.999], mean action: 1.733 [0.000, 3.000], mean observation: -0.047 [-0.889, 3.475], loss: 1.130459, mean_absolute_error: 29.402914, mean_q: 20.079539, mean_eps: 0.721608
  309481/2000000: episode: 3142, duration: 1.331s, episode steps: 91, steps per second: 68, episode reward: -113.050, mean reward: -1.242 [-100.000, 53.208], mean action: 1.615 [0.000, 3.000], mean observation: 0.142 [-1.175, 1.500], loss: 1.597834, mean_absolute_error: 30.021401, mean_q: 21.838361, mean_eps: 0.721508
  309634/2000000: episode: 3143, duration: 2.195s, episode steps: 153, steps per second: 70, episode reward: -106.318, mean reward: -0.695 [-100.000, 11.469], mean action: 1.680 [0.000, 3.000], mean observation: 0.019 [-1.116, 3.984], loss: 1.301106, mean_absolute_error: 29.699973, mean_q: 17.445666, mean_eps: 0.721398
  309758/2000000: episode: 3144, duration: 1.815s, episode steps: 124, steps per second: 68, episode reward: -120.908, mean reward: -0.975 [-100.000, 7.245], mean action: 1.669 [0.000, 3.000], mean observation: -0.113 [-1.010, 3.506], loss: 2.711978, mean_absolute_error: 31.381838, mean_q: 14.426983, mean_eps: 0.721274
  309862/2000000: episode: 3145, duration: 1.500s, episode steps: 104, steps per second: 69, episode reward: -123.103, mean reward: -1.184 [-100.000, 12.816], mean action: 1.702 [0.000, 3.000], mean observation: -0.136 [-0.908, 2.989], loss: 2.315274, mean_absolute_error: 30.736862, mean_q: 18.172330, mean_eps: 0.721171
  309941/2000000: episode: 3146, duration: 1.158s, episode steps: 79, steps per second: 68, episode reward: -166.008, mean reward: -2.101 [-100.000, 10.866], mean action: 1.671 [0.000, 3.000], mean observation: -0.153 [-1.353, 4.058], loss: 1.025845, mean_absolute_error: 29.180669, mean_q: 21.585210, mean_eps: 0.721088
  310074/2000000: episode: 3147, duration: 1.903s, episode steps: 133, steps per second: 70, episode reward: -143.870, mean reward: -1.082 [-100.000, 8.382], mean action: 1.526 [0.000, 3.000], mean observation: -0.056 [-1.117, 4.120], loss: 2.466327, mean_absolute_error: 31.939907, mean_q: 14.875771, mean_eps: 0.720993
  310149/2000000: episode: 3148, duration: 1.111s, episode steps: 75, steps per second: 67, episode reward: -152.822, mean reward: -2.038 [-100.000, 6.868], mean action: 1.720 [0.000, 3.000], mean observation: -0.155 [-1.242, 4.198], loss: 1.951825, mean_absolute_error: 32.202429, mean_q: 18.936667, mean_eps: 0.720899
  310221/2000000: episode: 3149, duration: 1.034s, episode steps: 72, steps per second: 70, episode reward: -124.030, mean reward: -1.723 [-100.000, 6.941], mean action: 1.681 [0.000, 3.000], mean observation: -0.022 [-4.041, 1.000], loss: 1.639193, mean_absolute_error: 30.676594, mean_q: 19.797533, mean_eps: 0.720833
  310294/2000000: episode: 3150, duration: 1.046s, episode steps: 73, steps per second: 70, episode reward: -190.221, mean reward: -2.606 [-100.000, 9.669], mean action: 1.945 [0.000, 3.000], mean observation: -0.127 [-4.737, 1.000], loss: 1.116081, mean_absolute_error: 30.330784, mean_q: 17.712494, mean_eps: 0.720768
  310371/2000000: episode: 3151, duration: 1.092s, episode steps: 77, steps per second: 71, episode reward: -128.011, mean reward: -1.662 [-100.000, 17.572], mean action: 1.429 [0.000, 3.000], mean observation: 0.097 [-1.362, 1.000], loss: 2.268203, mean_absolute_error: 30.448079, mean_q: 17.159660, mean_eps: 0.720701
  310437/2000000: episode: 3152, duration: 0.984s, episode steps: 66, steps per second: 67, episode reward: -113.019, mean reward: -1.712 [-100.000, 22.643], mean action: 1.818 [0.000, 3.000], mean observation: -0.167 [-1.158, 4.057], loss: 1.235610, mean_absolute_error: 30.434566, mean_q: 18.732162, mean_eps: 0.720636
  310552/2000000: episode: 3153, duration: 1.669s, episode steps: 115, steps per second: 69, episode reward: -129.428, mean reward: -1.125 [-100.000, 10.256], mean action: 1.748 [0.000, 3.000], mean observation: 0.017 [-1.182, 4.102], loss: 1.609986, mean_absolute_error: 31.561775, mean_q: 16.774876, mean_eps: 0.720555
  310639/2000000: episode: 3154, duration: 1.295s, episode steps: 87, steps per second: 67, episode reward: -97.504, mean reward: -1.121 [-100.000, 16.056], mean action: 1.701 [0.000, 3.000], mean observation: -0.038 [-1.169, 1.000], loss: 1.681967, mean_absolute_error: 30.620178, mean_q: 15.877075, mean_eps: 0.720465
  310714/2000000: episode: 3155, duration: 1.209s, episode steps: 75, steps per second: 62, episode reward: -83.346, mean reward: -1.111 [-100.000, 13.587], mean action: 1.587 [0.000, 3.000], mean observation: -0.053 [-1.213, 4.082], loss: 1.370909, mean_absolute_error: 30.429763, mean_q: 16.729676, mean_eps: 0.720392
  310813/2000000: episode: 3156, duration: 1.498s, episode steps: 99, steps per second: 66, episode reward: -305.736, mean reward: -3.088 [-100.000, 19.811], mean action: 1.768 [0.000, 3.000], mean observation: -0.283 [-1.984, 1.000], loss: 1.217627, mean_absolute_error: 31.621936, mean_q: 17.252680, mean_eps: 0.720312
  310951/2000000: episode: 3157, duration: 1.948s, episode steps: 138, steps per second: 71, episode reward: -131.148, mean reward: -0.950 [-100.000, 10.401], mean action: 1.558 [0.000, 3.000], mean observation: 0.067 [-3.441, 1.000], loss: 1.778076, mean_absolute_error: 30.823869, mean_q: 17.338744, mean_eps: 0.720206
  311017/2000000: episode: 3158, duration: 1.005s, episode steps: 66, steps per second: 66, episode reward: -103.333, mean reward: -1.566 [-100.000, 14.337], mean action: 1.621 [0.000, 3.000], mean observation: 0.079 [-1.303, 1.000], loss: 1.539303, mean_absolute_error: 30.379615, mean_q: 17.501708, mean_eps: 0.720114
  311159/2000000: episode: 3159, duration: 2.011s, episode steps: 142, steps per second: 71, episode reward: -115.591, mean reward: -0.814 [-100.000, 9.722], mean action: 1.627 [0.000, 3.000], mean observation: 0.054 [-3.225, 1.000], loss: 1.588028, mean_absolute_error: 30.755474, mean_q: 20.552978, mean_eps: 0.720021
  311276/2000000: episode: 3160, duration: 1.739s, episode steps: 117, steps per second: 67, episode reward: -270.804, mean reward: -2.315 [-100.000, 53.478], mean action: 1.675 [0.000, 3.000], mean observation: -0.214 [-2.341, 1.000], loss: 1.989332, mean_absolute_error: 32.501375, mean_q: 15.640846, mean_eps: 0.719906
  311396/2000000: episode: 3161, duration: 1.791s, episode steps: 120, steps per second: 67, episode reward: -94.794, mean reward: -0.790 [-100.000, 12.497], mean action: 1.658 [0.000, 3.000], mean observation: 0.029 [-1.156, 1.000], loss: 2.066101, mean_absolute_error: 31.463693, mean_q: 15.880153, mean_eps: 0.719799
  311495/2000000: episode: 3162, duration: 1.443s, episode steps: 99, steps per second: 69, episode reward: -86.152, mean reward: -0.870 [-100.000, 8.771], mean action: 1.465 [0.000, 3.000], mean observation: 0.079 [-2.927, 1.000], loss: 1.490729, mean_absolute_error: 30.365701, mean_q: 19.082747, mean_eps: 0.719700
  311567/2000000: episode: 3163, duration: 1.029s, episode steps: 72, steps per second: 70, episode reward: -114.904, mean reward: -1.596 [-100.000, 3.822], mean action: 1.681 [0.000, 3.000], mean observation: -0.124 [-1.523, 0.933], loss: 2.200234, mean_absolute_error: 30.189163, mean_q: 17.240722, mean_eps: 0.719623
  311718/2000000: episode: 3164, duration: 2.182s, episode steps: 151, steps per second: 69, episode reward: -111.942, mean reward: -0.741 [-100.000, 13.758], mean action: 1.795 [0.000, 3.000], mean observation: -0.053 [-1.315, 4.403], loss: 1.890824, mean_absolute_error: 30.017744, mean_q: 21.598396, mean_eps: 0.719522
  311828/2000000: episode: 3165, duration: 1.594s, episode steps: 110, steps per second: 69, episode reward: -149.713, mean reward: -1.361 [-100.000, 7.627], mean action: 1.582 [0.000, 3.000], mean observation: 0.199 [-3.661, 1.000], loss: 1.488110, mean_absolute_error: 31.085243, mean_q: 17.225805, mean_eps: 0.719405
  311957/2000000: episode: 3166, duration: 1.890s, episode steps: 129, steps per second: 68, episode reward: -157.760, mean reward: -1.223 [-100.000, 6.864], mean action: 1.605 [0.000, 3.000], mean observation: 0.119 [-1.032, 3.880], loss: 1.724808, mean_absolute_error: 31.257715, mean_q: 17.458347, mean_eps: 0.719297
  312062/2000000: episode: 3167, duration: 1.502s, episode steps: 105, steps per second: 70, episode reward: -97.395, mean reward: -0.928 [-100.000, 7.503], mean action: 1.619 [0.000, 3.000], mean observation: 0.123 [-3.364, 1.000], loss: 1.548592, mean_absolute_error: 31.046133, mean_q: 17.273521, mean_eps: 0.719191
  312149/2000000: episode: 3168, duration: 1.266s, episode steps: 87, steps per second: 69, episode reward: -71.973, mean reward: -0.827 [-100.000, 13.811], mean action: 1.736 [0.000, 3.000], mean observation: -0.034 [-2.783, 1.000], loss: 1.660240, mean_absolute_error: 31.121553, mean_q: 15.693692, mean_eps: 0.719105
  312269/2000000: episode: 3169, duration: 1.728s, episode steps: 120, steps per second: 69, episode reward: -65.260, mean reward: -0.544 [-100.000, 8.403], mean action: 1.617 [0.000, 3.000], mean observation: -0.026 [-2.349, 1.000], loss: 2.264339, mean_absolute_error: 31.397995, mean_q: 16.531482, mean_eps: 0.719011
  312390/2000000: episode: 3170, duration: 1.755s, episode steps: 121, steps per second: 69, episode reward: -53.115, mean reward: -0.439 [-100.000, 12.393], mean action: 1.818 [0.000, 3.000], mean observation: -0.094 [-0.750, 1.715], loss: 1.699411, mean_absolute_error: 30.683422, mean_q: 18.338557, mean_eps: 0.718903
  312523/2000000: episode: 3171, duration: 1.918s, episode steps: 133, steps per second: 69, episode reward: -74.352, mean reward: -0.559 [-100.000, 63.370], mean action: 1.504 [0.000, 3.000], mean observation: 0.146 [-1.403, 1.750], loss: 1.945266, mean_absolute_error: 32.075415, mean_q: 17.843516, mean_eps: 0.718790
  312619/2000000: episode: 3172, duration: 1.418s, episode steps: 96, steps per second: 68, episode reward: -104.297, mean reward: -1.086 [-100.000, 8.981], mean action: 1.521 [0.000, 3.000], mean observation: 0.126 [-1.017, 3.719], loss: 1.703519, mean_absolute_error: 31.662237, mean_q: 17.247706, mean_eps: 0.718687
  312717/2000000: episode: 3173, duration: 1.432s, episode steps: 98, steps per second: 68, episode reward: -117.446, mean reward: -1.198 [-100.000, 8.313], mean action: 1.724 [0.000, 3.000], mean observation: -0.047 [-1.061, 3.711], loss: 2.295988, mean_absolute_error: 30.622036, mean_q: 19.106915, mean_eps: 0.718599
  312817/2000000: episode: 3174, duration: 1.422s, episode steps: 100, steps per second: 70, episode reward: -79.823, mean reward: -0.798 [-100.000, 11.250], mean action: 1.540 [0.000, 3.000], mean observation: 0.053 [-0.871, 3.030], loss: 2.253982, mean_absolute_error: 31.425561, mean_q: 18.216152, mean_eps: 0.718509
  312943/2000000: episode: 3175, duration: 1.819s, episode steps: 126, steps per second: 69, episode reward: -137.878, mean reward: -1.094 [-100.000, 13.131], mean action: 1.492 [0.000, 3.000], mean observation: 0.028 [-1.212, 4.023], loss: 1.619476, mean_absolute_error: 30.228591, mean_q: 18.604476, mean_eps: 0.718408
  313015/2000000: episode: 3176, duration: 1.032s, episode steps: 72, steps per second: 70, episode reward: -109.282, mean reward: -1.518 [-100.000, 6.998], mean action: 1.764 [0.000, 3.000], mean observation: -0.146 [-4.748, 1.000], loss: 1.862388, mean_absolute_error: 31.076783, mean_q: 19.695329, mean_eps: 0.718320
  313168/2000000: episode: 3177, duration: 2.244s, episode steps: 153, steps per second: 68, episode reward: -175.581, mean reward: -1.148 [-100.000, 7.443], mean action: 1.693 [0.000, 3.000], mean observation: -0.144 [-1.007, 1.000], loss: 2.721552, mean_absolute_error: 31.682415, mean_q: 15.064302, mean_eps: 0.718219
  313296/2000000: episode: 3178, duration: 1.896s, episode steps: 128, steps per second: 68, episode reward: -95.213, mean reward: -0.744 [-100.000, 60.405], mean action: 1.594 [0.000, 3.000], mean observation: -0.069 [-1.409, 3.636], loss: 2.396579, mean_absolute_error: 32.336762, mean_q: 14.150242, mean_eps: 0.718093
  313423/2000000: episode: 3179, duration: 1.842s, episode steps: 127, steps per second: 69, episode reward: -95.342, mean reward: -0.751 [-100.000, 14.863], mean action: 1.622 [0.000, 3.000], mean observation: 0.070 [-1.230, 1.023], loss: 2.306729, mean_absolute_error: 30.869435, mean_q: 20.490394, mean_eps: 0.717978
  313533/2000000: episode: 3180, duration: 1.627s, episode steps: 110, steps per second: 68, episode reward: -143.478, mean reward: -1.304 [-100.000, 13.388], mean action: 1.636 [0.000, 3.000], mean observation: -0.148 [-1.051, 1.000], loss: 1.876099, mean_absolute_error: 31.221718, mean_q: 18.869386, mean_eps: 0.717870
  313697/2000000: episode: 3181, duration: 2.340s, episode steps: 164, steps per second: 70, episode reward: -62.009, mean reward: -0.378 [-100.000, 18.293], mean action: 1.549 [0.000, 3.000], mean observation: -0.048 [-2.706, 1.026], loss: 2.211926, mean_absolute_error: 31.055481, mean_q: 17.981432, mean_eps: 0.717746
  313830/2000000: episode: 3182, duration: 1.944s, episode steps: 133, steps per second: 68, episode reward: -119.385, mean reward: -0.898 [-100.000, 8.073], mean action: 1.602 [0.000, 3.000], mean observation: 0.028 [-4.043, 1.014], loss: 1.595113, mean_absolute_error: 30.580410, mean_q: 17.948749, mean_eps: 0.717612
  313904/2000000: episode: 3183, duration: 1.099s, episode steps: 74, steps per second: 67, episode reward: -79.137, mean reward: -1.069 [-100.000, 16.240], mean action: 1.581 [0.000, 3.000], mean observation: -0.045 [-1.176, 1.000], loss: 1.353081, mean_absolute_error: 30.025364, mean_q: 19.884545, mean_eps: 0.717521
  313982/2000000: episode: 3184, duration: 1.168s, episode steps: 78, steps per second: 67, episode reward: -90.393, mean reward: -1.159 [-100.000, 9.561], mean action: 1.628 [0.000, 3.000], mean observation: -0.062 [-2.856, 1.000], loss: 1.070495, mean_absolute_error: 29.808305, mean_q: 16.810074, mean_eps: 0.717452
  314062/2000000: episode: 3185, duration: 1.168s, episode steps: 80, steps per second: 68, episode reward: -110.973, mean reward: -1.387 [-100.000, 6.088], mean action: 1.688 [0.000, 3.000], mean observation: -0.115 [-1.105, 2.205], loss: 1.184875, mean_absolute_error: 31.353821, mean_q: 15.906063, mean_eps: 0.717380
  314150/2000000: episode: 3186, duration: 1.268s, episode steps: 88, steps per second: 69, episode reward: -89.321, mean reward: -1.015 [-100.000, 10.089], mean action: 1.818 [0.000, 3.000], mean observation: -0.080 [-0.937, 1.000], loss: 1.949576, mean_absolute_error: 31.324962, mean_q: 18.538054, mean_eps: 0.717305
  314227/2000000: episode: 3187, duration: 1.101s, episode steps: 77, steps per second: 70, episode reward: -87.600, mean reward: -1.138 [-100.000, 10.115], mean action: 1.545 [0.000, 3.000], mean observation: -0.026 [-1.095, 3.631], loss: 1.825096, mean_absolute_error: 31.462460, mean_q: 15.359096, mean_eps: 0.717231
  314314/2000000: episode: 3188, duration: 1.268s, episode steps: 87, steps per second: 69, episode reward: -133.265, mean reward: -1.532 [-100.000, 8.228], mean action: 1.621 [0.000, 3.000], mean observation: 0.027 [-4.290, 1.000], loss: 2.342893, mean_absolute_error: 32.379296, mean_q: 16.173563, mean_eps: 0.717157
  314433/2000000: episode: 3189, duration: 1.752s, episode steps: 119, steps per second: 68, episode reward: -122.158, mean reward: -1.027 [-100.000, 17.341], mean action: 1.546 [0.000, 3.000], mean observation: 0.024 [-3.737, 1.000], loss: 1.690166, mean_absolute_error: 30.104739, mean_q: 19.789569, mean_eps: 0.717063
  314517/2000000: episode: 3190, duration: 1.213s, episode steps: 84, steps per second: 69, episode reward: -173.328, mean reward: -2.063 [-100.000, 8.766], mean action: 1.762 [0.000, 3.000], mean observation: -0.032 [-1.418, 4.104], loss: 1.831027, mean_absolute_error: 31.576642, mean_q: 16.116046, mean_eps: 0.716972
  314610/2000000: episode: 3191, duration: 1.332s, episode steps: 93, steps per second: 70, episode reward: -112.948, mean reward: -1.214 [-100.000, 21.592], mean action: 1.613 [0.000, 3.000], mean observation: 0.061 [-1.028, 1.000], loss: 1.572004, mean_absolute_error: 30.204955, mean_q: 14.434784, mean_eps: 0.716892
  314700/2000000: episode: 3192, duration: 1.319s, episode steps: 90, steps per second: 68, episode reward: -115.280, mean reward: -1.281 [-100.000, 8.720], mean action: 1.600 [0.000, 3.000], mean observation: 0.022 [-3.397, 1.000], loss: 1.733506, mean_absolute_error: 30.112336, mean_q: 18.393495, mean_eps: 0.716811
  314778/2000000: episode: 3193, duration: 1.136s, episode steps: 78, steps per second: 69, episode reward: -140.973, mean reward: -1.807 [-100.000, 10.492], mean action: 1.436 [0.000, 3.000], mean observation: 0.122 [-1.360, 4.902], loss: 1.271466, mean_absolute_error: 30.731618, mean_q: 16.170121, mean_eps: 0.716736
  314907/2000000: episode: 3194, duration: 1.824s, episode steps: 129, steps per second: 71, episode reward: -109.888, mean reward: -0.852 [-100.000, 10.177], mean action: 1.574 [0.000, 3.000], mean observation: 0.083 [-0.984, 3.598], loss: 1.671027, mean_absolute_error: 30.546425, mean_q: 20.143121, mean_eps: 0.716642
  315023/2000000: episode: 3195, duration: 1.667s, episode steps: 116, steps per second: 70, episode reward: -144.706, mean reward: -1.247 [-100.000, 7.080], mean action: 1.603 [0.000, 3.000], mean observation: 0.091 [-4.460, 1.000], loss: 1.474177, mean_absolute_error: 30.945497, mean_q: 18.036418, mean_eps: 0.716532
  315183/2000000: episode: 3196, duration: 2.301s, episode steps: 160, steps per second: 70, episode reward: -134.824, mean reward: -0.843 [-100.000, 7.161], mean action: 1.594 [0.000, 3.000], mean observation: 0.064 [-4.353, 1.001], loss: 1.592985, mean_absolute_error: 30.681990, mean_q: 17.152780, mean_eps: 0.716408
  315249/2000000: episode: 3197, duration: 0.984s, episode steps: 66, steps per second: 67, episode reward: -115.324, mean reward: -1.747 [-100.000, 8.457], mean action: 1.652 [0.000, 3.000], mean observation: -0.055 [-1.408, 4.995], loss: 1.978042, mean_absolute_error: 30.989375, mean_q: 19.314218, mean_eps: 0.716306
  315348/2000000: episode: 3198, duration: 1.444s, episode steps: 99, steps per second: 69, episode reward: -90.387, mean reward: -0.913 [-100.000, 8.275], mean action: 1.485 [0.000, 3.000], mean observation: 0.037 [-3.486, 1.000], loss: 2.317072, mean_absolute_error: 32.089089, mean_q: 14.649776, mean_eps: 0.716232
  315412/2000000: episode: 3199, duration: 0.990s, episode steps: 64, steps per second: 65, episode reward: -117.995, mean reward: -1.844 [-100.000, 10.966], mean action: 1.516 [0.000, 3.000], mean observation: -0.004 [-1.507, 1.000], loss: 1.084837, mean_absolute_error: 30.079773, mean_q: 18.792569, mean_eps: 0.716160
  315502/2000000: episode: 3200, duration: 1.343s, episode steps: 90, steps per second: 67, episode reward: -72.506, mean reward: -0.806 [-100.000, 13.602], mean action: 1.644 [0.000, 3.000], mean observation: 0.007 [-1.048, 1.000], loss: 1.775151, mean_absolute_error: 31.138873, mean_q: 16.106569, mean_eps: 0.716090
  315617/2000000: episode: 3201, duration: 1.668s, episode steps: 115, steps per second: 69, episode reward: -137.268, mean reward: -1.194 [-100.000, 8.486], mean action: 1.626 [0.000, 3.000], mean observation: -0.063 [-3.999, 1.000], loss: 1.501154, mean_absolute_error: 30.003907, mean_q: 19.616208, mean_eps: 0.715996
  315751/2000000: episode: 3202, duration: 1.898s, episode steps: 134, steps per second: 71, episode reward: -137.564, mean reward: -1.027 [-100.000, 16.474], mean action: 1.672 [0.000, 3.000], mean observation: 0.092 [-1.119, 1.000], loss: 2.784647, mean_absolute_error: 32.058914, mean_q: 18.622785, mean_eps: 0.715884
  315834/2000000: episode: 3203, duration: 1.229s, episode steps: 83, steps per second: 68, episode reward: -191.113, mean reward: -2.303 [-100.000, 9.624], mean action: 1.928 [0.000, 3.000], mean observation: -0.055 [-3.121, 1.000], loss: 2.094353, mean_absolute_error: 32.801972, mean_q: 15.510893, mean_eps: 0.715787
  315931/2000000: episode: 3204, duration: 1.399s, episode steps: 97, steps per second: 69, episode reward: -72.583, mean reward: -0.748 [-100.000, 5.904], mean action: 1.814 [0.000, 3.000], mean observation: -0.006 [-0.905, 2.958], loss: 1.529179, mean_absolute_error: 30.548928, mean_q: 22.037030, mean_eps: 0.715706
  316012/2000000: episode: 3205, duration: 1.233s, episode steps: 81, steps per second: 66, episode reward: -94.831, mean reward: -1.171 [-100.000, 11.357], mean action: 1.728 [0.000, 3.000], mean observation: -0.098 [-1.057, 3.385], loss: 1.305979, mean_absolute_error: 31.745488, mean_q: 17.642486, mean_eps: 0.715627
  316110/2000000: episode: 3206, duration: 1.427s, episode steps: 98, steps per second: 69, episode reward: -84.051, mean reward: -0.858 [-100.000, 9.703], mean action: 1.612 [0.000, 3.000], mean observation: -0.086 [-1.006, 3.016], loss: 1.682806, mean_absolute_error: 30.186889, mean_q: 17.724250, mean_eps: 0.715546
  316252/2000000: episode: 3207, duration: 2.075s, episode steps: 142, steps per second: 68, episode reward: -198.455, mean reward: -1.398 [-100.000, 24.266], mean action: 1.697 [0.000, 3.000], mean observation: -0.129 [-1.244, 1.999], loss: 2.079894, mean_absolute_error: 31.615064, mean_q: 15.232088, mean_eps: 0.715438
  316390/2000000: episode: 3208, duration: 2.020s, episode steps: 138, steps per second: 68, episode reward: -106.825, mean reward: -0.774 [-100.000, 14.309], mean action: 1.652 [0.000, 3.000], mean observation: -0.012 [-0.966, 3.363], loss: 1.901248, mean_absolute_error: 31.209791, mean_q: 18.246891, mean_eps: 0.715312
  316488/2000000: episode: 3209, duration: 1.470s, episode steps: 98, steps per second: 67, episode reward: -156.606, mean reward: -1.598 [-100.000, 5.962], mean action: 1.653 [0.000, 3.000], mean observation: 0.010 [-1.243, 4.241], loss: 1.372149, mean_absolute_error: 31.056319, mean_q: 20.819814, mean_eps: 0.715206
  316605/2000000: episode: 3210, duration: 1.797s, episode steps: 117, steps per second: 65, episode reward: -146.353, mean reward: -1.251 [-100.000, 9.034], mean action: 1.778 [0.000, 3.000], mean observation: -0.073 [-1.212, 3.858], loss: 2.333740, mean_absolute_error: 31.202279, mean_q: 17.325288, mean_eps: 0.715109
  316728/2000000: episode: 3211, duration: 2.252s, episode steps: 123, steps per second: 55, episode reward: -97.575, mean reward: -0.793 [-100.000, 10.547], mean action: 1.472 [0.000, 3.000], mean observation: 0.032 [-1.138, 1.000], loss: 1.484843, mean_absolute_error: 30.804637, mean_q: 22.025199, mean_eps: 0.715001
  316806/2000000: episode: 3212, duration: 1.283s, episode steps: 78, steps per second: 61, episode reward: -121.127, mean reward: -1.553 [-100.000, 9.655], mean action: 1.462 [0.000, 3.000], mean observation: 0.091 [-3.971, 1.000], loss: 1.347806, mean_absolute_error: 30.191111, mean_q: 21.114130, mean_eps: 0.714911
  316891/2000000: episode: 3213, duration: 1.247s, episode steps: 85, steps per second: 68, episode reward: -95.085, mean reward: -1.119 [-100.000, 16.459], mean action: 1.682 [0.000, 3.000], mean observation: 0.041 [-1.141, 1.000], loss: 1.455646, mean_absolute_error: 31.765703, mean_q: 20.727272, mean_eps: 0.714837
  316971/2000000: episode: 3214, duration: 1.153s, episode steps: 80, steps per second: 69, episode reward: -88.165, mean reward: -1.102 [-100.000, 7.328], mean action: 1.762 [0.000, 3.000], mean observation: -0.100 [-1.088, 2.781], loss: 1.718840, mean_absolute_error: 31.313638, mean_q: 16.317369, mean_eps: 0.714763
  317054/2000000: episode: 3215, duration: 1.209s, episode steps: 83, steps per second: 69, episode reward: -82.807, mean reward: -0.998 [-100.000, 6.564], mean action: 1.675 [0.000, 3.000], mean observation: 0.030 [-3.352, 1.000], loss: 2.091424, mean_absolute_error: 33.590796, mean_q: 15.020507, mean_eps: 0.714689
  317185/2000000: episode: 3216, duration: 1.886s, episode steps: 131, steps per second: 69, episode reward: -34.833, mean reward: -0.266 [-100.000, 17.885], mean action: 1.641 [0.000, 3.000], mean observation: 0.009 [-0.765, 1.000], loss: 1.630517, mean_absolute_error: 31.156816, mean_q: 19.034794, mean_eps: 0.714592
  317293/2000000: episode: 3217, duration: 1.555s, episode steps: 108, steps per second: 69, episode reward: -102.689, mean reward: -0.951 [-100.000, 17.674], mean action: 1.620 [0.000, 3.000], mean observation: -0.034 [-0.999, 1.000], loss: 1.936351, mean_absolute_error: 31.668898, mean_q: 18.292141, mean_eps: 0.714484
  317384/2000000: episode: 3218, duration: 1.339s, episode steps: 91, steps per second: 68, episode reward: -162.866, mean reward: -1.790 [-100.000, 9.913], mean action: 1.549 [0.000, 3.000], mean observation: 0.076 [-3.615, 1.000], loss: 1.724116, mean_absolute_error: 31.943645, mean_q: 15.971858, mean_eps: 0.714396
  317460/2000000: episode: 3219, duration: 1.149s, episode steps: 76, steps per second: 66, episode reward: -130.281, mean reward: -1.714 [-100.000, 12.130], mean action: 1.803 [0.000, 3.000], mean observation: -0.076 [-1.203, 3.887], loss: 1.252530, mean_absolute_error: 29.486261, mean_q: 20.332266, mean_eps: 0.714322
  317547/2000000: episode: 3220, duration: 1.267s, episode steps: 87, steps per second: 69, episode reward: -86.373, mean reward: -0.993 [-100.000, 22.209], mean action: 1.632 [0.000, 3.000], mean observation: 0.039 [-1.156, 1.000], loss: 1.297707, mean_absolute_error: 29.588447, mean_q: 18.045774, mean_eps: 0.714248
  317607/2000000: episode: 3221, duration: 0.882s, episode steps: 60, steps per second: 68, episode reward: -90.268, mean reward: -1.504 [-100.000, 8.162], mean action: 1.750 [0.000, 3.000], mean observation: 0.076 [-1.218, 4.123], loss: 2.987244, mean_absolute_error: 32.431352, mean_q: 17.155443, mean_eps: 0.714182
  317691/2000000: episode: 3222, duration: 1.195s, episode steps: 84, steps per second: 70, episode reward: -63.299, mean reward: -0.754 [-100.000, 12.335], mean action: 1.714 [0.000, 3.000], mean observation: -0.067 [-3.178, 1.000], loss: 1.456138, mean_absolute_error: 31.020225, mean_q: 17.845700, mean_eps: 0.714117
  317824/2000000: episode: 3223, duration: 1.962s, episode steps: 133, steps per second: 68, episode reward: -109.748, mean reward: -0.825 [-100.000, 8.716], mean action: 1.699 [0.000, 3.000], mean observation: -0.075 [-3.197, 1.034], loss: 1.498137, mean_absolute_error: 30.638025, mean_q: 17.281300, mean_eps: 0.714020
  317950/2000000: episode: 3224, duration: 1.868s, episode steps: 126, steps per second: 67, episode reward: -160.060, mean reward: -1.270 [-100.000, 8.851], mean action: 1.746 [0.000, 3.000], mean observation: -0.110 [-1.130, 2.799], loss: 1.148916, mean_absolute_error: 29.782011, mean_q: 18.747572, mean_eps: 0.713903
  318063/2000000: episode: 3225, duration: 1.617s, episode steps: 113, steps per second: 70, episode reward: -90.823, mean reward: -0.804 [-100.000, 7.290], mean action: 1.646 [0.000, 3.000], mean observation: -0.024 [-0.971, 1.000], loss: 1.591298, mean_absolute_error: 30.667847, mean_q: 19.067735, mean_eps: 0.713795
  318156/2000000: episode: 3226, duration: 1.386s, episode steps: 93, steps per second: 67, episode reward: -104.981, mean reward: -1.129 [-100.000, 7.289], mean action: 1.581 [0.000, 3.000], mean observation: -0.040 [-1.054, 1.983], loss: 1.477069, mean_absolute_error: 30.693323, mean_q: 18.850169, mean_eps: 0.713703
  318273/2000000: episode: 3227, duration: 1.703s, episode steps: 117, steps per second: 69, episode reward: -277.972, mean reward: -2.376 [-100.000, 46.387], mean action: 1.436 [0.000, 3.000], mean observation: 0.213 [-1.086, 2.192], loss: 1.586162, mean_absolute_error: 29.535734, mean_q: 19.875722, mean_eps: 0.713607
  318411/2000000: episode: 3228, duration: 1.960s, episode steps: 138, steps per second: 70, episode reward: -71.472, mean reward: -0.518 [-100.000, 43.453], mean action: 1.594 [0.000, 3.000], mean observation: 0.157 [-1.470, 1.000], loss: 1.400648, mean_absolute_error: 30.502060, mean_q: 20.325931, mean_eps: 0.713492
  318510/2000000: episode: 3229, duration: 1.426s, episode steps: 99, steps per second: 69, episode reward: -95.930, mean reward: -0.969 [-100.000, 20.047], mean action: 1.556 [0.000, 3.000], mean observation: 0.022 [-1.203, 1.000], loss: 1.098030, mean_absolute_error: 30.539946, mean_q: 19.637030, mean_eps: 0.713386
  318629/2000000: episode: 3230, duration: 1.743s, episode steps: 119, steps per second: 68, episode reward: -108.564, mean reward: -0.912 [-100.000, 8.977], mean action: 1.681 [0.000, 3.000], mean observation: -0.015 [-1.067, 3.560], loss: 1.311095, mean_absolute_error: 29.518289, mean_q: 19.036259, mean_eps: 0.713287
  318756/2000000: episode: 3231, duration: 1.846s, episode steps: 127, steps per second: 69, episode reward: -78.499, mean reward: -0.618 [-100.000, 11.596], mean action: 1.591 [0.000, 3.000], mean observation: 0.191 [-0.946, 1.000], loss: 2.022742, mean_absolute_error: 32.020816, mean_q: 17.581294, mean_eps: 0.713177
  318850/2000000: episode: 3232, duration: 1.510s, episode steps: 94, steps per second: 62, episode reward: -106.595, mean reward: -1.134 [-100.000, 14.945], mean action: 1.585 [0.000, 3.000], mean observation: 0.114 [-0.993, 1.000], loss: 2.127479, mean_absolute_error: 30.545245, mean_q: 20.459384, mean_eps: 0.713078
  318966/2000000: episode: 3233, duration: 1.670s, episode steps: 116, steps per second: 69, episode reward: -206.660, mean reward: -1.782 [-100.000, 15.291], mean action: 1.759 [0.000, 3.000], mean observation: -0.033 [-4.768, 1.074], loss: 1.576624, mean_absolute_error: 31.813556, mean_q: 17.198283, mean_eps: 0.712983
  319061/2000000: episode: 3234, duration: 1.404s, episode steps: 95, steps per second: 68, episode reward: -131.538, mean reward: -1.385 [-100.000, 13.950], mean action: 1.642 [0.000, 3.000], mean observation: -0.013 [-1.350, 1.000], loss: 1.385263, mean_absolute_error: 30.376751, mean_q: 18.112211, mean_eps: 0.712887
  319206/2000000: episode: 3235, duration: 2.078s, episode steps: 145, steps per second: 70, episode reward: -110.422, mean reward: -0.762 [-100.000, 15.696], mean action: 1.628 [0.000, 3.000], mean observation: -0.058 [-3.464, 1.000], loss: 1.715920, mean_absolute_error: 31.148770, mean_q: 17.609933, mean_eps: 0.712779
  319283/2000000: episode: 3236, duration: 1.111s, episode steps: 77, steps per second: 69, episode reward: -116.825, mean reward: -1.517 [-100.000, 16.303], mean action: 1.753 [0.000, 3.000], mean observation: -0.123 [-1.207, 1.000], loss: 1.307841, mean_absolute_error: 30.726492, mean_q: 20.625045, mean_eps: 0.712680
  319406/2000000: episode: 3237, duration: 1.775s, episode steps: 123, steps per second: 69, episode reward: -204.974, mean reward: -1.666 [-100.000, 16.765], mean action: 1.805 [0.000, 3.000], mean observation: -0.024 [-1.641, 1.089], loss: 1.689212, mean_absolute_error: 31.878207, mean_q: 16.619662, mean_eps: 0.712590
  319516/2000000: episode: 3238, duration: 1.614s, episode steps: 110, steps per second: 68, episode reward: -103.635, mean reward: -0.942 [-100.000, 12.111], mean action: 1.591 [0.000, 3.000], mean observation: 0.073 [-3.068, 1.000], loss: 1.540708, mean_absolute_error: 31.156118, mean_q: 16.967652, mean_eps: 0.712486
  319610/2000000: episode: 3239, duration: 1.369s, episode steps: 94, steps per second: 69, episode reward: -138.442, mean reward: -1.473 [-100.000, 7.516], mean action: 1.383 [0.000, 3.000], mean observation: -0.044 [-1.312, 4.969], loss: 1.333179, mean_absolute_error: 30.843747, mean_q: 16.639462, mean_eps: 0.712394
  319705/2000000: episode: 3240, duration: 1.385s, episode steps: 95, steps per second: 69, episode reward: -95.748, mean reward: -1.008 [-100.000, 6.494], mean action: 1.663 [0.000, 3.000], mean observation: -0.017 [-3.618, 1.000], loss: 1.749431, mean_absolute_error: 30.138047, mean_q: 18.650132, mean_eps: 0.712308
  319800/2000000: episode: 3241, duration: 1.383s, episode steps: 95, steps per second: 69, episode reward: -118.620, mean reward: -1.249 [-100.000, 12.942], mean action: 1.642 [0.000, 3.000], mean observation: 0.047 [-1.026, 3.115], loss: 1.867148, mean_absolute_error: 31.309808, mean_q: 17.991791, mean_eps: 0.712223
  319905/2000000: episode: 3242, duration: 1.546s, episode steps: 105, steps per second: 68, episode reward: -109.269, mean reward: -1.041 [-100.000, 18.802], mean action: 1.657 [0.000, 3.000], mean observation: -0.025 [-1.215, 1.000], loss: 1.673970, mean_absolute_error: 31.299026, mean_q: 16.140815, mean_eps: 0.712133
  319989/2000000: episode: 3243, duration: 1.203s, episode steps: 84, steps per second: 70, episode reward: -126.687, mean reward: -1.508 [-100.000, 17.800], mean action: 1.643 [0.000, 3.000], mean observation: 0.089 [-1.191, 3.768], loss: 1.451794, mean_absolute_error: 29.792168, mean_q: 22.483423, mean_eps: 0.712047
  320094/2000000: episode: 3244, duration: 1.521s, episode steps: 105, steps per second: 69, episode reward: -129.187, mean reward: -1.230 [-100.000, 15.867], mean action: 1.667 [0.000, 3.000], mean observation: 0.032 [-1.017, 3.624], loss: 1.889624, mean_absolute_error: 31.351479, mean_q: 17.238935, mean_eps: 0.711962
  320193/2000000: episode: 3245, duration: 1.429s, episode steps: 99, steps per second: 69, episode reward: -116.101, mean reward: -1.173 [-100.000, 9.147], mean action: 1.556 [0.000, 3.000], mean observation: 0.099 [-1.115, 3.906], loss: 1.709374, mean_absolute_error: 31.424802, mean_q: 18.319826, mean_eps: 0.711870
  320279/2000000: episode: 3246, duration: 1.210s, episode steps: 86, steps per second: 71, episode reward: -104.006, mean reward: -1.209 [-100.000, 21.423], mean action: 1.663 [0.000, 3.000], mean observation: 0.063 [-1.160, 1.000], loss: 1.513278, mean_absolute_error: 32.039761, mean_q: 22.453204, mean_eps: 0.711788
  320369/2000000: episode: 3247, duration: 1.316s, episode steps: 90, steps per second: 68, episode reward: -103.129, mean reward: -1.146 [-100.000, 23.864], mean action: 1.533 [0.000, 3.000], mean observation: 0.060 [-1.074, 3.127], loss: 1.886720, mean_absolute_error: 31.239796, mean_q: 19.224735, mean_eps: 0.711708
  320489/2000000: episode: 3248, duration: 1.704s, episode steps: 120, steps per second: 70, episode reward: -115.290, mean reward: -0.961 [-100.000, 6.991], mean action: 1.692 [0.000, 3.000], mean observation: -0.104 [-1.081, 3.528], loss: 1.524109, mean_absolute_error: 31.544748, mean_q: 20.979695, mean_eps: 0.711613
  320680/2000000: episode: 3249, duration: 2.831s, episode steps: 191, steps per second: 67, episode reward: -181.628, mean reward: -0.951 [-100.000, 5.983], mean action: 1.707 [0.000, 3.000], mean observation: -0.081 [-4.410, 1.033], loss: 2.344171, mean_absolute_error: 32.215735, mean_q: 19.171026, mean_eps: 0.711474
  320764/2000000: episode: 3250, duration: 1.265s, episode steps: 84, steps per second: 66, episode reward: -79.806, mean reward: -0.950 [-100.000, 12.067], mean action: 1.679 [0.000, 3.000], mean observation: 0.023 [-3.596, 1.000], loss: 1.350503, mean_absolute_error: 31.089972, mean_q: 20.970841, mean_eps: 0.711352
  320904/2000000: episode: 3251, duration: 2.060s, episode steps: 140, steps per second: 68, episode reward: -82.792, mean reward: -0.591 [-100.000, 11.587], mean action: 1.750 [0.000, 3.000], mean observation: -0.077 [-1.099, 4.157], loss: 1.332246, mean_absolute_error: 31.106629, mean_q: 19.240658, mean_eps: 0.711251
  321034/2000000: episode: 3252, duration: 1.902s, episode steps: 130, steps per second: 68, episode reward: -110.832, mean reward: -0.853 [-100.000, 9.681], mean action: 1.623 [0.000, 3.000], mean observation: 0.111 [-0.966, 3.743], loss: 2.002985, mean_absolute_error: 31.958799, mean_q: 18.174335, mean_eps: 0.711129
  321146/2000000: episode: 3253, duration: 1.612s, episode steps: 112, steps per second: 69, episode reward: -245.138, mean reward: -2.189 [-100.000, 7.826], mean action: 1.661 [0.000, 3.000], mean observation: -0.161 [-1.446, 1.000], loss: 1.542069, mean_absolute_error: 32.987117, mean_q: 14.797509, mean_eps: 0.711019
  321243/2000000: episode: 3254, duration: 1.456s, episode steps: 97, steps per second: 67, episode reward: -130.867, mean reward: -1.349 [-100.000, 13.128], mean action: 1.629 [0.000, 3.000], mean observation: 0.074 [-1.244, 4.062], loss: 1.734292, mean_absolute_error: 31.070540, mean_q: 17.070644, mean_eps: 0.710925
  321333/2000000: episode: 3255, duration: 1.312s, episode steps: 90, steps per second: 69, episode reward: -127.952, mean reward: -1.422 [-100.000, 11.549], mean action: 1.733 [0.000, 3.000], mean observation: 0.003 [-1.161, 4.085], loss: 1.569796, mean_absolute_error: 32.959593, mean_q: 18.996128, mean_eps: 0.710841
  321477/2000000: episode: 3256, duration: 2.067s, episode steps: 144, steps per second: 70, episode reward: -51.405, mean reward: -0.357 [-100.000, 17.313], mean action: 1.729 [0.000, 3.000], mean observation: 0.018 [-0.812, 1.002], loss: 2.284645, mean_absolute_error: 32.673265, mean_q: 17.236259, mean_eps: 0.710735
  321580/2000000: episode: 3257, duration: 1.493s, episode steps: 103, steps per second: 69, episode reward: -98.157, mean reward: -0.953 [-100.000, 14.968], mean action: 1.777 [0.000, 3.000], mean observation: 0.000 [-1.138, 1.000], loss: 1.795856, mean_absolute_error: 31.865972, mean_q: 19.737574, mean_eps: 0.710625
  321709/2000000: episode: 3258, duration: 1.873s, episode steps: 129, steps per second: 69, episode reward: -152.359, mean reward: -1.181 [-100.000, 12.564], mean action: 1.612 [0.000, 3.000], mean observation: 0.118 [-3.889, 1.000], loss: 1.953520, mean_absolute_error: 32.590277, mean_q: 18.318322, mean_eps: 0.710520
  321789/2000000: episode: 3259, duration: 1.152s, episode steps: 80, steps per second: 69, episode reward: -106.356, mean reward: -1.329 [-100.000, 12.620], mean action: 1.613 [0.000, 3.000], mean observation: -0.012 [-1.200, 1.000], loss: 1.407847, mean_absolute_error: 32.163878, mean_q: 19.177798, mean_eps: 0.710425
  321963/2000000: episode: 3260, duration: 2.451s, episode steps: 174, steps per second: 71, episode reward: -119.941, mean reward: -0.689 [-100.000, 6.161], mean action: 1.759 [0.000, 3.000], mean observation: 0.083 [-2.902, 1.000], loss: 1.244869, mean_absolute_error: 31.299150, mean_q: 18.706871, mean_eps: 0.710312
  322121/2000000: episode: 3261, duration: 2.304s, episode steps: 158, steps per second: 69, episode reward: -27.818, mean reward: -0.176 [-100.000, 10.836], mean action: 1.791 [0.000, 3.000], mean observation: 0.127 [-0.617, 1.246], loss: 1.840033, mean_absolute_error: 31.790407, mean_q: 17.395035, mean_eps: 0.710162
  322230/2000000: episode: 3262, duration: 1.530s, episode steps: 109, steps per second: 71, episode reward: -98.062, mean reward: -0.900 [-100.000, 9.228], mean action: 1.661 [0.000, 3.000], mean observation: 0.113 [-3.181, 1.000], loss: 1.564766, mean_absolute_error: 31.030526, mean_q: 19.636435, mean_eps: 0.710042
  322354/2000000: episode: 3263, duration: 1.774s, episode steps: 124, steps per second: 70, episode reward: -132.397, mean reward: -1.068 [-100.000, 5.702], mean action: 1.476 [0.000, 3.000], mean observation: 0.128 [-2.837, 1.000], loss: 1.739237, mean_absolute_error: 32.484658, mean_q: 17.012982, mean_eps: 0.709937
  322434/2000000: episode: 3264, duration: 1.141s, episode steps: 80, steps per second: 70, episode reward: -84.916, mean reward: -1.061 [-100.000, 21.131], mean action: 1.562 [0.000, 3.000], mean observation: -0.080 [-1.188, 1.000], loss: 1.628795, mean_absolute_error: 32.609251, mean_q: 17.644324, mean_eps: 0.709845
  322509/2000000: episode: 3265, duration: 1.089s, episode steps: 75, steps per second: 69, episode reward: -107.075, mean reward: -1.428 [-100.000, 11.854], mean action: 1.653 [0.000, 3.000], mean observation: -0.085 [-3.271, 1.000], loss: 3.262970, mean_absolute_error: 34.394770, mean_q: 15.923719, mean_eps: 0.709775
  322594/2000000: episode: 3266, duration: 1.200s, episode steps: 85, steps per second: 71, episode reward: -117.753, mean reward: -1.385 [-100.000, 10.179], mean action: 1.729 [0.000, 3.000], mean observation: -0.052 [-1.205, 3.804], loss: 2.034283, mean_absolute_error: 32.463894, mean_q: 15.738740, mean_eps: 0.709703
  322722/2000000: episode: 3267, duration: 1.828s, episode steps: 128, steps per second: 70, episode reward: -192.616, mean reward: -1.505 [-100.000, 17.277], mean action: 1.672 [0.000, 3.000], mean observation: -0.120 [-1.424, 1.000], loss: 1.736665, mean_absolute_error: 31.631793, mean_q: 20.195231, mean_eps: 0.709608
  322854/2000000: episode: 3268, duration: 1.878s, episode steps: 132, steps per second: 70, episode reward: -162.545, mean reward: -1.231 [-100.000, 11.001], mean action: 1.576 [0.000, 3.000], mean observation: -0.039 [-1.109, 3.532], loss: 1.265381, mean_absolute_error: 30.267438, mean_q: 19.690748, mean_eps: 0.709491
  322943/2000000: episode: 3269, duration: 1.238s, episode steps: 89, steps per second: 72, episode reward: -110.622, mean reward: -1.243 [-100.000, 8.002], mean action: 1.629 [0.000, 3.000], mean observation: -0.123 [-1.156, 3.626], loss: 1.833493, mean_absolute_error: 31.731077, mean_q: 20.793840, mean_eps: 0.709392
  323062/2000000: episode: 3270, duration: 1.697s, episode steps: 119, steps per second: 70, episode reward: -103.855, mean reward: -0.873 [-100.000, 9.690], mean action: 1.555 [0.000, 3.000], mean observation: -0.007 [-0.963, 3.286], loss: 1.915465, mean_absolute_error: 33.049740, mean_q: 16.783340, mean_eps: 0.709298
  323182/2000000: episode: 3271, duration: 1.689s, episode steps: 120, steps per second: 71, episode reward: -94.273, mean reward: -0.786 [-100.000, 17.443], mean action: 1.658 [0.000, 3.000], mean observation: -0.023 [-1.137, 1.000], loss: 1.101767, mean_absolute_error: 31.563460, mean_q: 19.968480, mean_eps: 0.709190
  323261/2000000: episode: 3272, duration: 1.139s, episode steps: 79, steps per second: 69, episode reward: -118.610, mean reward: -1.501 [-100.000, 12.955], mean action: 1.544 [0.000, 3.000], mean observation: 0.090 [-1.085, 3.865], loss: 1.556324, mean_absolute_error: 30.427151, mean_q: 22.111992, mean_eps: 0.709100
  323378/2000000: episode: 3273, duration: 1.646s, episode steps: 117, steps per second: 71, episode reward: -108.373, mean reward: -0.926 [-100.000, 8.990], mean action: 1.564 [0.000, 3.000], mean observation: 0.116 [-3.499, 1.000], loss: 2.066358, mean_absolute_error: 31.664092, mean_q: 15.918298, mean_eps: 0.709012
  324378/2000000: episode: 3274, duration: 15.190s, episode steps: 1000, steps per second: 66, episode reward: 36.687, mean reward: 0.037 [-22.615, 45.077], mean action: 1.539 [0.000, 3.000], mean observation: 0.061 [-1.756, 1.000], loss: 1.789109, mean_absolute_error: 31.663634, mean_q: 19.093090, mean_eps: 0.708510
  324475/2000000: episode: 3275, duration: 1.367s, episode steps: 97, steps per second: 71, episode reward: -135.599, mean reward: -1.398 [-100.000, 10.268], mean action: 1.845 [0.000, 3.000], mean observation: -0.116 [-0.959, 3.072], loss: 1.796141, mean_absolute_error: 32.795424, mean_q: 17.588011, mean_eps: 0.708017
  324573/2000000: episode: 3276, duration: 1.403s, episode steps: 98, steps per second: 70, episode reward: -113.169, mean reward: -1.155 [-100.000, 18.156], mean action: 1.643 [0.000, 3.000], mean observation: -0.034 [-1.296, 1.000], loss: 1.763006, mean_absolute_error: 32.550302, mean_q: 18.666445, mean_eps: 0.707928
  324646/2000000: episode: 3277, duration: 1.041s, episode steps: 73, steps per second: 70, episode reward: -117.516, mean reward: -1.610 [-100.000, 17.468], mean action: 1.562 [0.000, 3.000], mean observation: 0.118 [-1.064, 4.097], loss: 1.633378, mean_absolute_error: 30.554095, mean_q: 19.465769, mean_eps: 0.707851
  324770/2000000: episode: 3278, duration: 2.136s, episode steps: 124, steps per second: 58, episode reward: -115.357, mean reward: -0.930 [-100.000, 6.987], mean action: 1.750 [0.000, 3.000], mean observation: -0.078 [-2.992, 1.000], loss: 1.689625, mean_absolute_error: 30.942088, mean_q: 19.227072, mean_eps: 0.707763
  324913/2000000: episode: 3279, duration: 2.110s, episode steps: 143, steps per second: 68, episode reward: -95.391, mean reward: -0.667 [-100.000, 13.030], mean action: 1.566 [0.000, 3.000], mean observation: 0.136 [-3.631, 1.023], loss: 1.995130, mean_absolute_error: 32.455527, mean_q: 16.002201, mean_eps: 0.707642
  325058/2000000: episode: 3280, duration: 2.048s, episode steps: 145, steps per second: 71, episode reward: -134.606, mean reward: -0.928 [-100.000, 17.551], mean action: 1.579 [0.000, 3.000], mean observation: -0.015 [-1.951, 1.000], loss: 2.026951, mean_absolute_error: 31.251190, mean_q: 18.589969, mean_eps: 0.707513
  325184/2000000: episode: 3281, duration: 1.809s, episode steps: 126, steps per second: 70, episode reward: -116.286, mean reward: -0.923 [-100.000, 9.656], mean action: 1.714 [0.000, 3.000], mean observation: -0.030 [-0.977, 3.019], loss: 1.985248, mean_absolute_error: 32.039603, mean_q: 19.338596, mean_eps: 0.707392
  325255/2000000: episode: 3282, duration: 1.020s, episode steps: 71, steps per second: 70, episode reward: -96.942, mean reward: -1.365 [-100.000, 7.335], mean action: 1.535 [0.000, 3.000], mean observation: 0.029 [-3.837, 1.000], loss: 1.793384, mean_absolute_error: 31.220565, mean_q: 17.131628, mean_eps: 0.707304
  325396/2000000: episode: 3283, duration: 2.037s, episode steps: 141, steps per second: 69, episode reward: -74.630, mean reward: -0.529 [-100.000, 21.021], mean action: 1.652 [0.000, 3.000], mean observation: 0.038 [-0.851, 2.433], loss: 1.194007, mean_absolute_error: 31.059743, mean_q: 21.995428, mean_eps: 0.707208
  325485/2000000: episode: 3284, duration: 1.316s, episode steps: 89, steps per second: 68, episode reward: -121.791, mean reward: -1.368 [-100.000, 15.832], mean action: 1.596 [0.000, 3.000], mean observation: 0.020 [-1.009, 1.000], loss: 1.743475, mean_absolute_error: 31.514270, mean_q: 18.365385, mean_eps: 0.707104
  325559/2000000: episode: 3285, duration: 1.036s, episode steps: 74, steps per second: 71, episode reward: -118.819, mean reward: -1.606 [-100.000, 7.062], mean action: 1.486 [0.000, 3.000], mean observation: 0.100 [-3.956, 1.000], loss: 2.122065, mean_absolute_error: 33.263946, mean_q: 16.301781, mean_eps: 0.707030
  325629/2000000: episode: 3286, duration: 1.020s, episode steps: 70, steps per second: 69, episode reward: -136.434, mean reward: -1.949 [-100.000, 10.780], mean action: 1.700 [0.000, 3.000], mean observation: -0.117 [-1.251, 3.627], loss: 1.478257, mean_absolute_error: 31.355075, mean_q: 18.692199, mean_eps: 0.706965
  325693/2000000: episode: 3287, duration: 0.919s, episode steps: 64, steps per second: 70, episode reward: -124.963, mean reward: -1.953 [-100.000, 10.127], mean action: 1.516 [0.000, 3.000], mean observation: 0.048 [-5.107, 1.000], loss: 1.147473, mean_absolute_error: 30.837867, mean_q: 19.304791, mean_eps: 0.706904
  325811/2000000: episode: 3288, duration: 1.650s, episode steps: 118, steps per second: 72, episode reward: -148.238, mean reward: -1.256 [-100.000, 13.830], mean action: 1.576 [0.000, 3.000], mean observation: -0.042 [-1.331, 3.433], loss: 1.778406, mean_absolute_error: 32.049620, mean_q: 19.629379, mean_eps: 0.706823
  325931/2000000: episode: 3289, duration: 1.683s, episode steps: 120, steps per second: 71, episode reward: -120.781, mean reward: -1.007 [-100.000, 19.295], mean action: 1.542 [0.000, 3.000], mean observation: 0.024 [-4.204, 1.000], loss: 1.881555, mean_absolute_error: 31.876315, mean_q: 20.288909, mean_eps: 0.706717
  326062/2000000: episode: 3290, duration: 1.887s, episode steps: 131, steps per second: 69, episode reward: -79.855, mean reward: -0.610 [-100.000, 16.254], mean action: 1.695 [0.000, 3.000], mean observation: 0.030 [-1.019, 1.000], loss: 1.859683, mean_absolute_error: 32.471176, mean_q: 16.944006, mean_eps: 0.706604
  326178/2000000: episode: 3291, duration: 1.665s, episode steps: 116, steps per second: 70, episode reward: -91.895, mean reward: -0.792 [-100.000, 11.700], mean action: 1.647 [0.000, 3.000], mean observation: 0.048 [-0.827, 2.569], loss: 1.710692, mean_absolute_error: 32.464051, mean_q: 14.229717, mean_eps: 0.706492
  326252/2000000: episode: 3292, duration: 1.093s, episode steps: 74, steps per second: 68, episode reward: -67.062, mean reward: -0.906 [-100.000, 16.374], mean action: 1.568 [0.000, 3.000], mean observation: -0.021 [-1.188, 1.000], loss: 1.132098, mean_absolute_error: 30.239057, mean_q: 23.212047, mean_eps: 0.706407
  326329/2000000: episode: 3293, duration: 1.123s, episode steps: 77, steps per second: 69, episode reward: -82.531, mean reward: -1.072 [-100.000, 7.571], mean action: 1.558 [0.000, 3.000], mean observation: 0.013 [-1.158, 3.912], loss: 1.373879, mean_absolute_error: 32.103849, mean_q: 17.628947, mean_eps: 0.706339
  326471/2000000: episode: 3294, duration: 1.981s, episode steps: 142, steps per second: 72, episode reward: -74.439, mean reward: -0.524 [-100.000, 17.441], mean action: 1.627 [0.000, 3.000], mean observation: 0.001 [-0.929, 2.587], loss: 1.631848, mean_absolute_error: 31.058087, mean_q: 20.116192, mean_eps: 0.706240
  326584/2000000: episode: 3295, duration: 1.643s, episode steps: 113, steps per second: 69, episode reward: -135.757, mean reward: -1.201 [-100.000, 7.719], mean action: 1.540 [0.000, 3.000], mean observation: -0.012 [-3.702, 1.000], loss: 1.658330, mean_absolute_error: 32.334880, mean_q: 17.549408, mean_eps: 0.706127
  326711/2000000: episode: 3296, duration: 1.810s, episode steps: 127, steps per second: 70, episode reward: -175.281, mean reward: -1.380 [-100.000, 10.426], mean action: 1.606 [0.000, 3.000], mean observation: 0.116 [-1.427, 1.045], loss: 1.314347, mean_absolute_error: 31.479124, mean_q: 19.077271, mean_eps: 0.706019
  326825/2000000: episode: 3297, duration: 1.724s, episode steps: 114, steps per second: 66, episode reward: -71.432, mean reward: -0.627 [-100.000, 17.737], mean action: 1.579 [0.000, 3.000], mean observation: -0.109 [-0.808, 1.000], loss: 1.833846, mean_absolute_error: 32.957879, mean_q: 19.397428, mean_eps: 0.705909
  326937/2000000: episode: 3298, duration: 1.593s, episode steps: 112, steps per second: 70, episode reward: -125.795, mean reward: -1.123 [-100.000, 5.046], mean action: 1.527 [0.000, 3.000], mean observation: 0.123 [-2.324, 0.979], loss: 1.529619, mean_absolute_error: 32.181972, mean_q: 16.999982, mean_eps: 0.705806
  327016/2000000: episode: 3299, duration: 1.135s, episode steps: 79, steps per second: 70, episode reward: -107.568, mean reward: -1.362 [-100.000, 13.002], mean action: 1.582 [0.000, 3.000], mean observation: 0.085 [-3.106, 1.000], loss: 1.245972, mean_absolute_error: 31.567212, mean_q: 21.465662, mean_eps: 0.705722
  327121/2000000: episode: 3300, duration: 1.681s, episode steps: 105, steps per second: 62, episode reward: -155.714, mean reward: -1.483 [-100.000, 9.807], mean action: 1.619 [0.000, 3.000], mean observation: -0.049 [-1.256, 3.986], loss: 1.797964, mean_absolute_error: 31.088665, mean_q: 20.507692, mean_eps: 0.705639
  327243/2000000: episode: 3301, duration: 1.747s, episode steps: 122, steps per second: 70, episode reward: -151.009, mean reward: -1.238 [-100.000, 6.709], mean action: 1.500 [0.000, 3.000], mean observation: 0.181 [-1.138, 3.704], loss: 1.456589, mean_absolute_error: 30.535885, mean_q: 19.398055, mean_eps: 0.705536
  327337/2000000: episode: 3302, duration: 1.399s, episode steps: 94, steps per second: 67, episode reward: -148.865, mean reward: -1.584 [-100.000, 6.691], mean action: 1.691 [0.000, 3.000], mean observation: 0.074 [-1.146, 2.871], loss: 1.521111, mean_absolute_error: 31.623092, mean_q: 16.555543, mean_eps: 0.705439
  327453/2000000: episode: 3303, duration: 1.638s, episode steps: 116, steps per second: 71, episode reward: -155.362, mean reward: -1.339 [-100.000, 6.178], mean action: 1.716 [0.000, 3.000], mean observation: 0.067 [-1.049, 3.319], loss: 1.227749, mean_absolute_error: 31.842481, mean_q: 20.018024, mean_eps: 0.705344
  327530/2000000: episode: 3304, duration: 1.112s, episode steps: 77, steps per second: 69, episode reward: -98.516, mean reward: -1.279 [-100.000, 22.949], mean action: 1.662 [0.000, 3.000], mean observation: -0.043 [-3.953, 1.000], loss: 1.787236, mean_absolute_error: 31.442281, mean_q: 19.704270, mean_eps: 0.705257
  327608/2000000: episode: 3305, duration: 1.144s, episode steps: 78, steps per second: 68, episode reward: -84.260, mean reward: -1.080 [-100.000, 11.939], mean action: 1.718 [0.000, 3.000], mean observation: -0.147 [-1.039, 1.000], loss: 2.625995, mean_absolute_error: 32.108137, mean_q: 18.063020, mean_eps: 0.705189
  327672/2000000: episode: 3306, duration: 0.953s, episode steps: 64, steps per second: 67, episode reward: -91.808, mean reward: -1.435 [-100.000, 7.602], mean action: 1.578 [0.000, 3.000], mean observation: 0.031 [-1.215, 4.463], loss: 1.495424, mean_absolute_error: 31.571120, mean_q: 20.208161, mean_eps: 0.705126
  327786/2000000: episode: 3307, duration: 1.642s, episode steps: 114, steps per second: 69, episode reward: -96.046, mean reward: -0.843 [-100.000, 10.510], mean action: 1.579 [0.000, 3.000], mean observation: 0.050 [-3.273, 1.000], loss: 1.702652, mean_absolute_error: 31.889018, mean_q: 18.267573, mean_eps: 0.705045
  327905/2000000: episode: 3308, duration: 1.716s, episode steps: 119, steps per second: 69, episode reward: -140.113, mean reward: -1.177 [-100.000, 7.239], mean action: 1.555 [0.000, 3.000], mean observation: 0.163 [-3.284, 1.000], loss: 1.359146, mean_absolute_error: 31.690617, mean_q: 18.378401, mean_eps: 0.704939
  327989/2000000: episode: 3309, duration: 1.201s, episode steps: 84, steps per second: 70, episode reward: -114.330, mean reward: -1.361 [-100.000, 6.785], mean action: 1.417 [0.000, 3.000], mean observation: -0.024 [-4.014, 1.000], loss: 2.359740, mean_absolute_error: 31.751344, mean_q: 15.957999, mean_eps: 0.704847
  328066/2000000: episode: 3310, duration: 1.090s, episode steps: 77, steps per second: 71, episode reward: -102.010, mean reward: -1.325 [-100.000, 19.127], mean action: 1.649 [0.000, 3.000], mean observation: 0.003 [-3.390, 1.000], loss: 1.465990, mean_absolute_error: 31.291066, mean_q: 19.996676, mean_eps: 0.704775
  328185/2000000: episode: 3311, duration: 1.711s, episode steps: 119, steps per second: 70, episode reward: -145.479, mean reward: -1.223 [-100.000, 9.575], mean action: 1.588 [0.000, 3.000], mean observation: -0.066 [-1.247, 4.749], loss: 1.428766, mean_absolute_error: 31.322416, mean_q: 19.625594, mean_eps: 0.704687
  328323/2000000: episode: 3312, duration: 1.951s, episode steps: 138, steps per second: 71, episode reward: -146.802, mean reward: -1.064 [-100.000, 12.281], mean action: 1.638 [0.000, 3.000], mean observation: 0.157 [-3.651, 1.045], loss: 2.460900, mean_absolute_error: 32.456111, mean_q: 18.486249, mean_eps: 0.704571
  328391/2000000: episode: 3313, duration: 0.975s, episode steps: 68, steps per second: 70, episode reward: -149.748, mean reward: -2.202 [-100.000, 9.712], mean action: 1.662 [0.000, 3.000], mean observation: 0.062 [-1.322, 4.394], loss: 1.981264, mean_absolute_error: 33.574979, mean_q: 16.909934, mean_eps: 0.704480
  328488/2000000: episode: 3314, duration: 1.432s, episode steps: 97, steps per second: 68, episode reward: -150.419, mean reward: -1.551 [-100.000, 7.063], mean action: 1.649 [0.000, 3.000], mean observation: 0.124 [-4.308, 1.000], loss: 1.503576, mean_absolute_error: 31.227823, mean_q: 20.389935, mean_eps: 0.704406
  328603/2000000: episode: 3315, duration: 1.651s, episode steps: 115, steps per second: 70, episode reward: -117.213, mean reward: -1.019 [-100.000, 15.369], mean action: 1.617 [0.000, 3.000], mean observation: -0.004 [-1.099, 3.779], loss: 1.947750, mean_absolute_error: 32.683260, mean_q: 16.283088, mean_eps: 0.704310
  328676/2000000: episode: 3316, duration: 1.071s, episode steps: 73, steps per second: 68, episode reward: -99.643, mean reward: -1.365 [-100.000, 12.938], mean action: 1.589 [0.000, 3.000], mean observation: -0.155 [-1.070, 1.000], loss: 1.739885, mean_absolute_error: 32.950797, mean_q: 17.232371, mean_eps: 0.704226
  328797/2000000: episode: 3317, duration: 1.787s, episode steps: 121, steps per second: 68, episode reward: -112.746, mean reward: -0.932 [-100.000, 16.840], mean action: 1.760 [0.000, 3.000], mean observation: -0.088 [-1.064, 2.559], loss: 1.555367, mean_absolute_error: 30.858075, mean_q: 17.690713, mean_eps: 0.704138
  328895/2000000: episode: 3318, duration: 1.384s, episode steps: 98, steps per second: 71, episode reward: -129.096, mean reward: -1.317 [-100.000, 19.898], mean action: 1.602 [0.000, 3.000], mean observation: -0.072 [-1.100, 4.187], loss: 1.380856, mean_absolute_error: 30.372634, mean_q: 19.515104, mean_eps: 0.704039
  329004/2000000: episode: 3319, duration: 1.604s, episode steps: 109, steps per second: 68, episode reward: -105.108, mean reward: -0.964 [-100.000, 17.545], mean action: 1.532 [0.000, 3.000], mean observation: -0.012 [-2.874, 1.000], loss: 1.373188, mean_absolute_error: 31.141882, mean_q: 18.732417, mean_eps: 0.703947
  329163/2000000: episode: 3320, duration: 2.264s, episode steps: 159, steps per second: 70, episode reward: -218.842, mean reward: -1.376 [-100.000, 27.506], mean action: 1.774 [0.000, 3.000], mean observation: -0.130 [-1.328, 1.385], loss: 1.428940, mean_absolute_error: 31.691056, mean_q: 20.073867, mean_eps: 0.703826
  329250/2000000: episode: 3321, duration: 1.243s, episode steps: 87, steps per second: 70, episode reward: -91.295, mean reward: -1.049 [-100.000, 16.799], mean action: 1.586 [0.000, 3.000], mean observation: 0.042 [-1.110, 1.000], loss: 1.464518, mean_absolute_error: 32.168094, mean_q: 18.255251, mean_eps: 0.703715
  329376/2000000: episode: 3322, duration: 1.825s, episode steps: 126, steps per second: 69, episode reward: -86.109, mean reward: -0.683 [-100.000, 14.389], mean action: 1.595 [0.000, 3.000], mean observation: -0.007 [-2.579, 1.000], loss: 1.545478, mean_absolute_error: 30.972458, mean_q: 21.193679, mean_eps: 0.703619
  329454/2000000: episode: 3323, duration: 1.155s, episode steps: 78, steps per second: 68, episode reward: -120.867, mean reward: -1.550 [-100.000, 9.754], mean action: 1.808 [0.000, 3.000], mean observation: -0.116 [-1.199, 2.501], loss: 1.985102, mean_absolute_error: 30.950107, mean_q: 18.734549, mean_eps: 0.703527
  329538/2000000: episode: 3324, duration: 1.216s, episode steps: 84, steps per second: 69, episode reward: -124.409, mean reward: -1.481 [-100.000, 10.097], mean action: 1.726 [0.000, 3.000], mean observation: -0.149 [-1.130, 1.000], loss: 1.603707, mean_absolute_error: 31.548002, mean_q: 19.415902, mean_eps: 0.703454
  329636/2000000: episode: 3325, duration: 1.396s, episode steps: 98, steps per second: 70, episode reward: -99.977, mean reward: -1.020 [-100.000, 22.588], mean action: 1.714 [0.000, 3.000], mean observation: -0.068 [-1.084, 1.000], loss: 1.408951, mean_absolute_error: 31.796685, mean_q: 20.463241, mean_eps: 0.703373
  329718/2000000: episode: 3326, duration: 1.218s, episode steps: 82, steps per second: 67, episode reward: -81.852, mean reward: -0.998 [-100.000, 9.586], mean action: 1.622 [0.000, 3.000], mean observation: -0.000 [-1.169, 1.000], loss: 2.155053, mean_absolute_error: 33.199520, mean_q: 18.487821, mean_eps: 0.703292
  329802/2000000: episode: 3327, duration: 1.189s, episode steps: 84, steps per second: 71, episode reward: -126.136, mean reward: -1.502 [-100.000, 6.465], mean action: 1.464 [0.000, 3.000], mean observation: 0.065 [-4.242, 1.000], loss: 1.250530, mean_absolute_error: 30.999983, mean_q: 19.928070, mean_eps: 0.703216
  329870/2000000: episode: 3328, duration: 0.984s, episode steps: 68, steps per second: 69, episode reward: -145.907, mean reward: -2.146 [-100.000, 6.995], mean action: 1.632 [0.000, 3.000], mean observation: -0.119 [-4.930, 1.000], loss: 1.207371, mean_absolute_error: 30.780343, mean_q: 20.846394, mean_eps: 0.703148
  330014/2000000: episode: 3329, duration: 2.104s, episode steps: 144, steps per second: 68, episode reward: -104.977, mean reward: -0.729 [-100.000, 12.729], mean action: 1.625 [0.000, 3.000], mean observation: -0.018 [-1.065, 3.598], loss: 2.028339, mean_absolute_error: 32.502752, mean_q: 17.041290, mean_eps: 0.703052
  330085/2000000: episode: 3330, duration: 1.034s, episode steps: 71, steps per second: 69, episode reward: -122.878, mean reward: -1.731 [-100.000, 8.660], mean action: 1.549 [0.000, 3.000], mean observation: 0.095 [-2.225, 1.000], loss: 1.350301, mean_absolute_error: 34.017276, mean_q: 16.128638, mean_eps: 0.702955
  330163/2000000: episode: 3331, duration: 1.091s, episode steps: 78, steps per second: 71, episode reward: -109.969, mean reward: -1.410 [-100.000, 11.379], mean action: 1.500 [0.000, 3.000], mean observation: 0.034 [-4.054, 1.000], loss: 1.485618, mean_absolute_error: 33.163099, mean_q: 16.565876, mean_eps: 0.702888
  330256/2000000: episode: 3332, duration: 1.359s, episode steps: 93, steps per second: 68, episode reward: -132.786, mean reward: -1.428 [-100.000, 6.159], mean action: 1.559 [0.000, 3.000], mean observation: -0.049 [-1.162, 3.983], loss: 1.683170, mean_absolute_error: 33.515321, mean_q: 17.952122, mean_eps: 0.702813
  330330/2000000: episode: 3333, duration: 1.080s, episode steps: 74, steps per second: 69, episode reward: -124.144, mean reward: -1.678 [-100.000, 8.307], mean action: 1.797 [0.000, 3.000], mean observation: -0.078 [-4.686, 1.000], loss: 1.908897, mean_absolute_error: 34.709608, mean_q: 16.984170, mean_eps: 0.702737
  330462/2000000: episode: 3334, duration: 1.888s, episode steps: 132, steps per second: 70, episode reward: -129.458, mean reward: -0.981 [-100.000, 13.197], mean action: 1.629 [0.000, 3.000], mean observation: 0.140 [-2.999, 1.000], loss: 1.679183, mean_absolute_error: 32.450395, mean_q: 21.037248, mean_eps: 0.702644
  330535/2000000: episode: 3335, duration: 1.033s, episode steps: 73, steps per second: 71, episode reward: -140.796, mean reward: -1.929 [-100.000, 7.825], mean action: 1.493 [0.000, 3.000], mean observation: 0.092 [-1.294, 3.810], loss: 1.681744, mean_absolute_error: 33.705477, mean_q: 15.567453, mean_eps: 0.702552
  330666/2000000: episode: 3336, duration: 1.873s, episode steps: 131, steps per second: 70, episode reward: -86.449, mean reward: -0.660 [-100.000, 15.059], mean action: 1.595 [0.000, 3.000], mean observation: 0.024 [-3.084, 1.000], loss: 2.410386, mean_absolute_error: 33.117617, mean_q: 20.136197, mean_eps: 0.702460
  330810/2000000: episode: 3337, duration: 2.065s, episode steps: 144, steps per second: 70, episode reward: -224.092, mean reward: -1.556 [-100.000, 27.314], mean action: 1.750 [0.000, 3.000], mean observation: -0.175 [-1.934, 1.000], loss: 1.669246, mean_absolute_error: 33.121248, mean_q: 18.280778, mean_eps: 0.702336
  330888/2000000: episode: 3338, duration: 1.160s, episode steps: 78, steps per second: 67, episode reward: -100.235, mean reward: -1.285 [-100.000, 15.877], mean action: 1.667 [0.000, 3.000], mean observation: -0.010 [-3.124, 1.000], loss: 1.235747, mean_absolute_error: 31.862782, mean_q: 21.578294, mean_eps: 0.702237
  330998/2000000: episode: 3339, duration: 1.602s, episode steps: 110, steps per second: 69, episode reward: -122.126, mean reward: -1.110 [-100.000, 10.221], mean action: 1.727 [0.000, 3.000], mean observation: -0.065 [-1.010, 2.763], loss: 1.194063, mean_absolute_error: 31.651373, mean_q: 19.700787, mean_eps: 0.702152
  331083/2000000: episode: 3340, duration: 1.222s, episode steps: 85, steps per second: 70, episode reward: -109.448, mean reward: -1.288 [-100.000, 17.022], mean action: 1.553 [0.000, 3.000], mean observation: -0.062 [-1.212, 1.000], loss: 1.784583, mean_absolute_error: 32.913848, mean_q: 19.804746, mean_eps: 0.702064
  331250/2000000: episode: 3341, duration: 2.364s, episode steps: 167, steps per second: 71, episode reward: -119.739, mean reward: -0.717 [-100.000, 10.906], mean action: 1.575 [0.000, 3.000], mean observation: -0.059 [-0.976, 2.712], loss: 1.582069, mean_absolute_error: 32.266178, mean_q: 20.799631, mean_eps: 0.701951
  331367/2000000: episode: 3342, duration: 1.672s, episode steps: 117, steps per second: 70, episode reward: -80.929, mean reward: -0.692 [-100.000, 14.620], mean action: 1.744 [0.000, 3.000], mean observation: 0.103 [-0.896, 2.950], loss: 1.768411, mean_absolute_error: 32.765074, mean_q: 18.112418, mean_eps: 0.701823
  331510/2000000: episode: 3343, duration: 2.063s, episode steps: 143, steps per second: 69, episode reward: -280.618, mean reward: -1.962 [-100.000, 75.462], mean action: 1.664 [0.000, 3.000], mean observation: 0.211 [-0.939, 2.397], loss: 1.813859, mean_absolute_error: 33.980880, mean_q: 17.996306, mean_eps: 0.701706
  331643/2000000: episode: 3344, duration: 1.911s, episode steps: 133, steps per second: 70, episode reward: -113.973, mean reward: -0.857 [-100.000, 31.665], mean action: 1.699 [0.000, 3.000], mean observation: -0.094 [-1.369, 1.971], loss: 1.323205, mean_absolute_error: 32.986312, mean_q: 16.801561, mean_eps: 0.701582
  331736/2000000: episode: 3345, duration: 1.380s, episode steps: 93, steps per second: 67, episode reward: -97.111, mean reward: -1.044 [-100.000, 12.616], mean action: 1.624 [0.000, 3.000], mean observation: 0.100 [-0.915, 1.000], loss: 2.121176, mean_absolute_error: 33.570983, mean_q: 18.159070, mean_eps: 0.701481
  331843/2000000: episode: 3346, duration: 1.548s, episode steps: 107, steps per second: 69, episode reward: -82.943, mean reward: -0.775 [-100.000, 8.272], mean action: 1.682 [0.000, 3.000], mean observation: 0.056 [-0.969, 3.313], loss: 1.910230, mean_absolute_error: 33.710268, mean_q: 17.276843, mean_eps: 0.701391
  331962/2000000: episode: 3347, duration: 1.728s, episode steps: 119, steps per second: 69, episode reward: -99.416, mean reward: -0.835 [-100.000, 8.941], mean action: 1.756 [0.000, 3.000], mean observation: -0.066 [-1.035, 3.208], loss: 2.451013, mean_absolute_error: 34.750152, mean_q: 17.820597, mean_eps: 0.701288
  332103/2000000: episode: 3348, duration: 2.019s, episode steps: 141, steps per second: 70, episode reward: -82.287, mean reward: -0.584 [-100.000, 12.529], mean action: 1.738 [0.000, 3.000], mean observation: -0.100 [-1.121, 1.000], loss: 1.769129, mean_absolute_error: 33.395806, mean_q: 19.375844, mean_eps: 0.701171
  332210/2000000: episode: 3349, duration: 1.510s, episode steps: 107, steps per second: 71, episode reward: -90.914, mean reward: -0.850 [-100.000, 14.566], mean action: 1.673 [0.000, 3.000], mean observation: -0.108 [-0.964, 1.000], loss: 1.488293, mean_absolute_error: 33.124414, mean_q: 20.027596, mean_eps: 0.701060
  332304/2000000: episode: 3350, duration: 1.373s, episode steps: 94, steps per second: 68, episode reward: -140.563, mean reward: -1.495 [-100.000, 9.016], mean action: 1.457 [0.000, 3.000], mean observation: 0.099 [-2.473, 1.000], loss: 1.629813, mean_absolute_error: 33.779100, mean_q: 16.561872, mean_eps: 0.700970
  332382/2000000: episode: 3351, duration: 1.159s, episode steps: 78, steps per second: 67, episode reward: -128.261, mean reward: -1.644 [-100.000, 8.004], mean action: 1.718 [0.000, 3.000], mean observation: -0.143 [-1.309, 3.921], loss: 1.515500, mean_absolute_error: 33.105417, mean_q: 19.809330, mean_eps: 0.700892
  332510/2000000: episode: 3352, duration: 1.844s, episode steps: 128, steps per second: 69, episode reward: -92.920, mean reward: -0.726 [-100.000, 46.609], mean action: 1.750 [0.000, 3.000], mean observation: -0.131 [-1.263, 1.339], loss: 2.162903, mean_absolute_error: 33.051046, mean_q: 18.249716, mean_eps: 0.700799
  332591/2000000: episode: 3353, duration: 1.156s, episode steps: 81, steps per second: 70, episode reward: -149.162, mean reward: -1.842 [-100.000, 6.624], mean action: 1.296 [0.000, 3.000], mean observation: 0.100 [-3.507, 1.000], loss: 1.005670, mean_absolute_error: 31.751025, mean_q: 22.097700, mean_eps: 0.700705
  332749/2000000: episode: 3354, duration: 2.288s, episode steps: 158, steps per second: 69, episode reward: -67.251, mean reward: -0.426 [-100.000, 14.543], mean action: 1.646 [0.000, 3.000], mean observation: -0.008 [-0.748, 1.000], loss: 1.869723, mean_absolute_error: 33.875708, mean_q: 18.315128, mean_eps: 0.700597
  332853/2000000: episode: 3355, duration: 1.471s, episode steps: 104, steps per second: 71, episode reward: -120.169, mean reward: -1.155 [-100.000, 7.218], mean action: 1.750 [0.000, 3.000], mean observation: -0.091 [-1.052, 3.622], loss: 1.735921, mean_absolute_error: 31.931616, mean_q: 20.383795, mean_eps: 0.700478
  332992/2000000: episode: 3356, duration: 2.024s, episode steps: 139, steps per second: 69, episode reward: -98.018, mean reward: -0.705 [-100.000, 10.439], mean action: 1.676 [0.000, 3.000], mean observation: 0.034 [-2.947, 1.000], loss: 1.645575, mean_absolute_error: 33.567676, mean_q: 19.215940, mean_eps: 0.700370
  333080/2000000: episode: 3357, duration: 1.310s, episode steps: 88, steps per second: 67, episode reward: -92.057, mean reward: -1.046 [-100.000, 12.688], mean action: 1.727 [0.000, 3.000], mean observation: -0.056 [-1.095, 3.718], loss: 2.016165, mean_absolute_error: 32.312926, mean_q: 21.759262, mean_eps: 0.700269
  333154/2000000: episode: 3358, duration: 1.070s, episode steps: 74, steps per second: 69, episode reward: -109.875, mean reward: -1.485 [-100.000, 6.156], mean action: 1.432 [0.000, 3.000], mean observation: 0.084 [-3.764, 1.000], loss: 3.273706, mean_absolute_error: 33.743001, mean_q: 21.454769, mean_eps: 0.700196
  333263/2000000: episode: 3359, duration: 1.526s, episode steps: 109, steps per second: 71, episode reward: -90.777, mean reward: -0.833 [-100.000, 8.920], mean action: 1.716 [0.000, 3.000], mean observation: -0.157 [-1.321, 1.000], loss: 1.925402, mean_absolute_error: 32.279400, mean_q: 18.312880, mean_eps: 0.700113
  333367/2000000: episode: 3360, duration: 1.493s, episode steps: 104, steps per second: 70, episode reward: -106.511, mean reward: -1.024 [-100.000, 12.142], mean action: 1.654 [0.000, 3.000], mean observation: 0.035 [-3.011, 1.000], loss: 1.578642, mean_absolute_error: 33.784233, mean_q: 18.576899, mean_eps: 0.700017
  333466/2000000: episode: 3361, duration: 1.427s, episode steps: 99, steps per second: 69, episode reward: -89.134, mean reward: -0.900 [-100.000, 23.989], mean action: 1.646 [0.000, 3.000], mean observation: -0.093 [-1.175, 1.281], loss: 1.406092, mean_absolute_error: 33.208093, mean_q: 18.131411, mean_eps: 0.699926
  333576/2000000: episode: 3362, duration: 1.628s, episode steps: 110, steps per second: 68, episode reward: -85.064, mean reward: -0.773 [-100.000, 10.399], mean action: 1.655 [0.000, 3.000], mean observation: 0.012 [-1.010, 1.000], loss: 1.294597, mean_absolute_error: 33.393115, mean_q: 19.121409, mean_eps: 0.699832
  333654/2000000: episode: 3363, duration: 1.153s, episode steps: 78, steps per second: 68, episode reward: -122.889, mean reward: -1.576 [-100.000, 12.002], mean action: 1.449 [0.000, 3.000], mean observation: 0.079 [-1.233, 1.000], loss: 1.575964, mean_absolute_error: 33.332454, mean_q: 17.279065, mean_eps: 0.699747
  333728/2000000: episode: 3364, duration: 1.103s, episode steps: 74, steps per second: 67, episode reward: -124.433, mean reward: -1.682 [-100.000, 5.586], mean action: 1.703 [0.000, 3.000], mean observation: -0.162 [-1.286, 3.805], loss: 1.561585, mean_absolute_error: 32.650991, mean_q: 19.739674, mean_eps: 0.699679
  333835/2000000: episode: 3365, duration: 1.573s, episode steps: 107, steps per second: 68, episode reward: -115.531, mean reward: -1.080 [-100.000, 11.130], mean action: 1.570 [0.000, 3.000], mean observation: -0.050 [-0.998, 3.335], loss: 2.611002, mean_absolute_error: 33.049945, mean_q: 20.433835, mean_eps: 0.699598
  333923/2000000: episode: 3366, duration: 1.264s, episode steps: 88, steps per second: 70, episode reward: -74.643, mean reward: -0.848 [-100.000, 17.398], mean action: 1.807 [0.000, 3.000], mean observation: -0.054 [-1.064, 1.000], loss: 2.163147, mean_absolute_error: 32.599324, mean_q: 18.657667, mean_eps: 0.699510
  334077/2000000: episode: 3367, duration: 2.238s, episode steps: 154, steps per second: 69, episode reward: -91.058, mean reward: -0.591 [-100.000, 17.760], mean action: 1.701 [0.000, 3.000], mean observation: 0.046 [-1.087, 1.000], loss: 1.579056, mean_absolute_error: 33.150510, mean_q: 19.782234, mean_eps: 0.699400
  334188/2000000: episode: 3368, duration: 1.599s, episode steps: 111, steps per second: 69, episode reward: -118.268, mean reward: -1.065 [-100.000, 7.879], mean action: 1.739 [0.000, 3.000], mean observation: -0.108 [-1.180, 3.973], loss: 1.729467, mean_absolute_error: 32.216212, mean_q: 21.840133, mean_eps: 0.699281
  334274/2000000: episode: 3369, duration: 1.259s, episode steps: 86, steps per second: 68, episode reward: -96.008, mean reward: -1.116 [-100.000, 7.730], mean action: 1.988 [0.000, 3.000], mean observation: -0.073 [-0.907, 2.790], loss: 1.764177, mean_absolute_error: 32.699340, mean_q: 18.502833, mean_eps: 0.699193
  334429/2000000: episode: 3370, duration: 2.232s, episode steps: 155, steps per second: 69, episode reward: -111.784, mean reward: -0.721 [-100.000, 7.678], mean action: 1.632 [0.000, 3.000], mean observation: 0.004 [-1.117, 3.856], loss: 1.623245, mean_absolute_error: 32.660218, mean_q: 20.126985, mean_eps: 0.699083
  334550/2000000: episode: 3371, duration: 1.758s, episode steps: 121, steps per second: 69, episode reward: -80.268, mean reward: -0.663 [-100.000, 20.360], mean action: 1.736 [0.000, 3.000], mean observation: -0.119 [-1.106, 1.000], loss: 1.505884, mean_absolute_error: 32.344486, mean_q: 21.322048, mean_eps: 0.698959
  334673/2000000: episode: 3372, duration: 1.773s, episode steps: 123, steps per second: 69, episode reward: -62.960, mean reward: -0.512 [-100.000, 15.105], mean action: 1.520 [0.000, 3.000], mean observation: 0.044 [-1.177, 3.471], loss: 1.552494, mean_absolute_error: 32.404807, mean_q: 18.704343, mean_eps: 0.698849
  334824/2000000: episode: 3373, duration: 2.177s, episode steps: 151, steps per second: 69, episode reward: -68.570, mean reward: -0.454 [-100.000, 13.543], mean action: 1.589 [0.000, 3.000], mean observation: 0.003 [-0.873, 1.000], loss: 1.813585, mean_absolute_error: 32.463531, mean_q: 20.842191, mean_eps: 0.698727
  334958/2000000: episode: 3374, duration: 1.949s, episode steps: 134, steps per second: 69, episode reward: -85.526, mean reward: -0.638 [-100.000, 15.718], mean action: 1.493 [0.000, 3.000], mean observation: 0.020 [-1.151, 3.703], loss: 1.685853, mean_absolute_error: 32.082935, mean_q: 21.248639, mean_eps: 0.698599
  335045/2000000: episode: 3375, duration: 1.276s, episode steps: 87, steps per second: 68, episode reward: -161.880, mean reward: -1.861 [-100.000, 10.381], mean action: 1.897 [0.000, 3.000], mean observation: -0.159 [-1.197, 4.085], loss: 1.529893, mean_absolute_error: 32.776794, mean_q: 18.580294, mean_eps: 0.698498
  335178/2000000: episode: 3376, duration: 1.917s, episode steps: 133, steps per second: 69, episode reward: -68.778, mean reward: -0.517 [-100.000, 17.107], mean action: 1.684 [0.000, 3.000], mean observation: 0.052 [-0.688, 1.982], loss: 1.634101, mean_absolute_error: 32.795557, mean_q: 20.059533, mean_eps: 0.698399
  335324/2000000: episode: 3377, duration: 2.213s, episode steps: 146, steps per second: 66, episode reward: -35.502, mean reward: -0.243 [-100.000, 17.294], mean action: 1.644 [0.000, 3.000], mean observation: 0.045 [-0.856, 1.016], loss: 1.840506, mean_absolute_error: 32.773744, mean_q: 19.287897, mean_eps: 0.698275
  335429/2000000: episode: 3378, duration: 1.677s, episode steps: 105, steps per second: 63, episode reward: -151.990, mean reward: -1.448 [-100.000, 6.906], mean action: 1.448 [0.000, 3.000], mean observation: 0.026 [-4.235, 1.000], loss: 1.275935, mean_absolute_error: 33.675026, mean_q: 17.514847, mean_eps: 0.698162
  335539/2000000: episode: 3379, duration: 1.623s, episode steps: 110, steps per second: 68, episode reward: -35.087, mean reward: -0.319 [-100.000, 20.131], mean action: 1.745 [0.000, 3.000], mean observation: 0.005 [-0.792, 1.110], loss: 2.128289, mean_absolute_error: 34.159248, mean_q: 17.671835, mean_eps: 0.698064
  335642/2000000: episode: 3380, duration: 1.464s, episode steps: 103, steps per second: 70, episode reward: -139.699, mean reward: -1.356 [-100.000, 10.130], mean action: 1.680 [0.000, 3.000], mean observation: 0.122 [-1.128, 3.995], loss: 1.652154, mean_absolute_error: 33.158802, mean_q: 18.868594, mean_eps: 0.697969
  336642/2000000: episode: 3381, duration: 15.281s, episode steps: 1000, steps per second: 65, episode reward: 55.858, mean reward: 0.056 [-23.679, 83.214], mean action: 1.753 [0.000, 3.000], mean observation: 0.046 [-1.933, 1.000], loss: 1.411413, mean_absolute_error: 32.512930, mean_q: 20.813999, mean_eps: 0.697472
  336744/2000000: episode: 3382, duration: 1.497s, episode steps: 102, steps per second: 68, episode reward: -144.100, mean reward: -1.413 [-100.000, 7.771], mean action: 1.647 [0.000, 3.000], mean observation: 0.081 [-3.777, 1.000], loss: 2.496231, mean_absolute_error: 33.735962, mean_q: 18.163364, mean_eps: 0.696977
  336850/2000000: episode: 3383, duration: 1.545s, episode steps: 106, steps per second: 69, episode reward: -123.333, mean reward: -1.164 [-100.000, 6.939], mean action: 1.557 [0.000, 3.000], mean observation: 0.041 [-3.291, 1.000], loss: 1.936126, mean_absolute_error: 32.968765, mean_q: 18.526466, mean_eps: 0.696884
  336953/2000000: episode: 3384, duration: 1.552s, episode steps: 103, steps per second: 66, episode reward: -150.348, mean reward: -1.460 [-100.000, 7.558], mean action: 1.621 [0.000, 3.000], mean observation: 0.092 [-3.739, 1.000], loss: 1.092820, mean_absolute_error: 33.154688, mean_q: 20.431123, mean_eps: 0.696788
  337055/2000000: episode: 3385, duration: 1.663s, episode steps: 102, steps per second: 61, episode reward: -232.792, mean reward: -2.282 [-100.000, 36.489], mean action: 1.559 [0.000, 3.000], mean observation: -0.179 [-2.536, 1.000], loss: 1.409883, mean_absolute_error: 33.435370, mean_q: 20.047742, mean_eps: 0.696696
  337142/2000000: episode: 3386, duration: 1.272s, episode steps: 87, steps per second: 68, episode reward: -163.242, mean reward: -1.876 [-100.000, 6.205], mean action: 1.494 [0.000, 3.000], mean observation: 0.111 [-3.845, 1.000], loss: 1.053832, mean_absolute_error: 32.091743, mean_q: 24.924834, mean_eps: 0.696612
  337265/2000000: episode: 3387, duration: 1.782s, episode steps: 123, steps per second: 69, episode reward: -120.666, mean reward: -0.981 [-100.000, 12.439], mean action: 1.707 [0.000, 3.000], mean observation: -0.138 [-1.050, 3.480], loss: 1.984230, mean_absolute_error: 33.153909, mean_q: 20.501064, mean_eps: 0.696516
  337327/2000000: episode: 3388, duration: 0.871s, episode steps: 62, steps per second: 71, episode reward: -112.848, mean reward: -1.820 [-100.000, 9.146], mean action: 1.661 [0.000, 3.000], mean observation: 0.028 [-1.413, 1.000], loss: 2.768023, mean_absolute_error: 34.330542, mean_q: 18.219351, mean_eps: 0.696434
  337407/2000000: episode: 3389, duration: 1.156s, episode steps: 80, steps per second: 69, episode reward: -123.627, mean reward: -1.545 [-100.000, 13.443], mean action: 1.562 [0.000, 3.000], mean observation: 0.016 [-4.093, 1.000], loss: 1.545451, mean_absolute_error: 32.549038, mean_q: 22.003094, mean_eps: 0.696371
  337497/2000000: episode: 3390, duration: 1.324s, episode steps: 90, steps per second: 68, episode reward: -49.015, mean reward: -0.545 [-100.000, 11.093], mean action: 1.822 [0.000, 3.000], mean observation: -0.108 [-2.463, 1.000], loss: 1.167669, mean_absolute_error: 32.645021, mean_q: 19.813668, mean_eps: 0.696293
  337597/2000000: episode: 3391, duration: 1.429s, episode steps: 100, steps per second: 70, episode reward: -80.899, mean reward: -0.809 [-100.000, 10.055], mean action: 1.820 [0.000, 3.000], mean observation: -0.025 [-2.498, 1.000], loss: 1.797236, mean_absolute_error: 32.587527, mean_q: 24.170706, mean_eps: 0.696207
  337702/2000000: episode: 3392, duration: 1.506s, episode steps: 105, steps per second: 70, episode reward: -135.536, mean reward: -1.291 [-100.000, 14.097], mean action: 1.695 [0.000, 3.000], mean observation: 0.112 [-4.612, 1.000], loss: 2.348296, mean_absolute_error: 32.549497, mean_q: 19.015577, mean_eps: 0.696115
  337799/2000000: episode: 3393, duration: 1.399s, episode steps: 97, steps per second: 69, episode reward: -100.344, mean reward: -1.034 [-100.000, 16.558], mean action: 1.649 [0.000, 3.000], mean observation: -0.037 [-1.411, 1.000], loss: 1.392464, mean_absolute_error: 32.303568, mean_q: 21.671576, mean_eps: 0.696025
  337935/2000000: episode: 3394, duration: 2.064s, episode steps: 136, steps per second: 66, episode reward: -114.503, mean reward: -0.842 [-100.000, 17.917], mean action: 1.588 [0.000, 3.000], mean observation: 0.063 [-1.118, 1.019], loss: 2.036192, mean_absolute_error: 32.634424, mean_q: 19.859685, mean_eps: 0.695921
  338068/2000000: episode: 3395, duration: 1.964s, episode steps: 133, steps per second: 68, episode reward: -177.393, mean reward: -1.334 [-100.000, 3.850], mean action: 1.722 [0.000, 3.000], mean observation: 0.174 [-0.877, 1.001], loss: 2.613292, mean_absolute_error: 35.465503, mean_q: 16.895996, mean_eps: 0.695800
  338163/2000000: episode: 3396, duration: 1.382s, episode steps: 95, steps per second: 69, episode reward: -148.354, mean reward: -1.562 [-100.000, 17.785], mean action: 1.684 [0.000, 3.000], mean observation: 0.104 [-3.630, 1.000], loss: 1.589395, mean_absolute_error: 32.118047, mean_q: 21.586461, mean_eps: 0.695697
  338249/2000000: episode: 3397, duration: 1.276s, episode steps: 86, steps per second: 67, episode reward: -78.609, mean reward: -0.914 [-100.000, 19.864], mean action: 1.698 [0.000, 3.000], mean observation: -0.035 [-1.161, 1.000], loss: 1.545615, mean_absolute_error: 34.702548, mean_q: 15.668969, mean_eps: 0.695615
  338336/2000000: episode: 3398, duration: 1.264s, episode steps: 87, steps per second: 69, episode reward: -90.393, mean reward: -1.039 [-100.000, 12.829], mean action: 1.667 [0.000, 3.000], mean observation: -0.104 [-0.945, 1.000], loss: 1.621607, mean_absolute_error: 31.990331, mean_q: 21.935803, mean_eps: 0.695537
  338415/2000000: episode: 3399, duration: 1.163s, episode steps: 79, steps per second: 68, episode reward: -71.148, mean reward: -0.901 [-100.000, 11.386], mean action: 1.873 [0.000, 3.000], mean observation: 0.064 [-0.924, 3.121], loss: 1.128182, mean_absolute_error: 31.722341, mean_q: 22.400387, mean_eps: 0.695463
  338520/2000000: episode: 3400, duration: 1.542s, episode steps: 105, steps per second: 68, episode reward: -75.368, mean reward: -0.718 [-100.000, 13.599], mean action: 1.667 [0.000, 3.000], mean observation: -0.007 [-0.928, 3.414], loss: 1.360441, mean_absolute_error: 32.648038, mean_q: 21.564404, mean_eps: 0.695381
  338643/2000000: episode: 3401, duration: 1.805s, episode steps: 123, steps per second: 68, episode reward: -109.638, mean reward: -0.891 [-100.000, 6.931], mean action: 1.577 [0.000, 3.000], mean observation: 0.101 [-3.057, 1.000], loss: 1.754788, mean_absolute_error: 33.603495, mean_q: 18.418531, mean_eps: 0.695278
  338780/2000000: episode: 3402, duration: 2.006s, episode steps: 137, steps per second: 68, episode reward: -138.370, mean reward: -1.010 [-100.000, 8.607], mean action: 1.599 [0.000, 3.000], mean observation: 0.104 [-1.072, 1.000], loss: 1.643347, mean_absolute_error: 33.647131, mean_q: 21.360449, mean_eps: 0.695161
  338854/2000000: episode: 3403, duration: 1.096s, episode steps: 74, steps per second: 68, episode reward: -139.742, mean reward: -1.888 [-100.000, 5.846], mean action: 1.838 [0.000, 3.000], mean observation: -0.127 [-1.203, 4.193], loss: 2.071179, mean_absolute_error: 33.246012, mean_q: 18.511899, mean_eps: 0.695066
  338944/2000000: episode: 3404, duration: 1.304s, episode steps: 90, steps per second: 69, episode reward: -118.648, mean reward: -1.318 [-100.000, 11.378], mean action: 1.744 [0.000, 3.000], mean observation: 0.046 [-0.952, 1.488], loss: 2.011382, mean_absolute_error: 31.824641, mean_q: 18.023571, mean_eps: 0.694992
  339065/2000000: episode: 3405, duration: 1.783s, episode steps: 121, steps per second: 68, episode reward: -103.013, mean reward: -0.851 [-100.000, 17.690], mean action: 1.653 [0.000, 3.000], mean observation: 0.079 [-1.025, 2.935], loss: 1.480413, mean_absolute_error: 31.952843, mean_q: 19.608928, mean_eps: 0.694896
  339159/2000000: episode: 3406, duration: 1.351s, episode steps: 94, steps per second: 70, episode reward: -140.088, mean reward: -1.490 [-100.000, 15.363], mean action: 1.457 [0.000, 3.000], mean observation: 0.130 [-3.605, 1.000], loss: 1.769148, mean_absolute_error: 33.388097, mean_q: 19.524389, mean_eps: 0.694799
  339247/2000000: episode: 3407, duration: 1.283s, episode steps: 88, steps per second: 69, episode reward: -103.606, mean reward: -1.177 [-100.000, 12.302], mean action: 1.443 [0.000, 3.000], mean observation: 0.045 [-1.072, 1.000], loss: 1.903843, mean_absolute_error: 32.385865, mean_q: 20.115099, mean_eps: 0.694718
  339336/2000000: episode: 3408, duration: 1.316s, episode steps: 89, steps per second: 68, episode reward: -209.750, mean reward: -2.357 [-100.000, 16.190], mean action: 1.596 [0.000, 3.000], mean observation: 0.159 [-1.104, 3.176], loss: 1.832742, mean_absolute_error: 33.017109, mean_q: 19.154132, mean_eps: 0.694639
  339462/2000000: episode: 3409, duration: 1.830s, episode steps: 126, steps per second: 69, episode reward: -82.208, mean reward: -0.652 [-100.000, 15.264], mean action: 1.635 [0.000, 3.000], mean observation: 0.100 [-3.091, 1.069], loss: 1.961667, mean_absolute_error: 33.015255, mean_q: 16.660878, mean_eps: 0.694542
  339566/2000000: episode: 3410, duration: 1.490s, episode steps: 104, steps per second: 70, episode reward: -74.015, mean reward: -0.712 [-100.000, 15.705], mean action: 1.788 [0.000, 3.000], mean observation: 0.054 [-0.850, 2.829], loss: 1.449719, mean_absolute_error: 32.595980, mean_q: 20.971252, mean_eps: 0.694437
  339659/2000000: episode: 3411, duration: 1.330s, episode steps: 93, steps per second: 70, episode reward: -85.898, mean reward: -0.924 [-100.000, 10.569], mean action: 1.527 [0.000, 3.000], mean observation: -0.091 [-3.460, 1.000], loss: 1.433742, mean_absolute_error: 31.798325, mean_q: 19.859888, mean_eps: 0.694349
  339750/2000000: episode: 3412, duration: 1.318s, episode steps: 91, steps per second: 69, episode reward: -112.485, mean reward: -1.236 [-100.000, 21.394], mean action: 1.659 [0.000, 3.000], mean observation: -0.082 [-1.141, 2.965], loss: 1.719766, mean_absolute_error: 33.034074, mean_q: 19.824642, mean_eps: 0.694266
  339869/2000000: episode: 3413, duration: 1.746s, episode steps: 119, steps per second: 68, episode reward: -70.624, mean reward: -0.593 [-100.000, 34.913], mean action: 1.529 [0.000, 3.000], mean observation: 0.141 [-3.161, 1.000], loss: 1.838195, mean_absolute_error: 33.647759, mean_q: 19.423483, mean_eps: 0.694171
  339964/2000000: episode: 3414, duration: 1.375s, episode steps: 95, steps per second: 69, episode reward: -99.026, mean reward: -1.042 [-100.000, 21.180], mean action: 1.758 [0.000, 3.000], mean observation: -0.083 [-0.988, 2.289], loss: 1.774879, mean_absolute_error: 34.018043, mean_q: 15.841462, mean_eps: 0.694076
  340039/2000000: episode: 3415, duration: 1.117s, episode steps: 75, steps per second: 67, episode reward: -141.949, mean reward: -1.893 [-100.000, 8.504], mean action: 1.667 [0.000, 3.000], mean observation: -0.175 [-1.202, 4.229], loss: 2.078234, mean_absolute_error: 32.927463, mean_q: 21.921674, mean_eps: 0.694000
  340198/2000000: episode: 3416, duration: 2.333s, episode steps: 159, steps per second: 68, episode reward: -116.728, mean reward: -0.734 [-100.000, 8.963], mean action: 1.522 [0.000, 3.000], mean observation: 0.051 [-1.001, 3.220], loss: 2.259337, mean_absolute_error: 33.366737, mean_q: 18.169323, mean_eps: 0.693894
  340300/2000000: episode: 3417, duration: 1.483s, episode steps: 102, steps per second: 69, episode reward: -88.055, mean reward: -0.863 [-100.000, 16.814], mean action: 1.637 [0.000, 3.000], mean observation: 0.027 [-1.015, 1.000], loss: 2.179878, mean_absolute_error: 34.504345, mean_q: 17.750104, mean_eps: 0.693777
  340403/2000000: episode: 3418, duration: 1.488s, episode steps: 103, steps per second: 69, episode reward: -108.966, mean reward: -1.058 [-100.000, 13.398], mean action: 1.534 [0.000, 3.000], mean observation: -0.005 [-1.115, 1.000], loss: 1.724288, mean_absolute_error: 33.724329, mean_q: 19.395948, mean_eps: 0.693685
  340546/2000000: episode: 3419, duration: 2.058s, episode steps: 143, steps per second: 69, episode reward: -88.602, mean reward: -0.620 [-100.000, 7.889], mean action: 1.601 [0.000, 3.000], mean observation: 0.131 [-2.960, 1.000], loss: 2.092556, mean_absolute_error: 34.318665, mean_q: 19.447447, mean_eps: 0.693573
  340680/2000000: episode: 3420, duration: 1.980s, episode steps: 134, steps per second: 68, episode reward: -136.221, mean reward: -1.017 [-100.000, 55.036], mean action: 1.836 [0.000, 3.000], mean observation: -0.103 [-1.348, 2.925], loss: 1.496443, mean_absolute_error: 34.280524, mean_q: 19.521882, mean_eps: 0.693449
  340828/2000000: episode: 3421, duration: 2.109s, episode steps: 148, steps per second: 70, episode reward: -81.227, mean reward: -0.549 [-100.000, 15.219], mean action: 1.595 [0.000, 3.000], mean observation: 0.065 [-3.026, 1.011], loss: 1.870731, mean_absolute_error: 33.404313, mean_q: 19.512472, mean_eps: 0.693323
  340954/2000000: episode: 3422, duration: 1.814s, episode steps: 126, steps per second: 69, episode reward: -125.812, mean reward: -0.999 [-100.000, 15.608], mean action: 1.786 [0.000, 3.000], mean observation: -0.062 [-1.039, 3.494], loss: 1.444674, mean_absolute_error: 33.187524, mean_q: 21.589972, mean_eps: 0.693199
  341016/2000000: episode: 3423, duration: 0.904s, episode steps: 62, steps per second: 69, episode reward: -107.672, mean reward: -1.737 [-100.000, 8.370], mean action: 1.871 [0.000, 3.000], mean observation: -0.154 [-4.684, 1.000], loss: 1.900968, mean_absolute_error: 32.569953, mean_q: 21.987506, mean_eps: 0.693114
  341131/2000000: episode: 3424, duration: 1.752s, episode steps: 115, steps per second: 66, episode reward: -86.851, mean reward: -0.755 [-100.000, 15.418], mean action: 1.661 [0.000, 3.000], mean observation: 0.017 [-1.104, 1.000], loss: 1.919212, mean_absolute_error: 32.515061, mean_q: 19.542022, mean_eps: 0.693035
  341258/2000000: episode: 3425, duration: 1.833s, episode steps: 127, steps per second: 69, episode reward: -145.722, mean reward: -1.147 [-100.000, 11.982], mean action: 1.677 [0.000, 3.000], mean observation: -0.093 [-0.944, 2.842], loss: 1.681874, mean_absolute_error: 32.955633, mean_q: 20.638965, mean_eps: 0.692925
  341385/2000000: episode: 3426, duration: 1.807s, episode steps: 127, steps per second: 70, episode reward: -96.892, mean reward: -0.763 [-100.000, 7.103], mean action: 1.575 [0.000, 3.000], mean observation: 0.080 [-0.787, 2.995], loss: 2.060626, mean_absolute_error: 34.616147, mean_q: 18.560420, mean_eps: 0.692810
  341510/2000000: episode: 3427, duration: 1.767s, episode steps: 125, steps per second: 71, episode reward: -84.374, mean reward: -0.675 [-100.000, 8.492], mean action: 1.592 [0.000, 3.000], mean observation: -0.064 [-2.745, 1.000], loss: 1.881307, mean_absolute_error: 32.904847, mean_q: 20.375777, mean_eps: 0.692697
  341648/2000000: episode: 3428, duration: 1.990s, episode steps: 138, steps per second: 69, episode reward: -54.751, mean reward: -0.397 [-100.000, 18.776], mean action: 1.652 [0.000, 3.000], mean observation: 0.101 [-0.542, 1.401], loss: 1.697703, mean_absolute_error: 33.967570, mean_q: 21.393261, mean_eps: 0.692580
  341797/2000000: episode: 3429, duration: 2.171s, episode steps: 149, steps per second: 69, episode reward: -112.987, mean reward: -0.758 [-100.000, 11.434], mean action: 1.584 [0.000, 3.000], mean observation: 0.068 [-0.895, 1.000], loss: 1.634417, mean_absolute_error: 33.918397, mean_q: 20.523712, mean_eps: 0.692450
  341897/2000000: episode: 3430, duration: 1.446s, episode steps: 100, steps per second: 69, episode reward: -138.307, mean reward: -1.383 [-100.000, 10.818], mean action: 1.670 [0.000, 3.000], mean observation: -0.011 [-4.447, 1.000], loss: 1.401822, mean_absolute_error: 33.206271, mean_q: 17.952946, mean_eps: 0.692337
  341998/2000000: episode: 3431, duration: 1.476s, episode steps: 101, steps per second: 68, episode reward: -86.319, mean reward: -0.855 [-100.000, 20.521], mean action: 1.733 [0.000, 3.000], mean observation: -0.071 [-1.001, 1.000], loss: 1.593166, mean_absolute_error: 33.582996, mean_q: 19.137670, mean_eps: 0.692247
  342096/2000000: episode: 3432, duration: 1.403s, episode steps: 98, steps per second: 70, episode reward: -135.043, mean reward: -1.378 [-100.000, 17.411], mean action: 1.816 [0.000, 3.000], mean observation: -0.028 [-1.230, 1.000], loss: 1.180653, mean_absolute_error: 32.207865, mean_q: 22.375842, mean_eps: 0.692159
  342166/2000000: episode: 3433, duration: 1.028s, episode steps: 70, steps per second: 68, episode reward: -94.083, mean reward: -1.344 [-100.000, 11.912], mean action: 1.557 [0.000, 3.000], mean observation: -0.070 [-1.207, 4.036], loss: 1.813219, mean_absolute_error: 33.653264, mean_q: 18.254837, mean_eps: 0.692083
  342237/2000000: episode: 3434, duration: 1.032s, episode steps: 71, steps per second: 69, episode reward: -117.029, mean reward: -1.648 [-100.000, 12.049], mean action: 1.746 [0.000, 3.000], mean observation: -0.130 [-1.166, 1.000], loss: 2.403721, mean_absolute_error: 34.981459, mean_q: 17.453021, mean_eps: 0.692018
  342396/2000000: episode: 3435, duration: 2.255s, episode steps: 159, steps per second: 71, episode reward: -139.570, mean reward: -0.878 [-100.000, 11.945], mean action: 1.692 [0.000, 3.000], mean observation: -0.060 [-1.253, 1.018], loss: 1.857444, mean_absolute_error: 33.074105, mean_q: 19.783644, mean_eps: 0.691916
  342480/2000000: episode: 3436, duration: 1.255s, episode steps: 84, steps per second: 67, episode reward: -148.690, mean reward: -1.770 [-100.000, 6.487], mean action: 1.714 [0.000, 3.000], mean observation: -0.144 [-1.125, 4.130], loss: 1.153033, mean_absolute_error: 32.483270, mean_q: 23.280140, mean_eps: 0.691808
  342578/2000000: episode: 3437, duration: 1.418s, episode steps: 98, steps per second: 69, episode reward: -146.139, mean reward: -1.491 [-100.000, 10.525], mean action: 1.561 [0.000, 3.000], mean observation: -0.004 [-1.214, 4.136], loss: 1.812050, mean_absolute_error: 34.657402, mean_q: 19.303587, mean_eps: 0.691725
  342726/2000000: episode: 3438, duration: 2.123s, episode steps: 148, steps per second: 70, episode reward: -29.217, mean reward: -0.197 [-100.000, 12.465], mean action: 1.601 [0.000, 3.000], mean observation: 0.034 [-1.172, 1.000], loss: 2.430104, mean_absolute_error: 33.958850, mean_q: 18.695819, mean_eps: 0.691613
  342807/2000000: episode: 3439, duration: 1.140s, episode steps: 81, steps per second: 71, episode reward: -79.246, mean reward: -0.978 [-100.000, 13.531], mean action: 1.617 [0.000, 3.000], mean observation: -0.077 [-1.042, 3.488], loss: 1.206575, mean_absolute_error: 33.561443, mean_q: 19.820362, mean_eps: 0.691511
  342904/2000000: episode: 3440, duration: 1.408s, episode steps: 97, steps per second: 69, episode reward: -133.763, mean reward: -1.379 [-100.000, 20.052], mean action: 1.732 [0.000, 3.000], mean observation: 0.002 [-1.327, 4.229], loss: 1.614917, mean_absolute_error: 33.465091, mean_q: 20.241760, mean_eps: 0.691431
  342986/2000000: episode: 3441, duration: 1.197s, episode steps: 82, steps per second: 69, episode reward: -112.229, mean reward: -1.369 [-100.000, 8.726], mean action: 1.646 [0.000, 3.000], mean observation: -0.088 [-3.500, 1.000], loss: 2.505912, mean_absolute_error: 36.116706, mean_q: 16.658554, mean_eps: 0.691350
  343100/2000000: episode: 3442, duration: 1.646s, episode steps: 114, steps per second: 69, episode reward: -124.435, mean reward: -1.092 [-100.000, 9.047], mean action: 1.579 [0.000, 3.000], mean observation: 0.024 [-3.392, 1.000], loss: 1.229034, mean_absolute_error: 33.052745, mean_q: 20.615358, mean_eps: 0.691262
  343183/2000000: episode: 3443, duration: 1.200s, episode steps: 83, steps per second: 69, episode reward: -71.428, mean reward: -0.861 [-100.000, 15.973], mean action: 1.627 [0.000, 3.000], mean observation: 0.087 [-3.375, 1.000], loss: 1.006231, mean_absolute_error: 34.061286, mean_q: 18.878951, mean_eps: 0.691174
  343286/2000000: episode: 3444, duration: 1.465s, episode steps: 103, steps per second: 70, episode reward: -100.899, mean reward: -0.980 [-100.000, 5.429], mean action: 1.718 [0.000, 3.000], mean observation: 0.040 [-0.767, 2.617], loss: 2.274271, mean_absolute_error: 34.166104, mean_q: 18.560118, mean_eps: 0.691089
  343368/2000000: episode: 3445, duration: 1.214s, episode steps: 82, steps per second: 68, episode reward: -130.440, mean reward: -1.591 [-100.000, 16.071], mean action: 1.634 [0.000, 3.000], mean observation: -0.005 [-1.406, 1.000], loss: 1.480976, mean_absolute_error: 35.312029, mean_q: 15.258987, mean_eps: 0.691007
  343458/2000000: episode: 3446, duration: 1.299s, episode steps: 90, steps per second: 69, episode reward: -115.881, mean reward: -1.288 [-100.000, 18.528], mean action: 1.656 [0.000, 3.000], mean observation: -0.100 [-2.760, 1.000], loss: 1.276191, mean_absolute_error: 33.167207, mean_q: 18.885724, mean_eps: 0.690929
  343606/2000000: episode: 3447, duration: 2.232s, episode steps: 148, steps per second: 66, episode reward: -64.044, mean reward: -0.433 [-100.000, 33.087], mean action: 1.696 [0.000, 3.000], mean observation: -0.063 [-1.117, 1.042], loss: 1.458068, mean_absolute_error: 32.523214, mean_q: 21.949155, mean_eps: 0.690821
  343765/2000000: episode: 3448, duration: 2.310s, episode steps: 159, steps per second: 69, episode reward: -33.775, mean reward: -0.212 [-100.000, 21.151], mean action: 1.692 [0.000, 3.000], mean observation: 0.020 [-1.022, 1.000], loss: 1.989162, mean_absolute_error: 34.160292, mean_q: 19.365595, mean_eps: 0.690683
  343887/2000000: episode: 3449, duration: 1.743s, episode steps: 122, steps per second: 70, episode reward: -87.723, mean reward: -0.719 [-100.000, 16.221], mean action: 1.590 [0.000, 3.000], mean observation: 0.061 [-0.947, 1.000], loss: 1.566335, mean_absolute_error: 33.529133, mean_q: 18.146130, mean_eps: 0.690557
  344008/2000000: episode: 3450, duration: 1.773s, episode steps: 121, steps per second: 68, episode reward: -102.421, mean reward: -0.846 [-100.000, 12.149], mean action: 1.562 [0.000, 3.000], mean observation: 0.066 [-2.984, 1.000], loss: 2.003304, mean_absolute_error: 34.343082, mean_q: 20.227407, mean_eps: 0.690449
  344106/2000000: episode: 3451, duration: 1.435s, episode steps: 98, steps per second: 68, episode reward: -143.430, mean reward: -1.464 [-100.000, 6.423], mean action: 1.551 [0.000, 3.000], mean observation: -0.096 [-1.202, 4.068], loss: 1.326756, mean_absolute_error: 33.393227, mean_q: 20.147081, mean_eps: 0.690350
  344210/2000000: episode: 3452, duration: 1.506s, episode steps: 104, steps per second: 69, episode reward: -166.747, mean reward: -1.603 [-100.000, 9.473], mean action: 1.702 [0.000, 3.000], mean observation: -0.124 [-1.388, 4.613], loss: 1.461989, mean_absolute_error: 33.241924, mean_q: 18.562341, mean_eps: 0.690258
  344340/2000000: episode: 3453, duration: 1.873s, episode steps: 130, steps per second: 69, episode reward: -92.407, mean reward: -0.711 [-100.000, 12.558], mean action: 1.615 [0.000, 3.000], mean observation: 0.050 [-1.266, 1.000], loss: 1.707792, mean_absolute_error: 33.400355, mean_q: 19.399295, mean_eps: 0.690153
  344525/2000000: episode: 3454, duration: 2.661s, episode steps: 185, steps per second: 70, episode reward: -89.705, mean reward: -0.485 [-100.000, 8.773], mean action: 1.627 [0.000, 3.000], mean observation: 0.059 [-3.158, 1.026], loss: 1.422228, mean_absolute_error: 32.484940, mean_q: 21.847775, mean_eps: 0.690011
  344632/2000000: episode: 3455, duration: 1.557s, episode steps: 107, steps per second: 69, episode reward: -115.853, mean reward: -1.083 [-100.000, 11.000], mean action: 1.701 [0.000, 3.000], mean observation: 0.064 [-0.995, 3.189], loss: 1.864273, mean_absolute_error: 35.047259, mean_q: 19.084870, mean_eps: 0.689880
  344727/2000000: episode: 3456, duration: 1.370s, episode steps: 95, steps per second: 69, episode reward: -89.776, mean reward: -0.945 [-100.000, 11.390], mean action: 1.716 [0.000, 3.000], mean observation: 0.045 [-2.519, 1.000], loss: 1.743924, mean_absolute_error: 33.970954, mean_q: 19.986937, mean_eps: 0.689790
  344831/2000000: episode: 3457, duration: 1.487s, episode steps: 104, steps per second: 70, episode reward: -121.408, mean reward: -1.167 [-100.000, 16.490], mean action: 1.721 [0.000, 3.000], mean observation: -0.117 [-1.042, 1.000], loss: 1.972438, mean_absolute_error: 32.458432, mean_q: 19.136036, mean_eps: 0.689700
  344925/2000000: episode: 3458, duration: 1.362s, episode steps: 94, steps per second: 69, episode reward: -82.474, mean reward: -0.877 [-100.000, 4.324], mean action: 1.809 [0.000, 3.000], mean observation: -0.097 [-2.764, 1.000], loss: 2.058893, mean_absolute_error: 33.491047, mean_q: 18.313942, mean_eps: 0.689610
  345045/2000000: episode: 3459, duration: 1.730s, episode steps: 120, steps per second: 69, episode reward: -126.051, mean reward: -1.050 [-100.000, 4.135], mean action: 1.758 [0.000, 3.000], mean observation: -0.085 [-2.136, 1.000], loss: 1.721276, mean_absolute_error: 34.076571, mean_q: 18.152904, mean_eps: 0.689513
  345136/2000000: episode: 3460, duration: 1.308s, episode steps: 91, steps per second: 70, episode reward: -19.089, mean reward: -0.210 [-100.000, 18.414], mean action: 1.714 [0.000, 3.000], mean observation: -0.056 [-1.097, 1.097], loss: 1.417086, mean_absolute_error: 34.444891, mean_q: 16.929703, mean_eps: 0.689419
  345241/2000000: episode: 3461, duration: 1.593s, episode steps: 105, steps per second: 66, episode reward: -86.998, mean reward: -0.829 [-100.000, 19.022], mean action: 1.648 [0.000, 3.000], mean observation: 0.090 [-1.084, 1.000], loss: 2.031496, mean_absolute_error: 35.871169, mean_q: 15.627997, mean_eps: 0.689331
  345359/2000000: episode: 3462, duration: 1.730s, episode steps: 118, steps per second: 68, episode reward: -221.845, mean reward: -1.880 [-100.000, 20.666], mean action: 1.788 [0.000, 3.000], mean observation: -0.107 [-3.982, 1.000], loss: 1.238223, mean_absolute_error: 33.965597, mean_q: 18.785651, mean_eps: 0.689230
  345485/2000000: episode: 3463, duration: 1.854s, episode steps: 126, steps per second: 68, episode reward: -114.924, mean reward: -0.912 [-100.000, 9.902], mean action: 1.524 [0.000, 3.000], mean observation: 0.033 [-3.698, 1.000], loss: 1.411186, mean_absolute_error: 34.155781, mean_q: 18.253410, mean_eps: 0.689120
  345612/2000000: episode: 3464, duration: 1.837s, episode steps: 127, steps per second: 69, episode reward: -113.727, mean reward: -0.895 [-100.000, 10.260], mean action: 1.591 [0.000, 3.000], mean observation: 0.072 [-4.171, 1.018], loss: 1.452827, mean_absolute_error: 32.911181, mean_q: 18.929965, mean_eps: 0.689007
  345763/2000000: episode: 3465, duration: 2.186s, episode steps: 151, steps per second: 69, episode reward: -103.816, mean reward: -0.688 [-100.000, 38.800], mean action: 1.656 [0.000, 3.000], mean observation: -0.097 [-1.136, 1.422], loss: 2.005216, mean_absolute_error: 33.819444, mean_q: 19.974794, mean_eps: 0.688883
  345876/2000000: episode: 3466, duration: 1.737s, episode steps: 113, steps per second: 65, episode reward: -152.850, mean reward: -1.353 [-100.000, 5.388], mean action: 1.646 [0.000, 3.000], mean observation: -0.116 [-4.355, 1.000], loss: 1.852043, mean_absolute_error: 34.664877, mean_q: 16.298163, mean_eps: 0.688764
  345944/2000000: episode: 3467, duration: 1.071s, episode steps: 68, steps per second: 63, episode reward: -67.595, mean reward: -0.994 [-100.000, 11.044], mean action: 1.735 [0.000, 3.000], mean observation: -0.159 [-3.549, 1.000], loss: 1.574375, mean_absolute_error: 33.993966, mean_q: 21.051623, mean_eps: 0.688683
  346027/2000000: episode: 3468, duration: 1.203s, episode steps: 83, steps per second: 69, episode reward: -146.412, mean reward: -1.764 [-100.000, 2.999], mean action: 1.723 [0.000, 3.000], mean observation: -0.112 [-4.401, 1.000], loss: 1.597104, mean_absolute_error: 34.581621, mean_q: 15.575789, mean_eps: 0.688614
  346153/2000000: episode: 3469, duration: 1.865s, episode steps: 126, steps per second: 68, episode reward: -156.311, mean reward: -1.241 [-100.000, 4.399], mean action: 1.651 [0.000, 3.000], mean observation: 0.108 [-2.258, 1.000], loss: 1.589630, mean_absolute_error: 33.345556, mean_q: 21.169647, mean_eps: 0.688519
  346227/2000000: episode: 3470, duration: 1.045s, episode steps: 74, steps per second: 71, episode reward: -110.312, mean reward: -1.491 [-100.000, 15.797], mean action: 1.581 [0.000, 3.000], mean observation: -0.185 [-4.155, 1.000], loss: 1.001451, mean_absolute_error: 33.780899, mean_q: 19.505352, mean_eps: 0.688429
  346311/2000000: episode: 3471, duration: 1.264s, episode steps: 84, steps per second: 66, episode reward: -40.348, mean reward: -0.480 [-100.000, 10.655], mean action: 1.607 [0.000, 3.000], mean observation: 0.047 [-2.599, 1.000], loss: 1.705639, mean_absolute_error: 33.520149, mean_q: 18.168198, mean_eps: 0.688359
  346385/2000000: episode: 3472, duration: 1.170s, episode steps: 74, steps per second: 63, episode reward: -78.123, mean reward: -1.056 [-100.000, 9.606], mean action: 1.541 [0.000, 3.000], mean observation: 0.028 [-1.166, 3.308], loss: 1.306534, mean_absolute_error: 33.269052, mean_q: 19.777128, mean_eps: 0.688287
  346516/2000000: episode: 3473, duration: 2.017s, episode steps: 131, steps per second: 65, episode reward: -116.505, mean reward: -0.889 [-100.000, 14.295], mean action: 1.557 [0.000, 3.000], mean observation: 0.158 [-1.132, 3.092], loss: 1.523863, mean_absolute_error: 34.168136, mean_q: 19.053261, mean_eps: 0.688195
  346678/2000000: episode: 3474, duration: 2.337s, episode steps: 162, steps per second: 69, episode reward: -72.534, mean reward: -0.448 [-100.000, 16.548], mean action: 1.654 [0.000, 3.000], mean observation: 0.025 [-0.861, 1.000], loss: 1.903173, mean_absolute_error: 33.165739, mean_q: 19.848337, mean_eps: 0.688064
  346758/2000000: episode: 3475, duration: 1.157s, episode steps: 80, steps per second: 69, episode reward: -86.003, mean reward: -1.075 [-100.000, 14.970], mean action: 1.475 [0.000, 3.000], mean observation: -0.048 [-3.902, 1.000], loss: 2.037788, mean_absolute_error: 32.934692, mean_q: 19.240297, mean_eps: 0.687954
  346886/2000000: episode: 3476, duration: 1.855s, episode steps: 128, steps per second: 69, episode reward: -65.012, mean reward: -0.508 [-100.000, 13.479], mean action: 1.641 [0.000, 3.000], mean observation: 0.127 [-3.322, 1.033], loss: 1.943981, mean_absolute_error: 34.331072, mean_q: 18.170114, mean_eps: 0.687860
  347012/2000000: episode: 3477, duration: 1.899s, episode steps: 126, steps per second: 66, episode reward: -21.602, mean reward: -0.171 [-100.000, 19.407], mean action: 1.690 [0.000, 3.000], mean observation: 0.053 [-0.895, 1.000], loss: 1.271222, mean_absolute_error: 33.923653, mean_q: 20.296970, mean_eps: 0.687747
  347116/2000000: episode: 3478, duration: 1.527s, episode steps: 104, steps per second: 68, episode reward: -101.896, mean reward: -0.980 [-100.000, 21.771], mean action: 1.356 [0.000, 3.000], mean observation: 0.086 [-2.893, 1.000], loss: 1.293336, mean_absolute_error: 33.050187, mean_q: 21.172861, mean_eps: 0.687644
  347217/2000000: episode: 3479, duration: 1.488s, episode steps: 101, steps per second: 68, episode reward: -106.614, mean reward: -1.056 [-100.000, 7.605], mean action: 1.564 [0.000, 3.000], mean observation: 0.032 [-3.994, 1.000], loss: 1.850630, mean_absolute_error: 35.130595, mean_q: 17.364213, mean_eps: 0.687551
  347308/2000000: episode: 3480, duration: 1.301s, episode steps: 91, steps per second: 70, episode reward: -127.775, mean reward: -1.404 [-100.000, 22.301], mean action: 1.703 [0.000, 3.000], mean observation: 0.052 [-3.696, 1.000], loss: 1.700262, mean_absolute_error: 35.088643, mean_q: 20.887715, mean_eps: 0.687464
  347415/2000000: episode: 3481, duration: 1.587s, episode steps: 107, steps per second: 67, episode reward: -68.825, mean reward: -0.643 [-100.000, 49.727], mean action: 1.710 [0.000, 3.000], mean observation: 0.054 [-1.000, 1.409], loss: 1.994813, mean_absolute_error: 33.314868, mean_q: 18.110892, mean_eps: 0.687376
  347500/2000000: episode: 3482, duration: 1.267s, episode steps: 85, steps per second: 67, episode reward: -124.472, mean reward: -1.464 [-100.000, 12.584], mean action: 1.624 [0.000, 3.000], mean observation: -0.149 [-1.171, 4.249], loss: 1.487863, mean_absolute_error: 33.847823, mean_q: 18.968952, mean_eps: 0.687290
  347613/2000000: episode: 3483, duration: 1.653s, episode steps: 113, steps per second: 68, episode reward: -122.069, mean reward: -1.080 [-100.000, 5.179], mean action: 1.575 [0.000, 3.000], mean observation: 0.038 [-3.218, 1.000], loss: 1.497578, mean_absolute_error: 34.151682, mean_q: 16.921834, mean_eps: 0.687200
  347703/2000000: episode: 3484, duration: 1.261s, episode steps: 90, steps per second: 71, episode reward: -25.758, mean reward: -0.286 [-100.000, 17.699], mean action: 1.711 [0.000, 3.000], mean observation: -0.027 [-0.873, 1.000], loss: 1.940447, mean_absolute_error: 34.234476, mean_q: 18.656219, mean_eps: 0.687108
  347871/2000000: episode: 3485, duration: 2.408s, episode steps: 168, steps per second: 70, episode reward: -86.334, mean reward: -0.514 [-100.000, 11.362], mean action: 1.607 [0.000, 3.000], mean observation: 0.070 [-0.836, 2.407], loss: 1.571773, mean_absolute_error: 32.991480, mean_q: 19.775015, mean_eps: 0.686993
  347990/2000000: episode: 3486, duration: 1.737s, episode steps: 119, steps per second: 68, episode reward: -106.951, mean reward: -0.899 [-100.000, 6.888], mean action: 1.655 [0.000, 3.000], mean observation: -0.005 [-3.099, 1.000], loss: 1.402328, mean_absolute_error: 32.716138, mean_q: 21.154763, mean_eps: 0.686863
  348091/2000000: episode: 3487, duration: 1.441s, episode steps: 101, steps per second: 70, episode reward: -143.532, mean reward: -1.421 [-100.000, 12.002], mean action: 1.762 [0.000, 3.000], mean observation: 0.034 [-1.011, 3.292], loss: 2.168471, mean_absolute_error: 33.984623, mean_q: 20.406383, mean_eps: 0.686764
  348163/2000000: episode: 3488, duration: 1.047s, episode steps: 72, steps per second: 69, episode reward: -130.529, mean reward: -1.813 [-100.000, 11.894], mean action: 1.681 [0.000, 3.000], mean observation: -0.045 [-1.272, 3.957], loss: 2.251449, mean_absolute_error: 33.806547, mean_q: 21.789962, mean_eps: 0.686687
  348277/2000000: episode: 3489, duration: 1.654s, episode steps: 114, steps per second: 69, episode reward: -167.246, mean reward: -1.467 [-100.000, 19.148], mean action: 1.667 [0.000, 3.000], mean observation: -0.074 [-1.393, 2.207], loss: 1.359677, mean_absolute_error: 33.652240, mean_q: 19.519143, mean_eps: 0.686602
  348447/2000000: episode: 3490, duration: 2.414s, episode steps: 170, steps per second: 70, episode reward: -106.537, mean reward: -0.627 [-100.000, 23.781], mean action: 1.576 [0.000, 3.000], mean observation: 0.148 [-1.116, 3.822], loss: 2.022051, mean_absolute_error: 34.018560, mean_q: 20.944727, mean_eps: 0.686474
  348546/2000000: episode: 3491, duration: 1.419s, episode steps: 99, steps per second: 70, episode reward: -131.484, mean reward: -1.328 [-100.000, 17.329], mean action: 1.545 [0.000, 3.000], mean observation: 0.122 [-0.997, 1.000], loss: 2.436219, mean_absolute_error: 34.120767, mean_q: 19.555920, mean_eps: 0.686354
  348640/2000000: episode: 3492, duration: 1.361s, episode steps: 94, steps per second: 69, episode reward: -128.195, mean reward: -1.364 [-100.000, 7.031], mean action: 1.617 [0.000, 3.000], mean observation: 0.116 [-2.840, 1.000], loss: 1.813976, mean_absolute_error: 35.481204, mean_q: 16.988479, mean_eps: 0.686267
  348748/2000000: episode: 3493, duration: 1.589s, episode steps: 108, steps per second: 68, episode reward: -356.851, mean reward: -3.304 [-100.000, 96.498], mean action: 1.676 [0.000, 3.000], mean observation: -0.214 [-3.189, 1.133], loss: 2.288049, mean_absolute_error: 34.607717, mean_q: 17.999006, mean_eps: 0.686177
  348821/2000000: episode: 3494, duration: 1.083s, episode steps: 73, steps per second: 67, episode reward: -67.501, mean reward: -0.925 [-100.000, 8.407], mean action: 1.589 [0.000, 3.000], mean observation: -0.117 [-1.164, 1.000], loss: 2.327648, mean_absolute_error: 35.337138, mean_q: 18.446796, mean_eps: 0.686094
  348910/2000000: episode: 3495, duration: 1.282s, episode steps: 89, steps per second: 69, episode reward: -93.628, mean reward: -1.052 [-100.000, 11.333], mean action: 1.652 [0.000, 3.000], mean observation: 0.050 [-2.726, 1.000], loss: 1.247613, mean_absolute_error: 32.350170, mean_q: 18.441360, mean_eps: 0.686021
  349008/2000000: episode: 3496, duration: 1.448s, episode steps: 98, steps per second: 68, episode reward: -73.770, mean reward: -0.753 [-100.000, 11.049], mean action: 1.592 [0.000, 3.000], mean observation: 0.045 [-0.872, 2.531], loss: 1.330270, mean_absolute_error: 34.311583, mean_q: 19.091825, mean_eps: 0.685938
  349122/2000000: episode: 3497, duration: 1.738s, episode steps: 114, steps per second: 66, episode reward: -105.287, mean reward: -0.924 [-100.000, 16.720], mean action: 1.500 [0.000, 3.000], mean observation: 0.026 [-1.031, 1.000], loss: 1.172443, mean_absolute_error: 33.921391, mean_q: 20.815060, mean_eps: 0.685842
  349257/2000000: episode: 3498, duration: 1.976s, episode steps: 135, steps per second: 68, episode reward: -89.010, mean reward: -0.659 [-100.000, 15.599], mean action: 1.548 [0.000, 3.000], mean observation: 0.019 [-0.960, 1.000], loss: 1.189517, mean_absolute_error: 32.917946, mean_q: 21.451563, mean_eps: 0.685729
  349356/2000000: episode: 3499, duration: 1.435s, episode steps: 99, steps per second: 69, episode reward: -79.458, mean reward: -0.803 [-100.000, 33.726], mean action: 1.636 [0.000, 3.000], mean observation: -0.162 [-1.231, 1.613], loss: 2.788811, mean_absolute_error: 35.526903, mean_q: 17.045944, mean_eps: 0.685625
  349417/2000000: episode: 3500, duration: 0.931s, episode steps: 61, steps per second: 66, episode reward: -117.640, mean reward: -1.929 [-100.000, 3.835], mean action: 1.541 [0.000, 3.000], mean observation: 0.059 [-1.293, 4.500], loss: 1.710839, mean_absolute_error: 33.787413, mean_q: 16.391434, mean_eps: 0.685553
  349543/2000000: episode: 3501, duration: 1.791s, episode steps: 126, steps per second: 70, episode reward: -98.989, mean reward: -0.786 [-100.000, 11.709], mean action: 1.619 [0.000, 3.000], mean observation: -0.106 [-1.208, 1.000], loss: 1.240208, mean_absolute_error: 33.180057, mean_q: 18.727371, mean_eps: 0.685468
  349618/2000000: episode: 3502, duration: 1.103s, episode steps: 75, steps per second: 68, episode reward: -105.182, mean reward: -1.402 [-100.000, 5.630], mean action: 1.720 [0.000, 3.000], mean observation: -0.160 [-3.454, 1.000], loss: 1.726571, mean_absolute_error: 34.210813, mean_q: 17.539695, mean_eps: 0.685378
  349846/2000000: episode: 3503, duration: 3.262s, episode steps: 228, steps per second: 70, episode reward: -142.859, mean reward: -0.627 [-100.000, 17.731], mean action: 1.680 [0.000, 3.000], mean observation: 0.151 [-0.924, 1.090], loss: 2.118270, mean_absolute_error: 34.273093, mean_q: 19.230120, mean_eps: 0.685241
  349928/2000000: episode: 3504, duration: 1.214s, episode steps: 82, steps per second: 68, episode reward: -99.869, mean reward: -1.218 [-100.000, 20.221], mean action: 1.671 [0.000, 3.000], mean observation: 0.088 [-0.849, 3.109], loss: 1.088272, mean_absolute_error: 33.217508, mean_q: 22.934122, mean_eps: 0.685103
  350077/2000000: episode: 3505, duration: 2.170s, episode steps: 149, steps per second: 69, episode reward: -249.942, mean reward: -1.677 [-100.000, 60.342], mean action: 1.557 [0.000, 3.000], mean observation: -0.113 [-2.096, 1.045], loss: 1.957186, mean_absolute_error: 34.677300, mean_q: 19.669935, mean_eps: 0.684998
  350167/2000000: episode: 3506, duration: 1.293s, episode steps: 90, steps per second: 70, episode reward: -126.225, mean reward: -1.402 [-100.000, 9.117], mean action: 1.656 [0.000, 3.000], mean observation: -0.009 [-4.201, 1.000], loss: 1.709997, mean_absolute_error: 34.142652, mean_q: 20.352539, mean_eps: 0.684890
  350271/2000000: episode: 3507, duration: 1.474s, episode steps: 104, steps per second: 71, episode reward: -114.054, mean reward: -1.097 [-100.000, 15.307], mean action: 1.663 [0.000, 3.000], mean observation: -0.073 [-0.966, 3.238], loss: 1.934661, mean_absolute_error: 35.184239, mean_q: 19.423545, mean_eps: 0.684804
  350391/2000000: episode: 3508, duration: 1.725s, episode steps: 120, steps per second: 70, episode reward: -93.899, mean reward: -0.782 [-100.000, 12.533], mean action: 1.617 [0.000, 3.000], mean observation: -0.065 [-0.904, 1.591], loss: 2.117755, mean_absolute_error: 35.093747, mean_q: 18.233809, mean_eps: 0.684703
  350491/2000000: episode: 3509, duration: 1.423s, episode steps: 100, steps per second: 70, episode reward: -76.589, mean reward: -0.766 [-100.000, 20.888], mean action: 1.670 [0.000, 3.000], mean observation: -0.049 [-0.944, 2.610], loss: 1.258425, mean_absolute_error: 33.338639, mean_q: 21.917546, mean_eps: 0.684604
  350599/2000000: episode: 3510, duration: 1.550s, episode steps: 108, steps per second: 70, episode reward: -126.383, mean reward: -1.170 [-100.000, 7.996], mean action: 1.676 [0.000, 3.000], mean observation: 0.050 [-1.002, 3.426], loss: 1.399730, mean_absolute_error: 33.872512, mean_q: 20.937988, mean_eps: 0.684510
  350705/2000000: episode: 3511, duration: 1.526s, episode steps: 106, steps per second: 69, episode reward: -102.058, mean reward: -0.963 [-100.000, 10.068], mean action: 1.623 [0.000, 3.000], mean observation: 0.095 [-0.955, 3.193], loss: 1.270671, mean_absolute_error: 33.677896, mean_q: 23.455674, mean_eps: 0.684413
  350842/2000000: episode: 3512, duration: 1.939s, episode steps: 137, steps per second: 71, episode reward: -111.050, mean reward: -0.811 [-100.000, 6.864], mean action: 1.555 [0.000, 3.000], mean observation: 0.094 [-0.797, 3.412], loss: 2.331445, mean_absolute_error: 35.873565, mean_q: 17.981041, mean_eps: 0.684303
  350914/2000000: episode: 3513, duration: 1.040s, episode steps: 72, steps per second: 69, episode reward: -162.608, mean reward: -2.258 [-100.000, 10.219], mean action: 1.181 [0.000, 3.000], mean observation: 0.035 [-1.416, 4.583], loss: 2.141785, mean_absolute_error: 34.907579, mean_q: 17.893487, mean_eps: 0.684210
  351040/2000000: episode: 3514, duration: 1.818s, episode steps: 126, steps per second: 69, episode reward: -107.346, mean reward: -0.852 [-100.000, 19.895], mean action: 1.603 [0.000, 3.000], mean observation: -0.042 [-0.892, 1.000], loss: 1.783279, mean_absolute_error: 33.411992, mean_q: 18.275871, mean_eps: 0.684122
  351137/2000000: episode: 3515, duration: 1.432s, episode steps: 97, steps per second: 68, episode reward: -118.823, mean reward: -1.225 [-100.000, 13.110], mean action: 1.649 [0.000, 3.000], mean observation: -0.057 [-1.162, 1.000], loss: 1.625995, mean_absolute_error: 33.429591, mean_q: 19.343479, mean_eps: 0.684021
  351273/2000000: episode: 3516, duration: 1.937s, episode steps: 136, steps per second: 70, episode reward: -92.392, mean reward: -0.679 [-100.000, 8.335], mean action: 1.706 [0.000, 3.000], mean observation: -0.049 [-1.928, 1.000], loss: 1.568735, mean_absolute_error: 33.766150, mean_q: 21.893929, mean_eps: 0.683915
  351384/2000000: episode: 3517, duration: 1.597s, episode steps: 111, steps per second: 70, episode reward: -87.094, mean reward: -0.785 [-100.000, 14.021], mean action: 1.604 [0.000, 3.000], mean observation: 0.064 [-0.984, 2.695], loss: 1.852581, mean_absolute_error: 34.884413, mean_q: 18.837556, mean_eps: 0.683805
  351507/2000000: episode: 3518, duration: 1.780s, episode steps: 123, steps per second: 69, episode reward: -114.363, mean reward: -0.930 [-100.000, 22.438], mean action: 1.715 [0.000, 3.000], mean observation: 0.078 [-1.388, 1.035], loss: 1.486053, mean_absolute_error: 33.789447, mean_q: 20.818131, mean_eps: 0.683700
  351572/2000000: episode: 3519, duration: 0.974s, episode steps: 65, steps per second: 67, episode reward: -112.681, mean reward: -1.734 [-100.000, 7.536], mean action: 1.615 [0.000, 3.000], mean observation: 0.017 [-4.237, 1.000], loss: 0.995610, mean_absolute_error: 33.844364, mean_q: 21.762409, mean_eps: 0.683616
  351686/2000000: episode: 3520, duration: 1.644s, episode steps: 114, steps per second: 69, episode reward: -59.821, mean reward: -0.525 [-100.000, 23.462], mean action: 1.456 [0.000, 3.000], mean observation: -0.021 [-2.530, 1.000], loss: 1.810185, mean_absolute_error: 34.741942, mean_q: 17.238571, mean_eps: 0.683535
  351797/2000000: episode: 3521, duration: 1.723s, episode steps: 111, steps per second: 64, episode reward: -153.029, mean reward: -1.379 [-100.000, 12.280], mean action: 1.631 [0.000, 3.000], mean observation: -0.056 [-1.254, 4.540], loss: 2.222453, mean_absolute_error: 34.570052, mean_q: 19.546213, mean_eps: 0.683432
  351865/2000000: episode: 3522, duration: 0.973s, episode steps: 68, steps per second: 70, episode reward: -101.966, mean reward: -1.500 [-100.000, 9.255], mean action: 1.632 [0.000, 3.000], mean observation: 0.098 [-1.080, 3.641], loss: 1.336744, mean_absolute_error: 34.272570, mean_q: 21.152739, mean_eps: 0.683351
  351941/2000000: episode: 3523, duration: 1.110s, episode steps: 76, steps per second: 68, episode reward: -72.651, mean reward: -0.956 [-100.000, 20.581], mean action: 1.671 [0.000, 3.000], mean observation: -0.103 [-1.117, 1.000], loss: 1.564102, mean_absolute_error: 33.842651, mean_q: 20.385650, mean_eps: 0.683286
  352036/2000000: episode: 3524, duration: 1.396s, episode steps: 95, steps per second: 68, episode reward: -123.613, mean reward: -1.301 [-100.000, 11.199], mean action: 1.484 [0.000, 3.000], mean observation: 0.139 [-2.793, 1.000], loss: 2.580089, mean_absolute_error: 34.897669, mean_q: 17.609736, mean_eps: 0.683211
  352177/2000000: episode: 3525, duration: 2.064s, episode steps: 141, steps per second: 68, episode reward: -75.249, mean reward: -0.534 [-100.000, 75.795], mean action: 1.560 [0.000, 3.000], mean observation: 0.146 [-1.510, 1.008], loss: 1.324099, mean_absolute_error: 33.487586, mean_q: 20.530932, mean_eps: 0.683105
  352289/2000000: episode: 3526, duration: 1.616s, episode steps: 112, steps per second: 69, episode reward: -104.782, mean reward: -0.936 [-100.000, 19.559], mean action: 1.661 [0.000, 3.000], mean observation: -0.076 [-1.071, 3.835], loss: 1.940882, mean_absolute_error: 34.239751, mean_q: 20.513295, mean_eps: 0.682989
  353289/2000000: episode: 3527, duration: 15.009s, episode steps: 1000, steps per second: 67, episode reward: 51.006, mean reward: 0.051 [-23.775, 35.960], mean action: 1.410 [0.000, 3.000], mean observation: 0.063 [-1.511, 1.093], loss: 1.778404, mean_absolute_error: 34.060887, mean_q: 19.783947, mean_eps: 0.682489
  353433/2000000: episode: 3528, duration: 2.018s, episode steps: 144, steps per second: 71, episode reward: -102.175, mean reward: -0.710 [-100.000, 7.299], mean action: 1.590 [0.000, 3.000], mean observation: 0.059 [-0.974, 3.053], loss: 1.503840, mean_absolute_error: 33.702332, mean_q: 18.180337, mean_eps: 0.681974
  353565/2000000: episode: 3529, duration: 1.853s, episode steps: 132, steps per second: 71, episode reward: -103.284, mean reward: -0.782 [-100.000, 10.734], mean action: 1.477 [0.000, 3.000], mean observation: -0.015 [-0.934, 3.184], loss: 1.154946, mean_absolute_error: 35.293720, mean_q: 21.311837, mean_eps: 0.681850
  353721/2000000: episode: 3530, duration: 2.214s, episode steps: 156, steps per second: 70, episode reward: -53.159, mean reward: -0.341 [-100.000, 11.380], mean action: 1.667 [0.000, 3.000], mean observation: 0.007 [-1.174, 1.000], loss: 1.207345, mean_absolute_error: 33.730202, mean_q: 21.843348, mean_eps: 0.681720
  353906/2000000: episode: 3531, duration: 2.614s, episode steps: 185, steps per second: 71, episode reward: -99.621, mean reward: -0.538 [-100.000, 13.028], mean action: 1.741 [0.000, 3.000], mean observation: 0.053 [-3.380, 1.015], loss: 1.693357, mean_absolute_error: 34.853685, mean_q: 18.100668, mean_eps: 0.681567
  353981/2000000: episode: 3532, duration: 1.068s, episode steps: 75, steps per second: 70, episode reward: -141.193, mean reward: -1.883 [-100.000, 17.523], mean action: 1.600 [0.000, 3.000], mean observation: 0.043 [-1.358, 4.962], loss: 1.741830, mean_absolute_error: 33.965072, mean_q: 19.004546, mean_eps: 0.681450
  354134/2000000: episode: 3533, duration: 2.128s, episode steps: 153, steps per second: 72, episode reward: -82.093, mean reward: -0.537 [-100.000, 13.678], mean action: 1.686 [0.000, 3.000], mean observation: 0.041 [-0.997, 3.412], loss: 1.347521, mean_absolute_error: 32.564481, mean_q: 22.827900, mean_eps: 0.681348
  354244/2000000: episode: 3534, duration: 1.604s, episode steps: 110, steps per second: 69, episode reward: -101.099, mean reward: -0.919 [-100.000, 11.327], mean action: 1.636 [0.000, 3.000], mean observation: 0.041 [-1.206, 1.000], loss: 1.590989, mean_absolute_error: 33.281384, mean_q: 19.754837, mean_eps: 0.681231
  354314/2000000: episode: 3535, duration: 1.021s, episode steps: 70, steps per second: 69, episode reward: -101.986, mean reward: -1.457 [-100.000, 11.515], mean action: 1.757 [0.000, 3.000], mean observation: 0.088 [-1.163, 1.000], loss: 1.197529, mean_absolute_error: 33.457271, mean_q: 21.587442, mean_eps: 0.681150
  354414/2000000: episode: 3536, duration: 1.443s, episode steps: 100, steps per second: 69, episode reward: -129.521, mean reward: -1.295 [-100.000, 20.925], mean action: 1.430 [0.000, 3.000], mean observation: 0.127 [-0.906, 1.000], loss: 1.750212, mean_absolute_error: 35.289037, mean_q: 18.324383, mean_eps: 0.681072
  354571/2000000: episode: 3537, duration: 2.233s, episode steps: 157, steps per second: 70, episode reward: -97.118, mean reward: -0.619 [-100.000, 15.514], mean action: 1.637 [0.000, 3.000], mean observation: -0.014 [-0.950, 2.918], loss: 1.636581, mean_absolute_error: 35.315638, mean_q: 17.385084, mean_eps: 0.680957
  354642/2000000: episode: 3538, duration: 1.040s, episode steps: 71, steps per second: 68, episode reward: -112.270, mean reward: -1.581 [-100.000, 6.361], mean action: 1.479 [0.000, 3.000], mean observation: 0.041 [-3.398, 1.000], loss: 1.745625, mean_absolute_error: 33.450420, mean_q: 19.798940, mean_eps: 0.680855
  354785/2000000: episode: 3539, duration: 2.049s, episode steps: 143, steps per second: 70, episode reward: -132.154, mean reward: -0.924 [-100.000, 11.173], mean action: 1.601 [0.000, 3.000], mean observation: 0.024 [-1.074, 3.630], loss: 1.317849, mean_absolute_error: 34.570221, mean_q: 19.584653, mean_eps: 0.680757
  354869/2000000: episode: 3540, duration: 1.217s, episode steps: 84, steps per second: 69, episode reward: -97.124, mean reward: -1.156 [-100.000, 10.148], mean action: 1.488 [0.000, 3.000], mean observation: -0.073 [-3.706, 1.000], loss: 1.627761, mean_absolute_error: 34.518630, mean_q: 18.647721, mean_eps: 0.680655
  354948/2000000: episode: 3541, duration: 1.156s, episode steps: 79, steps per second: 68, episode reward: -61.503, mean reward: -0.779 [-100.000, 12.530], mean action: 1.785 [0.000, 3.000], mean observation: -0.013 [-1.052, 2.969], loss: 2.971348, mean_absolute_error: 34.485701, mean_q: 17.688435, mean_eps: 0.680583
  355036/2000000: episode: 3542, duration: 1.308s, episode steps: 88, steps per second: 67, episode reward: -94.783, mean reward: -1.077 [-100.000, 17.532], mean action: 1.545 [0.000, 3.000], mean observation: -0.067 [-1.228, 1.000], loss: 1.431883, mean_absolute_error: 32.779713, mean_q: 21.542388, mean_eps: 0.680509
  355099/2000000: episode: 3543, duration: 0.910s, episode steps: 63, steps per second: 69, episode reward: -101.263, mean reward: -1.607 [-100.000, 17.173], mean action: 1.762 [0.000, 3.000], mean observation: -0.145 [-1.428, 3.919], loss: 1.158878, mean_absolute_error: 33.843719, mean_q: 20.998273, mean_eps: 0.680441
  355229/2000000: episode: 3544, duration: 1.885s, episode steps: 130, steps per second: 69, episode reward: -88.866, mean reward: -0.684 [-100.000, 18.534], mean action: 1.577 [0.000, 3.000], mean observation: -0.066 [-1.128, 1.050], loss: 1.740024, mean_absolute_error: 34.574974, mean_q: 16.398080, mean_eps: 0.680352
  355321/2000000: episode: 3545, duration: 1.327s, episode steps: 92, steps per second: 69, episode reward: -133.458, mean reward: -1.451 [-100.000, 10.957], mean action: 1.663 [0.000, 3.000], mean observation: -0.000 [-1.209, 4.093], loss: 2.260396, mean_absolute_error: 36.484321, mean_q: 17.055511, mean_eps: 0.680252
  355470/2000000: episode: 3546, duration: 2.109s, episode steps: 149, steps per second: 71, episode reward: -113.271, mean reward: -0.760 [-100.000, 8.074], mean action: 1.765 [0.000, 3.000], mean observation: 0.126 [-0.722, 2.534], loss: 1.630542, mean_absolute_error: 33.981886, mean_q: 20.880820, mean_eps: 0.680144
  355554/2000000: episode: 3547, duration: 1.211s, episode steps: 84, steps per second: 69, episode reward: -84.023, mean reward: -1.000 [-100.000, 7.418], mean action: 1.548 [0.000, 3.000], mean observation: 0.042 [-3.086, 1.000], loss: 2.296060, mean_absolute_error: 34.810653, mean_q: 19.789134, mean_eps: 0.680039
  355679/2000000: episode: 3548, duration: 1.779s, episode steps: 125, steps per second: 70, episode reward: -167.000, mean reward: -1.336 [-100.000, 6.986], mean action: 1.616 [0.000, 3.000], mean observation: -0.048 [-1.344, 4.502], loss: 1.804370, mean_absolute_error: 33.991561, mean_q: 19.353062, mean_eps: 0.679946
  355768/2000000: episode: 3549, duration: 1.290s, episode steps: 89, steps per second: 69, episode reward: -107.451, mean reward: -1.207 [-100.000, 13.007], mean action: 1.539 [0.000, 3.000], mean observation: 0.026 [-3.128, 1.000], loss: 1.438745, mean_absolute_error: 33.494657, mean_q: 21.437064, mean_eps: 0.679850
  355850/2000000: episode: 3550, duration: 1.188s, episode steps: 82, steps per second: 69, episode reward: -78.659, mean reward: -0.959 [-100.000, 18.164], mean action: 1.659 [0.000, 3.000], mean observation: -0.009 [-0.989, 2.869], loss: 1.439231, mean_absolute_error: 34.198815, mean_q: 17.557550, mean_eps: 0.679773
  356011/2000000: episode: 3551, duration: 2.266s, episode steps: 161, steps per second: 71, episode reward: -106.337, mean reward: -0.660 [-100.000, 12.378], mean action: 1.677 [0.000, 3.000], mean observation: 0.175 [-1.028, 1.143], loss: 2.266186, mean_absolute_error: 34.592132, mean_q: 21.185219, mean_eps: 0.679663
  356094/2000000: episode: 3552, duration: 1.197s, episode steps: 83, steps per second: 69, episode reward: -107.356, mean reward: -1.293 [-100.000, 7.646], mean action: 1.566 [0.000, 3.000], mean observation: 0.101 [-1.091, 3.881], loss: 2.100772, mean_absolute_error: 34.613363, mean_q: 19.260414, mean_eps: 0.679553
  356262/2000000: episode: 3553, duration: 2.357s, episode steps: 168, steps per second: 71, episode reward: -104.223, mean reward: -0.620 [-100.000, 18.342], mean action: 1.679 [0.000, 3.000], mean observation: 0.101 [-2.663, 1.000], loss: 1.770002, mean_absolute_error: 35.195800, mean_q: 19.916871, mean_eps: 0.679440
  356369/2000000: episode: 3554, duration: 1.535s, episode steps: 107, steps per second: 70, episode reward: -88.248, mean reward: -0.825 [-100.000, 13.260], mean action: 1.645 [0.000, 3.000], mean observation: 0.093 [-0.874, 1.000], loss: 2.029653, mean_absolute_error: 33.467900, mean_q: 18.873150, mean_eps: 0.679316
  356453/2000000: episode: 3555, duration: 1.189s, episode steps: 84, steps per second: 71, episode reward: -114.005, mean reward: -1.357 [-100.000, 15.537], mean action: 1.810 [0.000, 3.000], mean observation: -0.150 [-1.010, 1.000], loss: 2.532395, mean_absolute_error: 35.336735, mean_q: 19.043467, mean_eps: 0.679229
  356544/2000000: episode: 3556, duration: 1.304s, episode steps: 91, steps per second: 70, episode reward: -102.734, mean reward: -1.129 [-100.000, 11.352], mean action: 1.549 [0.000, 3.000], mean observation: 0.105 [-2.889, 1.000], loss: 2.011653, mean_absolute_error: 34.915932, mean_q: 18.021914, mean_eps: 0.679152
  356616/2000000: episode: 3557, duration: 1.080s, episode steps: 72, steps per second: 67, episode reward: -108.709, mean reward: -1.510 [-100.000, 7.736], mean action: 1.486 [0.000, 3.000], mean observation: 0.018 [-3.747, 1.000], loss: 3.154022, mean_absolute_error: 36.964833, mean_q: 11.410400, mean_eps: 0.679080
  356692/2000000: episode: 3558, duration: 1.132s, episode steps: 76, steps per second: 67, episode reward: -76.631, mean reward: -1.008 [-100.000, 11.375], mean action: 1.737 [0.000, 3.000], mean observation: -0.121 [-1.030, 1.000], loss: 1.908653, mean_absolute_error: 35.170035, mean_q: 18.573457, mean_eps: 0.679013
  356774/2000000: episode: 3559, duration: 1.195s, episode steps: 82, steps per second: 69, episode reward: -80.091, mean reward: -0.977 [-100.000, 13.187], mean action: 1.720 [0.000, 3.000], mean observation: 0.065 [-0.921, 3.389], loss: 1.341252, mean_absolute_error: 33.503960, mean_q: 17.892414, mean_eps: 0.678941
  356857/2000000: episode: 3560, duration: 1.199s, episode steps: 83, steps per second: 69, episode reward: -83.270, mean reward: -1.003 [-100.000, 9.826], mean action: 1.687 [0.000, 3.000], mean observation: 0.082 [-3.013, 1.000], loss: 1.750696, mean_absolute_error: 35.192469, mean_q: 17.489593, mean_eps: 0.678866
  357027/2000000: episode: 3561, duration: 2.381s, episode steps: 170, steps per second: 71, episode reward: -53.061, mean reward: -0.312 [-100.000, 15.214], mean action: 1.641 [0.000, 3.000], mean observation: -0.004 [-0.746, 1.022], loss: 1.737876, mean_absolute_error: 33.773446, mean_q: 20.998630, mean_eps: 0.678752
  357156/2000000: episode: 3562, duration: 1.883s, episode steps: 129, steps per second: 69, episode reward: -110.823, mean reward: -0.859 [-100.000, 5.895], mean action: 1.690 [0.000, 3.000], mean observation: 0.060 [-1.289, 4.073], loss: 1.705888, mean_absolute_error: 34.779373, mean_q: 18.787110, mean_eps: 0.678619
  357273/2000000: episode: 3563, duration: 1.706s, episode steps: 117, steps per second: 69, episode reward: -73.324, mean reward: -0.627 [-100.000, 22.680], mean action: 1.564 [0.000, 3.000], mean observation: 0.047 [-0.757, 1.000], loss: 1.848350, mean_absolute_error: 34.411159, mean_q: 17.844178, mean_eps: 0.678507
  357371/2000000: episode: 3564, duration: 1.388s, episode steps: 98, steps per second: 71, episode reward: -138.491, mean reward: -1.413 [-100.000, 6.544], mean action: 1.735 [0.000, 3.000], mean observation: 0.025 [-2.900, 1.000], loss: 1.318373, mean_absolute_error: 33.196609, mean_q: 22.688148, mean_eps: 0.678410
  357473/2000000: episode: 3565, duration: 1.473s, episode steps: 102, steps per second: 69, episode reward: -120.248, mean reward: -1.179 [-100.000, 12.021], mean action: 1.735 [0.000, 3.000], mean observation: 0.033 [-1.066, 1.000], loss: 2.064050, mean_absolute_error: 34.034428, mean_q: 18.236882, mean_eps: 0.678320
  357558/2000000: episode: 3566, duration: 1.221s, episode steps: 85, steps per second: 70, episode reward: -92.204, mean reward: -1.085 [-100.000, 21.146], mean action: 1.812 [0.000, 3.000], mean observation: -0.115 [-0.922, 1.000], loss: 1.163441, mean_absolute_error: 34.397674, mean_q: 15.038940, mean_eps: 0.678236
  357691/2000000: episode: 3567, duration: 1.903s, episode steps: 133, steps per second: 70, episode reward: -119.026, mean reward: -0.895 [-100.000, 7.374], mean action: 1.699 [0.000, 3.000], mean observation: 0.064 [-3.412, 1.037], loss: 2.404433, mean_absolute_error: 35.807836, mean_q: 18.116866, mean_eps: 0.678138
  357867/2000000: episode: 3568, duration: 2.486s, episode steps: 176, steps per second: 71, episode reward: -76.149, mean reward: -0.433 [-100.000, 15.702], mean action: 1.665 [0.000, 3.000], mean observation: -0.050 [-1.829, 1.000], loss: 1.725708, mean_absolute_error: 34.495769, mean_q: 18.797452, mean_eps: 0.678000
  357944/2000000: episode: 3569, duration: 1.142s, episode steps: 77, steps per second: 67, episode reward: -134.364, mean reward: -1.745 [-100.000, 5.673], mean action: 1.403 [0.000, 3.000], mean observation: 0.109 [-3.721, 1.000], loss: 1.321856, mean_absolute_error: 34.881204, mean_q: 21.778266, mean_eps: 0.677886
  358039/2000000: episode: 3570, duration: 1.396s, episode steps: 95, steps per second: 68, episode reward: -63.758, mean reward: -0.671 [-100.000, 13.254], mean action: 1.800 [0.000, 3.000], mean observation: -0.117 [-0.868, 2.921], loss: 1.716625, mean_absolute_error: 35.012002, mean_q: 20.195446, mean_eps: 0.677809
  358123/2000000: episode: 3571, duration: 1.199s, episode steps: 84, steps per second: 70, episode reward: -116.514, mean reward: -1.387 [-100.000, 9.471], mean action: 1.690 [0.000, 3.000], mean observation: -0.141 [-1.115, 3.887], loss: 1.513205, mean_absolute_error: 32.202922, mean_q: 19.948296, mean_eps: 0.677728
  358205/2000000: episode: 3572, duration: 1.188s, episode steps: 82, steps per second: 69, episode reward: -141.110, mean reward: -1.721 [-100.000, 11.022], mean action: 1.854 [0.000, 3.000], mean observation: -0.153 [-1.235, 1.000], loss: 1.478977, mean_absolute_error: 33.540454, mean_q: 21.342120, mean_eps: 0.677652
  358332/2000000: episode: 3573, duration: 1.828s, episode steps: 127, steps per second: 69, episode reward: -80.171, mean reward: -0.631 [-100.000, 9.952], mean action: 1.638 [0.000, 3.000], mean observation: 0.140 [-3.022, 1.000], loss: 1.595857, mean_absolute_error: 34.922251, mean_q: 18.136179, mean_eps: 0.677559
  358419/2000000: episode: 3574, duration: 1.273s, episode steps: 87, steps per second: 68, episode reward: -66.495, mean reward: -0.764 [-100.000, 14.220], mean action: 1.644 [0.000, 3.000], mean observation: 0.022 [-1.063, 1.000], loss: 1.588061, mean_absolute_error: 35.103113, mean_q: 21.373147, mean_eps: 0.677463
  358543/2000000: episode: 3575, duration: 1.780s, episode steps: 124, steps per second: 70, episode reward: -104.066, mean reward: -0.839 [-100.000, 13.297], mean action: 1.702 [0.000, 3.000], mean observation: 0.063 [-0.846, 3.042], loss: 1.883877, mean_absolute_error: 34.163967, mean_q: 18.958344, mean_eps: 0.677368
  358647/2000000: episode: 3576, duration: 1.531s, episode steps: 104, steps per second: 68, episode reward: -93.039, mean reward: -0.895 [-100.000, 24.785], mean action: 1.721 [0.000, 3.000], mean observation: -0.003 [-1.110, 1.000], loss: 1.266559, mean_absolute_error: 33.207141, mean_q: 20.876158, mean_eps: 0.677265
  358753/2000000: episode: 3577, duration: 1.544s, episode steps: 106, steps per second: 69, episode reward: -171.967, mean reward: -1.622 [-100.000, 8.802], mean action: 1.717 [0.000, 3.000], mean observation: 0.042 [-2.842, 1.000], loss: 2.194580, mean_absolute_error: 35.421357, mean_q: 16.185010, mean_eps: 0.677170
  358941/2000000: episode: 3578, duration: 2.670s, episode steps: 188, steps per second: 70, episode reward: -134.276, mean reward: -0.714 [-100.000, 10.483], mean action: 1.707 [0.000, 3.000], mean observation: 0.083 [-0.950, 1.000], loss: 1.317302, mean_absolute_error: 34.247817, mean_q: 19.982002, mean_eps: 0.677037
  359110/2000000: episode: 3579, duration: 2.386s, episode steps: 169, steps per second: 71, episode reward: -42.457, mean reward: -0.251 [-100.000, 17.210], mean action: 1.627 [0.000, 3.000], mean observation: 0.054 [-0.825, 1.065], loss: 1.736785, mean_absolute_error: 34.130125, mean_q: 20.971656, mean_eps: 0.676877
  359195/2000000: episode: 3580, duration: 1.233s, episode steps: 85, steps per second: 69, episode reward: -121.812, mean reward: -1.433 [-100.000, 12.026], mean action: 1.659 [0.000, 3.000], mean observation: 0.127 [-3.355, 1.000], loss: 2.347696, mean_absolute_error: 35.381809, mean_q: 17.017918, mean_eps: 0.676763
  359303/2000000: episode: 3581, duration: 1.531s, episode steps: 108, steps per second: 71, episode reward: -143.605, mean reward: -1.330 [-100.000, 5.928], mean action: 1.630 [0.000, 3.000], mean observation: 0.027 [-4.191, 1.000], loss: 1.848687, mean_absolute_error: 34.324345, mean_q: 20.109486, mean_eps: 0.676677
  359453/2000000: episode: 3582, duration: 2.150s, episode steps: 150, steps per second: 70, episode reward: -128.157, mean reward: -0.854 [-100.000, 7.214], mean action: 1.707 [0.000, 3.000], mean observation: 0.052 [-0.907, 1.000], loss: 1.122348, mean_absolute_error: 32.787596, mean_q: 20.988413, mean_eps: 0.676560
  359559/2000000: episode: 3583, duration: 1.493s, episode steps: 106, steps per second: 71, episode reward: -121.729, mean reward: -1.148 [-100.000, 7.321], mean action: 1.613 [0.000, 3.000], mean observation: 0.051 [-1.058, 3.693], loss: 1.862633, mean_absolute_error: 34.121626, mean_q: 19.292408, mean_eps: 0.676445
  359647/2000000: episode: 3584, duration: 1.252s, episode steps: 88, steps per second: 70, episode reward: -83.161, mean reward: -0.945 [-100.000, 15.560], mean action: 1.489 [0.000, 3.000], mean observation: 0.044 [-3.081, 1.000], loss: 2.612805, mean_absolute_error: 34.499848, mean_q: 20.019066, mean_eps: 0.676358
  359848/2000000: episode: 3585, duration: 2.915s, episode steps: 201, steps per second: 69, episode reward: -64.921, mean reward: -0.323 [-100.000, 13.746], mean action: 1.677 [0.000, 3.000], mean observation: 0.048 [-0.885, 1.000], loss: 1.854020, mean_absolute_error: 34.665167, mean_q: 18.111958, mean_eps: 0.676229
  360038/2000000: episode: 3586, duration: 2.851s, episode steps: 190, steps per second: 67, episode reward: -59.812, mean reward: -0.315 [-100.000, 23.153], mean action: 1.832 [0.000, 3.000], mean observation: -0.042 [-0.698, 1.767], loss: 1.519970, mean_absolute_error: 33.118974, mean_q: 20.846677, mean_eps: 0.676052
  360196/2000000: episode: 3587, duration: 2.290s, episode steps: 158, steps per second: 69, episode reward: -74.349, mean reward: -0.471 [-100.000, 17.077], mean action: 1.646 [0.000, 3.000], mean observation: 0.041 [-3.378, 1.046], loss: 1.484168, mean_absolute_error: 33.883722, mean_q: 21.581031, mean_eps: 0.675896
  360304/2000000: episode: 3588, duration: 1.579s, episode steps: 108, steps per second: 68, episode reward: -144.183, mean reward: -1.335 [-100.000, 6.186], mean action: 1.731 [0.000, 3.000], mean observation: 0.109 [-1.247, 3.338], loss: 1.686944, mean_absolute_error: 34.039540, mean_q: 19.600140, mean_eps: 0.675777
  360387/2000000: episode: 3589, duration: 1.199s, episode steps: 83, steps per second: 69, episode reward: -123.141, mean reward: -1.484 [-100.000, 7.837], mean action: 1.518 [0.000, 3.000], mean observation: 0.087 [-3.454, 1.000], loss: 2.506247, mean_absolute_error: 35.633718, mean_q: 16.425948, mean_eps: 0.675690
  360506/2000000: episode: 3590, duration: 1.701s, episode steps: 119, steps per second: 70, episode reward: -102.038, mean reward: -0.857 [-100.000, 8.227], mean action: 1.681 [0.000, 3.000], mean observation: 0.063 [-2.775, 1.000], loss: 1.998141, mean_absolute_error: 34.507544, mean_q: 20.161157, mean_eps: 0.675599
  360616/2000000: episode: 3591, duration: 1.580s, episode steps: 110, steps per second: 70, episode reward: -89.983, mean reward: -0.818 [-100.000, 18.262], mean action: 1.782 [0.000, 3.000], mean observation: -0.134 [-2.500, 1.000], loss: 3.145903, mean_absolute_error: 35.997063, mean_q: 18.406427, mean_eps: 0.675496
  360697/2000000: episode: 3592, duration: 1.193s, episode steps: 81, steps per second: 68, episode reward: -116.207, mean reward: -1.435 [-100.000, 9.511], mean action: 1.889 [0.000, 3.000], mean observation: -0.098 [-1.147, 1.853], loss: 1.845963, mean_absolute_error: 34.046295, mean_q: 21.323972, mean_eps: 0.675410
  360776/2000000: episode: 3593, duration: 1.134s, episode steps: 79, steps per second: 70, episode reward: -132.239, mean reward: -1.674 [-100.000, 6.234], mean action: 1.734 [0.000, 3.000], mean observation: 0.032 [-3.579, 1.000], loss: 2.529704, mean_absolute_error: 35.179554, mean_q: 21.926563, mean_eps: 0.675338
  360919/2000000: episode: 3594, duration: 2.047s, episode steps: 143, steps per second: 70, episode reward: -59.891, mean reward: -0.419 [-100.000, 20.089], mean action: 1.580 [0.000, 3.000], mean observation: 0.055 [-1.158, 1.110], loss: 1.433996, mean_absolute_error: 34.834250, mean_q: 19.672931, mean_eps: 0.675239
  361059/2000000: episode: 3595, duration: 1.988s, episode steps: 140, steps per second: 70, episode reward: -69.424, mean reward: -0.496 [-100.000, 13.803], mean action: 1.593 [0.000, 3.000], mean observation: 0.027 [-1.134, 1.075], loss: 1.655016, mean_absolute_error: 34.796975, mean_q: 20.490322, mean_eps: 0.675111
  361184/2000000: episode: 3596, duration: 1.824s, episode steps: 125, steps per second: 69, episode reward: -137.362, mean reward: -1.099 [-100.000, 7.958], mean action: 1.632 [0.000, 3.000], mean observation: 0.019 [-1.330, 4.890], loss: 1.192057, mean_absolute_error: 34.236080, mean_q: 21.259772, mean_eps: 0.674992
  361295/2000000: episode: 3597, duration: 1.629s, episode steps: 111, steps per second: 68, episode reward: -80.686, mean reward: -0.727 [-100.000, 23.956], mean action: 1.631 [0.000, 3.000], mean observation: -0.048 [-0.926, 1.000], loss: 1.672945, mean_absolute_error: 35.344225, mean_q: 21.201200, mean_eps: 0.674886
  361411/2000000: episode: 3598, duration: 1.650s, episode steps: 116, steps per second: 70, episode reward: -109.212, mean reward: -0.941 [-100.000, 7.922], mean action: 1.621 [0.000, 3.000], mean observation: -0.028 [-3.934, 1.000], loss: 1.362398, mean_absolute_error: 34.749812, mean_q: 21.189720, mean_eps: 0.674783
  361495/2000000: episode: 3599, duration: 1.323s, episode steps: 84, steps per second: 63, episode reward: -98.508, mean reward: -1.173 [-100.000, 23.187], mean action: 1.679 [0.000, 3.000], mean observation: -0.158 [-1.081, 3.646], loss: 1.774192, mean_absolute_error: 36.608749, mean_q: 16.204210, mean_eps: 0.674693
  361565/2000000: episode: 3600, duration: 1.186s, episode steps: 70, steps per second: 59, episode reward: -111.975, mean reward: -1.600 [-100.000, 5.757], mean action: 1.400 [0.000, 3.000], mean observation: 0.100 [-3.472, 1.000], loss: 1.168026, mean_absolute_error: 34.420599, mean_q: 18.254480, mean_eps: 0.674623
  361647/2000000: episode: 3601, duration: 1.156s, episode steps: 82, steps per second: 71, episode reward: -103.063, mean reward: -1.257 [-100.000, 7.067], mean action: 1.598 [0.000, 3.000], mean observation: 0.126 [-0.905, 1.000], loss: 1.721676, mean_absolute_error: 35.106680, mean_q: 20.305575, mean_eps: 0.674555
  362647/2000000: episode: 3602, duration: 15.583s, episode steps: 1000, steps per second: 64, episode reward: 24.593, mean reward: 0.025 [-23.475, 28.575], mean action: 1.277 [0.000, 3.000], mean observation: 0.041 [-1.421, 1.287], loss: 1.677921, mean_absolute_error: 34.968382, mean_q: 20.276625, mean_eps: 0.674069
  362744/2000000: episode: 3603, duration: 1.452s, episode steps: 97, steps per second: 67, episode reward: -82.332, mean reward: -0.849 [-100.000, 14.182], mean action: 1.680 [0.000, 3.000], mean observation: -0.033 [-0.952, 3.533], loss: 2.312117, mean_absolute_error: 36.076778, mean_q: 18.859847, mean_eps: 0.673575
  362868/2000000: episode: 3604, duration: 1.879s, episode steps: 124, steps per second: 66, episode reward: -80.538, mean reward: -0.649 [-100.000, 9.941], mean action: 1.629 [0.000, 3.000], mean observation: -0.079 [-0.868, 2.064], loss: 1.601150, mean_absolute_error: 34.926624, mean_q: 19.374648, mean_eps: 0.673476
  362950/2000000: episode: 3605, duration: 1.221s, episode steps: 82, steps per second: 67, episode reward: -103.911, mean reward: -1.267 [-100.000, 6.684], mean action: 1.500 [0.000, 3.000], mean observation: -0.037 [-3.700, 1.000], loss: 1.923453, mean_absolute_error: 34.990440, mean_q: 19.865415, mean_eps: 0.673383
  363045/2000000: episode: 3606, duration: 1.383s, episode steps: 95, steps per second: 69, episode reward: -110.655, mean reward: -1.165 [-100.000, 12.912], mean action: 1.505 [0.000, 3.000], mean observation: -0.041 [-1.222, 3.865], loss: 1.898340, mean_absolute_error: 35.940998, mean_q: 19.240845, mean_eps: 0.673302
  363186/2000000: episode: 3607, duration: 2.021s, episode steps: 141, steps per second: 70, episode reward: -127.547, mean reward: -0.905 [-100.000, 9.428], mean action: 1.688 [0.000, 3.000], mean observation: 0.145 [-4.340, 1.045], loss: 1.445991, mean_absolute_error: 34.083216, mean_q: 19.687092, mean_eps: 0.673196
  363304/2000000: episode: 3608, duration: 1.760s, episode steps: 118, steps per second: 67, episode reward: -88.835, mean reward: -0.753 [-100.000, 11.344], mean action: 1.627 [0.000, 3.000], mean observation: 0.089 [-2.440, 1.000], loss: 1.657075, mean_absolute_error: 35.040449, mean_q: 20.506735, mean_eps: 0.673080
  363370/2000000: episode: 3609, duration: 0.972s, episode steps: 66, steps per second: 68, episode reward: -88.979, mean reward: -1.348 [-100.000, 11.419], mean action: 1.727 [0.000, 3.000], mean observation: -0.148 [-3.750, 1.000], loss: 2.154313, mean_absolute_error: 35.704900, mean_q: 20.212103, mean_eps: 0.672998
  363473/2000000: episode: 3610, duration: 1.490s, episode steps: 103, steps per second: 69, episode reward: -96.371, mean reward: -0.936 [-100.000, 24.082], mean action: 1.680 [0.000, 3.000], mean observation: -0.059 [-0.925, 3.433], loss: 1.709993, mean_absolute_error: 34.590200, mean_q: 20.484370, mean_eps: 0.672920
  363570/2000000: episode: 3611, duration: 1.388s, episode steps: 97, steps per second: 70, episode reward: -106.537, mean reward: -1.098 [-100.000, 6.249], mean action: 1.577 [0.000, 3.000], mean observation: -0.053 [-3.680, 1.000], loss: 1.982972, mean_absolute_error: 34.244931, mean_q: 21.221536, mean_eps: 0.672830
  363715/2000000: episode: 3612, duration: 2.079s, episode steps: 145, steps per second: 70, episode reward: -120.660, mean reward: -0.832 [-100.000, 11.781], mean action: 1.738 [0.000, 3.000], mean observation: 0.026 [-1.423, 1.000], loss: 1.770536, mean_absolute_error: 35.900490, mean_q: 19.742994, mean_eps: 0.672722
  363817/2000000: episode: 3613, duration: 1.472s, episode steps: 102, steps per second: 69, episode reward: -98.472, mean reward: -0.965 [-100.000, 12.936], mean action: 1.637 [0.000, 3.000], mean observation: -0.118 [-1.081, 3.376], loss: 1.792576, mean_absolute_error: 35.749654, mean_q: 20.864851, mean_eps: 0.672611
  363962/2000000: episode: 3614, duration: 2.056s, episode steps: 145, steps per second: 71, episode reward: -104.567, mean reward: -0.721 [-100.000, 20.303], mean action: 1.628 [0.000, 3.000], mean observation: 0.062 [-1.819, 1.000], loss: 2.155002, mean_absolute_error: 35.688408, mean_q: 20.315016, mean_eps: 0.672499
  364069/2000000: episode: 3615, duration: 1.522s, episode steps: 107, steps per second: 70, episode reward: -114.259, mean reward: -1.068 [-100.000, 16.747], mean action: 1.617 [0.000, 3.000], mean observation: -0.097 [-0.992, 3.163], loss: 2.004723, mean_absolute_error: 34.409341, mean_q: 20.026249, mean_eps: 0.672386
  364249/2000000: episode: 3616, duration: 2.543s, episode steps: 180, steps per second: 71, episode reward: -82.444, mean reward: -0.458 [-100.000, 34.789], mean action: 1.694 [0.000, 3.000], mean observation: 0.188 [-1.285, 1.175], loss: 1.805700, mean_absolute_error: 35.810077, mean_q: 19.106050, mean_eps: 0.672256
  364331/2000000: episode: 3617, duration: 1.136s, episode steps: 82, steps per second: 72, episode reward: -129.635, mean reward: -1.581 [-100.000, 4.494], mean action: 1.561 [0.000, 3.000], mean observation: 0.104 [-2.581, 1.000], loss: 1.418319, mean_absolute_error: 34.810349, mean_q: 21.251747, mean_eps: 0.672139
  364406/2000000: episode: 3618, duration: 1.070s, episode steps: 75, steps per second: 70, episode reward: -127.706, mean reward: -1.703 [-100.000, 4.586], mean action: 1.720 [0.000, 3.000], mean observation: -0.114 [-4.624, 1.000], loss: 2.282100, mean_absolute_error: 34.737399, mean_q: 19.182270, mean_eps: 0.672069
  364491/2000000: episode: 3619, duration: 1.199s, episode steps: 85, steps per second: 71, episode reward: -129.183, mean reward: -1.520 [-100.000, 3.271], mean action: 1.706 [0.000, 3.000], mean observation: 0.088 [-0.996, 3.426], loss: 1.262924, mean_absolute_error: 35.073094, mean_q: 20.272758, mean_eps: 0.671997
  364565/2000000: episode: 3620, duration: 1.083s, episode steps: 74, steps per second: 68, episode reward: -112.497, mean reward: -1.520 [-100.000, 10.264], mean action: 1.514 [0.000, 3.000], mean observation: 0.066 [-1.205, 3.826], loss: 1.645624, mean_absolute_error: 34.420261, mean_q: 19.217159, mean_eps: 0.671925
  364666/2000000: episode: 3621, duration: 1.424s, episode steps: 101, steps per second: 71, episode reward: -93.495, mean reward: -0.926 [-100.000, 12.087], mean action: 1.554 [0.000, 3.000], mean observation: -0.086 [-1.528, 1.032], loss: 1.947779, mean_absolute_error: 35.142935, mean_q: 19.876293, mean_eps: 0.671846
  364771/2000000: episode: 3622, duration: 1.466s, episode steps: 105, steps per second: 72, episode reward: -79.730, mean reward: -0.759 [-100.000, 15.866], mean action: 1.562 [0.000, 3.000], mean observation: -0.121 [-2.616, 1.000], loss: 1.461380, mean_absolute_error: 35.125894, mean_q: 20.534518, mean_eps: 0.671754
  364886/2000000: episode: 3623, duration: 1.636s, episode steps: 115, steps per second: 70, episode reward: -112.898, mean reward: -0.982 [-100.000, 7.954], mean action: 1.548 [0.000, 3.000], mean observation: -0.060 [-1.082, 4.054], loss: 2.097698, mean_absolute_error: 34.872523, mean_q: 19.193287, mean_eps: 0.671655
  364961/2000000: episode: 3624, duration: 1.065s, episode steps: 75, steps per second: 70, episode reward: -130.745, mean reward: -1.743 [-100.000, 7.968], mean action: 1.533 [0.000, 3.000], mean observation: 0.021 [-1.174, 3.489], loss: 1.285102, mean_absolute_error: 32.784606, mean_q: 21.380893, mean_eps: 0.671568
  365111/2000000: episode: 3625, duration: 2.141s, episode steps: 150, steps per second: 70, episode reward: -137.581, mean reward: -0.917 [-100.000, 9.993], mean action: 1.707 [0.000, 3.000], mean observation: -0.098 [-4.075, 1.097], loss: 1.605651, mean_absolute_error: 34.612000, mean_q: 18.713716, mean_eps: 0.671468
  365185/2000000: episode: 3626, duration: 1.098s, episode steps: 74, steps per second: 67, episode reward: -131.045, mean reward: -1.771 [-100.000, 6.005], mean action: 1.595 [0.000, 3.000], mean observation: 0.023 [-3.411, 1.000], loss: 2.054890, mean_absolute_error: 36.803962, mean_q: 17.931260, mean_eps: 0.671367
  365300/2000000: episode: 3627, duration: 1.636s, episode steps: 115, steps per second: 70, episode reward: -74.577, mean reward: -0.648 [-100.000, 19.393], mean action: 1.643 [0.000, 3.000], mean observation: -0.066 [-0.900, 1.000], loss: 1.159124, mean_absolute_error: 35.646927, mean_q: 20.214860, mean_eps: 0.671282
  365381/2000000: episode: 3628, duration: 1.186s, episode steps: 81, steps per second: 68, episode reward: -98.149, mean reward: -1.212 [-100.000, 12.701], mean action: 1.741 [0.000, 3.000], mean observation: -0.051 [-1.037, 1.000], loss: 1.718509, mean_absolute_error: 35.496882, mean_q: 19.600750, mean_eps: 0.671194
  365513/2000000: episode: 3629, duration: 1.882s, episode steps: 132, steps per second: 70, episode reward: -139.149, mean reward: -1.054 [-100.000, 5.979], mean action: 1.606 [0.000, 3.000], mean observation: -0.070 [-3.608, 1.000], loss: 1.425419, mean_absolute_error: 33.752039, mean_q: 20.130707, mean_eps: 0.671097
  365677/2000000: episode: 3630, duration: 2.680s, episode steps: 164, steps per second: 61, episode reward: -133.155, mean reward: -0.812 [-100.000, 10.844], mean action: 1.689 [0.000, 3.000], mean observation: 0.056 [-1.865, 1.001], loss: 1.720749, mean_absolute_error: 35.053006, mean_q: 19.132520, mean_eps: 0.670964
  365804/2000000: episode: 3631, duration: 1.816s, episode steps: 127, steps per second: 70, episode reward: -103.385, mean reward: -0.814 [-100.000, 14.522], mean action: 1.575 [0.000, 3.000], mean observation: -0.140 [-1.003, 1.276], loss: 1.550210, mean_absolute_error: 35.210748, mean_q: 21.440581, mean_eps: 0.670834
  365904/2000000: episode: 3632, duration: 1.483s, episode steps: 100, steps per second: 67, episode reward: -112.118, mean reward: -1.121 [-100.000, 12.346], mean action: 1.640 [0.000, 3.000], mean observation: -0.091 [-0.958, 3.114], loss: 2.360983, mean_absolute_error: 34.931051, mean_q: 20.421174, mean_eps: 0.670733
  365985/2000000: episode: 3633, duration: 1.224s, episode steps: 81, steps per second: 66, episode reward: -122.126, mean reward: -1.508 [-100.000, 6.149], mean action: 1.691 [0.000, 3.000], mean observation: 0.064 [-3.939, 1.000], loss: 1.400747, mean_absolute_error: 33.986169, mean_q: 20.934094, mean_eps: 0.670650
  366066/2000000: episode: 3634, duration: 1.218s, episode steps: 81, steps per second: 66, episode reward: -97.419, mean reward: -1.203 [-100.000, 7.068], mean action: 1.580 [0.000, 3.000], mean observation: 0.099 [-0.975, 3.535], loss: 1.764381, mean_absolute_error: 37.395680, mean_q: 16.228266, mean_eps: 0.670577
  366159/2000000: episode: 3635, duration: 1.307s, episode steps: 93, steps per second: 71, episode reward: -96.103, mean reward: -1.033 [-100.000, 10.743], mean action: 1.753 [0.000, 3.000], mean observation: -0.099 [-0.960, 1.930], loss: 1.259006, mean_absolute_error: 34.625297, mean_q: 19.820706, mean_eps: 0.670499
  366250/2000000: episode: 3636, duration: 1.325s, episode steps: 91, steps per second: 69, episode reward: -73.329, mean reward: -0.806 [-100.000, 19.889], mean action: 1.615 [0.000, 3.000], mean observation: -0.022 [-1.041, 1.000], loss: 2.551063, mean_absolute_error: 34.959400, mean_q: 20.164105, mean_eps: 0.670416
  366340/2000000: episode: 3637, duration: 1.295s, episode steps: 90, steps per second: 70, episode reward: -86.311, mean reward: -0.959 [-100.000, 13.122], mean action: 1.778 [0.000, 3.000], mean observation: 0.100 [-0.905, 1.000], loss: 1.916714, mean_absolute_error: 35.583599, mean_q: 18.697371, mean_eps: 0.670335
  366431/2000000: episode: 3638, duration: 1.286s, episode steps: 91, steps per second: 71, episode reward: -105.436, mean reward: -1.159 [-100.000, 15.829], mean action: 1.593 [0.000, 3.000], mean observation: 0.102 [-3.064, 1.000], loss: 1.409019, mean_absolute_error: 35.464785, mean_q: 21.818648, mean_eps: 0.670254
  366576/2000000: episode: 3639, duration: 2.116s, episode steps: 145, steps per second: 69, episode reward: -90.115, mean reward: -0.621 [-100.000, 61.792], mean action: 1.634 [0.000, 3.000], mean observation: -0.107 [-1.053, 1.521], loss: 1.707891, mean_absolute_error: 34.843992, mean_q: 20.843518, mean_eps: 0.670148
  366700/2000000: episode: 3640, duration: 1.801s, episode steps: 124, steps per second: 69, episode reward: -80.600, mean reward: -0.650 [-100.000, 21.296], mean action: 1.637 [0.000, 3.000], mean observation: 0.023 [-1.240, 1.027], loss: 1.669650, mean_absolute_error: 33.781662, mean_q: 24.783092, mean_eps: 0.670028
  366813/2000000: episode: 3641, duration: 1.621s, episode steps: 113, steps per second: 70, episode reward: -113.493, mean reward: -1.004 [-100.000, 20.930], mean action: 1.602 [0.000, 3.000], mean observation: -0.126 [-1.776, 1.000], loss: 1.633733, mean_absolute_error: 35.147900, mean_q: 16.938513, mean_eps: 0.669920
  366919/2000000: episode: 3642, duration: 1.496s, episode steps: 106, steps per second: 71, episode reward: -110.351, mean reward: -1.041 [-100.000, 15.851], mean action: 1.651 [0.000, 3.000], mean observation: -0.076 [-1.040, 1.000], loss: 1.225364, mean_absolute_error: 33.810966, mean_q: 21.222825, mean_eps: 0.669821
  367058/2000000: episode: 3643, duration: 1.965s, episode steps: 139, steps per second: 71, episode reward: -156.690, mean reward: -1.127 [-100.000, 7.239], mean action: 1.640 [0.000, 3.000], mean observation: 0.109 [-0.922, 3.058], loss: 2.345466, mean_absolute_error: 35.661370, mean_q: 20.359557, mean_eps: 0.669711
  367166/2000000: episode: 3644, duration: 1.517s, episode steps: 108, steps per second: 71, episode reward: -57.005, mean reward: -0.528 [-100.000, 16.179], mean action: 1.676 [0.000, 3.000], mean observation: -0.058 [-0.823, 1.000], loss: 1.836425, mean_absolute_error: 36.037826, mean_q: 17.096443, mean_eps: 0.669599
  367290/2000000: episode: 3645, duration: 1.768s, episode steps: 124, steps per second: 70, episode reward: -135.650, mean reward: -1.094 [-100.000, 12.556], mean action: 1.621 [0.000, 3.000], mean observation: 0.014 [-1.138, 3.789], loss: 1.891397, mean_absolute_error: 34.746915, mean_q: 22.802050, mean_eps: 0.669495
  367412/2000000: episode: 3646, duration: 1.751s, episode steps: 122, steps per second: 70, episode reward: -110.465, mean reward: -0.905 [-100.000, 8.449], mean action: 1.492 [0.000, 3.000], mean observation: -0.096 [-3.767, 1.000], loss: 2.065296, mean_absolute_error: 35.546495, mean_q: 19.461452, mean_eps: 0.669385
  367539/2000000: episode: 3647, duration: 1.814s, episode steps: 127, steps per second: 70, episode reward: -67.876, mean reward: -0.534 [-100.000, 19.150], mean action: 1.528 [0.000, 3.000], mean observation: -0.033 [-2.200, 1.000], loss: 1.608871, mean_absolute_error: 34.876233, mean_q: 21.433068, mean_eps: 0.669273
  367692/2000000: episode: 3648, duration: 2.204s, episode steps: 153, steps per second: 69, episode reward: -57.805, mean reward: -0.378 [-100.000, 14.798], mean action: 1.542 [0.000, 3.000], mean observation: -0.023 [-0.717, 1.000], loss: 1.790339, mean_absolute_error: 35.517676, mean_q: 20.065945, mean_eps: 0.669147
  367812/2000000: episode: 3649, duration: 1.757s, episode steps: 120, steps per second: 68, episode reward: -157.043, mean reward: -1.309 [-100.000, 9.239], mean action: 1.658 [0.000, 3.000], mean observation: -0.035 [-1.383, 4.289], loss: 1.839530, mean_absolute_error: 35.809149, mean_q: 20.716637, mean_eps: 0.669025
  367925/2000000: episode: 3650, duration: 1.646s, episode steps: 113, steps per second: 69, episode reward: -136.065, mean reward: -1.204 [-100.000, 8.202], mean action: 1.699 [0.000, 3.000], mean observation: 0.008 [-1.042, 3.413], loss: 1.566051, mean_absolute_error: 34.321658, mean_q: 21.192076, mean_eps: 0.668919
  368045/2000000: episode: 3651, duration: 1.719s, episode steps: 120, steps per second: 70, episode reward: -169.904, mean reward: -1.416 [-100.000, 7.535], mean action: 1.450 [0.000, 3.000], mean observation: 0.186 [-3.652, 1.017], loss: 2.704270, mean_absolute_error: 35.117810, mean_q: 19.239935, mean_eps: 0.668813
  368128/2000000: episode: 3652, duration: 1.181s, episode steps: 83, steps per second: 70, episode reward: -93.768, mean reward: -1.130 [-100.000, 7.150], mean action: 1.711 [0.000, 3.000], mean observation: -0.133 [-3.011, 1.000], loss: 1.837773, mean_absolute_error: 34.367072, mean_q: 21.304007, mean_eps: 0.668723
  368200/2000000: episode: 3653, duration: 1.165s, episode steps: 72, steps per second: 62, episode reward: -120.127, mean reward: -1.668 [-100.000, 7.138], mean action: 1.486 [0.000, 3.000], mean observation: 0.077 [-1.190, 4.147], loss: 1.906121, mean_absolute_error: 34.596669, mean_q: 19.916327, mean_eps: 0.668654
  368356/2000000: episode: 3654, duration: 2.350s, episode steps: 156, steps per second: 66, episode reward: -84.515, mean reward: -0.542 [-100.000, 10.619], mean action: 1.647 [0.000, 3.000], mean observation: 0.037 [-2.721, 1.000], loss: 2.491256, mean_absolute_error: 35.854315, mean_q: 20.267975, mean_eps: 0.668552
  368493/2000000: episode: 3655, duration: 2.025s, episode steps: 137, steps per second: 68, episode reward: -85.482, mean reward: -0.624 [-100.000, 7.269], mean action: 1.693 [0.000, 3.000], mean observation: 0.032 [-0.811, 3.253], loss: 2.040149, mean_absolute_error: 34.436384, mean_q: 22.289027, mean_eps: 0.668418
  368613/2000000: episode: 3656, duration: 1.704s, episode steps: 120, steps per second: 70, episode reward: -140.801, mean reward: -1.173 [-100.000, 13.856], mean action: 1.758 [0.000, 3.000], mean observation: 0.099 [-0.876, 2.458], loss: 1.433494, mean_absolute_error: 34.282315, mean_q: 20.507047, mean_eps: 0.668301
  368762/2000000: episode: 3657, duration: 2.128s, episode steps: 149, steps per second: 70, episode reward: -120.043, mean reward: -0.806 [-100.000, 11.604], mean action: 1.658 [0.000, 3.000], mean observation: 0.069 [-2.936, 1.000], loss: 2.144860, mean_absolute_error: 35.892207, mean_q: 19.413458, mean_eps: 0.668181
  368895/2000000: episode: 3658, duration: 1.883s, episode steps: 133, steps per second: 71, episode reward: -95.296, mean reward: -0.717 [-100.000, 12.455], mean action: 1.759 [0.000, 3.000], mean observation: 0.124 [-0.867, 2.810], loss: 2.175845, mean_absolute_error: 34.592177, mean_q: 21.438295, mean_eps: 0.668055
  369018/2000000: episode: 3659, duration: 1.769s, episode steps: 123, steps per second: 70, episode reward: -101.586, mean reward: -0.826 [-100.000, 16.066], mean action: 1.626 [0.000, 3.000], mean observation: -0.092 [-1.014, 3.223], loss: 1.445191, mean_absolute_error: 34.359550, mean_q: 18.943498, mean_eps: 0.667940
  369156/2000000: episode: 3660, duration: 1.978s, episode steps: 138, steps per second: 70, episode reward: -108.895, mean reward: -0.789 [-100.000, 10.490], mean action: 1.667 [0.000, 3.000], mean observation: 0.119 [-0.737, 2.420], loss: 2.402291, mean_absolute_error: 35.582857, mean_q: 20.245700, mean_eps: 0.667823
  369309/2000000: episode: 3661, duration: 2.223s, episode steps: 153, steps per second: 69, episode reward: -130.779, mean reward: -0.855 [-100.000, 13.487], mean action: 1.654 [0.000, 3.000], mean observation: 0.158 [-0.783, 2.838], loss: 1.404388, mean_absolute_error: 34.884974, mean_q: 20.161929, mean_eps: 0.667691
  369404/2000000: episode: 3662, duration: 1.364s, episode steps: 95, steps per second: 70, episode reward: -83.631, mean reward: -0.880 [-100.000, 26.787], mean action: 1.695 [0.000, 3.000], mean observation: -0.114 [-0.936, 1.000], loss: 1.418975, mean_absolute_error: 34.952584, mean_q: 17.010386, mean_eps: 0.667580
  369525/2000000: episode: 3663, duration: 1.776s, episode steps: 121, steps per second: 68, episode reward: -109.522, mean reward: -0.905 [-100.000, 9.501], mean action: 1.645 [0.000, 3.000], mean observation: 0.053 [-2.173, 1.000], loss: 1.852744, mean_absolute_error: 35.252547, mean_q: 20.544270, mean_eps: 0.667482
  369609/2000000: episode: 3664, duration: 1.285s, episode steps: 84, steps per second: 65, episode reward: -89.074, mean reward: -1.060 [-100.000, 9.388], mean action: 1.893 [0.000, 3.000], mean observation: -0.049 [-1.065, 3.433], loss: 1.626031, mean_absolute_error: 34.040414, mean_q: 20.853578, mean_eps: 0.667389
  369717/2000000: episode: 3665, duration: 1.651s, episode steps: 108, steps per second: 65, episode reward: -78.124, mean reward: -0.723 [-100.000, 19.075], mean action: 1.556 [0.000, 3.000], mean observation: -0.019 [-0.971, 3.180], loss: 2.317962, mean_absolute_error: 35.183650, mean_q: 18.881964, mean_eps: 0.667302
  369833/2000000: episode: 3666, duration: 1.658s, episode steps: 116, steps per second: 70, episode reward: -73.238, mean reward: -0.631 [-100.000, 19.028], mean action: 1.621 [0.000, 3.000], mean observation: 0.108 [-1.081, 1.000], loss: 1.868415, mean_absolute_error: 35.279217, mean_q: 19.635826, mean_eps: 0.667202
  369997/2000000: episode: 3667, duration: 2.375s, episode steps: 164, steps per second: 69, episode reward: -68.823, mean reward: -0.420 [-100.000, 16.405], mean action: 1.677 [0.000, 3.000], mean observation: 0.021 [-2.646, 1.010], loss: 1.426896, mean_absolute_error: 33.274296, mean_q: 20.430056, mean_eps: 0.667076
  370107/2000000: episode: 3668, duration: 1.573s, episode steps: 110, steps per second: 70, episode reward: -99.163, mean reward: -0.901 [-100.000, 5.386], mean action: 1.618 [0.000, 3.000], mean observation: -0.129 [-2.288, 1.197], loss: 2.554403, mean_absolute_error: 37.276486, mean_q: 17.942265, mean_eps: 0.666953
  370202/2000000: episode: 3669, duration: 1.370s, episode steps: 95, steps per second: 69, episode reward: -59.580, mean reward: -0.627 [-100.000, 12.253], mean action: 1.821 [0.000, 3.000], mean observation: -0.114 [-0.848, 1.000], loss: 2.271539, mean_absolute_error: 34.962571, mean_q: 20.739861, mean_eps: 0.666861
  370283/2000000: episode: 3670, duration: 1.153s, episode steps: 81, steps per second: 70, episode reward: -96.666, mean reward: -1.193 [-100.000, 7.307], mean action: 1.741 [0.000, 3.000], mean observation: -0.133 [-1.082, 3.234], loss: 1.770604, mean_absolute_error: 37.704814, mean_q: 16.364876, mean_eps: 0.666782
  370386/2000000: episode: 3671, duration: 1.512s, episode steps: 103, steps per second: 68, episode reward: -104.655, mean reward: -1.016 [-100.000, 11.201], mean action: 1.670 [0.000, 3.000], mean observation: 0.088 [-3.223, 1.000], loss: 1.219513, mean_absolute_error: 35.476122, mean_q: 22.084470, mean_eps: 0.666699
  370525/2000000: episode: 3672, duration: 1.998s, episode steps: 139, steps per second: 70, episode reward: -128.970, mean reward: -0.928 [-100.000, 15.904], mean action: 1.734 [0.000, 3.000], mean observation: -0.098 [-0.932, 3.086], loss: 1.955541, mean_absolute_error: 35.490400, mean_q: 18.958161, mean_eps: 0.666590
  370691/2000000: episode: 3673, duration: 2.380s, episode steps: 166, steps per second: 70, episode reward: -76.255, mean reward: -0.459 [-100.000, 16.611], mean action: 1.578 [0.000, 3.000], mean observation: 0.053 [-1.031, 1.034], loss: 1.814769, mean_absolute_error: 34.335321, mean_q: 20.119542, mean_eps: 0.666453
  370799/2000000: episode: 3674, duration: 1.560s, episode steps: 108, steps per second: 69, episode reward: -119.597, mean reward: -1.107 [-100.000, 9.656], mean action: 1.639 [0.000, 3.000], mean observation: 0.004 [-3.019, 1.000], loss: 2.032500, mean_absolute_error: 36.783847, mean_q: 17.803740, mean_eps: 0.666330
  370885/2000000: episode: 3675, duration: 1.260s, episode steps: 86, steps per second: 68, episode reward: -108.659, mean reward: -1.263 [-100.000, 18.421], mean action: 1.674 [0.000, 3.000], mean observation: -0.146 [-1.073, 1.000], loss: 2.425905, mean_absolute_error: 36.677912, mean_q: 21.319836, mean_eps: 0.666242
  370981/2000000: episode: 3676, duration: 1.384s, episode steps: 96, steps per second: 69, episode reward: -117.235, mean reward: -1.221 [-100.000, 7.531], mean action: 1.771 [0.000, 3.000], mean observation: -0.110 [-0.889, 2.848], loss: 1.618708, mean_absolute_error: 35.712448, mean_q: 23.292411, mean_eps: 0.666159
  371118/2000000: episode: 3677, duration: 1.950s, episode steps: 137, steps per second: 70, episode reward: -72.565, mean reward: -0.530 [-100.000, 19.161], mean action: 1.664 [0.000, 3.000], mean observation: 0.059 [-1.601, 1.000], loss: 2.396671, mean_absolute_error: 36.745672, mean_q: 20.179159, mean_eps: 0.666055
  371187/2000000: episode: 3678, duration: 0.982s, episode steps: 69, steps per second: 70, episode reward: -72.150, mean reward: -1.046 [-100.000, 13.167], mean action: 1.841 [0.000, 3.000], mean observation: -0.089 [-1.142, 1.000], loss: 2.481140, mean_absolute_error: 34.964188, mean_q: 19.070479, mean_eps: 0.665963
  371290/2000000: episode: 3679, duration: 1.487s, episode steps: 103, steps per second: 69, episode reward: -108.759, mean reward: -1.056 [-100.000, 7.433], mean action: 1.621 [0.000, 3.000], mean observation: 0.104 [-1.000, 3.648], loss: 2.166052, mean_absolute_error: 35.207120, mean_q: 20.013373, mean_eps: 0.665886
  371365/2000000: episode: 3680, duration: 1.100s, episode steps: 75, steps per second: 68, episode reward: -82.779, mean reward: -1.104 [-100.000, 13.358], mean action: 1.587 [0.000, 3.000], mean observation: -0.151 [-1.068, 1.000], loss: 2.475602, mean_absolute_error: 37.636935, mean_q: 19.659021, mean_eps: 0.665805
  371463/2000000: episode: 3681, duration: 1.396s, episode steps: 98, steps per second: 70, episode reward: -126.720, mean reward: -1.293 [-100.000, 7.189], mean action: 1.633 [0.000, 3.000], mean observation: 0.079 [-3.472, 1.000], loss: 3.105860, mean_absolute_error: 36.081615, mean_q: 17.384066, mean_eps: 0.665727
  371602/2000000: episode: 3682, duration: 2.006s, episode steps: 139, steps per second: 69, episode reward: -88.495, mean reward: -0.637 [-100.000, 78.163], mean action: 1.640 [0.000, 3.000], mean observation: 0.165 [-0.978, 1.700], loss: 2.165521, mean_absolute_error: 36.620149, mean_q: 18.552760, mean_eps: 0.665621
  371719/2000000: episode: 3683, duration: 1.672s, episode steps: 117, steps per second: 70, episode reward: -125.714, mean reward: -1.074 [-100.000, 10.204], mean action: 1.624 [0.000, 3.000], mean observation: 0.077 [-0.984, 3.454], loss: 1.691694, mean_absolute_error: 36.060255, mean_q: 17.955624, mean_eps: 0.665506
  371839/2000000: episode: 3684, duration: 1.719s, episode steps: 120, steps per second: 70, episode reward: -102.910, mean reward: -0.858 [-100.000, 7.333], mean action: 1.725 [0.000, 3.000], mean observation: 0.032 [-2.398, 1.000], loss: 2.761178, mean_absolute_error: 35.946361, mean_q: 17.677660, mean_eps: 0.665400
  371940/2000000: episode: 3685, duration: 1.471s, episode steps: 101, steps per second: 69, episode reward: -109.726, mean reward: -1.086 [-100.000, 10.833], mean action: 1.574 [0.000, 3.000], mean observation: -0.085 [-1.166, 4.373], loss: 1.644036, mean_absolute_error: 35.601209, mean_q: 17.222911, mean_eps: 0.665301
  372038/2000000: episode: 3686, duration: 1.455s, episode steps: 98, steps per second: 67, episode reward: -104.821, mean reward: -1.070 [-100.000, 7.362], mean action: 1.673 [0.000, 3.000], mean observation: -0.066 [-3.363, 1.000], loss: 1.297274, mean_absolute_error: 34.682988, mean_q: 21.669015, mean_eps: 0.665211
  372097/2000000: episode: 3687, duration: 0.857s, episode steps: 59, steps per second: 69, episode reward: -165.151, mean reward: -2.799 [-100.000, 16.311], mean action: 1.593 [0.000, 3.000], mean observation: -0.153 [-4.300, 1.000], loss: 1.485431, mean_absolute_error: 34.873487, mean_q: 21.410874, mean_eps: 0.665139
  372188/2000000: episode: 3688, duration: 1.321s, episode steps: 91, steps per second: 69, episode reward: -97.135, mean reward: -1.067 [-100.000, 14.306], mean action: 1.571 [0.000, 3.000], mean observation: -0.121 [-0.944, 2.916], loss: 2.456532, mean_absolute_error: 37.340429, mean_q: 16.711242, mean_eps: 0.665072
  372297/2000000: episode: 3689, duration: 1.609s, episode steps: 109, steps per second: 68, episode reward: -92.742, mean reward: -0.851 [-100.000, 12.736], mean action: 1.651 [0.000, 3.000], mean observation: 0.017 [-3.195, 1.000], loss: 1.990417, mean_absolute_error: 36.557593, mean_q: 20.482831, mean_eps: 0.664982
  372451/2000000: episode: 3690, duration: 2.322s, episode steps: 154, steps per second: 66, episode reward: -91.799, mean reward: -0.596 [-100.000, 7.663], mean action: 1.656 [0.000, 3.000], mean observation: -0.034 [-0.750, 1.000], loss: 2.414093, mean_absolute_error: 35.239086, mean_q: 19.726175, mean_eps: 0.664863
  372629/2000000: episode: 3691, duration: 2.549s, episode steps: 178, steps per second: 70, episode reward: -103.442, mean reward: -0.581 [-100.000, 10.523], mean action: 1.736 [0.000, 3.000], mean observation: 0.159 [-1.340, 1.049], loss: 1.906467, mean_absolute_error: 35.056621, mean_q: 20.702397, mean_eps: 0.664714
  372724/2000000: episode: 3692, duration: 1.361s, episode steps: 95, steps per second: 70, episode reward: -102.313, mean reward: -1.077 [-100.000, 6.476], mean action: 1.758 [0.000, 3.000], mean observation: 0.050 [-1.003, 1.000], loss: 2.281472, mean_absolute_error: 34.843898, mean_q: 19.827798, mean_eps: 0.664592
  372860/2000000: episode: 3693, duration: 1.968s, episode steps: 136, steps per second: 69, episode reward: -71.601, mean reward: -0.526 [-100.000, 35.812], mean action: 1.728 [0.000, 3.000], mean observation: 0.120 [-0.786, 2.455], loss: 2.154546, mean_absolute_error: 35.583536, mean_q: 19.216861, mean_eps: 0.664489
  372992/2000000: episode: 3694, duration: 1.925s, episode steps: 132, steps per second: 69, episode reward: -68.045, mean reward: -0.515 [-100.000, 13.001], mean action: 1.689 [0.000, 3.000], mean observation: -0.045 [-0.907, 1.000], loss: 1.631945, mean_absolute_error: 34.869654, mean_q: 18.109166, mean_eps: 0.664368
  373121/2000000: episode: 3695, duration: 1.851s, episode steps: 129, steps per second: 70, episode reward: -66.014, mean reward: -0.512 [-100.000, 19.281], mean action: 1.612 [0.000, 3.000], mean observation: 0.032 [-0.903, 1.000], loss: 1.833479, mean_absolute_error: 35.416450, mean_q: 19.280037, mean_eps: 0.664250
  373198/2000000: episode: 3696, duration: 1.081s, episode steps: 77, steps per second: 71, episode reward: -138.441, mean reward: -1.798 [-100.000, 6.526], mean action: 1.649 [0.000, 3.000], mean observation: -0.135 [-4.007, 1.000], loss: 1.677498, mean_absolute_error: 34.760713, mean_q: 21.071923, mean_eps: 0.664156
  373324/2000000: episode: 3697, duration: 1.794s, episode steps: 126, steps per second: 70, episode reward: -84.622, mean reward: -0.672 [-100.000, 10.064], mean action: 1.738 [0.000, 3.000], mean observation: -0.046 [-3.009, 1.007], loss: 1.890931, mean_absolute_error: 35.078657, mean_q: 18.427078, mean_eps: 0.664066
  373432/2000000: episode: 3698, duration: 1.568s, episode steps: 108, steps per second: 69, episode reward: -106.643, mean reward: -0.987 [-100.000, 17.699], mean action: 1.676 [0.000, 3.000], mean observation: 0.071 [-1.022, 3.509], loss: 1.506481, mean_absolute_error: 36.519123, mean_q: 22.196932, mean_eps: 0.663962
  373519/2000000: episode: 3699, duration: 1.284s, episode steps: 87, steps per second: 68, episode reward: -45.248, mean reward: -0.520 [-100.000, 11.635], mean action: 1.644 [0.000, 3.000], mean observation: -0.120 [-2.311, 1.000], loss: 2.265935, mean_absolute_error: 35.723580, mean_q: 20.697674, mean_eps: 0.663873
  373633/2000000: episode: 3700, duration: 1.658s, episode steps: 114, steps per second: 69, episode reward: -112.146, mean reward: -0.984 [-100.000, 6.634], mean action: 1.649 [0.000, 3.000], mean observation: 0.049 [-0.893, 3.279], loss: 1.838472, mean_absolute_error: 35.224936, mean_q: 21.291641, mean_eps: 0.663782
  373699/2000000: episode: 3701, duration: 0.917s, episode steps: 66, steps per second: 72, episode reward: -91.717, mean reward: -1.390 [-100.000, 16.741], mean action: 1.530 [0.000, 3.000], mean observation: 0.078 [-3.756, 1.000], loss: 1.303504, mean_absolute_error: 34.395018, mean_q: 19.930031, mean_eps: 0.663701
  373785/2000000: episode: 3702, duration: 1.262s, episode steps: 86, steps per second: 68, episode reward: -53.941, mean reward: -0.627 [-100.000, 14.111], mean action: 1.779 [0.000, 3.000], mean observation: -0.147 [-0.975, 2.307], loss: 1.464884, mean_absolute_error: 35.974846, mean_q: 18.318838, mean_eps: 0.663632
  373865/2000000: episode: 3703, duration: 1.149s, episode steps: 80, steps per second: 70, episode reward: -149.241, mean reward: -1.866 [-100.000, 5.898], mean action: 1.425 [0.000, 3.000], mean observation: 0.072 [-4.270, 1.000], loss: 2.155288, mean_absolute_error: 34.339367, mean_q: 19.497611, mean_eps: 0.663557
  373947/2000000: episode: 3704, duration: 1.138s, episode steps: 82, steps per second: 72, episode reward: -126.309, mean reward: -1.540 [-100.000, 10.805], mean action: 1.598 [0.000, 3.000], mean observation: 0.105 [-4.611, 1.000], loss: 2.429916, mean_absolute_error: 36.318285, mean_q: 21.093657, mean_eps: 0.663485
  374011/2000000: episode: 3705, duration: 0.922s, episode steps: 64, steps per second: 69, episode reward: -158.356, mean reward: -2.474 [-100.000, 8.280], mean action: 1.609 [0.000, 3.000], mean observation: -0.172 [-4.554, 1.000], loss: 2.274636, mean_absolute_error: 36.317495, mean_q: 21.038881, mean_eps: 0.663420
  374089/2000000: episode: 3706, duration: 1.154s, episode steps: 78, steps per second: 68, episode reward: -44.008, mean reward: -0.564 [-100.000, 13.162], mean action: 1.654 [0.000, 3.000], mean observation: -0.069 [-1.003, 1.000], loss: 2.416093, mean_absolute_error: 35.578516, mean_q: 19.386664, mean_eps: 0.663355
  374170/2000000: episode: 3707, duration: 1.153s, episode steps: 81, steps per second: 70, episode reward: -93.772, mean reward: -1.158 [-100.000, 19.003], mean action: 1.642 [0.000, 3.000], mean observation: -0.027 [-1.194, 3.738], loss: 1.803108, mean_absolute_error: 36.090024, mean_q: 21.683747, mean_eps: 0.663283
  374296/2000000: episode: 3708, duration: 1.834s, episode steps: 126, steps per second: 69, episode reward: -114.135, mean reward: -0.906 [-100.000, 11.452], mean action: 1.635 [0.000, 3.000], mean observation: 0.089 [-1.039, 3.080], loss: 1.570154, mean_absolute_error: 34.574496, mean_q: 20.164612, mean_eps: 0.663191
  374409/2000000: episode: 3709, duration: 1.671s, episode steps: 113, steps per second: 68, episode reward: -81.986, mean reward: -0.726 [-100.000, 13.090], mean action: 1.575 [0.000, 3.000], mean observation: 0.135 [-0.881, 1.000], loss: 1.787036, mean_absolute_error: 34.269849, mean_q: 22.685946, mean_eps: 0.663083
  374519/2000000: episode: 3710, duration: 1.537s, episode steps: 110, steps per second: 72, episode reward: -136.711, mean reward: -1.243 [-100.000, 8.224], mean action: 1.582 [0.000, 3.000], mean observation: 0.129 [-3.297, 1.000], loss: 1.647836, mean_absolute_error: 34.904291, mean_q: 21.810712, mean_eps: 0.662982
  374611/2000000: episode: 3711, duration: 1.329s, episode steps: 92, steps per second: 69, episode reward: -120.861, mean reward: -1.314 [-100.000, 23.148], mean action: 1.543 [0.000, 3.000], mean observation: 0.108 [-3.766, 1.000], loss: 2.555908, mean_absolute_error: 35.387126, mean_q: 20.589251, mean_eps: 0.662892
  374750/2000000: episode: 3712, duration: 1.993s, episode steps: 139, steps per second: 70, episode reward: -36.742, mean reward: -0.264 [-100.000, 63.988], mean action: 1.655 [0.000, 3.000], mean observation: -0.120 [-1.010, 1.234], loss: 2.072138, mean_absolute_error: 36.155364, mean_q: 19.426997, mean_eps: 0.662788
  374872/2000000: episode: 3713, duration: 1.814s, episode steps: 122, steps per second: 67, episode reward: -127.744, mean reward: -1.047 [-100.000, 9.723], mean action: 1.607 [0.000, 3.000], mean observation: -0.036 [-0.860, 1.000], loss: 2.127166, mean_absolute_error: 36.714910, mean_q: 15.805043, mean_eps: 0.662671
  374971/2000000: episode: 3714, duration: 1.433s, episode steps: 99, steps per second: 69, episode reward: -148.182, mean reward: -1.497 [-100.000, 21.708], mean action: 1.737 [0.000, 3.000], mean observation: 0.115 [-1.726, 1.011], loss: 1.382540, mean_absolute_error: 35.296235, mean_q: 20.726557, mean_eps: 0.662572
  375080/2000000: episode: 3715, duration: 1.637s, episode steps: 109, steps per second: 67, episode reward: -75.907, mean reward: -0.696 [-100.000, 17.704], mean action: 1.624 [0.000, 3.000], mean observation: -0.017 [-0.887, 1.000], loss: 2.517540, mean_absolute_error: 36.286471, mean_q: 20.335638, mean_eps: 0.662478
  375207/2000000: episode: 3716, duration: 1.831s, episode steps: 127, steps per second: 69, episode reward: -98.532, mean reward: -0.776 [-100.000, 22.125], mean action: 1.756 [0.000, 3.000], mean observation: -0.095 [-2.943, 1.000], loss: 2.308503, mean_absolute_error: 36.643113, mean_q: 18.433054, mean_eps: 0.662372
  375325/2000000: episode: 3717, duration: 1.729s, episode steps: 118, steps per second: 68, episode reward: -94.391, mean reward: -0.800 [-100.000, 12.907], mean action: 1.703 [0.000, 3.000], mean observation: 0.110 [-2.915, 1.000], loss: 1.513398, mean_absolute_error: 34.954629, mean_q: 19.366731, mean_eps: 0.662261
  375445/2000000: episode: 3718, duration: 1.720s, episode steps: 120, steps per second: 70, episode reward: -45.360, mean reward: -0.378 [-100.000, 19.533], mean action: 1.725 [0.000, 3.000], mean observation: -0.077 [-0.850, 1.000], loss: 2.819650, mean_absolute_error: 36.079713, mean_q: 16.768793, mean_eps: 0.662153
  375522/2000000: episode: 3719, duration: 1.107s, episode steps: 77, steps per second: 70, episode reward: -110.412, mean reward: -1.434 [-100.000, 9.882], mean action: 1.416 [0.000, 3.000], mean observation: 0.036 [-3.839, 1.000], loss: 1.339148, mean_absolute_error: 34.939679, mean_q: 22.176791, mean_eps: 0.662064
  375631/2000000: episode: 3720, duration: 1.551s, episode steps: 109, steps per second: 70, episode reward: -125.240, mean reward: -1.149 [-100.000, 6.302], mean action: 1.651 [0.000, 3.000], mean observation: -0.044 [-1.168, 4.019], loss: 1.753562, mean_absolute_error: 34.436749, mean_q: 22.488417, mean_eps: 0.661982
  375781/2000000: episode: 3721, duration: 2.156s, episode steps: 150, steps per second: 70, episode reward: -60.907, mean reward: -0.406 [-100.000, 11.661], mean action: 1.693 [0.000, 3.000], mean observation: -0.066 [-0.620, 2.126], loss: 2.003935, mean_absolute_error: 34.399522, mean_q: 18.552654, mean_eps: 0.661865
  375884/2000000: episode: 3722, duration: 1.472s, episode steps: 103, steps per second: 70, episode reward: -115.158, mean reward: -1.118 [-100.000, 10.849], mean action: 1.544 [0.000, 3.000], mean observation: 0.059 [-3.447, 1.000], loss: 1.438247, mean_absolute_error: 36.462267, mean_q: 18.225428, mean_eps: 0.661751
  375968/2000000: episode: 3723, duration: 1.247s, episode steps: 84, steps per second: 67, episode reward: -94.919, mean reward: -1.130 [-100.000, 13.388], mean action: 1.774 [0.000, 3.000], mean observation: -0.110 [-1.073, 1.000], loss: 1.572898, mean_absolute_error: 33.622901, mean_q: 18.997168, mean_eps: 0.661668
  376083/2000000: episode: 3724, duration: 1.659s, episode steps: 115, steps per second: 69, episode reward: -78.966, mean reward: -0.687 [-100.000, 12.560], mean action: 1.739 [0.000, 3.000], mean observation: 0.006 [-0.887, 1.000], loss: 2.336530, mean_absolute_error: 36.233945, mean_q: 21.427617, mean_eps: 0.661578
  376204/2000000: episode: 3725, duration: 1.777s, episode steps: 121, steps per second: 68, episode reward: -108.202, mean reward: -0.894 [-100.000, 7.989], mean action: 1.612 [0.000, 3.000], mean observation: 0.151 [-3.134, 1.028], loss: 1.976568, mean_absolute_error: 36.432332, mean_q: 20.301712, mean_eps: 0.661472
  376295/2000000: episode: 3726, duration: 1.289s, episode steps: 91, steps per second: 71, episode reward: -75.455, mean reward: -0.829 [-100.000, 9.799], mean action: 1.692 [0.000, 3.000], mean observation: -0.016 [-0.925, 3.065], loss: 2.260213, mean_absolute_error: 34.754168, mean_q: 21.614602, mean_eps: 0.661377
  376401/2000000: episode: 3727, duration: 1.558s, episode steps: 106, steps per second: 68, episode reward: -106.154, mean reward: -1.001 [-100.000, 7.623], mean action: 1.660 [0.000, 3.000], mean observation: -0.070 [-3.628, 1.000], loss: 1.806735, mean_absolute_error: 35.442877, mean_q: 19.680946, mean_eps: 0.661287
  376518/2000000: episode: 3728, duration: 1.763s, episode steps: 117, steps per second: 66, episode reward: -119.582, mean reward: -1.022 [-100.000, 6.154], mean action: 1.615 [0.000, 3.000], mean observation: 0.077 [-3.076, 1.000], loss: 1.789854, mean_absolute_error: 34.976583, mean_q: 20.480171, mean_eps: 0.661186
  376629/2000000: episode: 3729, duration: 1.607s, episode steps: 111, steps per second: 69, episode reward: -151.717, mean reward: -1.367 [-100.000, 10.909], mean action: 1.748 [0.000, 3.000], mean observation: -0.016 [-1.140, 3.637], loss: 2.905364, mean_absolute_error: 36.225392, mean_q: 19.560919, mean_eps: 0.661083
  376760/2000000: episode: 3730, duration: 1.916s, episode steps: 131, steps per second: 68, episode reward: -84.373, mean reward: -0.644 [-100.000, 7.756], mean action: 1.664 [0.000, 3.000], mean observation: 0.088 [-3.666, 1.014], loss: 1.918820, mean_absolute_error: 35.420703, mean_q: 21.453119, mean_eps: 0.660975
  376860/2000000: episode: 3731, duration: 1.496s, episode steps: 100, steps per second: 67, episode reward: -129.806, mean reward: -1.298 [-100.000, 9.602], mean action: 1.760 [0.000, 3.000], mean observation: -0.086 [-1.169, 3.981], loss: 1.758160, mean_absolute_error: 36.775286, mean_q: 19.517855, mean_eps: 0.660873
  376984/2000000: episode: 3732, duration: 1.814s, episode steps: 124, steps per second: 68, episode reward: -119.398, mean reward: -0.963 [-100.000, 7.906], mean action: 1.742 [0.000, 3.000], mean observation: -0.081 [-2.751, 1.000], loss: 1.927930, mean_absolute_error: 35.783786, mean_q: 20.961746, mean_eps: 0.660772
  377076/2000000: episode: 3733, duration: 1.398s, episode steps: 92, steps per second: 66, episode reward: -69.244, mean reward: -0.753 [-100.000, 11.998], mean action: 1.630 [0.000, 3.000], mean observation: -0.152 [-1.037, 2.663], loss: 1.958933, mean_absolute_error: 35.338699, mean_q: 20.531691, mean_eps: 0.660675
  377145/2000000: episode: 3734, duration: 1.029s, episode steps: 69, steps per second: 67, episode reward: -109.556, mean reward: -1.588 [-100.000, 4.730], mean action: 1.739 [0.000, 3.000], mean observation: -0.153 [-4.207, 1.000], loss: 1.362568, mean_absolute_error: 34.609359, mean_q: 22.344498, mean_eps: 0.660601
  377256/2000000: episode: 3735, duration: 1.591s, episode steps: 111, steps per second: 70, episode reward: -96.783, mean reward: -0.872 [-100.000, 9.480], mean action: 1.658 [0.000, 3.000], mean observation: 0.049 [-2.833, 1.000], loss: 2.212175, mean_absolute_error: 34.745075, mean_q: 20.535595, mean_eps: 0.660520
  377363/2000000: episode: 3736, duration: 1.557s, episode steps: 107, steps per second: 69, episode reward: -120.911, mean reward: -1.130 [-100.000, 9.522], mean action: 1.710 [0.000, 3.000], mean observation: -0.092 [-1.052, 1.000], loss: 1.780117, mean_absolute_error: 36.208779, mean_q: 20.749801, mean_eps: 0.660423
  377442/2000000: episode: 3737, duration: 1.143s, episode steps: 79, steps per second: 69, episode reward: -126.737, mean reward: -1.604 [-100.000, 9.067], mean action: 1.861 [0.000, 3.000], mean observation: -0.157 [-1.114, 3.624], loss: 2.301554, mean_absolute_error: 36.254430, mean_q: 15.813677, mean_eps: 0.660338
  377555/2000000: episode: 3738, duration: 1.589s, episode steps: 113, steps per second: 71, episode reward: -148.105, mean reward: -1.311 [-100.000, 10.187], mean action: 1.681 [0.000, 3.000], mean observation: 0.025 [-1.160, 3.564], loss: 1.571750, mean_absolute_error: 35.222207, mean_q: 19.401609, mean_eps: 0.660252
  377646/2000000: episode: 3739, duration: 1.341s, episode steps: 91, steps per second: 68, episode reward: -101.973, mean reward: -1.121 [-100.000, 21.280], mean action: 1.582 [0.000, 3.000], mean observation: -0.056 [-0.951, 2.803], loss: 1.837118, mean_absolute_error: 36.152999, mean_q: 17.334292, mean_eps: 0.660160
  377764/2000000: episode: 3740, duration: 1.719s, episode steps: 118, steps per second: 69, episode reward: -97.254, mean reward: -0.824 [-100.000, 7.301], mean action: 1.678 [0.000, 3.000], mean observation: 0.063 [-3.071, 1.000], loss: 1.365222, mean_absolute_error: 34.501284, mean_q: 19.522486, mean_eps: 0.660066
  377923/2000000: episode: 3741, duration: 2.293s, episode steps: 159, steps per second: 69, episode reward: -128.353, mean reward: -0.807 [-100.000, 11.322], mean action: 1.642 [0.000, 3.000], mean observation: 0.080 [-3.302, 1.000], loss: 1.555745, mean_absolute_error: 35.060626, mean_q: 19.896305, mean_eps: 0.659942
  377994/2000000: episode: 3742, duration: 1.075s, episode steps: 71, steps per second: 66, episode reward: -139.729, mean reward: -1.968 [-100.000, 9.550], mean action: 1.549 [0.000, 3.000], mean observation: 0.008 [-4.321, 1.000], loss: 1.274585, mean_absolute_error: 34.876810, mean_q: 23.133330, mean_eps: 0.659838
  378129/2000000: episode: 3743, duration: 1.932s, episode steps: 135, steps per second: 70, episode reward: -141.822, mean reward: -1.051 [-100.000, 5.671], mean action: 1.689 [0.000, 3.000], mean observation: 0.108 [-0.876, 1.011], loss: 1.619039, mean_absolute_error: 34.740178, mean_q: 19.933233, mean_eps: 0.659744
  378242/2000000: episode: 3744, duration: 1.622s, episode steps: 113, steps per second: 70, episode reward: -37.875, mean reward: -0.335 [-100.000, 9.509], mean action: 1.867 [0.000, 3.000], mean observation: -0.022 [-0.816, 1.528], loss: 1.377540, mean_absolute_error: 34.947152, mean_q: 22.720689, mean_eps: 0.659633
  378381/2000000: episode: 3745, duration: 2.249s, episode steps: 139, steps per second: 62, episode reward: -106.940, mean reward: -0.769 [-100.000, 10.339], mean action: 1.683 [0.000, 3.000], mean observation: 0.030 [-0.828, 1.000], loss: 1.391126, mean_absolute_error: 34.930735, mean_q: 21.459134, mean_eps: 0.659519
  378463/2000000: episode: 3746, duration: 1.166s, episode steps: 82, steps per second: 70, episode reward: -83.471, mean reward: -1.018 [-100.000, 15.039], mean action: 1.780 [0.000, 3.000], mean observation: -0.151 [-0.883, 1.000], loss: 1.928781, mean_absolute_error: 36.610575, mean_q: 18.753072, mean_eps: 0.659420
  378573/2000000: episode: 3747, duration: 1.611s, episode steps: 110, steps per second: 68, episode reward: -99.967, mean reward: -0.909 [-100.000, 16.642], mean action: 1.591 [0.000, 3.000], mean observation: 0.038 [-0.983, 1.000], loss: 1.692991, mean_absolute_error: 35.289256, mean_q: 20.527073, mean_eps: 0.659334
  378655/2000000: episode: 3748, duration: 1.138s, episode steps: 82, steps per second: 72, episode reward: -64.900, mean reward: -0.791 [-100.000, 20.104], mean action: 1.695 [0.000, 3.000], mean observation: -0.061 [-2.545, 1.000], loss: 1.460458, mean_absolute_error: 36.311427, mean_q: 23.587708, mean_eps: 0.659247
  378777/2000000: episode: 3749, duration: 1.765s, episode steps: 122, steps per second: 69, episode reward: -101.743, mean reward: -0.834 [-100.000, 17.651], mean action: 1.705 [0.000, 3.000], mean observation: 0.155 [-1.188, 3.829], loss: 1.818241, mean_absolute_error: 35.330990, mean_q: 21.301770, mean_eps: 0.659156
  378929/2000000: episode: 3750, duration: 2.182s, episode steps: 152, steps per second: 70, episode reward: -104.894, mean reward: -0.690 [-100.000, 10.831], mean action: 1.757 [0.000, 3.000], mean observation: -0.035 [-0.861, 1.000], loss: 1.935163, mean_absolute_error: 35.517448, mean_q: 22.535799, mean_eps: 0.659031
  379047/2000000: episode: 3751, duration: 1.683s, episode steps: 118, steps per second: 70, episode reward: -126.850, mean reward: -1.075 [-100.000, 8.757], mean action: 1.661 [0.000, 3.000], mean observation: 0.038 [-0.968, 3.666], loss: 1.604652, mean_absolute_error: 34.814797, mean_q: 21.189038, mean_eps: 0.658911
  379232/2000000: episode: 3752, duration: 2.705s, episode steps: 185, steps per second: 68, episode reward: -118.558, mean reward: -0.641 [-100.000, 13.971], mean action: 1.670 [0.000, 3.000], mean observation: 0.073 [-0.841, 1.039], loss: 1.496029, mean_absolute_error: 35.823572, mean_q: 21.291013, mean_eps: 0.658776
  379371/2000000: episode: 3753, duration: 2.022s, episode steps: 139, steps per second: 69, episode reward: -35.068, mean reward: -0.252 [-100.000, 15.455], mean action: 1.683 [0.000, 3.000], mean observation: 0.015 [-0.695, 1.000], loss: 2.296070, mean_absolute_error: 35.773651, mean_q: 20.666515, mean_eps: 0.658630
  379452/2000000: episode: 3754, duration: 1.191s, episode steps: 81, steps per second: 68, episode reward: -117.977, mean reward: -1.457 [-100.000, 12.110], mean action: 1.654 [0.000, 3.000], mean observation: 0.019 [-1.134, 3.794], loss: 1.495698, mean_absolute_error: 34.857962, mean_q: 20.864004, mean_eps: 0.658531
  379577/2000000: episode: 3755, duration: 1.817s, episode steps: 125, steps per second: 69, episode reward: -125.886, mean reward: -1.007 [-100.000, 12.622], mean action: 1.632 [0.000, 3.000], mean observation: 0.062 [-1.122, 1.000], loss: 2.578269, mean_absolute_error: 34.830836, mean_q: 20.150518, mean_eps: 0.658437
  379717/2000000: episode: 3756, duration: 1.997s, episode steps: 140, steps per second: 70, episode reward: -164.956, mean reward: -1.178 [-100.000, 10.789], mean action: 1.700 [0.000, 3.000], mean observation: -0.062 [-1.201, 1.000], loss: 2.327895, mean_absolute_error: 36.451854, mean_q: 21.375465, mean_eps: 0.658317
  379895/2000000: episode: 3757, duration: 2.515s, episode steps: 178, steps per second: 71, episode reward: -50.520, mean reward: -0.284 [-100.000, 12.713], mean action: 1.596 [0.000, 3.000], mean observation: 0.036 [-0.837, 1.023], loss: 1.736345, mean_absolute_error: 34.911216, mean_q: 22.916670, mean_eps: 0.658175
  380006/2000000: episode: 3758, duration: 1.604s, episode steps: 111, steps per second: 69, episode reward: -85.987, mean reward: -0.775 [-100.000, 13.417], mean action: 1.658 [0.000, 3.000], mean observation: -0.000 [-0.873, 3.128], loss: 1.556337, mean_absolute_error: 35.359058, mean_q: 20.090515, mean_eps: 0.658045
  380154/2000000: episode: 3759, duration: 2.095s, episode steps: 148, steps per second: 71, episode reward: -104.482, mean reward: -0.706 [-100.000, 16.142], mean action: 1.649 [0.000, 3.000], mean observation: 0.180 [-3.713, 1.065], loss: 1.901002, mean_absolute_error: 35.597101, mean_q: 21.828751, mean_eps: 0.657928
  380248/2000000: episode: 3760, duration: 1.370s, episode steps: 94, steps per second: 69, episode reward: -91.603, mean reward: -0.975 [-100.000, 7.764], mean action: 1.606 [0.000, 3.000], mean observation: -0.017 [-3.839, 1.000], loss: 1.988304, mean_absolute_error: 34.407256, mean_q: 21.865502, mean_eps: 0.657820
  380397/2000000: episode: 3761, duration: 2.181s, episode steps: 149, steps per second: 68, episode reward: -82.039, mean reward: -0.551 [-100.000, 16.561], mean action: 1.758 [0.000, 3.000], mean observation: -0.109 [-0.868, 2.232], loss: 1.628553, mean_absolute_error: 36.144579, mean_q: 19.966097, mean_eps: 0.657710
  380510/2000000: episode: 3762, duration: 1.605s, episode steps: 113, steps per second: 70, episode reward: -96.646, mean reward: -0.855 [-100.000, 17.557], mean action: 1.593 [0.000, 3.000], mean observation: 0.128 [-2.986, 1.000], loss: 1.493225, mean_absolute_error: 35.406243, mean_q: 19.604816, mean_eps: 0.657591
  380642/2000000: episode: 3763, duration: 1.881s, episode steps: 132, steps per second: 70, episode reward: -121.889, mean reward: -0.923 [-100.000, 10.873], mean action: 1.553 [0.000, 3.000], mean observation: 0.157 [-2.901, 1.009], loss: 1.757993, mean_absolute_error: 34.663152, mean_q: 19.592730, mean_eps: 0.657482
  380726/2000000: episode: 3764, duration: 1.202s, episode steps: 84, steps per second: 70, episode reward: -89.535, mean reward: -1.066 [-100.000, 11.849], mean action: 1.417 [0.000, 3.000], mean observation: 0.085 [-0.973, 1.000], loss: 1.664198, mean_absolute_error: 34.712148, mean_q: 21.251497, mean_eps: 0.657384
  380813/2000000: episode: 3765, duration: 1.242s, episode steps: 87, steps per second: 70, episode reward: -129.044, mean reward: -1.483 [-100.000, 10.968], mean action: 1.736 [0.000, 3.000], mean observation: -0.145 [-1.155, 3.004], loss: 1.711891, mean_absolute_error: 34.317136, mean_q: 23.798082, mean_eps: 0.657307
  380911/2000000: episode: 3766, duration: 1.360s, episode steps: 98, steps per second: 72, episode reward: -92.135, mean reward: -0.940 [-100.000, 20.515], mean action: 1.633 [0.000, 3.000], mean observation: 0.033 [-0.943, 1.000], loss: 2.393798, mean_absolute_error: 36.319846, mean_q: 20.757651, mean_eps: 0.657224
  380988/2000000: episode: 3767, duration: 1.125s, episode steps: 77, steps per second: 68, episode reward: -97.149, mean reward: -1.262 [-100.000, 8.868], mean action: 1.675 [0.000, 3.000], mean observation: -0.156 [-0.906, 2.951], loss: 1.801406, mean_absolute_error: 35.553439, mean_q: 17.581639, mean_eps: 0.657147
  381163/2000000: episode: 3768, duration: 2.526s, episode steps: 175, steps per second: 69, episode reward: -95.291, mean reward: -0.545 [-100.000, 30.091], mean action: 1.663 [0.000, 3.000], mean observation: 0.127 [-1.451, 1.000], loss: 1.652245, mean_absolute_error: 35.625441, mean_q: 19.514834, mean_eps: 0.657033
  381337/2000000: episode: 3769, duration: 2.520s, episode steps: 174, steps per second: 69, episode reward: -97.026, mean reward: -0.558 [-100.000, 18.800], mean action: 1.598 [0.000, 3.000], mean observation: 0.062 [-1.233, 3.705], loss: 1.907938, mean_absolute_error: 34.641401, mean_q: 20.823597, mean_eps: 0.656875
  381422/2000000: episode: 3770, duration: 1.225s, episode steps: 85, steps per second: 69, episode reward: -130.086, mean reward: -1.530 [-100.000, 7.295], mean action: 1.576 [0.000, 3.000], mean observation: -0.079 [-4.311, 1.000], loss: 2.071044, mean_absolute_error: 35.131483, mean_q: 19.315042, mean_eps: 0.656758
  381516/2000000: episode: 3771, duration: 1.376s, episode steps: 94, steps per second: 68, episode reward: -66.624, mean reward: -0.709 [-100.000, 30.144], mean action: 1.681 [0.000, 3.000], mean observation: 0.108 [-0.975, 3.146], loss: 1.637966, mean_absolute_error: 33.784028, mean_q: 20.283263, mean_eps: 0.656679
  381631/2000000: episode: 3772, duration: 1.653s, episode steps: 115, steps per second: 70, episode reward: -95.588, mean reward: -0.831 [-100.000, 10.163], mean action: 1.791 [0.000, 3.000], mean observation: 0.057 [-3.338, 1.000], loss: 1.819967, mean_absolute_error: 34.857560, mean_q: 23.401811, mean_eps: 0.656585
  381719/2000000: episode: 3773, duration: 1.264s, episode steps: 88, steps per second: 70, episode reward: -121.674, mean reward: -1.383 [-100.000, 8.972], mean action: 1.727 [0.000, 3.000], mean observation: -0.068 [-1.086, 3.764], loss: 2.203269, mean_absolute_error: 35.661427, mean_q: 21.495425, mean_eps: 0.656493
  381876/2000000: episode: 3774, duration: 2.277s, episode steps: 157, steps per second: 69, episode reward: -47.392, mean reward: -0.302 [-100.000, 12.998], mean action: 1.554 [0.000, 3.000], mean observation: 0.034 [-0.938, 1.027], loss: 2.043390, mean_absolute_error: 35.316373, mean_q: 22.204409, mean_eps: 0.656384
  382019/2000000: episode: 3775, duration: 2.066s, episode steps: 143, steps per second: 69, episode reward: -128.689, mean reward: -0.900 [-100.000, 13.877], mean action: 1.706 [0.000, 3.000], mean observation: -0.098 [-0.949, 1.956], loss: 1.938806, mean_absolute_error: 35.632429, mean_q: 21.580666, mean_eps: 0.656249
  382097/2000000: episode: 3776, duration: 1.147s, episode steps: 78, steps per second: 68, episode reward: -93.854, mean reward: -1.203 [-100.000, 10.469], mean action: 1.808 [0.000, 3.000], mean observation: -0.108 [-1.013, 2.867], loss: 1.759702, mean_absolute_error: 34.431096, mean_q: 19.750831, mean_eps: 0.656148
  382204/2000000: episode: 3777, duration: 1.556s, episode steps: 107, steps per second: 69, episode reward: -236.834, mean reward: -2.213 [-100.000, 61.373], mean action: 1.692 [0.000, 3.000], mean observation: 0.174 [-0.964, 2.195], loss: 1.347935, mean_absolute_error: 33.909018, mean_q: 20.655620, mean_eps: 0.656065
  382345/2000000: episode: 3778, duration: 2.053s, episode steps: 141, steps per second: 69, episode reward: -106.446, mean reward: -0.755 [-100.000, 8.639], mean action: 1.567 [0.000, 3.000], mean observation: 0.099 [-3.082, 1.000], loss: 1.996735, mean_absolute_error: 34.541672, mean_q: 19.282915, mean_eps: 0.655953
  382478/2000000: episode: 3779, duration: 1.952s, episode steps: 133, steps per second: 68, episode reward: -118.611, mean reward: -0.892 [-100.000, 9.106], mean action: 1.654 [0.000, 3.000], mean observation: 0.167 [-4.044, 1.041], loss: 2.432984, mean_absolute_error: 35.079684, mean_q: 20.864797, mean_eps: 0.655829
  382600/2000000: episode: 3780, duration: 1.763s, episode steps: 122, steps per second: 69, episode reward: -145.540, mean reward: -1.193 [-100.000, 7.777], mean action: 1.582 [0.000, 3.000], mean observation: -0.153 [-3.809, 1.000], loss: 2.036792, mean_absolute_error: 35.874500, mean_q: 17.327215, mean_eps: 0.655716
  382736/2000000: episode: 3781, duration: 2.013s, episode steps: 136, steps per second: 68, episode reward: -150.519, mean reward: -1.107 [-100.000, 11.256], mean action: 1.603 [0.000, 3.000], mean observation: 0.148 [-1.045, 1.008], loss: 1.626856, mean_absolute_error: 35.344235, mean_q: 21.838631, mean_eps: 0.655601
  382814/2000000: episode: 3782, duration: 1.155s, episode steps: 78, steps per second: 68, episode reward: -138.276, mean reward: -1.773 [-100.000, 6.008], mean action: 1.577 [0.000, 3.000], mean observation: -0.087 [-1.154, 3.228], loss: 1.991999, mean_absolute_error: 35.185885, mean_q: 19.810564, mean_eps: 0.655503
  382894/2000000: episode: 3783, duration: 1.145s, episode steps: 80, steps per second: 70, episode reward: -108.890, mean reward: -1.361 [-100.000, 10.231], mean action: 1.550 [0.000, 3.000], mean observation: -0.129 [-1.162, 3.909], loss: 1.714919, mean_absolute_error: 34.769791, mean_q: 21.909578, mean_eps: 0.655431
  383047/2000000: episode: 3784, duration: 2.187s, episode steps: 153, steps per second: 70, episode reward: -82.463, mean reward: -0.539 [-100.000, 14.786], mean action: 1.686 [0.000, 3.000], mean observation: -0.075 [-0.768, 1.000], loss: 2.188967, mean_absolute_error: 34.696082, mean_q: 20.368820, mean_eps: 0.655327
  383137/2000000: episode: 3785, duration: 1.332s, episode steps: 90, steps per second: 68, episode reward: -99.728, mean reward: -1.108 [-100.000, 7.442], mean action: 1.522 [0.000, 3.000], mean observation: -0.006 [-3.710, 1.000], loss: 1.278158, mean_absolute_error: 35.688352, mean_q: 20.794321, mean_eps: 0.655217
  383295/2000000: episode: 3786, duration: 2.239s, episode steps: 158, steps per second: 71, episode reward: -254.721, mean reward: -1.612 [-100.000, 71.648], mean action: 1.658 [0.000, 3.000], mean observation: 0.221 [-0.963, 1.982], loss: 1.558027, mean_absolute_error: 35.331838, mean_q: 23.336385, mean_eps: 0.655106
  383444/2000000: episode: 3787, duration: 2.177s, episode steps: 149, steps per second: 68, episode reward: -42.829, mean reward: -0.287 [-100.000, 21.170], mean action: 1.658 [0.000, 3.000], mean observation: -0.115 [-1.011, 1.000], loss: 2.177035, mean_absolute_error: 35.904874, mean_q: 20.595297, mean_eps: 0.654969
  383511/2000000: episode: 3788, duration: 0.994s, episode steps: 67, steps per second: 67, episode reward: -113.686, mean reward: -1.697 [-100.000, 4.912], mean action: 1.821 [0.000, 3.000], mean observation: -0.142 [-3.796, 1.000], loss: 2.822899, mean_absolute_error: 37.355175, mean_q: 21.486093, mean_eps: 0.654872
  383634/2000000: episode: 3789, duration: 1.923s, episode steps: 123, steps per second: 64, episode reward: -115.146, mean reward: -0.936 [-100.000, 11.417], mean action: 1.618 [0.000, 3.000], mean observation: 0.050 [-2.964, 1.000], loss: 1.764300, mean_absolute_error: 35.517138, mean_q: 20.475300, mean_eps: 0.654785
  383752/2000000: episode: 3790, duration: 1.736s, episode steps: 118, steps per second: 68, episode reward: -19.548, mean reward: -0.166 [-100.000, 16.461], mean action: 1.695 [0.000, 3.000], mean observation: 0.053 [-0.725, 1.000], loss: 1.945169, mean_absolute_error: 35.047450, mean_q: 19.959045, mean_eps: 0.654677
  383832/2000000: episode: 3791, duration: 1.201s, episode steps: 80, steps per second: 67, episode reward: -91.475, mean reward: -1.143 [-100.000, 11.172], mean action: 1.738 [0.000, 3.000], mean observation: -0.135 [-0.962, 3.014], loss: 2.994927, mean_absolute_error: 36.419499, mean_q: 16.962201, mean_eps: 0.654589
  383943/2000000: episode: 3792, duration: 1.613s, episode steps: 111, steps per second: 69, episode reward: -111.276, mean reward: -1.002 [-100.000, 21.340], mean action: 1.649 [0.000, 3.000], mean observation: 0.065 [-1.119, 3.766], loss: 2.273366, mean_absolute_error: 35.669359, mean_q: 19.969267, mean_eps: 0.654503
  384033/2000000: episode: 3793, duration: 1.323s, episode steps: 90, steps per second: 68, episode reward: -74.645, mean reward: -0.829 [-100.000, 12.395], mean action: 1.711 [0.000, 3.000], mean observation: 0.072 [-3.258, 1.000], loss: 1.472934, mean_absolute_error: 34.853251, mean_q: 21.677917, mean_eps: 0.654411
  384095/2000000: episode: 3794, duration: 0.865s, episode steps: 62, steps per second: 72, episode reward: -66.938, mean reward: -1.080 [-100.000, 10.346], mean action: 1.468 [0.000, 3.000], mean observation: 0.054 [-1.160, 3.833], loss: 1.637212, mean_absolute_error: 34.297133, mean_q: 22.962995, mean_eps: 0.654342
  384251/2000000: episode: 3795, duration: 2.233s, episode steps: 156, steps per second: 70, episode reward: -118.245, mean reward: -0.758 [-100.000, 7.499], mean action: 1.737 [0.000, 3.000], mean observation: -0.005 [-1.296, 4.658], loss: 1.728107, mean_absolute_error: 34.641651, mean_q: 22.024840, mean_eps: 0.654245
  384364/2000000: episode: 3796, duration: 1.677s, episode steps: 113, steps per second: 67, episode reward: -131.001, mean reward: -1.159 [-100.000, 13.188], mean action: 1.602 [0.000, 3.000], mean observation: -0.111 [-1.543, 1.000], loss: 2.237207, mean_absolute_error: 38.269399, mean_q: 16.645427, mean_eps: 0.654125
  384448/2000000: episode: 3797, duration: 1.251s, episode steps: 84, steps per second: 67, episode reward: -126.776, mean reward: -1.509 [-100.000, 8.249], mean action: 1.631 [0.000, 3.000], mean observation: 0.040 [-4.128, 1.000], loss: 1.439726, mean_absolute_error: 35.697244, mean_q: 22.352186, mean_eps: 0.654036
  384592/2000000: episode: 3798, duration: 2.114s, episode steps: 144, steps per second: 68, episode reward: -88.806, mean reward: -0.617 [-100.000, 10.694], mean action: 1.715 [0.000, 3.000], mean observation: -0.101 [-0.878, 1.000], loss: 1.572021, mean_absolute_error: 34.625283, mean_q: 21.546309, mean_eps: 0.653934
  384716/2000000: episode: 3799, duration: 1.894s, episode steps: 124, steps per second: 65, episode reward: -94.969, mean reward: -0.766 [-100.000, 18.201], mean action: 1.766 [0.000, 3.000], mean observation: 0.041 [-0.994, 1.000], loss: 1.905304, mean_absolute_error: 35.309911, mean_q: 20.595507, mean_eps: 0.653813
  384796/2000000: episode: 3800, duration: 1.256s, episode steps: 80, steps per second: 64, episode reward: -98.042, mean reward: -1.226 [-100.000, 10.143], mean action: 1.625 [0.000, 3.000], mean observation: 0.059 [-3.640, 1.000], loss: 1.481509, mean_absolute_error: 34.405962, mean_q: 22.037206, mean_eps: 0.653721
  384881/2000000: episode: 3801, duration: 1.298s, episode steps: 85, steps per second: 65, episode reward: -122.430, mean reward: -1.440 [-100.000, 9.646], mean action: 1.635 [0.000, 3.000], mean observation: 0.119 [-3.793, 1.000], loss: 1.686495, mean_absolute_error: 34.063676, mean_q: 18.856973, mean_eps: 0.653646
  385012/2000000: episode: 3802, duration: 1.890s, episode steps: 131, steps per second: 69, episode reward: -87.423, mean reward: -0.667 [-100.000, 15.878], mean action: 1.618 [0.000, 3.000], mean observation: 0.023 [-0.853, 2.972], loss: 1.526858, mean_absolute_error: 35.467172, mean_q: 20.124836, mean_eps: 0.653549
  386012/2000000: episode: 3803, duration: 15.691s, episode steps: 1000, steps per second: 64, episode reward: -12.183, mean reward: -0.012 [-24.298, 25.847], mean action: 1.633 [0.000, 3.000], mean observation: 0.224 [-0.721, 1.000], loss: 2.084723, mean_absolute_error: 35.775114, mean_q: 20.844591, mean_eps: 0.653041
  386158/2000000: episode: 3804, duration: 2.127s, episode steps: 146, steps per second: 69, episode reward: -101.424, mean reward: -0.695 [-100.000, 6.724], mean action: 1.678 [0.000, 3.000], mean observation: 0.106 [-0.962, 1.762], loss: 2.073570, mean_absolute_error: 35.396212, mean_q: 20.401722, mean_eps: 0.652524
  386241/2000000: episode: 3805, duration: 1.196s, episode steps: 83, steps per second: 69, episode reward: -117.973, mean reward: -1.421 [-100.000, 11.197], mean action: 1.651 [0.000, 3.000], mean observation: -0.088 [-1.169, 1.000], loss: 1.517680, mean_absolute_error: 35.574207, mean_q: 18.438311, mean_eps: 0.652420
  386371/2000000: episode: 3806, duration: 2.113s, episode steps: 130, steps per second: 62, episode reward: -72.027, mean reward: -0.554 [-100.000, 10.401], mean action: 1.723 [0.000, 3.000], mean observation: 0.080 [-0.874, 2.577], loss: 2.482718, mean_absolute_error: 36.658327, mean_q: 18.368690, mean_eps: 0.652325
  386446/2000000: episode: 3807, duration: 1.138s, episode steps: 75, steps per second: 66, episode reward: -135.408, mean reward: -1.805 [-100.000, 4.296], mean action: 1.520 [0.000, 3.000], mean observation: 0.050 [-1.238, 1.000], loss: 1.145594, mean_absolute_error: 35.988933, mean_q: 25.323715, mean_eps: 0.652233
  386622/2000000: episode: 3808, duration: 2.559s, episode steps: 176, steps per second: 69, episode reward: -72.097, mean reward: -0.410 [-100.000, 13.729], mean action: 1.733 [0.000, 3.000], mean observation: -0.004 [-2.757, 1.000], loss: 1.420474, mean_absolute_error: 35.453383, mean_q: 21.355114, mean_eps: 0.652119
  386746/2000000: episode: 3809, duration: 1.796s, episode steps: 124, steps per second: 69, episode reward: -63.814, mean reward: -0.515 [-100.000, 14.552], mean action: 1.718 [0.000, 3.000], mean observation: -0.025 [-0.894, 1.000], loss: 1.518554, mean_absolute_error: 34.418653, mean_q: 23.374885, mean_eps: 0.651984
  386907/2000000: episode: 3810, duration: 2.271s, episode steps: 161, steps per second: 71, episode reward: -90.903, mean reward: -0.565 [-100.000, 14.956], mean action: 1.646 [0.000, 3.000], mean observation: -0.011 [-0.848, 1.032], loss: 1.459329, mean_absolute_error: 34.754101, mean_q: 21.346562, mean_eps: 0.651857
  386991/2000000: episode: 3811, duration: 1.217s, episode steps: 84, steps per second: 69, episode reward: -143.440, mean reward: -1.708 [-100.000, 10.585], mean action: 1.643 [0.000, 3.000], mean observation: 0.116 [-4.155, 1.000], loss: 1.627428, mean_absolute_error: 36.068029, mean_q: 16.247050, mean_eps: 0.651747
  387104/2000000: episode: 3812, duration: 1.674s, episode steps: 113, steps per second: 67, episode reward: -119.920, mean reward: -1.061 [-100.000, 7.688], mean action: 1.593 [0.000, 3.000], mean observation: -0.116 [-0.860, 3.208], loss: 1.434373, mean_absolute_error: 35.218259, mean_q: 21.671496, mean_eps: 0.651659
  387184/2000000: episode: 3813, duration: 1.200s, episode steps: 80, steps per second: 67, episode reward: -138.803, mean reward: -1.735 [-100.000, 14.858], mean action: 1.650 [0.000, 3.000], mean observation: -0.069 [-1.115, 1.000], loss: 1.805788, mean_absolute_error: 36.219224, mean_q: 20.907690, mean_eps: 0.651572
  387370/2000000: episode: 3814, duration: 2.682s, episode steps: 186, steps per second: 69, episode reward: -61.168, mean reward: -0.329 [-100.000, 22.372], mean action: 1.780 [0.000, 3.000], mean observation: 0.025 [-0.842, 1.000], loss: 2.356788, mean_absolute_error: 35.646151, mean_q: 21.302436, mean_eps: 0.651452
  387504/2000000: episode: 3815, duration: 2.141s, episode steps: 134, steps per second: 63, episode reward: -138.134, mean reward: -1.031 [-100.000, 7.996], mean action: 1.664 [0.000, 3.000], mean observation: 0.061 [-1.645, 1.000], loss: 1.476911, mean_absolute_error: 35.242920, mean_q: 19.982677, mean_eps: 0.651308
  387584/2000000: episode: 3816, duration: 1.259s, episode steps: 80, steps per second: 64, episode reward: -81.487, mean reward: -1.019 [-100.000, 8.049], mean action: 1.613 [0.000, 3.000], mean observation: -0.019 [-3.448, 1.000], loss: 1.502691, mean_absolute_error: 36.179240, mean_q: 20.908430, mean_eps: 0.651212
  387666/2000000: episode: 3817, duration: 1.249s, episode steps: 82, steps per second: 66, episode reward: -148.884, mean reward: -1.816 [-100.000, 19.008], mean action: 1.720 [0.000, 3.000], mean observation: -0.141 [-3.996, 1.000], loss: 1.190756, mean_absolute_error: 34.047882, mean_q: 22.132604, mean_eps: 0.651138
  387792/2000000: episode: 3818, duration: 1.852s, episode steps: 126, steps per second: 68, episode reward: -110.706, mean reward: -0.879 [-100.000, 6.476], mean action: 1.675 [0.000, 3.000], mean observation: 0.019 [-3.185, 1.000], loss: 1.553013, mean_absolute_error: 35.391425, mean_q: 18.727361, mean_eps: 0.651045
  387953/2000000: episode: 3819, duration: 2.505s, episode steps: 161, steps per second: 64, episode reward: -95.161, mean reward: -0.591 [-100.000, 18.647], mean action: 1.615 [0.000, 3.000], mean observation: 0.061 [-0.924, 1.000], loss: 2.927832, mean_absolute_error: 35.707321, mean_q: 19.195439, mean_eps: 0.650915
  388070/2000000: episode: 3820, duration: 1.711s, episode steps: 117, steps per second: 68, episode reward: -76.645, mean reward: -0.655 [-100.000, 8.567], mean action: 1.718 [0.000, 3.000], mean observation: -0.089 [-0.873, 2.842], loss: 1.610526, mean_absolute_error: 36.190143, mean_q: 20.726886, mean_eps: 0.650789
  388145/2000000: episode: 3821, duration: 1.124s, episode steps: 75, steps per second: 67, episode reward: -120.085, mean reward: -1.601 [-100.000, 11.426], mean action: 1.613 [0.000, 3.000], mean observation: 0.047 [-3.018, 1.000], loss: 1.220194, mean_absolute_error: 33.108412, mean_q: 24.928779, mean_eps: 0.650703
  388239/2000000: episode: 3822, duration: 1.322s, episode steps: 94, steps per second: 71, episode reward: -99.326, mean reward: -1.057 [-100.000, 14.798], mean action: 1.574 [0.000, 3.000], mean observation: -0.056 [-3.536, 1.000], loss: 1.662314, mean_absolute_error: 34.143152, mean_q: 21.194139, mean_eps: 0.650627
  388352/2000000: episode: 3823, duration: 1.667s, episode steps: 113, steps per second: 68, episode reward: -83.427, mean reward: -0.738 [-100.000, 22.259], mean action: 1.593 [0.000, 3.000], mean observation: 0.024 [-0.963, 1.000], loss: 1.352892, mean_absolute_error: 35.449166, mean_q: 20.938965, mean_eps: 0.650535
  388449/2000000: episode: 3824, duration: 1.429s, episode steps: 97, steps per second: 68, episode reward: -164.753, mean reward: -1.698 [-100.000, 16.175], mean action: 1.753 [0.000, 3.000], mean observation: -0.164 [-3.810, 1.000], loss: 1.613825, mean_absolute_error: 35.873190, mean_q: 21.784141, mean_eps: 0.650440
  388575/2000000: episode: 3825, duration: 1.781s, episode steps: 126, steps per second: 71, episode reward: -129.157, mean reward: -1.025 [-100.000, 6.242], mean action: 1.754 [0.000, 3.000], mean observation: 0.049 [-0.956, 1.000], loss: 1.645737, mean_absolute_error: 34.278299, mean_q: 21.393876, mean_eps: 0.650339
  388656/2000000: episode: 3826, duration: 1.202s, episode steps: 81, steps per second: 67, episode reward: -116.375, mean reward: -1.437 [-100.000, 19.100], mean action: 1.691 [0.000, 3.000], mean observation: -0.085 [-4.047, 1.000], loss: 2.197827, mean_absolute_error: 36.087361, mean_q: 17.416929, mean_eps: 0.650247
  388747/2000000: episode: 3827, duration: 1.367s, episode steps: 91, steps per second: 67, episode reward: -154.136, mean reward: -1.694 [-100.000, 15.771], mean action: 1.648 [0.000, 3.000], mean observation: -0.081 [-1.157, 3.953], loss: 1.797255, mean_absolute_error: 36.467770, mean_q: 21.254104, mean_eps: 0.650170
  388841/2000000: episode: 3828, duration: 1.465s, episode steps: 94, steps per second: 64, episode reward: -14.006, mean reward: -0.149 [-100.000, 18.212], mean action: 1.745 [0.000, 3.000], mean observation: -0.037 [-0.878, 1.000], loss: 1.631725, mean_absolute_error: 35.648680, mean_q: 21.120310, mean_eps: 0.650085
  388921/2000000: episode: 3829, duration: 1.155s, episode steps: 80, steps per second: 69, episode reward: -92.419, mean reward: -1.155 [-100.000, 5.779], mean action: 1.637 [0.000, 3.000], mean observation: 0.063 [-1.076, 3.599], loss: 1.749095, mean_absolute_error: 35.503126, mean_q: 19.462021, mean_eps: 0.650006
  389042/2000000: episode: 3830, duration: 1.770s, episode steps: 121, steps per second: 68, episode reward: -91.314, mean reward: -0.755 [-100.000, 18.230], mean action: 1.826 [0.000, 3.000], mean observation: 0.149 [-1.073, 1.000], loss: 2.212850, mean_absolute_error: 35.807964, mean_q: 20.432598, mean_eps: 0.649916
  390042/2000000: episode: 3831, duration: 15.493s, episode steps: 1000, steps per second: 65, episode reward: 35.325, mean reward: 0.035 [-23.382, 24.621], mean action: 1.609 [0.000, 3.000], mean observation: 0.100 [-0.775, 1.000], loss: 2.012190, mean_absolute_error: 35.137483, mean_q: 20.218741, mean_eps: 0.649412
  390163/2000000: episode: 3832, duration: 1.740s, episode steps: 121, steps per second: 70, episode reward: -104.075, mean reward: -0.860 [-100.000, 13.328], mean action: 1.653 [0.000, 3.000], mean observation: 0.045 [-0.852, 1.000], loss: 1.237137, mean_absolute_error: 34.130319, mean_q: 25.133023, mean_eps: 0.648908
  390307/2000000: episode: 3833, duration: 2.077s, episode steps: 144, steps per second: 69, episode reward: -52.353, mean reward: -0.364 [-100.000, 15.968], mean action: 1.771 [0.000, 3.000], mean observation: 0.078 [-0.824, 2.959], loss: 1.801282, mean_absolute_error: 35.839259, mean_q: 20.159056, mean_eps: 0.648789
  390445/2000000: episode: 3834, duration: 2.071s, episode steps: 138, steps per second: 67, episode reward: -62.249, mean reward: -0.451 [-100.000, 18.557], mean action: 1.688 [0.000, 3.000], mean observation: 0.036 [-1.810, 1.000], loss: 2.302100, mean_absolute_error: 35.247248, mean_q: 21.855954, mean_eps: 0.648662
  390594/2000000: episode: 3835, duration: 2.138s, episode steps: 149, steps per second: 70, episode reward: -41.728, mean reward: -0.280 [-100.000, 17.979], mean action: 1.725 [0.000, 3.000], mean observation: 0.015 [-1.545, 1.000], loss: 1.326472, mean_absolute_error: 36.100201, mean_q: 21.078890, mean_eps: 0.648532
  390723/2000000: episode: 3836, duration: 1.856s, episode steps: 129, steps per second: 69, episode reward: -117.126, mean reward: -0.908 [-100.000, 8.392], mean action: 1.713 [0.000, 3.000], mean observation: -0.080 [-0.778, 1.719], loss: 1.174687, mean_absolute_error: 33.779023, mean_q: 23.101685, mean_eps: 0.648408
  390943/2000000: episode: 3837, duration: 3.163s, episode steps: 220, steps per second: 70, episode reward: -123.935, mean reward: -0.563 [-100.000, 12.585], mean action: 1.755 [0.000, 3.000], mean observation: 0.073 [-0.640, 1.000], loss: 2.098734, mean_absolute_error: 35.737830, mean_q: 20.504559, mean_eps: 0.648251
  391066/2000000: episode: 3838, duration: 1.789s, episode steps: 123, steps per second: 69, episode reward: -61.820, mean reward: -0.503 [-100.000, 14.332], mean action: 1.650 [0.000, 3.000], mean observation: -0.049 [-0.746, 1.488], loss: 1.590241, mean_absolute_error: 36.366025, mean_q: 19.181274, mean_eps: 0.648096
  391202/2000000: episode: 3839, duration: 1.944s, episode steps: 136, steps per second: 70, episode reward: -63.472, mean reward: -0.467 [-100.000, 20.097], mean action: 1.603 [0.000, 3.000], mean observation: 0.183 [-1.809, 1.022], loss: 1.760662, mean_absolute_error: 36.236045, mean_q: 20.887746, mean_eps: 0.647979
  391334/2000000: episode: 3840, duration: 1.916s, episode steps: 132, steps per second: 69, episode reward: -59.229, mean reward: -0.449 [-100.000, 17.242], mean action: 1.644 [0.000, 3.000], mean observation: 0.028 [-0.829, 1.000], loss: 2.504310, mean_absolute_error: 37.136902, mean_q: 19.173119, mean_eps: 0.647859
  391449/2000000: episode: 3841, duration: 1.669s, episode steps: 115, steps per second: 69, episode reward: -160.601, mean reward: -1.397 [-100.000, 16.124], mean action: 1.670 [0.000, 3.000], mean observation: -0.056 [-1.054, 3.092], loss: 1.249180, mean_absolute_error: 35.232612, mean_q: 22.594377, mean_eps: 0.647747
  391695/2000000: episode: 3842, duration: 3.520s, episode steps: 246, steps per second: 70, episode reward: -78.934, mean reward: -0.321 [-100.000, 17.861], mean action: 1.593 [0.000, 3.000], mean observation: 0.054 [-2.302, 1.000], loss: 1.953683, mean_absolute_error: 36.387367, mean_q: 22.255010, mean_eps: 0.647585
  391812/2000000: episode: 3843, duration: 1.739s, episode steps: 117, steps per second: 67, episode reward: -68.273, mean reward: -0.584 [-100.000, 20.187], mean action: 1.667 [0.000, 3.000], mean observation: 0.060 [-0.872, 2.680], loss: 1.679697, mean_absolute_error: 37.018927, mean_q: 19.645563, mean_eps: 0.647423
  391925/2000000: episode: 3844, duration: 1.665s, episode steps: 113, steps per second: 68, episode reward: -37.414, mean reward: -0.331 [-100.000, 15.256], mean action: 1.761 [0.000, 3.000], mean observation: -0.012 [-0.828, 1.000], loss: 1.766868, mean_absolute_error: 35.576279, mean_q: 19.818637, mean_eps: 0.647319
  392053/2000000: episode: 3845, duration: 1.833s, episode steps: 128, steps per second: 70, episode reward: -142.678, mean reward: -1.115 [-100.000, 6.582], mean action: 1.680 [0.000, 3.000], mean observation: -0.070 [-3.530, 1.000], loss: 1.593493, mean_absolute_error: 35.961840, mean_q: 22.697533, mean_eps: 0.647209
  392210/2000000: episode: 3846, duration: 2.248s, episode steps: 157, steps per second: 70, episode reward: -49.248, mean reward: -0.314 [-100.000, 22.437], mean action: 1.720 [0.000, 3.000], mean observation: 0.033 [-2.316, 1.000], loss: 2.284258, mean_absolute_error: 36.143641, mean_q: 18.279935, mean_eps: 0.647081
  392311/2000000: episode: 3847, duration: 1.451s, episode steps: 101, steps per second: 70, episode reward: -110.201, mean reward: -1.091 [-100.000, 9.653], mean action: 1.713 [0.000, 3.000], mean observation: -0.037 [-0.986, 2.897], loss: 1.620585, mean_absolute_error: 34.604924, mean_q: 22.797191, mean_eps: 0.646966
  392436/2000000: episode: 3848, duration: 1.839s, episode steps: 125, steps per second: 68, episode reward: -218.031, mean reward: -1.744 [-100.000, 73.127], mean action: 1.696 [0.000, 3.000], mean observation: -0.143 [-1.962, 1.000], loss: 2.420849, mean_absolute_error: 37.204415, mean_q: 18.696660, mean_eps: 0.646865
  392535/2000000: episode: 3849, duration: 1.470s, episode steps: 99, steps per second: 67, episode reward: -118.762, mean reward: -1.200 [-100.000, 9.318], mean action: 1.657 [0.000, 3.000], mean observation: 0.070 [-3.072, 1.000], loss: 1.598576, mean_absolute_error: 36.861469, mean_q: 20.921090, mean_eps: 0.646764
  393535/2000000: episode: 3850, duration: 15.466s, episode steps: 1000, steps per second: 65, episode reward: 48.239, mean reward: 0.048 [-23.641, 46.912], mean action: 1.425 [0.000, 3.000], mean observation: 0.086 [-1.552, 1.042], loss: 1.764749, mean_absolute_error: 35.662293, mean_q: 21.510863, mean_eps: 0.646269
  393628/2000000: episode: 3851, duration: 1.381s, episode steps: 93, steps per second: 67, episode reward: -139.158, mean reward: -1.496 [-100.000, 8.525], mean action: 1.817 [0.000, 3.000], mean observation: -0.154 [-0.934, 2.330], loss: 2.425369, mean_absolute_error: 36.819434, mean_q: 21.995101, mean_eps: 0.645778
  393774/2000000: episode: 3852, duration: 2.118s, episode steps: 146, steps per second: 69, episode reward: -80.528, mean reward: -0.552 [-100.000, 19.283], mean action: 1.671 [0.000, 3.000], mean observation: 0.007 [-2.298, 1.000], loss: 1.810210, mean_absolute_error: 36.327153, mean_q: 21.664131, mean_eps: 0.645670
  393883/2000000: episode: 3853, duration: 1.554s, episode steps: 109, steps per second: 70, episode reward: -114.797, mean reward: -1.053 [-100.000, 10.297], mean action: 1.771 [0.000, 3.000], mean observation: -0.131 [-1.021, 3.284], loss: 3.004171, mean_absolute_error: 37.014561, mean_q: 19.702110, mean_eps: 0.645555
  394052/2000000: episode: 3854, duration: 2.458s, episode steps: 169, steps per second: 69, episode reward: -63.522, mean reward: -0.376 [-100.000, 18.183], mean action: 1.710 [0.000, 3.000], mean observation: -0.014 [-1.014, 1.000], loss: 1.761953, mean_absolute_error: 35.627198, mean_q: 23.312045, mean_eps: 0.645431
  394147/2000000: episode: 3855, duration: 1.386s, episode steps: 95, steps per second: 69, episode reward: -110.083, mean reward: -1.159 [-100.000, 13.961], mean action: 1.674 [0.000, 3.000], mean observation: -0.042 [-1.087, 3.744], loss: 1.444700, mean_absolute_error: 35.344530, mean_q: 24.067254, mean_eps: 0.645312
  394265/2000000: episode: 3856, duration: 1.696s, episode steps: 118, steps per second: 70, episode reward: -153.690, mean reward: -1.302 [-100.000, 9.669], mean action: 1.678 [0.000, 3.000], mean observation: 0.014 [-0.972, 3.043], loss: 2.307604, mean_absolute_error: 36.069061, mean_q: 22.252984, mean_eps: 0.645215
  394402/2000000: episode: 3857, duration: 1.943s, episode steps: 137, steps per second: 71, episode reward: -112.381, mean reward: -0.820 [-100.000, 27.476], mean action: 1.511 [0.000, 3.000], mean observation: 0.126 [-2.366, 1.000], loss: 2.580725, mean_absolute_error: 36.774696, mean_q: 18.678713, mean_eps: 0.645099
  394565/2000000: episode: 3858, duration: 2.358s, episode steps: 163, steps per second: 69, episode reward: -81.152, mean reward: -0.498 [-100.000, 7.386], mean action: 1.687 [0.000, 3.000], mean observation: 0.027 [-0.818, 3.096], loss: 1.487712, mean_absolute_error: 35.551025, mean_q: 21.512010, mean_eps: 0.644964
  394699/2000000: episode: 3859, duration: 1.903s, episode steps: 134, steps per second: 70, episode reward: -81.242, mean reward: -0.606 [-100.000, 13.550], mean action: 1.604 [0.000, 3.000], mean observation: -0.065 [-2.484, 1.000], loss: 1.903670, mean_absolute_error: 35.274294, mean_q: 21.985434, mean_eps: 0.644831
  394824/2000000: episode: 3860, duration: 1.824s, episode steps: 125, steps per second: 69, episode reward: -130.694, mean reward: -1.046 [-100.000, 17.292], mean action: 1.472 [0.000, 3.000], mean observation: 0.039 [-3.040, 1.000], loss: 1.710520, mean_absolute_error: 36.353953, mean_q: 21.797872, mean_eps: 0.644716
  394938/2000000: episode: 3861, duration: 1.655s, episode steps: 114, steps per second: 69, episode reward: -99.661, mean reward: -0.874 [-100.000, 7.231], mean action: 1.702 [0.000, 3.000], mean observation: -0.116 [-0.933, 2.881], loss: 2.143122, mean_absolute_error: 35.689939, mean_q: 18.913214, mean_eps: 0.644608
  395007/2000000: episode: 3862, duration: 0.967s, episode steps: 69, steps per second: 71, episode reward: -117.956, mean reward: -1.710 [-100.000, 18.098], mean action: 1.507 [0.000, 3.000], mean observation: 0.053 [-1.307, 1.488], loss: 2.368352, mean_absolute_error: 35.411117, mean_q: 21.223097, mean_eps: 0.644525
  395156/2000000: episode: 3863, duration: 2.143s, episode steps: 149, steps per second: 70, episode reward: -140.998, mean reward: -0.946 [-100.000, 5.840], mean action: 1.651 [0.000, 3.000], mean observation: 0.060 [-0.936, 3.197], loss: 1.697779, mean_absolute_error: 34.914880, mean_q: 21.280497, mean_eps: 0.644428
  395275/2000000: episode: 3864, duration: 1.739s, episode steps: 119, steps per second: 68, episode reward: -126.047, mean reward: -1.059 [-100.000, 9.511], mean action: 1.664 [0.000, 3.000], mean observation: -0.035 [-1.155, 4.115], loss: 1.557907, mean_absolute_error: 34.953328, mean_q: 23.165558, mean_eps: 0.644307
  395371/2000000: episode: 3865, duration: 1.371s, episode steps: 96, steps per second: 70, episode reward: -150.274, mean reward: -1.565 [-100.000, 11.000], mean action: 1.771 [0.000, 3.000], mean observation: -0.150 [-0.983, 3.016], loss: 1.891293, mean_absolute_error: 36.219656, mean_q: 19.732494, mean_eps: 0.644210
  395592/2000000: episode: 3866, duration: 3.216s, episode steps: 221, steps per second: 69, episode reward: -133.845, mean reward: -0.606 [-100.000, 6.278], mean action: 1.665 [0.000, 3.000], mean observation: -0.066 [-1.135, 1.000], loss: 1.870596, mean_absolute_error: 35.005075, mean_q: 20.413916, mean_eps: 0.644068
  395697/2000000: episode: 3867, duration: 1.626s, episode steps: 105, steps per second: 65, episode reward: -126.690, mean reward: -1.207 [-100.000, 5.718], mean action: 1.781 [0.000, 3.000], mean observation: -0.092 [-0.890, 3.091], loss: 2.141517, mean_absolute_error: 37.319482, mean_q: 18.211262, mean_eps: 0.643920
  395796/2000000: episode: 3868, duration: 1.433s, episode steps: 99, steps per second: 69, episode reward: -127.402, mean reward: -1.287 [-100.000, 8.243], mean action: 1.465 [0.000, 3.000], mean observation: 0.062 [-0.873, 3.177], loss: 1.926980, mean_absolute_error: 35.992448, mean_q: 25.807415, mean_eps: 0.643829
  395869/2000000: episode: 3869, duration: 1.102s, episode steps: 73, steps per second: 66, episode reward: -66.549, mean reward: -0.912 [-100.000, 12.054], mean action: 1.849 [0.000, 3.000], mean observation: -0.117 [-1.024, 1.000], loss: 3.034148, mean_absolute_error: 37.169967, mean_q: 20.754413, mean_eps: 0.643751
  396052/2000000: episode: 3870, duration: 2.618s, episode steps: 183, steps per second: 70, episode reward: -64.721, mean reward: -0.354 [-100.000, 15.133], mean action: 1.617 [0.000, 3.000], mean observation: 0.047 [-0.999, 2.602], loss: 1.608071, mean_absolute_error: 36.214407, mean_q: 22.402240, mean_eps: 0.643636
  396200/2000000: episode: 3871, duration: 2.174s, episode steps: 148, steps per second: 68, episode reward: -69.969, mean reward: -0.473 [-100.000, 5.010], mean action: 1.696 [0.000, 3.000], mean observation: -0.044 [-0.722, 1.034], loss: 2.074685, mean_absolute_error: 34.949927, mean_q: 23.613770, mean_eps: 0.643488
  396296/2000000: episode: 3872, duration: 1.419s, episode steps: 96, steps per second: 68, episode reward: -106.843, mean reward: -1.113 [-100.000, 6.499], mean action: 1.625 [0.000, 3.000], mean observation: 0.068 [-3.175, 1.000], loss: 1.365030, mean_absolute_error: 35.529251, mean_q: 21.773779, mean_eps: 0.643379
  396403/2000000: episode: 3873, duration: 1.521s, episode steps: 107, steps per second: 70, episode reward: -94.279, mean reward: -0.881 [-100.000, 12.238], mean action: 1.664 [0.000, 3.000], mean observation: 0.030 [-2.704, 1.000], loss: 1.646898, mean_absolute_error: 36.953700, mean_q: 20.382310, mean_eps: 0.643287
  396530/2000000: episode: 3874, duration: 1.805s, episode steps: 127, steps per second: 70, episode reward: -93.103, mean reward: -0.733 [-100.000, 10.585], mean action: 1.693 [0.000, 3.000], mean observation: -0.069 [-0.791, 2.767], loss: 1.263285, mean_absolute_error: 35.594819, mean_q: 22.269842, mean_eps: 0.643181
  396655/2000000: episode: 3875, duration: 1.800s, episode steps: 125, steps per second: 69, episode reward: -59.094, mean reward: -0.473 [-100.000, 16.380], mean action: 1.752 [0.000, 3.000], mean observation: -0.028 [-0.915, 2.045], loss: 1.693548, mean_absolute_error: 35.930257, mean_q: 18.712012, mean_eps: 0.643067
  396763/2000000: episode: 3876, duration: 1.585s, episode steps: 108, steps per second: 68, episode reward: -79.027, mean reward: -0.732 [-100.000, 14.896], mean action: 1.583 [0.000, 3.000], mean observation: -0.001 [-0.947, 2.897], loss: 2.682256, mean_absolute_error: 37.255638, mean_q: 18.096312, mean_eps: 0.642963
  396920/2000000: episode: 3877, duration: 2.280s, episode steps: 157, steps per second: 69, episode reward: -143.059, mean reward: -0.911 [-100.000, 8.757], mean action: 1.561 [0.000, 3.000], mean observation: 0.124 [-0.891, 1.028], loss: 2.120419, mean_absolute_error: 35.364878, mean_q: 22.000337, mean_eps: 0.642844
  397073/2000000: episode: 3878, duration: 2.228s, episode steps: 153, steps per second: 69, episode reward: -147.112, mean reward: -0.962 [-100.000, 7.450], mean action: 1.804 [0.000, 3.000], mean observation: -0.008 [-0.918, 3.407], loss: 2.297871, mean_absolute_error: 35.796490, mean_q: 21.474737, mean_eps: 0.642704
  397195/2000000: episode: 3879, duration: 1.733s, episode steps: 122, steps per second: 70, episode reward: -61.910, mean reward: -0.507 [-100.000, 18.889], mean action: 1.820 [0.000, 3.000], mean observation: -0.051 [-0.796, 2.556], loss: 1.785009, mean_absolute_error: 36.194893, mean_q: 24.900479, mean_eps: 0.642579
  397309/2000000: episode: 3880, duration: 1.670s, episode steps: 114, steps per second: 68, episode reward: -72.318, mean reward: -0.634 [-100.000, 12.481], mean action: 1.614 [0.000, 3.000], mean observation: 0.063 [-0.694, 2.550], loss: 1.412206, mean_absolute_error: 35.420557, mean_q: 21.673898, mean_eps: 0.642473
  397447/2000000: episode: 3881, duration: 2.031s, episode steps: 138, steps per second: 68, episode reward: -135.215, mean reward: -0.980 [-100.000, 13.355], mean action: 1.761 [0.000, 3.000], mean observation: 0.083 [-0.848, 2.406], loss: 1.441851, mean_absolute_error: 35.123098, mean_q: 24.371429, mean_eps: 0.642360
  397594/2000000: episode: 3882, duration: 2.112s, episode steps: 147, steps per second: 70, episode reward: -98.306, mean reward: -0.669 [-100.000, 9.706], mean action: 1.707 [0.000, 3.000], mean observation: 0.030 [-3.236, 1.017], loss: 1.332184, mean_absolute_error: 35.171722, mean_q: 24.248517, mean_eps: 0.642232
  397673/2000000: episode: 3883, duration: 1.206s, episode steps: 79, steps per second: 66, episode reward: -54.944, mean reward: -0.695 [-100.000, 124.845], mean action: 1.785 [0.000, 3.000], mean observation: -0.068 [-1.472, 1.865], loss: 1.655487, mean_absolute_error: 36.517462, mean_q: 21.766313, mean_eps: 0.642129
  397767/2000000: episode: 3884, duration: 1.305s, episode steps: 94, steps per second: 72, episode reward: -96.523, mean reward: -1.027 [-100.000, 12.265], mean action: 1.585 [0.000, 3.000], mean observation: -0.048 [-3.533, 1.000], loss: 1.575978, mean_absolute_error: 35.856481, mean_q: 23.116790, mean_eps: 0.642052
  397916/2000000: episode: 3885, duration: 2.141s, episode steps: 149, steps per second: 70, episode reward: -85.099, mean reward: -0.571 [-100.000, 9.855], mean action: 1.711 [0.000, 3.000], mean observation: 0.036 [-0.669, 2.137], loss: 1.607190, mean_absolute_error: 34.299663, mean_q: 23.701345, mean_eps: 0.641944
  397997/2000000: episode: 3886, duration: 1.233s, episode steps: 81, steps per second: 66, episode reward: -125.704, mean reward: -1.552 [-100.000, 11.504], mean action: 1.741 [0.000, 3.000], mean observation: -0.139 [-3.603, 1.000], loss: 1.120798, mean_absolute_error: 34.845946, mean_q: 20.475312, mean_eps: 0.641840
  398144/2000000: episode: 3887, duration: 2.157s, episode steps: 147, steps per second: 68, episode reward: -54.741, mean reward: -0.372 [-100.000, 15.279], mean action: 1.769 [0.000, 3.000], mean observation: 0.026 [-1.926, 1.000], loss: 2.058762, mean_absolute_error: 36.566263, mean_q: 19.791940, mean_eps: 0.641737
  398234/2000000: episode: 3888, duration: 1.294s, episode steps: 90, steps per second: 70, episode reward: -103.192, mean reward: -1.147 [-100.000, 14.516], mean action: 1.678 [0.000, 3.000], mean observation: -0.006 [-3.271, 1.000], loss: 1.574716, mean_absolute_error: 35.963618, mean_q: 22.319819, mean_eps: 0.641631
  398394/2000000: episode: 3889, duration: 2.279s, episode steps: 160, steps per second: 70, episode reward: -71.500, mean reward: -0.447 [-100.000, 17.157], mean action: 1.712 [0.000, 3.000], mean observation: -0.061 [-0.686, 1.010], loss: 1.519762, mean_absolute_error: 35.605796, mean_q: 25.061807, mean_eps: 0.641517
  398518/2000000: episode: 3890, duration: 1.770s, episode steps: 124, steps per second: 70, episode reward: -162.411, mean reward: -1.310 [-100.000, 10.017], mean action: 1.637 [0.000, 3.000], mean observation: -0.007 [-1.257, 3.539], loss: 1.690372, mean_absolute_error: 36.788585, mean_q: 20.941228, mean_eps: 0.641390
  398628/2000000: episode: 3891, duration: 1.595s, episode steps: 110, steps per second: 69, episode reward: -112.428, mean reward: -1.022 [-100.000, 12.605], mean action: 1.609 [0.000, 3.000], mean observation: 0.038 [-1.260, 1.000], loss: 1.998863, mean_absolute_error: 36.457809, mean_q: 19.883427, mean_eps: 0.641285
  398723/2000000: episode: 3892, duration: 1.412s, episode steps: 95, steps per second: 67, episode reward: -61.337, mean reward: -0.646 [-100.000, 10.190], mean action: 1.579 [0.000, 3.000], mean observation: -0.076 [-2.521, 1.000], loss: 2.338839, mean_absolute_error: 36.351104, mean_q: 19.426169, mean_eps: 0.641193
  398854/2000000: episode: 3893, duration: 2.102s, episode steps: 131, steps per second: 62, episode reward: -139.639, mean reward: -1.066 [-100.000, 8.530], mean action: 1.710 [0.000, 3.000], mean observation: 0.069 [-0.989, 1.000], loss: 2.007571, mean_absolute_error: 35.840413, mean_q: 24.077946, mean_eps: 0.641091
  398976/2000000: episode: 3894, duration: 1.759s, episode steps: 122, steps per second: 69, episode reward: -113.502, mean reward: -0.930 [-100.000, 6.584], mean action: 1.656 [0.000, 3.000], mean observation: 0.126 [-2.695, 1.000], loss: 2.345024, mean_absolute_error: 36.168944, mean_q: 23.909558, mean_eps: 0.640977
  399075/2000000: episode: 3895, duration: 1.429s, episode steps: 99, steps per second: 69, episode reward: -65.857, mean reward: -0.665 [-100.000, 19.238], mean action: 1.848 [0.000, 3.000], mean observation: -0.030 [-0.921, 1.000], loss: 2.740162, mean_absolute_error: 36.171277, mean_q: 19.682603, mean_eps: 0.640878
  399151/2000000: episode: 3896, duration: 1.106s, episode steps: 76, steps per second: 69, episode reward: -75.179, mean reward: -0.989 [-100.000, 28.560], mean action: 1.816 [0.000, 3.000], mean observation: -0.126 [-1.075, 1.824], loss: 1.695002, mean_absolute_error: 35.362364, mean_q: 21.241403, mean_eps: 0.640799
  399335/2000000: episode: 3897, duration: 2.740s, episode steps: 184, steps per second: 67, episode reward: -190.359, mean reward: -1.035 [-100.000, 4.208], mean action: 1.701 [0.000, 3.000], mean observation: 0.136 [-0.595, 1.001], loss: 1.964209, mean_absolute_error: 35.588382, mean_q: 21.800580, mean_eps: 0.640682
  399480/2000000: episode: 3898, duration: 2.122s, episode steps: 145, steps per second: 68, episode reward: -85.245, mean reward: -0.588 [-100.000, 12.244], mean action: 1.731 [0.000, 3.000], mean observation: 0.193 [-3.064, 1.057], loss: 1.834918, mean_absolute_error: 35.781513, mean_q: 20.407873, mean_eps: 0.640535
  399633/2000000: episode: 3899, duration: 2.246s, episode steps: 153, steps per second: 68, episode reward: -55.329, mean reward: -0.362 [-100.000, 33.671], mean action: 1.791 [0.000, 3.000], mean observation: 0.060 [-0.946, 1.705], loss: 1.948781, mean_absolute_error: 36.872452, mean_q: 18.640282, mean_eps: 0.640400
  399759/2000000: episode: 3900, duration: 1.780s, episode steps: 126, steps per second: 71, episode reward: -144.602, mean reward: -1.148 [-100.000, 10.270], mean action: 1.540 [0.000, 3.000], mean observation: 0.087 [-1.031, 3.192], loss: 1.930299, mean_absolute_error: 35.999029, mean_q: 23.129087, mean_eps: 0.640274
  399840/2000000: episode: 3901, duration: 1.208s, episode steps: 81, steps per second: 67, episode reward: -80.088, mean reward: -0.989 [-100.000, 9.881], mean action: 1.642 [0.000, 3.000], mean observation: 0.015 [-3.579, 1.000], loss: 1.406705, mean_absolute_error: 35.236101, mean_q: 19.539510, mean_eps: 0.640182
  399931/2000000: episode: 3902, duration: 1.308s, episode steps: 91, steps per second: 70, episode reward: -93.368, mean reward: -1.026 [-100.000, 7.437], mean action: 1.802 [0.000, 3.000], mean observation: 0.152 [-0.961, 3.135], loss: 2.789461, mean_absolute_error: 36.187416, mean_q: 21.237761, mean_eps: 0.640104
  400070/2000000: episode: 3903, duration: 1.997s, episode steps: 139, steps per second: 70, episode reward: -91.282, mean reward: -0.657 [-100.000, 12.007], mean action: 1.698 [0.000, 3.000], mean observation: -0.038 [-0.950, 3.202], loss: 1.573722, mean_absolute_error: 35.912926, mean_q: 21.973434, mean_eps: 0.640000
  400144/2000000: episode: 3904, duration: 1.093s, episode steps: 74, steps per second: 68, episode reward: -110.733, mean reward: -1.496 [-100.000, 23.550], mean action: 1.554 [0.000, 3.000], mean observation: -0.120 [-2.150, 1.000], loss: 1.157956, mean_absolute_error: 36.098823, mean_q: 23.855452, mean_eps: 0.639905
  400266/2000000: episode: 3905, duration: 1.755s, episode steps: 122, steps per second: 70, episode reward: -191.557, mean reward: -1.570 [-100.000, 8.291], mean action: 1.705 [0.000, 3.000], mean observation: 0.135 [-1.519, 1.000], loss: 1.324125, mean_absolute_error: 35.845352, mean_q: 23.124986, mean_eps: 0.639816
  400514/2000000: episode: 3906, duration: 3.583s, episode steps: 248, steps per second: 69, episode reward: -62.283, mean reward: -0.251 [-100.000, 18.405], mean action: 1.669 [0.000, 3.000], mean observation: 0.124 [-0.993, 1.000], loss: 1.585479, mean_absolute_error: 35.511967, mean_q: 20.900570, mean_eps: 0.639649
  400588/2000000: episode: 3907, duration: 1.098s, episode steps: 74, steps per second: 67, episode reward: -107.294, mean reward: -1.450 [-100.000, 8.505], mean action: 1.635 [0.000, 3.000], mean observation: 0.038 [-3.845, 1.000], loss: 1.495507, mean_absolute_error: 36.774542, mean_q: 19.101928, mean_eps: 0.639505
  400701/2000000: episode: 3908, duration: 1.643s, episode steps: 113, steps per second: 69, episode reward: -83.701, mean reward: -0.741 [-100.000, 11.739], mean action: 1.796 [0.000, 3.000], mean observation: -0.095 [-0.689, 1.013], loss: 1.352632, mean_absolute_error: 36.232430, mean_q: 21.325594, mean_eps: 0.639420
  400798/2000000: episode: 3909, duration: 1.379s, episode steps: 97, steps per second: 70, episode reward: -107.682, mean reward: -1.110 [-100.000, 9.595], mean action: 1.814 [0.000, 3.000], mean observation: -0.127 [-0.992, 3.032], loss: 1.661814, mean_absolute_error: 34.800306, mean_q: 22.451666, mean_eps: 0.639325
  400936/2000000: episode: 3910, duration: 2.093s, episode steps: 138, steps per second: 66, episode reward: -82.440, mean reward: -0.597 [-100.000, 11.071], mean action: 1.732 [0.000, 3.000], mean observation: 0.001 [-0.772, 1.000], loss: 1.253859, mean_absolute_error: 36.155335, mean_q: 20.626691, mean_eps: 0.639221
  401038/2000000: episode: 3911, duration: 1.559s, episode steps: 102, steps per second: 65, episode reward: -86.389, mean reward: -0.847 [-100.000, 17.547], mean action: 1.667 [0.000, 3.000], mean observation: 0.162 [-3.123, 1.000], loss: 1.842073, mean_absolute_error: 35.342727, mean_q: 17.410245, mean_eps: 0.639113
  401117/2000000: episode: 3912, duration: 1.155s, episode steps: 79, steps per second: 68, episode reward: -106.078, mean reward: -1.343 [-100.000, 7.095], mean action: 1.671 [0.000, 3.000], mean observation: -0.090 [-1.049, 3.648], loss: 1.688406, mean_absolute_error: 35.568547, mean_q: 21.551617, mean_eps: 0.639030
  401216/2000000: episode: 3913, duration: 1.450s, episode steps: 99, steps per second: 68, episode reward: -63.175, mean reward: -0.638 [-100.000, 12.706], mean action: 1.626 [0.000, 3.000], mean observation: 0.067 [-2.369, 1.000], loss: 1.184619, mean_absolute_error: 34.834040, mean_q: 20.462086, mean_eps: 0.638951
  401383/2000000: episode: 3914, duration: 2.395s, episode steps: 167, steps per second: 70, episode reward: -97.347, mean reward: -0.583 [-100.000, 6.359], mean action: 1.557 [0.000, 3.000], mean observation: 0.017 [-3.708, 1.005], loss: 1.792620, mean_absolute_error: 35.932763, mean_q: 18.780783, mean_eps: 0.638832
  401471/2000000: episode: 3915, duration: 1.274s, episode steps: 88, steps per second: 69, episode reward: -64.616, mean reward: -0.734 [-100.000, 10.369], mean action: 1.864 [0.000, 3.000], mean observation: -0.107 [-0.859, 2.514], loss: 1.131758, mean_absolute_error: 35.042663, mean_q: 25.991474, mean_eps: 0.638717
  401581/2000000: episode: 3916, duration: 1.592s, episode steps: 110, steps per second: 69, episode reward: -110.428, mean reward: -1.004 [-100.000, 6.570], mean action: 1.718 [0.000, 3.000], mean observation: 0.018 [-3.384, 1.000], loss: 1.668612, mean_absolute_error: 35.769644, mean_q: 21.864492, mean_eps: 0.638627
  401658/2000000: episode: 3917, duration: 1.081s, episode steps: 77, steps per second: 71, episode reward: -114.208, mean reward: -1.483 [-100.000, 6.299], mean action: 1.649 [0.000, 3.000], mean observation: 0.036 [-3.837, 1.000], loss: 1.774967, mean_absolute_error: 35.258100, mean_q: 24.458720, mean_eps: 0.638542
  402658/2000000: episode: 3918, duration: 15.330s, episode steps: 1000, steps per second: 65, episode reward: 53.298, mean reward: 0.053 [-23.453, 36.002], mean action: 1.569 [0.000, 3.000], mean observation: 0.095 [-0.786, 1.000], loss: 1.696409, mean_absolute_error: 35.577466, mean_q: 21.058351, mean_eps: 0.638058
  402824/2000000: episode: 3919, duration: 2.433s, episode steps: 166, steps per second: 68, episode reward: -124.025, mean reward: -0.747 [-100.000, 6.568], mean action: 1.645 [0.000, 3.000], mean observation: 0.012 [-3.183, 1.000], loss: 1.997270, mean_absolute_error: 36.382002, mean_q: 19.670193, mean_eps: 0.637534
  403063/2000000: episode: 3920, duration: 3.456s, episode steps: 239, steps per second: 69, episode reward: -77.514, mean reward: -0.324 [-100.000, 16.981], mean action: 1.841 [0.000, 3.000], mean observation: 0.039 [-0.778, 2.384], loss: 1.523984, mean_absolute_error: 36.057815, mean_q: 21.227395, mean_eps: 0.637352
  403177/2000000: episode: 3921, duration: 1.651s, episode steps: 114, steps per second: 69, episode reward: -114.024, mean reward: -1.000 [-100.000, 9.774], mean action: 1.579 [0.000, 3.000], mean observation: -0.043 [-1.067, 3.511], loss: 1.420028, mean_absolute_error: 35.384116, mean_q: 24.120914, mean_eps: 0.637192
  403287/2000000: episode: 3922, duration: 1.551s, episode steps: 110, steps per second: 71, episode reward: -43.179, mean reward: -0.393 [-100.000, 21.465], mean action: 1.709 [0.000, 3.000], mean observation: 0.121 [-1.629, 1.000], loss: 2.213848, mean_absolute_error: 35.686254, mean_q: 22.913840, mean_eps: 0.637091
  403424/2000000: episode: 3923, duration: 1.988s, episode steps: 137, steps per second: 69, episode reward: -90.422, mean reward: -0.660 [-100.000, 9.074], mean action: 1.759 [0.000, 3.000], mean observation: -0.059 [-2.759, 1.000], loss: 2.088849, mean_absolute_error: 35.613983, mean_q: 20.548276, mean_eps: 0.636981
  403658/2000000: episode: 3924, duration: 3.396s, episode steps: 234, steps per second: 69, episode reward: -115.574, mean reward: -0.494 [-100.000, 14.719], mean action: 1.628 [0.000, 3.000], mean observation: 0.095 [-2.805, 1.090], loss: 1.592970, mean_absolute_error: 34.957731, mean_q: 22.630863, mean_eps: 0.636814
  404658/2000000: episode: 3925, duration: 16.408s, episode steps: 1000, steps per second: 61, episode reward: 23.962, mean reward: 0.024 [-22.643, 23.352], mean action: 1.551 [0.000, 3.000], mean observation: 0.186 [-0.708, 1.537], loss: 1.920633, mean_absolute_error: 35.461065, mean_q: 21.270924, mean_eps: 0.636258
  404820/2000000: episode: 3926, duration: 2.377s, episode steps: 162, steps per second: 68, episode reward: -13.891, mean reward: -0.086 [-100.000, 16.440], mean action: 1.704 [0.000, 3.000], mean observation: 0.004 [-1.341, 1.000], loss: 1.513691, mean_absolute_error: 35.130910, mean_q: 20.888447, mean_eps: 0.635736
  404967/2000000: episode: 3927, duration: 2.124s, episode steps: 147, steps per second: 69, episode reward: -87.676, mean reward: -0.596 [-100.000, 10.722], mean action: 1.687 [0.000, 3.000], mean observation: 0.112 [-1.573, 1.000], loss: 1.742246, mean_absolute_error: 36.021577, mean_q: 20.939975, mean_eps: 0.635597
  405118/2000000: episode: 3928, duration: 2.175s, episode steps: 151, steps per second: 69, episode reward: -77.039, mean reward: -0.510 [-100.000, 17.775], mean action: 1.722 [0.000, 3.000], mean observation: -0.096 [-0.742, 1.810], loss: 1.702278, mean_absolute_error: 35.119082, mean_q: 19.756872, mean_eps: 0.635462
  405261/2000000: episode: 3929, duration: 2.069s, episode steps: 143, steps per second: 69, episode reward: -100.029, mean reward: -0.700 [-100.000, 12.343], mean action: 1.706 [0.000, 3.000], mean observation: 0.096 [-1.030, 3.600], loss: 2.167731, mean_absolute_error: 35.775944, mean_q: 21.193943, mean_eps: 0.635329
  405340/2000000: episode: 3930, duration: 1.159s, episode steps: 79, steps per second: 68, episode reward: -103.880, mean reward: -1.315 [-100.000, 17.289], mean action: 1.506 [0.000, 3.000], mean observation: 0.109 [-2.974, 1.000], loss: 2.354650, mean_absolute_error: 36.309487, mean_q: 20.299661, mean_eps: 0.635230
  405423/2000000: episode: 3931, duration: 1.195s, episode steps: 83, steps per second: 69, episode reward: -100.116, mean reward: -1.206 [-100.000, 12.484], mean action: 1.554 [0.000, 3.000], mean observation: 0.137 [-0.931, 1.000], loss: 1.725539, mean_absolute_error: 35.681179, mean_q: 20.470435, mean_eps: 0.635158
  405507/2000000: episode: 3932, duration: 1.228s, episode steps: 84, steps per second: 68, episode reward: -96.912, mean reward: -1.154 [-100.000, 9.048], mean action: 1.714 [0.000, 3.000], mean observation: 0.042 [-3.206, 1.000], loss: 1.489560, mean_absolute_error: 35.756560, mean_q: 19.341053, mean_eps: 0.635082
  405601/2000000: episode: 3933, duration: 1.366s, episode steps: 94, steps per second: 69, episode reward: -64.620, mean reward: -0.687 [-100.000, 9.818], mean action: 1.649 [0.000, 3.000], mean observation: 0.026 [-2.667, 1.000], loss: 1.441487, mean_absolute_error: 35.146824, mean_q: 18.997720, mean_eps: 0.635001
  405707/2000000: episode: 3934, duration: 1.488s, episode steps: 106, steps per second: 71, episode reward: -117.964, mean reward: -1.113 [-100.000, 20.243], mean action: 1.679 [0.000, 3.000], mean observation: -0.024 [-1.115, 3.660], loss: 2.786733, mean_absolute_error: 37.386377, mean_q: 20.045759, mean_eps: 0.634911
  405801/2000000: episode: 3935, duration: 1.389s, episode steps: 94, steps per second: 68, episode reward: -121.196, mean reward: -1.289 [-100.000, 10.182], mean action: 1.617 [0.000, 3.000], mean observation: 0.132 [-4.538, 1.000], loss: 1.977986, mean_absolute_error: 36.519710, mean_q: 18.074468, mean_eps: 0.634821
  405900/2000000: episode: 3936, duration: 1.414s, episode steps: 99, steps per second: 70, episode reward: -57.528, mean reward: -0.581 [-100.000, 16.326], mean action: 1.636 [0.000, 3.000], mean observation: 0.057 [-2.147, 1.000], loss: 1.553975, mean_absolute_error: 36.234369, mean_q: 23.539112, mean_eps: 0.634735
  406022/2000000: episode: 3937, duration: 1.778s, episode steps: 122, steps per second: 69, episode reward: -106.608, mean reward: -0.874 [-100.000, 6.679], mean action: 1.730 [0.000, 3.000], mean observation: -0.009 [-2.944, 1.000], loss: 1.446613, mean_absolute_error: 35.556691, mean_q: 24.101289, mean_eps: 0.634636
  407022/2000000: episode: 3938, duration: 15.288s, episode steps: 1000, steps per second: 65, episode reward: 46.798, mean reward: 0.047 [-24.923, 32.244], mean action: 1.612 [0.000, 3.000], mean observation: 0.233 [-0.937, 1.593], loss: 1.621884, mean_absolute_error: 35.568773, mean_q: 22.009991, mean_eps: 0.634130
  407156/2000000: episode: 3939, duration: 1.966s, episode steps: 134, steps per second: 68, episode reward: -91.851, mean reward: -0.685 [-100.000, 6.323], mean action: 1.709 [0.000, 3.000], mean observation: -0.045 [-0.809, 3.104], loss: 2.515816, mean_absolute_error: 36.199709, mean_q: 20.241832, mean_eps: 0.633621
  407250/2000000: episode: 3940, duration: 1.392s, episode steps: 94, steps per second: 68, episode reward: -100.409, mean reward: -1.068 [-100.000, 17.063], mean action: 1.766 [0.000, 3.000], mean observation: -0.094 [-1.103, 1.000], loss: 1.868853, mean_absolute_error: 36.256014, mean_q: 22.851347, mean_eps: 0.633518
  408250/2000000: episode: 3941, duration: 15.468s, episode steps: 1000, steps per second: 65, episode reward: -16.650, mean reward: -0.017 [-24.214, 24.170], mean action: 1.605 [0.000, 3.000], mean observation: 0.264 [-0.703, 1.294], loss: 1.917409, mean_absolute_error: 36.008437, mean_q: 20.527222, mean_eps: 0.633025
  408428/2000000: episode: 3942, duration: 2.599s, episode steps: 178, steps per second: 68, episode reward: -42.212, mean reward: -0.237 [-100.000, 11.824], mean action: 1.697 [0.000, 3.000], mean observation: 0.052 [-1.936, 1.000], loss: 1.617904, mean_absolute_error: 36.511705, mean_q: 21.548505, mean_eps: 0.632496
  408527/2000000: episode: 3943, duration: 1.436s, episode steps: 99, steps per second: 69, episode reward: -78.270, mean reward: -0.791 [-100.000, 6.806], mean action: 1.707 [0.000, 3.000], mean observation: -0.047 [-2.424, 1.000], loss: 1.693247, mean_absolute_error: 36.676843, mean_q: 20.733676, mean_eps: 0.632372
  408635/2000000: episode: 3944, duration: 1.557s, episode steps: 108, steps per second: 69, episode reward: -63.238, mean reward: -0.586 [-100.000, 10.904], mean action: 1.843 [0.000, 3.000], mean observation: -0.087 [-1.485, 1.000], loss: 1.335243, mean_absolute_error: 35.597163, mean_q: 22.268536, mean_eps: 0.632278
  408724/2000000: episode: 3945, duration: 1.298s, episode steps: 89, steps per second: 69, episode reward: -97.079, mean reward: -1.091 [-100.000, 16.618], mean action: 1.584 [0.000, 3.000], mean observation: 0.119 [-1.127, 3.589], loss: 2.530096, mean_absolute_error: 36.063361, mean_q: 17.385081, mean_eps: 0.632190
  408898/2000000: episode: 3946, duration: 2.636s, episode steps: 174, steps per second: 66, episode reward: -110.267, mean reward: -0.634 [-100.000, 7.540], mean action: 1.672 [0.000, 3.000], mean observation: -0.037 [-1.022, 3.156], loss: 1.714289, mean_absolute_error: 35.215791, mean_q: 23.097847, mean_eps: 0.632071
  409098/2000000: episode: 3947, duration: 2.936s, episode steps: 200, steps per second: 68, episode reward: -80.776, mean reward: -0.404 [-100.000, 8.453], mean action: 1.635 [0.000, 3.000], mean observation: 0.018 [-2.550, 1.026], loss: 1.666701, mean_absolute_error: 34.588852, mean_q: 22.392347, mean_eps: 0.631902
  409213/2000000: episode: 3948, duration: 1.669s, episode steps: 115, steps per second: 69, episode reward: -76.955, mean reward: -0.669 [-100.000, 12.468], mean action: 1.748 [0.000, 3.000], mean observation: -0.037 [-0.960, 2.901], loss: 1.149962, mean_absolute_error: 34.751867, mean_q: 23.035108, mean_eps: 0.631760
  409346/2000000: episode: 3949, duration: 1.913s, episode steps: 133, steps per second: 70, episode reward: -122.550, mean reward: -0.921 [-100.000, 12.045], mean action: 1.699 [0.000, 3.000], mean observation: 0.040 [-0.782, 1.000], loss: 2.304831, mean_absolute_error: 36.486154, mean_q: 24.471403, mean_eps: 0.631648
  409418/2000000: episode: 3950, duration: 1.037s, episode steps: 72, steps per second: 69, episode reward: -150.307, mean reward: -2.088 [-100.000, 7.521], mean action: 1.639 [0.000, 3.000], mean observation: -0.144 [-3.819, 1.000], loss: 1.352664, mean_absolute_error: 34.900009, mean_q: 21.600526, mean_eps: 0.631556
  409514/2000000: episode: 3951, duration: 1.403s, episode steps: 96, steps per second: 68, episode reward: -95.571, mean reward: -0.996 [-100.000, 6.915], mean action: 1.708 [0.000, 3.000], mean observation: -0.112 [-0.909, 1.000], loss: 2.305339, mean_absolute_error: 36.047349, mean_q: 17.269729, mean_eps: 0.631481
  409619/2000000: episode: 3952, duration: 1.494s, episode steps: 105, steps per second: 70, episode reward: -114.283, mean reward: -1.088 [-100.000, 9.037], mean action: 1.562 [0.000, 3.000], mean observation: 0.099 [-4.222, 1.000], loss: 1.770338, mean_absolute_error: 36.725410, mean_q: 19.649596, mean_eps: 0.631391
  409771/2000000: episode: 3953, duration: 2.176s, episode steps: 152, steps per second: 70, episode reward: -68.168, mean reward: -0.448 [-100.000, 10.092], mean action: 1.684 [0.000, 3.000], mean observation: -0.013 [-0.793, 1.805], loss: 2.005526, mean_absolute_error: 35.685669, mean_q: 23.022967, mean_eps: 0.631275
  410061/2000000: episode: 3954, duration: 4.255s, episode steps: 290, steps per second: 68, episode reward: -151.337, mean reward: -0.522 [-100.000, 9.880], mean action: 1.693 [0.000, 3.000], mean observation: 0.092 [-2.693, 1.000], loss: 1.834180, mean_absolute_error: 36.091096, mean_q: 19.743627, mean_eps: 0.631076
  410243/2000000: episode: 3955, duration: 2.571s, episode steps: 182, steps per second: 71, episode reward: -173.052, mean reward: -0.951 [-100.000, 4.809], mean action: 1.659 [0.000, 3.000], mean observation: 0.149 [-0.537, 1.002], loss: 1.883291, mean_absolute_error: 35.632215, mean_q: 21.974346, mean_eps: 0.630863
  410354/2000000: episode: 3956, duration: 1.623s, episode steps: 111, steps per second: 68, episode reward: -78.102, mean reward: -0.704 [-100.000, 14.128], mean action: 1.694 [0.000, 3.000], mean observation: 0.030 [-3.103, 1.000], loss: 1.736362, mean_absolute_error: 35.847170, mean_q: 20.231992, mean_eps: 0.630732
  410453/2000000: episode: 3957, duration: 1.403s, episode steps: 99, steps per second: 71, episode reward: -107.292, mean reward: -1.084 [-100.000, 24.979], mean action: 1.636 [0.000, 3.000], mean observation: 0.045 [-1.054, 1.000], loss: 1.465957, mean_absolute_error: 35.446009, mean_q: 26.467985, mean_eps: 0.630636
  410533/2000000: episode: 3958, duration: 1.132s, episode steps: 80, steps per second: 71, episode reward: -126.275, mean reward: -1.578 [-100.000, 7.237], mean action: 1.488 [0.000, 3.000], mean observation: 0.112 [-3.114, 1.000], loss: 2.393577, mean_absolute_error: 37.461376, mean_q: 17.028302, mean_eps: 0.630555
  410608/2000000: episode: 3959, duration: 1.098s, episode steps: 75, steps per second: 68, episode reward: -96.169, mean reward: -1.282 [-100.000, 13.363], mean action: 1.587 [0.000, 3.000], mean observation: 0.047 [-2.696, 1.000], loss: 1.839397, mean_absolute_error: 36.556114, mean_q: 20.897279, mean_eps: 0.630487
  410754/2000000: episode: 3960, duration: 2.186s, episode steps: 146, steps per second: 67, episode reward: -135.896, mean reward: -0.931 [-100.000, 11.539], mean action: 1.685 [0.000, 3.000], mean observation: 0.064 [-0.867, 1.000], loss: 1.678787, mean_absolute_error: 36.994606, mean_q: 20.565373, mean_eps: 0.630388
  410867/2000000: episode: 3961, duration: 1.821s, episode steps: 113, steps per second: 62, episode reward: -94.465, mean reward: -0.836 [-100.000, 31.730], mean action: 1.558 [0.000, 3.000], mean observation: 0.134 [-0.966, 1.000], loss: 2.022565, mean_absolute_error: 36.620684, mean_q: 24.562649, mean_eps: 0.630271
  411019/2000000: episode: 3962, duration: 2.241s, episode steps: 152, steps per second: 68, episode reward: -39.735, mean reward: -0.261 [-100.000, 17.864], mean action: 1.605 [0.000, 3.000], mean observation: 0.001 [-0.765, 1.464], loss: 1.189131, mean_absolute_error: 34.632434, mean_q: 21.286806, mean_eps: 0.630152
  411108/2000000: episode: 3963, duration: 1.322s, episode steps: 89, steps per second: 67, episode reward: -123.282, mean reward: -1.385 [-100.000, 10.358], mean action: 1.640 [0.000, 3.000], mean observation: 0.036 [-1.218, 3.683], loss: 1.198900, mean_absolute_error: 34.285016, mean_q: 23.567011, mean_eps: 0.630044
  411243/2000000: episode: 3964, duration: 2.019s, episode steps: 135, steps per second: 67, episode reward: -120.947, mean reward: -0.896 [-100.000, 12.789], mean action: 1.607 [0.000, 3.000], mean observation: 0.041 [-3.226, 1.000], loss: 1.815990, mean_absolute_error: 35.941731, mean_q: 22.197952, mean_eps: 0.629943
  411424/2000000: episode: 3965, duration: 2.754s, episode steps: 181, steps per second: 66, episode reward: -146.170, mean reward: -0.808 [-100.000, 18.052], mean action: 1.691 [0.000, 3.000], mean observation: -0.006 [-0.574, 1.000], loss: 1.424380, mean_absolute_error: 35.210562, mean_q: 21.878706, mean_eps: 0.629801
  411575/2000000: episode: 3966, duration: 2.276s, episode steps: 151, steps per second: 66, episode reward: -95.071, mean reward: -0.630 [-100.000, 23.991], mean action: 1.689 [0.000, 3.000], mean observation: 0.017 [-1.039, 1.000], loss: 1.675344, mean_absolute_error: 34.939105, mean_q: 23.099666, mean_eps: 0.629652
  411660/2000000: episode: 3967, duration: 1.259s, episode steps: 85, steps per second: 68, episode reward: -114.003, mean reward: -1.341 [-100.000, 11.284], mean action: 1.600 [0.000, 3.000], mean observation: -0.089 [-3.327, 1.000], loss: 1.746993, mean_absolute_error: 35.449959, mean_q: 22.697617, mean_eps: 0.629546
  411749/2000000: episode: 3968, duration: 1.428s, episode steps: 89, steps per second: 62, episode reward: -274.282, mean reward: -3.082 [-100.000, 88.369], mean action: 1.831 [0.000, 3.000], mean observation: 0.071 [-1.155, 2.513], loss: 1.358155, mean_absolute_error: 35.808051, mean_q: 20.616439, mean_eps: 0.629466
  411850/2000000: episode: 3969, duration: 1.451s, episode steps: 101, steps per second: 70, episode reward: -89.379, mean reward: -0.885 [-100.000, 22.205], mean action: 1.673 [0.000, 3.000], mean observation: 0.125 [-0.855, 1.000], loss: 2.357331, mean_absolute_error: 36.034368, mean_q: 21.718340, mean_eps: 0.629380
  411937/2000000: episode: 3970, duration: 1.327s, episode steps: 87, steps per second: 66, episode reward: -99.116, mean reward: -1.139 [-100.000, 7.360], mean action: 1.701 [0.000, 3.000], mean observation: 0.083 [-4.047, 1.000], loss: 1.180863, mean_absolute_error: 34.699507, mean_q: 19.975332, mean_eps: 0.629295
  412051/2000000: episode: 3971, duration: 1.622s, episode steps: 114, steps per second: 70, episode reward: -104.027, mean reward: -0.913 [-100.000, 16.477], mean action: 1.711 [0.000, 3.000], mean observation: 0.132 [-0.907, 1.000], loss: 1.648845, mean_absolute_error: 35.674901, mean_q: 20.820373, mean_eps: 0.629205
  412207/2000000: episode: 3972, duration: 2.235s, episode steps: 156, steps per second: 70, episode reward: -96.284, mean reward: -0.617 [-100.000, 6.382], mean action: 1.654 [0.000, 3.000], mean observation: 0.022 [-0.819, 2.591], loss: 1.621780, mean_absolute_error: 36.484476, mean_q: 20.899617, mean_eps: 0.629085
  412399/2000000: episode: 3973, duration: 2.763s, episode steps: 192, steps per second: 69, episode reward: -89.305, mean reward: -0.465 [-100.000, 14.961], mean action: 1.568 [0.000, 3.000], mean observation: 0.031 [-0.993, 3.798], loss: 2.907348, mean_absolute_error: 36.696475, mean_q: 21.905739, mean_eps: 0.628928
  412498/2000000: episode: 3974, duration: 1.437s, episode steps: 99, steps per second: 69, episode reward: -65.666, mean reward: -0.663 [-100.000, 17.447], mean action: 1.636 [0.000, 3.000], mean observation: 0.146 [-0.772, 1.000], loss: 1.279701, mean_absolute_error: 35.751956, mean_q: 22.823580, mean_eps: 0.628797
  413498/2000000: episode: 3975, duration: 15.724s, episode steps: 1000, steps per second: 64, episode reward: 68.301, mean reward: 0.068 [-23.224, 23.476], mean action: 1.533 [0.000, 3.000], mean observation: 0.160 [-1.493, 1.000], loss: 1.605486, mean_absolute_error: 35.638831, mean_q: 21.112153, mean_eps: 0.628302
  413583/2000000: episode: 3976, duration: 1.230s, episode steps: 85, steps per second: 69, episode reward: -74.924, mean reward: -0.881 [-100.000, 10.883], mean action: 1.706 [0.000, 3.000], mean observation: 0.007 [-0.995, 1.000], loss: 1.791363, mean_absolute_error: 35.217006, mean_q: 21.901668, mean_eps: 0.627814
  413718/2000000: episode: 3977, duration: 1.959s, episode steps: 135, steps per second: 69, episode reward: -86.076, mean reward: -0.638 [-100.000, 16.281], mean action: 1.496 [0.000, 3.000], mean observation: 0.021 [-0.922, 1.000], loss: 1.370651, mean_absolute_error: 35.440808, mean_q: 22.906827, mean_eps: 0.627715
  413807/2000000: episode: 3978, duration: 1.293s, episode steps: 89, steps per second: 69, episode reward: -187.008, mean reward: -2.101 [-100.000, 8.765], mean action: 1.966 [0.000, 3.000], mean observation: -0.187 [-3.565, 1.000], loss: 1.473440, mean_absolute_error: 35.079628, mean_q: 20.437749, mean_eps: 0.627614
  413914/2000000: episode: 3979, duration: 1.522s, episode steps: 107, steps per second: 70, episode reward: -136.944, mean reward: -1.280 [-100.000, 9.749], mean action: 1.738 [0.000, 3.000], mean observation: -0.066 [-1.159, 3.889], loss: 2.430815, mean_absolute_error: 35.406383, mean_q: 19.471968, mean_eps: 0.627526
  413992/2000000: episode: 3980, duration: 1.162s, episode steps: 78, steps per second: 67, episode reward: -90.383, mean reward: -1.159 [-100.000, 18.346], mean action: 1.654 [0.000, 3.000], mean observation: 0.079 [-1.050, 4.071], loss: 1.450582, mean_absolute_error: 36.547224, mean_q: 23.769544, mean_eps: 0.627443
  414102/2000000: episode: 3981, duration: 1.622s, episode steps: 110, steps per second: 68, episode reward: -43.116, mean reward: -0.392 [-100.000, 26.828], mean action: 1.773 [0.000, 3.000], mean observation: -0.139 [-0.883, 2.855], loss: 1.451538, mean_absolute_error: 35.408156, mean_q: 21.942807, mean_eps: 0.627359
  414202/2000000: episode: 3982, duration: 1.443s, episode steps: 100, steps per second: 69, episode reward: -112.964, mean reward: -1.130 [-100.000, 11.606], mean action: 1.620 [0.000, 3.000], mean observation: 0.079 [-1.080, 3.788], loss: 1.165826, mean_absolute_error: 35.134627, mean_q: 22.352169, mean_eps: 0.627263
  414328/2000000: episode: 3983, duration: 1.843s, episode steps: 126, steps per second: 68, episode reward: -87.465, mean reward: -0.694 [-100.000, 17.519], mean action: 1.762 [0.000, 3.000], mean observation: -0.100 [-0.820, 2.959], loss: 1.522211, mean_absolute_error: 34.830896, mean_q: 21.472250, mean_eps: 0.627162
  414455/2000000: episode: 3984, duration: 1.843s, episode steps: 127, steps per second: 69, episode reward: -77.760, mean reward: -0.612 [-100.000, 12.788], mean action: 1.638 [0.000, 3.000], mean observation: 0.005 [-0.953, 1.000], loss: 1.637515, mean_absolute_error: 36.110809, mean_q: 21.467167, mean_eps: 0.627049
  414562/2000000: episode: 3985, duration: 1.576s, episode steps: 107, steps per second: 68, episode reward: -77.641, mean reward: -0.726 [-100.000, 9.109], mean action: 1.850 [0.000, 3.000], mean observation: -0.129 [-0.743, 1.075], loss: 1.566060, mean_absolute_error: 35.448664, mean_q: 23.244601, mean_eps: 0.626943
  414775/2000000: episode: 3986, duration: 3.066s, episode steps: 213, steps per second: 69, episode reward: -75.804, mean reward: -0.356 [-100.000, 12.815], mean action: 1.648 [0.000, 3.000], mean observation: 0.060 [-1.459, 1.000], loss: 1.841350, mean_absolute_error: 35.550775, mean_q: 21.071884, mean_eps: 0.626799
  414870/2000000: episode: 3987, duration: 1.374s, episode steps: 95, steps per second: 69, episode reward: -111.593, mean reward: -1.175 [-100.000, 9.090], mean action: 1.758 [0.000, 3.000], mean observation: 0.002 [-3.485, 1.000], loss: 2.019564, mean_absolute_error: 35.789088, mean_q: 19.167737, mean_eps: 0.626660
  414969/2000000: episode: 3988, duration: 1.447s, episode steps: 99, steps per second: 68, episode reward: -86.640, mean reward: -0.875 [-100.000, 14.936], mean action: 1.788 [0.000, 3.000], mean observation: -0.109 [-0.854, 2.870], loss: 1.333771, mean_absolute_error: 35.440723, mean_q: 22.659391, mean_eps: 0.626572
  415185/2000000: episode: 3989, duration: 3.117s, episode steps: 216, steps per second: 69, episode reward: -93.986, mean reward: -0.435 [-100.000, 11.288], mean action: 1.639 [0.000, 3.000], mean observation: 0.074 [-0.817, 2.104], loss: 2.107649, mean_absolute_error: 36.592200, mean_q: 20.621510, mean_eps: 0.626430
  415385/2000000: episode: 3990, duration: 2.885s, episode steps: 200, steps per second: 69, episode reward: -38.369, mean reward: -0.192 [-100.000, 12.752], mean action: 1.625 [0.000, 3.000], mean observation: 0.058 [-0.783, 1.022], loss: 1.705000, mean_absolute_error: 36.529404, mean_q: 21.365453, mean_eps: 0.626243
  415461/2000000: episode: 3991, duration: 1.112s, episode steps: 76, steps per second: 68, episode reward: -114.719, mean reward: -1.509 [-100.000, 11.255], mean action: 1.513 [0.000, 3.000], mean observation: 0.022 [-1.088, 1.000], loss: 1.491560, mean_absolute_error: 35.007611, mean_q: 24.491552, mean_eps: 0.626118
  415541/2000000: episode: 3992, duration: 1.161s, episode steps: 80, steps per second: 69, episode reward: -129.060, mean reward: -1.613 [-100.000, 9.662], mean action: 1.562 [0.000, 3.000], mean observation: 0.046 [-4.175, 1.000], loss: 1.582843, mean_absolute_error: 35.095445, mean_q: 21.615987, mean_eps: 0.626048
  415642/2000000: episode: 3993, duration: 1.432s, episode steps: 101, steps per second: 71, episode reward: -55.056, mean reward: -0.545 [-100.000, 10.535], mean action: 1.634 [0.000, 3.000], mean observation: 0.064 [-2.468, 1.000], loss: 1.592153, mean_absolute_error: 34.716476, mean_q: 23.147204, mean_eps: 0.625967
  415716/2000000: episode: 3994, duration: 1.085s, episode steps: 74, steps per second: 68, episode reward: -155.261, mean reward: -2.098 [-100.000, 5.193], mean action: 1.649 [0.000, 3.000], mean observation: -0.140 [-2.649, 1.000], loss: 1.536789, mean_absolute_error: 35.755689, mean_q: 22.488140, mean_eps: 0.625890
  415818/2000000: episode: 3995, duration: 1.571s, episode steps: 102, steps per second: 65, episode reward: -157.715, mean reward: -1.546 [-100.000, 39.806], mean action: 1.686 [0.000, 3.000], mean observation: -0.057 [-1.242, 1.620], loss: 1.345575, mean_absolute_error: 35.139031, mean_q: 19.646695, mean_eps: 0.625811
  415904/2000000: episode: 3996, duration: 1.277s, episode steps: 86, steps per second: 67, episode reward: -89.086, mean reward: -1.036 [-100.000, 8.277], mean action: 1.814 [0.000, 3.000], mean observation: 0.069 [-3.599, 1.000], loss: 1.932492, mean_absolute_error: 36.446355, mean_q: 18.777548, mean_eps: 0.625726
  416028/2000000: episode: 3997, duration: 1.821s, episode steps: 124, steps per second: 68, episode reward: -109.505, mean reward: -0.883 [-100.000, 9.260], mean action: 1.710 [0.000, 3.000], mean observation: 0.081 [-0.761, 2.247], loss: 1.500577, mean_absolute_error: 36.028784, mean_q: 21.014583, mean_eps: 0.625632
  416140/2000000: episode: 3998, duration: 1.666s, episode steps: 112, steps per second: 67, episode reward: -61.508, mean reward: -0.549 [-100.000, 77.002], mean action: 1.777 [0.000, 3.000], mean observation: 0.079 [-1.070, 1.000], loss: 1.813409, mean_absolute_error: 34.847241, mean_q: 22.329571, mean_eps: 0.625526
  416247/2000000: episode: 3999, duration: 1.560s, episode steps: 107, steps per second: 69, episode reward: -47.298, mean reward: -0.442 [-100.000, 16.914], mean action: 1.738 [0.000, 3.000], mean observation: 0.101 [-2.485, 1.000], loss: 1.673675, mean_absolute_error: 35.379846, mean_q: 22.624381, mean_eps: 0.625427
  416342/2000000: episode: 4000, duration: 1.396s, episode steps: 95, steps per second: 68, episode reward: -25.213, mean reward: -0.265 [-100.000, 13.228], mean action: 1.726 [0.000, 3.000], mean observation: 0.042 [-2.899, 1.000], loss: 1.318580, mean_absolute_error: 35.580849, mean_q: 21.035575, mean_eps: 0.625335
  416444/2000000: episode: 4001, duration: 1.481s, episode steps: 102, steps per second: 69, episode reward: -113.895, mean reward: -1.117 [-100.000, 26.284], mean action: 1.735 [0.000, 3.000], mean observation: -0.161 [-1.013, 3.514], loss: 1.329277, mean_absolute_error: 35.985681, mean_q: 22.428715, mean_eps: 0.625247
  416565/2000000: episode: 4002, duration: 1.758s, episode steps: 121, steps per second: 69, episode reward: -70.202, mean reward: -0.580 [-100.000, 13.804], mean action: 1.711 [0.000, 3.000], mean observation: -0.089 [-0.802, 2.898], loss: 1.224449, mean_absolute_error: 35.136700, mean_q: 23.532121, mean_eps: 0.625146
  416640/2000000: episode: 4003, duration: 1.083s, episode steps: 75, steps per second: 69, episode reward: -101.364, mean reward: -1.352 [-100.000, 10.112], mean action: 1.680 [0.000, 3.000], mean observation: -0.161 [-2.783, 1.000], loss: 1.316498, mean_absolute_error: 36.193951, mean_q: 23.474958, mean_eps: 0.625058
  416764/2000000: episode: 4004, duration: 1.824s, episode steps: 124, steps per second: 68, episode reward: -140.406, mean reward: -1.132 [-100.000, 23.970], mean action: 1.750 [0.000, 3.000], mean observation: -0.093 [-1.249, 4.286], loss: 1.772076, mean_absolute_error: 35.475983, mean_q: 22.889058, mean_eps: 0.624970
  416891/2000000: episode: 4005, duration: 1.830s, episode steps: 127, steps per second: 69, episode reward: -64.976, mean reward: -0.512 [-100.000, 11.483], mean action: 1.630 [0.000, 3.000], mean observation: 0.110 [-0.781, 1.000], loss: 1.331081, mean_absolute_error: 34.800822, mean_q: 24.195508, mean_eps: 0.624857
  417003/2000000: episode: 4006, duration: 1.659s, episode steps: 112, steps per second: 68, episode reward: -64.620, mean reward: -0.577 [-100.000, 18.892], mean action: 1.688 [0.000, 3.000], mean observation: -0.040 [-0.974, 1.000], loss: 1.765686, mean_absolute_error: 35.656543, mean_q: 20.177672, mean_eps: 0.624749
  417120/2000000: episode: 4007, duration: 1.816s, episode steps: 117, steps per second: 64, episode reward: -123.901, mean reward: -1.059 [-100.000, 7.587], mean action: 1.675 [0.000, 3.000], mean observation: 0.028 [-2.629, 1.000], loss: 1.301655, mean_absolute_error: 35.358751, mean_q: 20.628426, mean_eps: 0.624646
  417253/2000000: episode: 4008, duration: 1.952s, episode steps: 133, steps per second: 68, episode reward: -69.616, mean reward: -0.523 [-100.000, 20.393], mean action: 1.677 [0.000, 3.000], mean observation: -0.045 [-1.637, 1.000], loss: 1.670842, mean_absolute_error: 34.720004, mean_q: 22.392209, mean_eps: 0.624533
  417368/2000000: episode: 4009, duration: 1.638s, episode steps: 115, steps per second: 70, episode reward: -88.144, mean reward: -0.766 [-100.000, 45.075], mean action: 1.548 [0.000, 3.000], mean observation: 0.123 [-4.436, 1.010], loss: 2.263294, mean_absolute_error: 36.692078, mean_q: 20.173865, mean_eps: 0.624421
  417494/2000000: episode: 4010, duration: 1.832s, episode steps: 126, steps per second: 69, episode reward: -142.110, mean reward: -1.128 [-100.000, 6.910], mean action: 1.675 [0.000, 3.000], mean observation: 0.088 [-1.029, 1.000], loss: 1.343358, mean_absolute_error: 34.527197, mean_q: 20.652875, mean_eps: 0.624313
  417593/2000000: episode: 4011, duration: 1.436s, episode steps: 99, steps per second: 69, episode reward: -56.376, mean reward: -0.569 [-100.000, 15.833], mean action: 1.545 [0.000, 3.000], mean observation: -0.005 [-0.879, 1.000], loss: 2.143853, mean_absolute_error: 36.063692, mean_q: 20.596019, mean_eps: 0.624210
  417777/2000000: episode: 4012, duration: 2.633s, episode steps: 184, steps per second: 70, episode reward: -90.544, mean reward: -0.492 [-100.000, 12.632], mean action: 1.658 [0.000, 3.000], mean observation: 0.018 [-3.557, 1.000], loss: 1.841330, mean_absolute_error: 35.845732, mean_q: 22.954402, mean_eps: 0.624083
  417892/2000000: episode: 4013, duration: 1.656s, episode steps: 115, steps per second: 69, episode reward: -55.789, mean reward: -0.485 [-100.000, 14.106], mean action: 1.739 [0.000, 3.000], mean observation: -0.020 [-0.727, 2.183], loss: 1.194929, mean_absolute_error: 36.267213, mean_q: 21.429076, mean_eps: 0.623949
  418016/2000000: episode: 4014, duration: 1.811s, episode steps: 124, steps per second: 68, episode reward: -107.285, mean reward: -0.865 [-100.000, 10.567], mean action: 1.661 [0.000, 3.000], mean observation: -0.090 [-3.359, 1.000], loss: 1.389861, mean_absolute_error: 36.238748, mean_q: 22.359912, mean_eps: 0.623843
  418155/2000000: episode: 4015, duration: 1.987s, episode steps: 139, steps per second: 70, episode reward: -45.812, mean reward: -0.330 [-100.000, 16.675], mean action: 1.719 [0.000, 3.000], mean observation: 0.043 [-0.982, 1.025], loss: 1.526645, mean_absolute_error: 34.865138, mean_q: 22.196462, mean_eps: 0.623724
  418256/2000000: episode: 4016, duration: 1.493s, episode steps: 101, steps per second: 68, episode reward: -35.042, mean reward: -0.347 [-100.000, 19.443], mean action: 1.604 [0.000, 3.000], mean observation: -0.023 [-0.824, 2.086], loss: 1.829221, mean_absolute_error: 35.448418, mean_q: 18.952032, mean_eps: 0.623616
  418383/2000000: episode: 4017, duration: 1.838s, episode steps: 127, steps per second: 69, episode reward: -60.196, mean reward: -0.474 [-100.000, 28.501], mean action: 1.811 [0.000, 3.000], mean observation: 0.001 [-0.746, 1.863], loss: 1.677014, mean_absolute_error: 35.771065, mean_q: 24.689908, mean_eps: 0.623514
  418493/2000000: episode: 4018, duration: 1.602s, episode steps: 110, steps per second: 69, episode reward: -84.514, mean reward: -0.768 [-100.000, 11.863], mean action: 1.745 [0.000, 3.000], mean observation: 0.020 [-1.007, 2.926], loss: 1.216853, mean_absolute_error: 35.694822, mean_q: 21.300379, mean_eps: 0.623406
  418683/2000000: episode: 4019, duration: 2.744s, episode steps: 190, steps per second: 69, episode reward: -74.669, mean reward: -0.393 [-100.000, 13.792], mean action: 1.768 [0.000, 3.000], mean observation: -0.050 [-0.735, 1.523], loss: 1.413077, mean_absolute_error: 35.200396, mean_q: 23.518140, mean_eps: 0.623271
  418849/2000000: episode: 4020, duration: 2.427s, episode steps: 166, steps per second: 68, episode reward: -79.927, mean reward: -0.481 [-100.000, 13.033], mean action: 1.723 [0.000, 3.000], mean observation: 0.015 [-0.865, 1.000], loss: 2.236291, mean_absolute_error: 36.733338, mean_q: 22.357208, mean_eps: 0.623111
  418933/2000000: episode: 4021, duration: 1.384s, episode steps: 84, steps per second: 61, episode reward: -148.225, mean reward: -1.765 [-100.000, 11.888], mean action: 1.595 [0.000, 3.000], mean observation: 0.038 [-4.090, 1.000], loss: 1.439453, mean_absolute_error: 36.249592, mean_q: 21.123691, mean_eps: 0.622997
  419025/2000000: episode: 4022, duration: 1.314s, episode steps: 92, steps per second: 70, episode reward: -120.163, mean reward: -1.306 [-100.000, 9.239], mean action: 1.522 [0.000, 3.000], mean observation: -0.082 [-1.130, 3.066], loss: 1.543552, mean_absolute_error: 34.027531, mean_q: 27.951050, mean_eps: 0.622918
  419226/2000000: episode: 4023, duration: 2.874s, episode steps: 201, steps per second: 70, episode reward: -53.989, mean reward: -0.269 [-100.000, 22.148], mean action: 1.667 [0.000, 3.000], mean observation: -0.013 [-0.561, 1.407], loss: 2.025709, mean_absolute_error: 36.041324, mean_q: 20.888210, mean_eps: 0.622787
  419311/2000000: episode: 4024, duration: 1.223s, episode steps: 85, steps per second: 69, episode reward: -142.941, mean reward: -1.682 [-100.000, 21.163], mean action: 1.588 [0.000, 3.000], mean observation: 0.084 [-1.188, 3.693], loss: 1.548547, mean_absolute_error: 35.291596, mean_q: 25.111730, mean_eps: 0.622659
  419495/2000000: episode: 4025, duration: 2.665s, episode steps: 184, steps per second: 69, episode reward: -114.071, mean reward: -0.620 [-100.000, 14.745], mean action: 1.674 [0.000, 3.000], mean observation: 0.001 [-0.957, 1.567], loss: 2.026340, mean_absolute_error: 35.592082, mean_q: 22.180793, mean_eps: 0.622538
  419648/2000000: episode: 4026, duration: 2.259s, episode steps: 153, steps per second: 68, episode reward: -119.372, mean reward: -0.780 [-100.000, 24.850], mean action: 1.712 [0.000, 3.000], mean observation: -0.076 [-0.967, 1.991], loss: 1.727137, mean_absolute_error: 34.961657, mean_q: 21.552920, mean_eps: 0.622387
  419794/2000000: episode: 4027, duration: 2.131s, episode steps: 146, steps per second: 69, episode reward: -79.567, mean reward: -0.545 [-100.000, 5.927], mean action: 1.644 [0.000, 3.000], mean observation: -0.030 [-2.738, 1.000], loss: 2.003100, mean_absolute_error: 35.722801, mean_q: 22.186437, mean_eps: 0.622252
  419904/2000000: episode: 4028, duration: 1.623s, episode steps: 110, steps per second: 68, episode reward: -78.231, mean reward: -0.711 [-100.000, 22.292], mean action: 1.691 [0.000, 3.000], mean observation: 0.017 [-0.851, 1.000], loss: 1.176541, mean_absolute_error: 33.682318, mean_q: 21.539061, mean_eps: 0.622137
  420000/2000000: episode: 4029, duration: 1.432s, episode steps: 96, steps per second: 67, episode reward: -87.668, mean reward: -0.913 [-100.000, 10.736], mean action: 1.698 [0.000, 3.000], mean observation: 0.027 [-0.929, 1.000], loss: 1.454258, mean_absolute_error: 35.696604, mean_q: 24.055136, mean_eps: 0.622045
  420080/2000000: episode: 4030, duration: 1.219s, episode steps: 80, steps per second: 66, episode reward: -118.123, mean reward: -1.477 [-100.000, 18.374], mean action: 1.550 [0.000, 3.000], mean observation: -0.104 [-3.280, 1.000], loss: 1.859793, mean_absolute_error: 35.167508, mean_q: 20.423988, mean_eps: 0.621966
  420316/2000000: episode: 4031, duration: 3.525s, episode steps: 236, steps per second: 67, episode reward: -131.798, mean reward: -0.558 [-100.000, 5.024], mean action: 1.614 [0.000, 3.000], mean observation: 0.219 [-0.315, 1.022], loss: 1.406054, mean_absolute_error: 36.393134, mean_q: 21.503277, mean_eps: 0.621824
  420462/2000000: episode: 4032, duration: 2.138s, episode steps: 146, steps per second: 68, episode reward: -143.427, mean reward: -0.982 [-100.000, 9.095], mean action: 1.603 [0.000, 3.000], mean observation: 0.117 [-0.968, 3.229], loss: 1.742361, mean_absolute_error: 35.669346, mean_q: 20.486279, mean_eps: 0.621651
  420565/2000000: episode: 4033, duration: 1.503s, episode steps: 103, steps per second: 69, episode reward: -82.529, mean reward: -0.801 [-100.000, 40.823], mean action: 1.728 [0.000, 3.000], mean observation: -0.111 [-1.050, 1.775], loss: 2.639179, mean_absolute_error: 37.525581, mean_q: 18.101366, mean_eps: 0.621537
  420666/2000000: episode: 4034, duration: 1.454s, episode steps: 101, steps per second: 69, episode reward: -104.954, mean reward: -1.039 [-100.000, 15.619], mean action: 1.683 [0.000, 3.000], mean observation: -0.009 [-3.188, 1.000], loss: 1.665090, mean_absolute_error: 35.397273, mean_q: 22.350500, mean_eps: 0.621446
  420903/2000000: episode: 4035, duration: 3.459s, episode steps: 237, steps per second: 69, episode reward: -88.480, mean reward: -0.373 [-100.000, 12.859], mean action: 1.810 [0.000, 3.000], mean observation: 0.035 [-0.546, 1.168], loss: 1.975077, mean_absolute_error: 36.584770, mean_q: 21.628180, mean_eps: 0.621294
  420998/2000000: episode: 4036, duration: 1.394s, episode steps: 95, steps per second: 68, episode reward: -101.550, mean reward: -1.069 [-100.000, 10.543], mean action: 1.653 [0.000, 3.000], mean observation: 0.020 [-3.520, 1.000], loss: 1.935016, mean_absolute_error: 37.367521, mean_q: 21.177344, mean_eps: 0.621145
  421124/2000000: episode: 4037, duration: 1.841s, episode steps: 126, steps per second: 68, episode reward: -93.459, mean reward: -0.742 [-100.000, 6.563], mean action: 1.619 [0.000, 3.000], mean observation: -0.028 [-1.047, 1.000], loss: 2.491397, mean_absolute_error: 37.175323, mean_q: 18.455489, mean_eps: 0.621046
  421253/2000000: episode: 4038, duration: 1.937s, episode steps: 129, steps per second: 67, episode reward: -60.954, mean reward: -0.473 [-100.000, 31.086], mean action: 1.643 [0.000, 3.000], mean observation: 0.028 [-2.883, 1.000], loss: 1.430637, mean_absolute_error: 35.467655, mean_q: 22.865759, mean_eps: 0.620931
  421335/2000000: episode: 4039, duration: 1.145s, episode steps: 82, steps per second: 72, episode reward: -85.542, mean reward: -1.043 [-100.000, 17.224], mean action: 1.793 [0.000, 3.000], mean observation: -0.113 [-0.996, 2.577], loss: 1.247332, mean_absolute_error: 35.152383, mean_q: 24.773195, mean_eps: 0.620835
  421498/2000000: episode: 4040, duration: 2.368s, episode steps: 163, steps per second: 69, episode reward: -94.777, mean reward: -0.581 [-100.000, 12.386], mean action: 1.656 [0.000, 3.000], mean observation: 0.114 [-0.921, 3.346], loss: 1.633443, mean_absolute_error: 35.201180, mean_q: 22.317003, mean_eps: 0.620726
  422498/2000000: episode: 4041, duration: 15.220s, episode steps: 1000, steps per second: 66, episode reward: 39.906, mean reward: 0.040 [-23.444, 43.356], mean action: 1.641 [0.000, 3.000], mean observation: 0.080 [-1.051, 1.268], loss: 1.771789, mean_absolute_error: 35.376394, mean_q: 21.066882, mean_eps: 0.620202
  422626/2000000: episode: 4042, duration: 1.850s, episode steps: 128, steps per second: 69, episode reward: -46.762, mean reward: -0.365 [-100.000, 14.584], mean action: 1.680 [0.000, 3.000], mean observation: -0.104 [-1.904, 1.000], loss: 1.507919, mean_absolute_error: 36.011051, mean_q: 22.272319, mean_eps: 0.619694
  422756/2000000: episode: 4043, duration: 1.886s, episode steps: 130, steps per second: 69, episode reward: -100.776, mean reward: -0.775 [-100.000, 9.816], mean action: 1.723 [0.000, 3.000], mean observation: -0.032 [-0.732, 2.527], loss: 1.280943, mean_absolute_error: 35.496018, mean_q: 22.991825, mean_eps: 0.619579
  422819/2000000: episode: 4044, duration: 0.927s, episode steps: 63, steps per second: 68, episode reward: -47.220, mean reward: -0.750 [-100.000, 59.228], mean action: 1.587 [0.000, 3.000], mean observation: -0.124 [-3.869, 1.000], loss: 1.162786, mean_absolute_error: 34.606657, mean_q: 18.945125, mean_eps: 0.619493
  422911/2000000: episode: 4045, duration: 1.331s, episode steps: 92, steps per second: 69, episode reward: -86.566, mean reward: -0.941 [-100.000, 20.403], mean action: 1.620 [0.000, 3.000], mean observation: 0.084 [-1.016, 1.000], loss: 1.964525, mean_absolute_error: 35.800393, mean_q: 19.883268, mean_eps: 0.619422
  422998/2000000: episode: 4046, duration: 1.286s, episode steps: 87, steps per second: 68, episode reward: -87.047, mean reward: -1.001 [-100.000, 12.834], mean action: 1.632 [0.000, 3.000], mean observation: -0.112 [-0.868, 2.938], loss: 1.266443, mean_absolute_error: 36.750431, mean_q: 23.543060, mean_eps: 0.619341
  423998/2000000: episode: 4047, duration: 15.899s, episode steps: 1000, steps per second: 63, episode reward: 79.288, mean reward: 0.079 [-24.248, 42.609], mean action: 1.608 [0.000, 3.000], mean observation: 0.167 [-1.209, 1.568], loss: 1.819828, mean_absolute_error: 35.424123, mean_q: 21.634450, mean_eps: 0.618852
  424143/2000000: episode: 4048, duration: 2.090s, episode steps: 145, steps per second: 69, episode reward: -79.179, mean reward: -0.546 [-100.000, 9.951], mean action: 1.621 [0.000, 3.000], mean observation: 0.004 [-0.816, 1.000], loss: 2.130307, mean_absolute_error: 36.117995, mean_q: 21.064977, mean_eps: 0.618337
  424233/2000000: episode: 4049, duration: 1.325s, episode steps: 90, steps per second: 68, episode reward: -126.899, mean reward: -1.410 [-100.000, 8.157], mean action: 1.711 [0.000, 3.000], mean observation: -0.141 [-1.095, 1.000], loss: 2.806070, mean_absolute_error: 36.661566, mean_q: 16.809715, mean_eps: 0.618231
  424351/2000000: episode: 4050, duration: 1.669s, episode steps: 118, steps per second: 71, episode reward: -53.346, mean reward: -0.452 [-100.000, 18.871], mean action: 1.653 [0.000, 3.000], mean observation: -0.080 [-0.778, 1.000], loss: 1.667128, mean_absolute_error: 35.543472, mean_q: 19.830092, mean_eps: 0.618137
  424518/2000000: episode: 4051, duration: 2.386s, episode steps: 167, steps per second: 70, episode reward: -36.058, mean reward: -0.216 [-100.000, 20.542], mean action: 1.653 [0.000, 3.000], mean observation: 0.038 [-0.987, 1.000], loss: 2.343827, mean_absolute_error: 36.866688, mean_q: 19.494457, mean_eps: 0.618009
  424615/2000000: episode: 4052, duration: 1.372s, episode steps: 97, steps per second: 71, episode reward: -87.759, mean reward: -0.905 [-100.000, 14.431], mean action: 1.629 [0.000, 3.000], mean observation: -0.041 [-1.245, 1.000], loss: 1.591180, mean_absolute_error: 34.688403, mean_q: 20.976757, mean_eps: 0.617891
  424716/2000000: episode: 4053, duration: 1.502s, episode steps: 101, steps per second: 67, episode reward: -65.362, mean reward: -0.647 [-100.000, 7.210], mean action: 1.782 [0.000, 3.000], mean observation: -0.056 [-0.868, 2.654], loss: 1.469466, mean_absolute_error: 35.991618, mean_q: 22.439507, mean_eps: 0.617802
  424842/2000000: episode: 4054, duration: 1.852s, episode steps: 126, steps per second: 68, episode reward: -92.215, mean reward: -0.732 [-100.000, 11.483], mean action: 1.651 [0.000, 3.000], mean observation: 0.072 [-1.141, 1.010], loss: 1.517229, mean_absolute_error: 35.964476, mean_q: 20.921400, mean_eps: 0.617700
  424945/2000000: episode: 4055, duration: 1.490s, episode steps: 103, steps per second: 69, episode reward: -77.648, mean reward: -0.754 [-100.000, 14.400], mean action: 1.718 [0.000, 3.000], mean observation: 0.044 [-1.023, 1.000], loss: 1.549924, mean_absolute_error: 36.342603, mean_q: 21.699657, mean_eps: 0.617595
  425030/2000000: episode: 4056, duration: 1.216s, episode steps: 85, steps per second: 70, episode reward: -65.716, mean reward: -0.773 [-100.000, 10.066], mean action: 1.647 [0.000, 3.000], mean observation: 0.077 [-1.002, 1.000], loss: 2.046904, mean_absolute_error: 36.129638, mean_q: 20.964087, mean_eps: 0.617511
  425132/2000000: episode: 4057, duration: 1.692s, episode steps: 102, steps per second: 60, episode reward: -127.250, mean reward: -1.248 [-100.000, 11.964], mean action: 1.686 [0.000, 3.000], mean observation: 0.025 [-1.002, 1.000], loss: 1.488431, mean_absolute_error: 34.944418, mean_q: 20.155904, mean_eps: 0.617428
  425244/2000000: episode: 4058, duration: 1.684s, episode steps: 112, steps per second: 67, episode reward: -99.069, mean reward: -0.885 [-100.000, 17.052], mean action: 1.571 [0.000, 3.000], mean observation: 0.014 [-0.951, 1.000], loss: 1.724187, mean_absolute_error: 36.177226, mean_q: 21.759272, mean_eps: 0.617333
  425319/2000000: episode: 4059, duration: 1.089s, episode steps: 75, steps per second: 69, episode reward: -109.962, mean reward: -1.466 [-100.000, 13.936], mean action: 1.787 [0.000, 3.000], mean observation: 0.072 [-1.157, 1.000], loss: 1.727255, mean_absolute_error: 35.728707, mean_q: 19.233074, mean_eps: 0.617248
  425467/2000000: episode: 4060, duration: 2.128s, episode steps: 148, steps per second: 70, episode reward: -113.400, mean reward: -0.766 [-100.000, 8.223], mean action: 1.568 [0.000, 3.000], mean observation: 0.027 [-2.770, 1.000], loss: 1.209468, mean_absolute_error: 34.620286, mean_q: 23.425782, mean_eps: 0.617147
  425539/2000000: episode: 4061, duration: 1.041s, episode steps: 72, steps per second: 69, episode reward: -102.409, mean reward: -1.422 [-100.000, 8.499], mean action: 1.792 [0.000, 3.000], mean observation: -0.135 [-1.010, 2.789], loss: 1.741879, mean_absolute_error: 37.013765, mean_q: 18.294240, mean_eps: 0.617048
  425678/2000000: episode: 4062, duration: 2.146s, episode steps: 139, steps per second: 65, episode reward: -89.268, mean reward: -0.642 [-100.000, 12.373], mean action: 1.835 [0.000, 3.000], mean observation: 0.006 [-0.862, 1.000], loss: 2.003396, mean_absolute_error: 35.999890, mean_q: 19.837495, mean_eps: 0.616953
  425843/2000000: episode: 4063, duration: 2.366s, episode steps: 165, steps per second: 70, episode reward: -116.155, mean reward: -0.704 [-100.000, 18.926], mean action: 1.764 [0.000, 3.000], mean observation: 0.061 [-2.456, 1.000], loss: 1.409207, mean_absolute_error: 35.866381, mean_q: 20.869048, mean_eps: 0.616816
  425952/2000000: episode: 4064, duration: 1.595s, episode steps: 109, steps per second: 68, episode reward: -61.924, mean reward: -0.568 [-100.000, 14.054], mean action: 1.596 [0.000, 3.000], mean observation: 0.025 [-2.131, 1.000], loss: 1.777500, mean_absolute_error: 35.627309, mean_q: 23.113700, mean_eps: 0.616694
  426026/2000000: episode: 4065, duration: 1.091s, episode steps: 74, steps per second: 68, episode reward: -124.668, mean reward: -1.685 [-100.000, 10.826], mean action: 1.689 [0.000, 3.000], mean observation: -0.095 [-1.271, 4.139], loss: 1.451461, mean_absolute_error: 36.041269, mean_q: 18.724900, mean_eps: 0.616611
  426200/2000000: episode: 4066, duration: 2.532s, episode steps: 174, steps per second: 69, episode reward: -88.000, mean reward: -0.506 [-100.000, 18.266], mean action: 1.736 [0.000, 3.000], mean observation: 0.084 [-1.226, 1.172], loss: 1.479783, mean_absolute_error: 35.258476, mean_q: 22.102620, mean_eps: 0.616499
  426291/2000000: episode: 4067, duration: 1.320s, episode steps: 91, steps per second: 69, episode reward: -136.913, mean reward: -1.505 [-100.000, 10.162], mean action: 1.692 [0.000, 3.000], mean observation: 0.067 [-0.959, 1.002], loss: 1.484519, mean_absolute_error: 35.353733, mean_q: 19.500431, mean_eps: 0.616380
  426393/2000000: episode: 4068, duration: 1.540s, episode steps: 102, steps per second: 66, episode reward: -30.216, mean reward: -0.296 [-100.000, 27.777], mean action: 1.775 [0.000, 3.000], mean observation: -0.075 [-2.310, 1.000], loss: 2.033012, mean_absolute_error: 35.537771, mean_q: 21.473596, mean_eps: 0.616292
  426553/2000000: episode: 4069, duration: 2.303s, episode steps: 160, steps per second: 69, episode reward: -86.369, mean reward: -0.540 [-100.000, 10.745], mean action: 1.663 [0.000, 3.000], mean observation: 0.011 [-2.842, 1.000], loss: 2.059775, mean_absolute_error: 36.256827, mean_q: 20.685231, mean_eps: 0.616173
  426666/2000000: episode: 4070, duration: 1.724s, episode steps: 113, steps per second: 66, episode reward: -141.837, mean reward: -1.255 [-100.000, 11.939], mean action: 1.761 [0.000, 3.000], mean observation: -0.052 [-1.044, 1.000], loss: 3.085101, mean_absolute_error: 37.016224, mean_q: 22.191868, mean_eps: 0.616051
  426816/2000000: episode: 4071, duration: 2.492s, episode steps: 150, steps per second: 60, episode reward: -141.023, mean reward: -0.940 [-100.000, 9.586], mean action: 1.687 [0.000, 3.000], mean observation: -0.062 [-1.058, 1.098], loss: 1.914855, mean_absolute_error: 35.948397, mean_q: 22.577994, mean_eps: 0.615934
  426914/2000000: episode: 4072, duration: 1.527s, episode steps: 98, steps per second: 64, episode reward: -115.617, mean reward: -1.180 [-100.000, 6.361], mean action: 1.735 [0.000, 3.000], mean observation: 0.062 [-1.261, 1.000], loss: 2.028730, mean_absolute_error: 34.924271, mean_q: 21.176016, mean_eps: 0.615822
  427036/2000000: episode: 4073, duration: 1.810s, episode steps: 122, steps per second: 67, episode reward: -147.916, mean reward: -1.212 [-100.000, 8.353], mean action: 1.779 [0.000, 3.000], mean observation: -0.115 [-0.724, 1.563], loss: 1.656385, mean_absolute_error: 34.641517, mean_q: 20.376405, mean_eps: 0.615723
  427185/2000000: episode: 4074, duration: 2.209s, episode steps: 149, steps per second: 67, episode reward: -24.276, mean reward: -0.163 [-100.000, 17.827], mean action: 1.725 [0.000, 3.000], mean observation: 0.050 [-0.868, 1.448], loss: 1.742747, mean_absolute_error: 36.790673, mean_q: 21.968921, mean_eps: 0.615601
  427378/2000000: episode: 4075, duration: 2.787s, episode steps: 193, steps per second: 69, episode reward: -55.741, mean reward: -0.289 [-100.000, 12.519], mean action: 1.668 [0.000, 3.000], mean observation: 0.087 [-1.669, 1.000], loss: 2.633171, mean_absolute_error: 35.555242, mean_q: 21.209932, mean_eps: 0.615446
  427470/2000000: episode: 4076, duration: 1.354s, episode steps: 92, steps per second: 68, episode reward: -133.011, mean reward: -1.446 [-100.000, 15.309], mean action: 1.804 [0.000, 3.000], mean observation: 0.028 [-0.991, 2.849], loss: 1.310222, mean_absolute_error: 35.489288, mean_q: 16.518250, mean_eps: 0.615318
  427679/2000000: episode: 4077, duration: 3.064s, episode steps: 209, steps per second: 68, episode reward: -101.495, mean reward: -0.486 [-100.000, 14.077], mean action: 1.742 [0.000, 3.000], mean observation: 0.022 [-0.704, 2.785], loss: 1.698829, mean_absolute_error: 35.367808, mean_q: 22.380821, mean_eps: 0.615183
  427741/2000000: episode: 4078, duration: 0.942s, episode steps: 62, steps per second: 66, episode reward: -103.862, mean reward: -1.675 [-100.000, 13.788], mean action: 1.742 [0.000, 3.000], mean observation: 0.067 [-1.105, 3.557], loss: 2.015881, mean_absolute_error: 35.324686, mean_q: 22.162520, mean_eps: 0.615061
  427875/2000000: episode: 4079, duration: 1.899s, episode steps: 134, steps per second: 71, episode reward: -101.519, mean reward: -0.758 [-100.000, 12.385], mean action: 1.493 [0.000, 3.000], mean observation: 0.059 [-1.180, 1.000], loss: 1.310624, mean_absolute_error: 35.034327, mean_q: 25.326999, mean_eps: 0.614973
  428002/2000000: episode: 4080, duration: 1.844s, episode steps: 127, steps per second: 69, episode reward: -127.155, mean reward: -1.001 [-100.000, 6.472], mean action: 1.780 [0.000, 3.000], mean observation: 0.024 [-1.014, 3.686], loss: 2.143394, mean_absolute_error: 36.850349, mean_q: 21.166754, mean_eps: 0.614856
  428170/2000000: episode: 4081, duration: 2.411s, episode steps: 168, steps per second: 70, episode reward: -101.137, mean reward: -0.602 [-100.000, 16.301], mean action: 1.690 [0.000, 3.000], mean observation: 0.028 [-0.897, 2.944], loss: 1.914279, mean_absolute_error: 36.214458, mean_q: 19.053604, mean_eps: 0.614723
  428253/2000000: episode: 4082, duration: 1.224s, episode steps: 83, steps per second: 68, episode reward: -117.678, mean reward: -1.418 [-100.000, 13.521], mean action: 1.639 [0.000, 3.000], mean observation: 0.040 [-0.936, 1.933], loss: 2.203735, mean_absolute_error: 37.273440, mean_q: 18.081666, mean_eps: 0.614609
  428358/2000000: episode: 4083, duration: 1.506s, episode steps: 105, steps per second: 70, episode reward: -95.291, mean reward: -0.908 [-100.000, 14.716], mean action: 1.648 [0.000, 3.000], mean observation: -0.025 [-0.846, 2.682], loss: 1.518179, mean_absolute_error: 35.421861, mean_q: 21.957471, mean_eps: 0.614525
  428466/2000000: episode: 4084, duration: 1.538s, episode steps: 108, steps per second: 70, episode reward: -79.908, mean reward: -0.740 [-100.000, 9.199], mean action: 1.630 [0.000, 3.000], mean observation: -0.131 [-0.793, 2.579], loss: 1.833218, mean_absolute_error: 35.505939, mean_q: 21.453644, mean_eps: 0.614429
  428552/2000000: episode: 4085, duration: 1.262s, episode steps: 86, steps per second: 68, episode reward: -123.872, mean reward: -1.440 [-100.000, 9.725], mean action: 1.663 [0.000, 3.000], mean observation: 0.117 [-1.142, 4.030], loss: 1.399949, mean_absolute_error: 33.622625, mean_q: 21.434378, mean_eps: 0.614343
  428680/2000000: episode: 4086, duration: 1.868s, episode steps: 128, steps per second: 69, episode reward: -138.041, mean reward: -1.078 [-100.000, 8.710], mean action: 1.680 [0.000, 3.000], mean observation: -0.108 [-1.005, 3.061], loss: 1.384692, mean_absolute_error: 35.463900, mean_q: 21.093385, mean_eps: 0.614247
  428797/2000000: episode: 4087, duration: 1.727s, episode steps: 117, steps per second: 68, episode reward: -74.859, mean reward: -0.640 [-100.000, 11.957], mean action: 1.735 [0.000, 3.000], mean observation: 0.071 [-0.638, 1.338], loss: 1.900911, mean_absolute_error: 34.310569, mean_q: 21.197937, mean_eps: 0.614136
  428920/2000000: episode: 4088, duration: 1.774s, episode steps: 123, steps per second: 69, episode reward: -117.179, mean reward: -0.953 [-100.000, 11.070], mean action: 1.707 [0.000, 3.000], mean observation: 0.003 [-2.310, 1.000], loss: 1.712978, mean_absolute_error: 35.884441, mean_q: 23.160692, mean_eps: 0.614028
  428997/2000000: episode: 4089, duration: 1.157s, episode steps: 77, steps per second: 67, episode reward: -95.337, mean reward: -1.238 [-100.000, 18.879], mean action: 1.532 [0.000, 3.000], mean observation: 0.049 [-3.759, 1.000], loss: 2.569896, mean_absolute_error: 36.302368, mean_q: 19.283682, mean_eps: 0.613938
  429149/2000000: episode: 4090, duration: 2.170s, episode steps: 152, steps per second: 70, episode reward: -80.113, mean reward: -0.527 [-100.000, 8.665], mean action: 1.651 [0.000, 3.000], mean observation: 0.006 [-0.836, 2.685], loss: 1.510571, mean_absolute_error: 35.491614, mean_q: 21.643796, mean_eps: 0.613833
  429250/2000000: episode: 4091, duration: 1.442s, episode steps: 101, steps per second: 70, episode reward: -92.824, mean reward: -0.919 [-100.000, 9.919], mean action: 1.594 [0.000, 3.000], mean observation: -0.022 [-0.937, 1.000], loss: 1.482229, mean_absolute_error: 35.738650, mean_q: 22.061938, mean_eps: 0.613720
  429325/2000000: episode: 4092, duration: 1.080s, episode steps: 75, steps per second: 69, episode reward: -142.713, mean reward: -1.903 [-100.000, 9.628], mean action: 1.787 [0.000, 3.000], mean observation: -0.119 [-1.285, 3.732], loss: 1.748466, mean_absolute_error: 34.930597, mean_q: 22.416213, mean_eps: 0.613641
  429473/2000000: episode: 4093, duration: 2.106s, episode steps: 148, steps per second: 70, episode reward: -56.655, mean reward: -0.383 [-100.000, 17.704], mean action: 1.561 [0.000, 3.000], mean observation: 0.020 [-2.372, 1.006], loss: 2.272057, mean_absolute_error: 36.537848, mean_q: 19.112667, mean_eps: 0.613540
  429586/2000000: episode: 4094, duration: 1.600s, episode steps: 113, steps per second: 71, episode reward: -38.852, mean reward: -0.344 [-100.000, 18.598], mean action: 1.637 [0.000, 3.000], mean observation: 0.044 [-1.943, 1.000], loss: 1.464052, mean_absolute_error: 35.701500, mean_q: 20.754915, mean_eps: 0.613423
  429689/2000000: episode: 4095, duration: 1.546s, episode steps: 103, steps per second: 67, episode reward: -133.074, mean reward: -1.292 [-100.000, 8.032], mean action: 1.621 [0.000, 3.000], mean observation: 0.086 [-1.677, 1.000], loss: 1.918609, mean_absolute_error: 35.890417, mean_q: 18.150180, mean_eps: 0.613326
  429789/2000000: episode: 4096, duration: 1.441s, episode steps: 100, steps per second: 69, episode reward: -93.074, mean reward: -0.931 [-100.000, 7.903], mean action: 1.590 [0.000, 3.000], mean observation: -0.003 [-3.743, 1.000], loss: 2.419788, mean_absolute_error: 36.126193, mean_q: 20.372754, mean_eps: 0.613234
  429927/2000000: episode: 4097, duration: 1.978s, episode steps: 138, steps per second: 70, episode reward: -60.695, mean reward: -0.440 [-100.000, 7.892], mean action: 1.645 [0.000, 3.000], mean observation: -0.092 [-0.756, 1.402], loss: 1.572262, mean_absolute_error: 35.741828, mean_q: 21.143397, mean_eps: 0.613128
  430129/2000000: episode: 4098, duration: 2.922s, episode steps: 202, steps per second: 69, episode reward: -82.069, mean reward: -0.406 [-100.000, 19.973], mean action: 1.649 [0.000, 3.000], mean observation: -0.001 [-0.575, 1.519], loss: 1.845257, mean_absolute_error: 36.287602, mean_q: 19.939336, mean_eps: 0.612975
  430229/2000000: episode: 4099, duration: 1.462s, episode steps: 100, steps per second: 68, episode reward: -31.095, mean reward: -0.311 [-100.000, 11.669], mean action: 1.730 [0.000, 3.000], mean observation: -0.065 [-0.806, 1.000], loss: 1.847337, mean_absolute_error: 35.679243, mean_q: 18.783453, mean_eps: 0.612838
  430357/2000000: episode: 4100, duration: 1.823s, episode steps: 128, steps per second: 70, episode reward: -73.051, mean reward: -0.571 [-100.000, 13.655], mean action: 1.609 [0.000, 3.000], mean observation: -0.006 [-2.342, 1.000], loss: 1.674802, mean_absolute_error: 35.092079, mean_q: 19.676180, mean_eps: 0.612735
  430471/2000000: episode: 4101, duration: 1.611s, episode steps: 114, steps per second: 71, episode reward: -119.615, mean reward: -1.049 [-100.000, 18.217], mean action: 1.746 [0.000, 3.000], mean observation: 0.114 [-0.804, 2.048], loss: 1.613952, mean_absolute_error: 35.292501, mean_q: 22.117002, mean_eps: 0.612627
  430568/2000000: episode: 4102, duration: 1.409s, episode steps: 97, steps per second: 69, episode reward: -73.874, mean reward: -0.762 [-100.000, 11.292], mean action: 1.691 [0.000, 3.000], mean observation: -0.024 [-2.764, 1.000], loss: 1.332547, mean_absolute_error: 35.502773, mean_q: 23.225836, mean_eps: 0.612534
  430732/2000000: episode: 4103, duration: 2.378s, episode steps: 164, steps per second: 69, episode reward: -61.699, mean reward: -0.376 [-100.000, 17.562], mean action: 1.640 [0.000, 3.000], mean observation: 0.049 [-0.951, 1.008], loss: 1.928334, mean_absolute_error: 35.281663, mean_q: 20.366417, mean_eps: 0.612417
  430826/2000000: episode: 4104, duration: 1.388s, episode steps: 94, steps per second: 68, episode reward: -124.987, mean reward: -1.330 [-100.000, 17.123], mean action: 1.702 [0.000, 3.000], mean observation: 0.023 [-1.039, 2.608], loss: 1.340872, mean_absolute_error: 36.077872, mean_q: 17.311974, mean_eps: 0.612300
  430967/2000000: episode: 4105, duration: 2.001s, episode steps: 141, steps per second: 70, episode reward: -122.980, mean reward: -0.872 [-100.000, 10.685], mean action: 1.738 [0.000, 3.000], mean observation: 0.002 [-3.338, 1.000], loss: 1.596574, mean_absolute_error: 35.684041, mean_q: 21.130847, mean_eps: 0.612194
  431104/2000000: episode: 4106, duration: 1.989s, episode steps: 137, steps per second: 69, episode reward: -81.832, mean reward: -0.597 [-100.000, 17.803], mean action: 1.664 [0.000, 3.000], mean observation: 0.112 [-2.631, 1.000], loss: 1.378795, mean_absolute_error: 34.669586, mean_q: 21.020317, mean_eps: 0.612069
  432104/2000000: episode: 4107, duration: 15.224s, episode steps: 1000, steps per second: 66, episode reward: -0.442, mean reward: -0.000 [-23.559, 23.208], mean action: 1.793 [0.000, 3.000], mean observation: 0.045 [-1.491, 1.000], loss: 1.974445, mean_absolute_error: 35.772006, mean_q: 19.665047, mean_eps: 0.611558
  432294/2000000: episode: 4108, duration: 2.738s, episode steps: 190, steps per second: 69, episode reward: -92.666, mean reward: -0.488 [-100.000, 21.213], mean action: 1.763 [0.000, 3.000], mean observation: -0.062 [-1.683, 1.000], loss: 1.513379, mean_absolute_error: 34.552173, mean_q: 22.402038, mean_eps: 0.611022
  432379/2000000: episode: 4109, duration: 1.208s, episode steps: 85, steps per second: 70, episode reward: -59.533, mean reward: -0.700 [-100.000, 18.343], mean action: 1.494 [0.000, 3.000], mean observation: 0.014 [-0.904, 1.000], loss: 1.640972, mean_absolute_error: 34.863283, mean_q: 22.949930, mean_eps: 0.610898
  432528/2000000: episode: 4110, duration: 2.157s, episode steps: 149, steps per second: 69, episode reward: -78.621, mean reward: -0.528 [-100.000, 11.796], mean action: 1.651 [0.000, 3.000], mean observation: -0.014 [-0.781, 2.761], loss: 1.389333, mean_absolute_error: 35.002837, mean_q: 20.193057, mean_eps: 0.610793
  432614/2000000: episode: 4111, duration: 1.250s, episode steps: 86, steps per second: 69, episode reward: -103.813, mean reward: -1.207 [-100.000, 17.582], mean action: 1.756 [0.000, 3.000], mean observation: -0.174 [-2.641, 1.000], loss: 1.308358, mean_absolute_error: 35.287578, mean_q: 20.387394, mean_eps: 0.610687
  432680/2000000: episode: 4112, duration: 0.966s, episode steps: 66, steps per second: 68, episode reward: -80.628, mean reward: -1.222 [-100.000, 7.725], mean action: 1.652 [0.000, 3.000], mean observation: -0.136 [-3.688, 1.000], loss: 1.242926, mean_absolute_error: 35.081515, mean_q: 18.496072, mean_eps: 0.610619
  432776/2000000: episode: 4113, duration: 1.396s, episode steps: 96, steps per second: 69, episode reward: -88.326, mean reward: -0.920 [-100.000, 8.629], mean action: 1.573 [0.000, 3.000], mean observation: 0.090 [-2.963, 1.000], loss: 1.112479, mean_absolute_error: 35.214403, mean_q: 21.625643, mean_eps: 0.610547
  432917/2000000: episode: 4114, duration: 2.051s, episode steps: 141, steps per second: 69, episode reward: -89.617, mean reward: -0.636 [-100.000, 17.072], mean action: 1.652 [0.000, 3.000], mean observation: 0.020 [-2.331, 1.000], loss: 1.486181, mean_absolute_error: 35.110228, mean_q: 21.612512, mean_eps: 0.610439
  433059/2000000: episode: 4115, duration: 2.042s, episode steps: 142, steps per second: 70, episode reward: -94.117, mean reward: -0.663 [-100.000, 13.048], mean action: 1.746 [0.000, 3.000], mean observation: 0.032 [-0.811, 1.000], loss: 2.161288, mean_absolute_error: 36.871939, mean_q: 17.425073, mean_eps: 0.610311
  433152/2000000: episode: 4116, duration: 1.402s, episode steps: 93, steps per second: 66, episode reward: -75.984, mean reward: -0.817 [-100.000, 16.721], mean action: 1.753 [0.000, 3.000], mean observation: 0.058 [-0.809, 1.000], loss: 1.457441, mean_absolute_error: 35.266237, mean_q: 18.664532, mean_eps: 0.610206
  433253/2000000: episode: 4117, duration: 1.571s, episode steps: 101, steps per second: 64, episode reward: -75.080, mean reward: -0.743 [-100.000, 19.117], mean action: 1.772 [0.000, 3.000], mean observation: 0.050 [-0.920, 1.000], loss: 3.088962, mean_absolute_error: 36.001838, mean_q: 19.717508, mean_eps: 0.610118
  433356/2000000: episode: 4118, duration: 1.499s, episode steps: 103, steps per second: 69, episode reward: -135.863, mean reward: -1.319 [-100.000, 7.873], mean action: 1.583 [0.000, 3.000], mean observation: -0.057 [-1.518, 1.000], loss: 2.040889, mean_absolute_error: 35.740034, mean_q: 19.731802, mean_eps: 0.610026
  433463/2000000: episode: 4119, duration: 1.563s, episode steps: 107, steps per second: 68, episode reward: -182.027, mean reward: -1.701 [-100.000, 7.928], mean action: 1.692 [0.000, 3.000], mean observation: 0.018 [-0.967, 1.055], loss: 1.924283, mean_absolute_error: 36.669911, mean_q: 19.286037, mean_eps: 0.609933
  433638/2000000: episode: 4120, duration: 2.501s, episode steps: 175, steps per second: 70, episode reward: -135.603, mean reward: -0.775 [-100.000, 15.293], mean action: 1.709 [0.000, 3.000], mean observation: -0.077 [-2.756, 1.044], loss: 1.672928, mean_absolute_error: 34.887708, mean_q: 22.276763, mean_eps: 0.609805
  433756/2000000: episode: 4121, duration: 1.739s, episode steps: 118, steps per second: 68, episode reward: -99.900, mean reward: -0.847 [-100.000, 26.472], mean action: 1.576 [0.000, 3.000], mean observation: 0.118 [-3.375, 1.000], loss: 2.021390, mean_absolute_error: 36.106597, mean_q: 18.991308, mean_eps: 0.609674
  433863/2000000: episode: 4122, duration: 1.533s, episode steps: 107, steps per second: 70, episode reward: -68.930, mean reward: -0.644 [-100.000, 12.862], mean action: 1.654 [0.000, 3.000], mean observation: 0.039 [-2.551, 1.000], loss: 1.442252, mean_absolute_error: 34.091245, mean_q: 21.850135, mean_eps: 0.609573
  433958/2000000: episode: 4123, duration: 1.376s, episode steps: 95, steps per second: 69, episode reward: -114.916, mean reward: -1.210 [-100.000, 12.953], mean action: 1.642 [0.000, 3.000], mean observation: 0.093 [-0.987, 1.000], loss: 1.946256, mean_absolute_error: 34.552459, mean_q: 22.193045, mean_eps: 0.609481
  434071/2000000: episode: 4124, duration: 1.605s, episode steps: 113, steps per second: 70, episode reward: -114.240, mean reward: -1.011 [-100.000, 17.691], mean action: 1.832 [0.000, 3.000], mean observation: -0.062 [-0.906, 2.589], loss: 2.002017, mean_absolute_error: 36.327265, mean_q: 19.228638, mean_eps: 0.609387
  434148/2000000: episode: 4125, duration: 1.136s, episode steps: 77, steps per second: 68, episode reward: -96.832, mean reward: -1.258 [-100.000, 4.443], mean action: 1.675 [0.000, 3.000], mean observation: -0.125 [-1.015, 1.000], loss: 1.293231, mean_absolute_error: 35.772030, mean_q: 20.997346, mean_eps: 0.609303
  434326/2000000: episode: 4126, duration: 2.565s, episode steps: 178, steps per second: 69, episode reward: -43.131, mean reward: -0.242 [-100.000, 15.268], mean action: 1.612 [0.000, 3.000], mean observation: 0.033 [-0.735, 1.011], loss: 1.491137, mean_absolute_error: 35.576832, mean_q: 21.535939, mean_eps: 0.609188
  434424/2000000: episode: 4127, duration: 1.426s, episode steps: 98, steps per second: 69, episode reward: -114.747, mean reward: -1.171 [-100.000, 8.262], mean action: 1.786 [0.000, 3.000], mean observation: 0.013 [-3.268, 1.000], loss: 1.508663, mean_absolute_error: 36.015431, mean_q: 18.845343, mean_eps: 0.609063
  434549/2000000: episode: 4128, duration: 1.820s, episode steps: 125, steps per second: 69, episode reward: -96.779, mean reward: -0.774 [-100.000, 11.764], mean action: 1.512 [0.000, 3.000], mean observation: 0.057 [-3.457, 1.000], loss: 2.259176, mean_absolute_error: 36.406935, mean_q: 16.514892, mean_eps: 0.608963
  434707/2000000: episode: 4129, duration: 2.221s, episode steps: 158, steps per second: 71, episode reward: -60.599, mean reward: -0.384 [-100.000, 18.749], mean action: 1.646 [0.000, 3.000], mean observation: 0.093 [-2.543, 1.000], loss: 1.627936, mean_absolute_error: 35.552850, mean_q: 19.768982, mean_eps: 0.608835
  434832/2000000: episode: 4130, duration: 1.835s, episode steps: 125, steps per second: 68, episode reward: -64.141, mean reward: -0.513 [-100.000, 28.199], mean action: 1.784 [0.000, 3.000], mean observation: -0.132 [-1.491, 1.000], loss: 1.657495, mean_absolute_error: 35.530559, mean_q: 18.247160, mean_eps: 0.608709
  434924/2000000: episode: 4131, duration: 1.377s, episode steps: 92, steps per second: 67, episode reward: -133.433, mean reward: -1.450 [-100.000, 43.319], mean action: 1.522 [0.000, 3.000], mean observation: 0.015 [-3.165, 1.000], loss: 1.379163, mean_absolute_error: 36.406350, mean_q: 24.066730, mean_eps: 0.608612
  435021/2000000: episode: 4132, duration: 1.410s, episode steps: 97, steps per second: 69, episode reward: -51.256, mean reward: -0.528 [-100.000, 18.910], mean action: 1.526 [0.000, 3.000], mean observation: 0.033 [-0.958, 1.000], loss: 1.942881, mean_absolute_error: 36.452022, mean_q: 19.543377, mean_eps: 0.608525
  435132/2000000: episode: 4133, duration: 1.611s, episode steps: 111, steps per second: 69, episode reward: -99.760, mean reward: -0.899 [-100.000, 10.525], mean action: 1.739 [0.000, 3.000], mean observation: -0.114 [-1.000, 3.079], loss: 1.281291, mean_absolute_error: 34.896015, mean_q: 22.426607, mean_eps: 0.608432
  435240/2000000: episode: 4134, duration: 1.593s, episode steps: 108, steps per second: 68, episode reward: -85.136, mean reward: -0.788 [-100.000, 9.115], mean action: 1.676 [0.000, 3.000], mean observation: -0.080 [-0.849, 2.781], loss: 1.349550, mean_absolute_error: 36.322355, mean_q: 21.254009, mean_eps: 0.608334
  435400/2000000: episode: 4135, duration: 2.314s, episode steps: 160, steps per second: 69, episode reward: -130.886, mean reward: -0.818 [-100.000, 3.821], mean action: 1.669 [0.000, 3.000], mean observation: -0.114 [-1.004, 1.014], loss: 1.433612, mean_absolute_error: 35.307627, mean_q: 20.816432, mean_eps: 0.608214
  435517/2000000: episode: 4136, duration: 1.705s, episode steps: 117, steps per second: 69, episode reward: -104.928, mean reward: -0.897 [-100.000, 10.450], mean action: 1.795 [0.000, 3.000], mean observation: -0.040 [-0.961, 2.897], loss: 1.628218, mean_absolute_error: 36.320092, mean_q: 22.263476, mean_eps: 0.608088
  435704/2000000: episode: 4137, duration: 2.693s, episode steps: 187, steps per second: 69, episode reward: -96.728, mean reward: -0.517 [-100.000, 9.801], mean action: 1.701 [0.000, 3.000], mean observation: 0.144 [-0.800, 1.649], loss: 1.378885, mean_absolute_error: 35.238608, mean_q: 20.685419, mean_eps: 0.607951
  435865/2000000: episode: 4138, duration: 2.379s, episode steps: 161, steps per second: 68, episode reward: -64.906, mean reward: -0.403 [-100.000, 14.655], mean action: 1.764 [0.000, 3.000], mean observation: -0.052 [-1.201, 1.179], loss: 1.426945, mean_absolute_error: 34.996645, mean_q: 23.056205, mean_eps: 0.607794
  436031/2000000: episode: 4139, duration: 2.382s, episode steps: 166, steps per second: 70, episode reward: -125.535, mean reward: -0.756 [-100.000, 27.445], mean action: 1.789 [0.000, 3.000], mean observation: -0.051 [-0.929, 3.822], loss: 1.921637, mean_absolute_error: 35.368362, mean_q: 18.044139, mean_eps: 0.607647
  436132/2000000: episode: 4140, duration: 1.491s, episode steps: 101, steps per second: 68, episode reward: -127.650, mean reward: -1.264 [-100.000, 10.671], mean action: 1.624 [0.000, 3.000], mean observation: -0.084 [-1.115, 3.498], loss: 1.422501, mean_absolute_error: 34.538557, mean_q: 24.049403, mean_eps: 0.607528
  436343/2000000: episode: 4141, duration: 3.026s, episode steps: 211, steps per second: 70, episode reward: -146.830, mean reward: -0.696 [-100.000, 9.656], mean action: 1.697 [0.000, 3.000], mean observation: 0.156 [-1.626, 1.000], loss: 1.800564, mean_absolute_error: 35.732642, mean_q: 19.068366, mean_eps: 0.607388
  436454/2000000: episode: 4142, duration: 1.616s, episode steps: 111, steps per second: 69, episode reward: -62.651, mean reward: -0.564 [-100.000, 12.986], mean action: 1.703 [0.000, 3.000], mean observation: -0.040 [-0.769, 1.707], loss: 1.494154, mean_absolute_error: 36.189513, mean_q: 16.476063, mean_eps: 0.607242
  436626/2000000: episode: 4143, duration: 2.469s, episode steps: 172, steps per second: 70, episode reward: -114.293, mean reward: -0.664 [-100.000, 14.932], mean action: 1.640 [0.000, 3.000], mean observation: 0.186 [-1.051, 1.000], loss: 1.672163, mean_absolute_error: 36.150123, mean_q: 20.646373, mean_eps: 0.607114
  436724/2000000: episode: 4144, duration: 1.449s, episode steps: 98, steps per second: 68, episode reward: -72.481, mean reward: -0.740 [-100.000, 9.671], mean action: 1.745 [0.000, 3.000], mean observation: 0.046 [-2.253, 1.000], loss: 1.383050, mean_absolute_error: 35.344114, mean_q: 20.820548, mean_eps: 0.606993
  436859/2000000: episode: 4145, duration: 1.970s, episode steps: 135, steps per second: 69, episode reward: -98.064, mean reward: -0.726 [-100.000, 14.128], mean action: 1.622 [0.000, 3.000], mean observation: 0.016 [-1.134, 1.000], loss: 1.545876, mean_absolute_error: 36.648632, mean_q: 18.988440, mean_eps: 0.606889
  437859/2000000: episode: 4146, duration: 15.343s, episode steps: 1000, steps per second: 65, episode reward: -10.546, mean reward: -0.011 [-26.074, 44.386], mean action: 1.284 [0.000, 3.000], mean observation: 0.041 [-1.126, 1.773], loss: 1.857567, mean_absolute_error: 35.958203, mean_q: 20.348297, mean_eps: 0.606378
  438031/2000000: episode: 4147, duration: 2.492s, episode steps: 172, steps per second: 69, episode reward: -81.328, mean reward: -0.473 [-100.000, 17.440], mean action: 1.605 [0.000, 3.000], mean observation: -0.032 [-2.715, 1.000], loss: 1.787940, mean_absolute_error: 36.647269, mean_q: 19.437266, mean_eps: 0.605850
  438118/2000000: episode: 4148, duration: 1.277s, episode steps: 87, steps per second: 68, episode reward: -116.801, mean reward: -1.343 [-100.000, 7.178], mean action: 1.632 [0.000, 3.000], mean observation: 0.066 [-3.188, 1.000], loss: 1.709647, mean_absolute_error: 34.642859, mean_q: 18.530295, mean_eps: 0.605733
  438276/2000000: episode: 4149, duration: 2.318s, episode steps: 158, steps per second: 68, episode reward: -193.238, mean reward: -1.223 [-100.000, 11.734], mean action: 1.684 [0.000, 3.000], mean observation: 0.123 [-0.677, 1.262], loss: 1.319830, mean_absolute_error: 34.696997, mean_q: 21.572791, mean_eps: 0.605624
  438443/2000000: episode: 4150, duration: 2.406s, episode steps: 167, steps per second: 69, episode reward: -108.809, mean reward: -0.652 [-100.000, 12.143], mean action: 1.713 [0.000, 3.000], mean observation: -0.084 [-0.888, 1.000], loss: 1.294545, mean_absolute_error: 35.437596, mean_q: 19.398961, mean_eps: 0.605478
  438621/2000000: episode: 4151, duration: 2.612s, episode steps: 178, steps per second: 68, episode reward: -88.505, mean reward: -0.497 [-100.000, 9.504], mean action: 1.708 [0.000, 3.000], mean observation: 0.061 [-1.819, 1.038], loss: 1.602719, mean_absolute_error: 35.491576, mean_q: 17.127695, mean_eps: 0.605321
  438816/2000000: episode: 4152, duration: 2.830s, episode steps: 195, steps per second: 69, episode reward: -58.720, mean reward: -0.301 [-100.000, 16.871], mean action: 1.662 [0.000, 3.000], mean observation: 0.030 [-1.796, 1.000], loss: 1.476786, mean_absolute_error: 35.145590, mean_q: 20.771473, mean_eps: 0.605154
  438919/2000000: episode: 4153, duration: 1.495s, episode steps: 103, steps per second: 69, episode reward: -55.307, mean reward: -0.537 [-100.000, 7.440], mean action: 1.728 [0.000, 3.000], mean observation: -0.051 [-0.803, 1.000], loss: 1.956249, mean_absolute_error: 36.473871, mean_q: 20.169433, mean_eps: 0.605021
  438994/2000000: episode: 4154, duration: 1.099s, episode steps: 75, steps per second: 68, episode reward: -87.725, mean reward: -1.170 [-100.000, 11.528], mean action: 1.693 [0.000, 3.000], mean observation: -0.044 [-1.098, 3.494], loss: 1.562196, mean_absolute_error: 35.850966, mean_q: 20.773036, mean_eps: 0.604940
  439097/2000000: episode: 4155, duration: 1.482s, episode steps: 103, steps per second: 69, episode reward: -96.781, mean reward: -0.940 [-100.000, 12.097], mean action: 1.631 [0.000, 3.000], mean observation: -0.121 [-0.770, 2.345], loss: 1.848069, mean_absolute_error: 36.205745, mean_q: 18.518069, mean_eps: 0.604859
  439239/2000000: episode: 4156, duration: 2.027s, episode steps: 142, steps per second: 70, episode reward: -101.300, mean reward: -0.713 [-100.000, 43.310], mean action: 1.711 [0.000, 3.000], mean observation: -0.015 [-1.187, 2.745], loss: 2.158068, mean_absolute_error: 36.167196, mean_q: 19.430495, mean_eps: 0.604749
  439343/2000000: episode: 4157, duration: 1.495s, episode steps: 104, steps per second: 70, episode reward: -58.883, mean reward: -0.566 [-100.000, 8.196], mean action: 1.654 [0.000, 3.000], mean observation: 0.093 [-2.740, 1.000], loss: 2.392023, mean_absolute_error: 37.657825, mean_q: 18.415781, mean_eps: 0.604639
  439435/2000000: episode: 4158, duration: 1.343s, episode steps: 92, steps per second: 69, episode reward: -109.870, mean reward: -1.194 [-100.000, 9.888], mean action: 1.815 [0.000, 3.000], mean observation: -0.078 [-1.056, 3.098], loss: 1.787845, mean_absolute_error: 35.901886, mean_q: 20.573231, mean_eps: 0.604551
  439571/2000000: episode: 4159, duration: 1.952s, episode steps: 136, steps per second: 70, episode reward: -76.796, mean reward: -0.565 [-100.000, 12.495], mean action: 1.610 [0.000, 3.000], mean observation: 0.156 [-2.613, 1.000], loss: 1.918327, mean_absolute_error: 36.299190, mean_q: 19.306400, mean_eps: 0.604448
  439726/2000000: episode: 4160, duration: 2.230s, episode steps: 155, steps per second: 69, episode reward: -103.675, mean reward: -0.669 [-100.000, 6.981], mean action: 1.710 [0.000, 3.000], mean observation: 0.062 [-2.601, 1.000], loss: 2.261851, mean_absolute_error: 36.206728, mean_q: 19.416000, mean_eps: 0.604317
  439946/2000000: episode: 4161, duration: 3.141s, episode steps: 220, steps per second: 70, episode reward: -91.162, mean reward: -0.414 [-100.000, 22.173], mean action: 1.632 [0.000, 3.000], mean observation: 0.042 [-2.946, 1.000], loss: 1.581051, mean_absolute_error: 35.280502, mean_q: 22.549261, mean_eps: 0.604148
  440122/2000000: episode: 4162, duration: 2.510s, episode steps: 176, steps per second: 70, episode reward: -75.702, mean reward: -0.430 [-100.000, 12.792], mean action: 1.795 [0.000, 3.000], mean observation: -0.029 [-0.797, 2.793], loss: 2.206992, mean_absolute_error: 35.892602, mean_q: 22.295623, mean_eps: 0.603969
  440291/2000000: episode: 4163, duration: 2.397s, episode steps: 169, steps per second: 71, episode reward: -48.755, mean reward: -0.288 [-100.000, 67.189], mean action: 1.669 [0.000, 3.000], mean observation: 0.110 [-0.841, 1.099], loss: 1.649721, mean_absolute_error: 35.794039, mean_q: 19.034655, mean_eps: 0.603815
  440379/2000000: episode: 4164, duration: 1.270s, episode steps: 88, steps per second: 69, episode reward: -130.012, mean reward: -1.477 [-100.000, 16.704], mean action: 1.625 [0.000, 3.000], mean observation: 0.126 [-3.593, 1.000], loss: 1.445461, mean_absolute_error: 36.663334, mean_q: 21.300491, mean_eps: 0.603699
  440454/2000000: episode: 4165, duration: 1.075s, episode steps: 75, steps per second: 70, episode reward: -111.033, mean reward: -1.480 [-100.000, 7.478], mean action: 1.533 [0.000, 3.000], mean observation: 0.076 [-1.196, 4.353], loss: 1.867649, mean_absolute_error: 36.254770, mean_q: 19.745647, mean_eps: 0.603626
  440596/2000000: episode: 4166, duration: 2.027s, episode steps: 142, steps per second: 70, episode reward: -42.077, mean reward: -0.296 [-100.000, 20.406], mean action: 1.669 [0.000, 3.000], mean observation: -0.016 [-2.323, 1.000], loss: 1.597572, mean_absolute_error: 36.568574, mean_q: 21.698746, mean_eps: 0.603528
  440744/2000000: episode: 4167, duration: 2.160s, episode steps: 148, steps per second: 69, episode reward: -107.735, mean reward: -0.728 [-100.000, 10.840], mean action: 1.784 [0.000, 3.000], mean observation: -0.002 [-0.927, 2.987], loss: 2.192015, mean_absolute_error: 36.882957, mean_q: 18.625263, mean_eps: 0.603399
  440870/2000000: episode: 4168, duration: 1.802s, episode steps: 126, steps per second: 70, episode reward: -72.058, mean reward: -0.572 [-100.000, 15.849], mean action: 1.643 [0.000, 3.000], mean observation: -0.021 [-0.772, 1.343], loss: 1.627416, mean_absolute_error: 35.318667, mean_q: 20.684683, mean_eps: 0.603275
  441008/2000000: episode: 4169, duration: 1.974s, episode steps: 138, steps per second: 70, episode reward: -101.571, mean reward: -0.736 [-100.000, 9.082], mean action: 1.696 [0.000, 3.000], mean observation: -0.045 [-2.940, 1.000], loss: 1.897413, mean_absolute_error: 35.753522, mean_q: 19.462065, mean_eps: 0.603156
  441100/2000000: episode: 4170, duration: 1.358s, episode steps: 92, steps per second: 68, episode reward: -129.878, mean reward: -1.412 [-100.000, 16.430], mean action: 1.500 [0.000, 3.000], mean observation: -0.097 [-1.123, 3.386], loss: 1.769390, mean_absolute_error: 35.336547, mean_q: 20.511970, mean_eps: 0.603053
  441230/2000000: episode: 4171, duration: 1.877s, episode steps: 130, steps per second: 69, episode reward: -88.968, mean reward: -0.684 [-100.000, 11.383], mean action: 1.708 [0.000, 3.000], mean observation: -0.023 [-1.812, 1.000], loss: 2.029275, mean_absolute_error: 36.928387, mean_q: 20.367452, mean_eps: 0.602952
  441370/2000000: episode: 4172, duration: 1.997s, episode steps: 140, steps per second: 70, episode reward: -74.952, mean reward: -0.535 [-100.000, 12.784], mean action: 1.707 [0.000, 3.000], mean observation: -0.045 [-2.560, 1.000], loss: 1.292074, mean_absolute_error: 34.804803, mean_q: 22.238359, mean_eps: 0.602830
  441537/2000000: episode: 4173, duration: 2.507s, episode steps: 167, steps per second: 67, episode reward: -109.486, mean reward: -0.656 [-100.000, 8.610], mean action: 1.647 [0.000, 3.000], mean observation: 0.034 [-3.221, 1.000], loss: 1.717555, mean_absolute_error: 34.935881, mean_q: 21.020420, mean_eps: 0.602691
  441626/2000000: episode: 4174, duration: 1.323s, episode steps: 89, steps per second: 67, episode reward: -78.931, mean reward: -0.887 [-100.000, 18.174], mean action: 1.629 [0.000, 3.000], mean observation: -0.104 [-2.855, 1.000], loss: 1.169024, mean_absolute_error: 33.673329, mean_q: 23.865782, mean_eps: 0.602576
  441786/2000000: episode: 4175, duration: 2.321s, episode steps: 160, steps per second: 69, episode reward: -92.119, mean reward: -0.576 [-100.000, 7.327], mean action: 1.738 [0.000, 3.000], mean observation: 0.008 [-0.785, 2.703], loss: 1.658368, mean_absolute_error: 36.081325, mean_q: 21.050131, mean_eps: 0.602465
  441903/2000000: episode: 4176, duration: 1.652s, episode steps: 117, steps per second: 71, episode reward: -105.795, mean reward: -0.904 [-100.000, 13.017], mean action: 1.581 [0.000, 3.000], mean observation: -0.107 [-1.890, 1.000], loss: 1.797862, mean_absolute_error: 37.074429, mean_q: 20.233061, mean_eps: 0.602340
  442039/2000000: episode: 4177, duration: 1.965s, episode steps: 136, steps per second: 69, episode reward: -132.151, mean reward: -0.972 [-100.000, 17.698], mean action: 1.662 [0.000, 3.000], mean observation: 0.083 [-0.726, 3.029], loss: 1.424651, mean_absolute_error: 35.950160, mean_q: 18.862150, mean_eps: 0.602227
  442177/2000000: episode: 4178, duration: 1.989s, episode steps: 138, steps per second: 69, episode reward: -81.345, mean reward: -0.589 [-100.000, 5.544], mean action: 1.638 [0.000, 3.000], mean observation: -0.073 [-0.704, 1.528], loss: 1.483282, mean_absolute_error: 35.412041, mean_q: 21.278783, mean_eps: 0.602103
  442321/2000000: episode: 4179, duration: 2.064s, episode steps: 144, steps per second: 70, episode reward: -108.531, mean reward: -0.754 [-100.000, 7.744], mean action: 1.465 [0.000, 3.000], mean observation: 0.154 [-0.842, 1.712], loss: 1.412072, mean_absolute_error: 35.398597, mean_q: 21.933743, mean_eps: 0.601975
  442417/2000000: episode: 4180, duration: 1.390s, episode steps: 96, steps per second: 69, episode reward: -85.839, mean reward: -0.894 [-100.000, 7.607], mean action: 1.708 [0.000, 3.000], mean observation: 0.109 [-2.898, 1.000], loss: 2.352082, mean_absolute_error: 36.758460, mean_q: 16.468017, mean_eps: 0.601867
  442555/2000000: episode: 4181, duration: 1.933s, episode steps: 138, steps per second: 71, episode reward: -120.983, mean reward: -0.877 [-100.000, 10.084], mean action: 1.580 [0.000, 3.000], mean observation: 0.033 [-3.211, 1.000], loss: 1.749924, mean_absolute_error: 35.091006, mean_q: 23.277956, mean_eps: 0.601763
  442740/2000000: episode: 4182, duration: 2.664s, episode steps: 185, steps per second: 69, episode reward: -72.991, mean reward: -0.395 [-100.000, 17.861], mean action: 1.649 [0.000, 3.000], mean observation: 0.063 [-0.950, 1.000], loss: 2.041350, mean_absolute_error: 35.995200, mean_q: 18.444161, mean_eps: 0.601619
  442883/2000000: episode: 4183, duration: 2.043s, episode steps: 143, steps per second: 70, episode reward: -96.414, mean reward: -0.674 [-100.000, 7.634], mean action: 1.517 [0.000, 3.000], mean observation: 0.014 [-2.876, 1.000], loss: 2.155645, mean_absolute_error: 35.650038, mean_q: 21.521521, mean_eps: 0.601471
  443031/2000000: episode: 4184, duration: 2.113s, episode steps: 148, steps per second: 70, episode reward: -74.153, mean reward: -0.501 [-100.000, 12.770], mean action: 1.628 [0.000, 3.000], mean observation: 0.023 [-3.238, 1.012], loss: 1.999124, mean_absolute_error: 36.690772, mean_q: 21.598309, mean_eps: 0.601340
  443114/2000000: episode: 4185, duration: 1.188s, episode steps: 83, steps per second: 70, episode reward: -59.107, mean reward: -0.712 [-100.000, 17.227], mean action: 1.518 [0.000, 3.000], mean observation: 0.002 [-0.917, 1.000], loss: 1.496100, mean_absolute_error: 35.788407, mean_q: 19.242860, mean_eps: 0.601235
  443290/2000000: episode: 4186, duration: 2.506s, episode steps: 176, steps per second: 70, episode reward: -46.816, mean reward: -0.266 [-100.000, 14.859], mean action: 1.733 [0.000, 3.000], mean observation: -0.092 [-0.761, 1.000], loss: 1.543165, mean_absolute_error: 35.469070, mean_q: 20.479174, mean_eps: 0.601118
  443455/2000000: episode: 4187, duration: 2.350s, episode steps: 165, steps per second: 70, episode reward: -29.672, mean reward: -0.180 [-100.000, 19.026], mean action: 1.697 [0.000, 3.000], mean observation: -0.005 [-0.612, 1.000], loss: 1.631822, mean_absolute_error: 35.259131, mean_q: 20.539297, mean_eps: 0.600965
  443571/2000000: episode: 4188, duration: 1.665s, episode steps: 116, steps per second: 70, episode reward: -44.875, mean reward: -0.387 [-100.000, 14.210], mean action: 1.750 [0.000, 3.000], mean observation: 0.072 [-2.548, 1.000], loss: 1.440528, mean_absolute_error: 35.330085, mean_q: 20.922994, mean_eps: 0.600839
  443696/2000000: episode: 4189, duration: 1.822s, episode steps: 125, steps per second: 69, episode reward: -58.144, mean reward: -0.465 [-100.000, 12.380], mean action: 1.656 [0.000, 3.000], mean observation: -0.104 [-0.891, 3.002], loss: 1.261417, mean_absolute_error: 35.287932, mean_q: 22.344891, mean_eps: 0.600731
  443836/2000000: episode: 4190, duration: 2.024s, episode steps: 140, steps per second: 69, episode reward: -19.992, mean reward: -0.143 [-100.000, 16.553], mean action: 1.650 [0.000, 3.000], mean observation: -0.065 [-0.991, 1.264], loss: 2.279279, mean_absolute_error: 35.977918, mean_q: 19.132459, mean_eps: 0.600612
  443944/2000000: episode: 4191, duration: 1.585s, episode steps: 108, steps per second: 68, episode reward: -109.121, mean reward: -1.010 [-100.000, 12.649], mean action: 1.676 [0.000, 3.000], mean observation: -0.075 [-0.902, 2.755], loss: 2.227477, mean_absolute_error: 36.217233, mean_q: 22.720019, mean_eps: 0.600501
  444059/2000000: episode: 4192, duration: 1.638s, episode steps: 115, steps per second: 70, episode reward: -85.995, mean reward: -0.748 [-100.000, 20.912], mean action: 1.583 [0.000, 3.000], mean observation: -0.134 [-0.817, 2.227], loss: 1.538472, mean_absolute_error: 35.477753, mean_q: 21.923897, mean_eps: 0.600400
  444195/2000000: episode: 4193, duration: 1.951s, episode steps: 136, steps per second: 70, episode reward: -123.823, mean reward: -0.910 [-100.000, 16.631], mean action: 1.596 [0.000, 3.000], mean observation: -0.018 [-2.005, 1.000], loss: 1.652466, mean_absolute_error: 35.923374, mean_q: 20.316676, mean_eps: 0.600287
  444330/2000000: episode: 4194, duration: 1.957s, episode steps: 135, steps per second: 69, episode reward: -51.556, mean reward: -0.382 [-100.000, 17.312], mean action: 1.570 [0.000, 3.000], mean observation: -0.008 [-0.698, 1.000], loss: 1.749882, mean_absolute_error: 35.549575, mean_q: 20.675548, mean_eps: 0.600164
  444418/2000000: episode: 4195, duration: 1.270s, episode steps: 88, steps per second: 69, episode reward: -133.373, mean reward: -1.516 [-100.000, 6.070], mean action: 1.500 [0.000, 3.000], mean observation: 0.094 [-3.030, 1.000], loss: 1.851093, mean_absolute_error: 36.149072, mean_q: 19.585217, mean_eps: 0.600063
  444571/2000000: episode: 4196, duration: 2.180s, episode steps: 153, steps per second: 70, episode reward: -91.700, mean reward: -0.599 [-100.000, 13.225], mean action: 1.641 [0.000, 3.000], mean observation: -0.048 [-0.851, 1.023], loss: 1.871248, mean_absolute_error: 36.494347, mean_q: 17.997653, mean_eps: 0.599955
  445571/2000000: episode: 4197, duration: 15.887s, episode steps: 1000, steps per second: 63, episode reward: 51.810, mean reward: 0.052 [-22.026, 22.771], mean action: 1.697 [0.000, 3.000], mean observation: 0.155 [-1.400, 1.000], loss: 1.783407, mean_absolute_error: 35.495700, mean_q: 21.821186, mean_eps: 0.599437
  445744/2000000: episode: 4198, duration: 2.555s, episode steps: 173, steps per second: 68, episode reward: -107.700, mean reward: -0.623 [-100.000, 44.955], mean action: 1.711 [0.000, 3.000], mean observation: 0.121 [-1.192, 1.028], loss: 1.515320, mean_absolute_error: 35.872228, mean_q: 20.264864, mean_eps: 0.598910
  445890/2000000: episode: 4199, duration: 2.130s, episode steps: 146, steps per second: 69, episode reward: -91.042, mean reward: -0.624 [-100.000, 7.552], mean action: 1.603 [0.000, 3.000], mean observation: 0.147 [-0.988, 1.000], loss: 1.799704, mean_absolute_error: 35.881039, mean_q: 17.832622, mean_eps: 0.598766
  446017/2000000: episode: 4200, duration: 1.851s, episode steps: 127, steps per second: 69, episode reward: -113.499, mean reward: -0.894 [-100.000, 11.271], mean action: 1.787 [0.000, 3.000], mean observation: 0.028 [-0.740, 2.807], loss: 1.749122, mean_absolute_error: 35.806672, mean_q: 21.301973, mean_eps: 0.598641
  446207/2000000: episode: 4201, duration: 2.700s, episode steps: 190, steps per second: 70, episode reward: -75.437, mean reward: -0.397 [-100.000, 9.395], mean action: 1.689 [0.000, 3.000], mean observation: -0.006 [-3.197, 1.000], loss: 1.946065, mean_absolute_error: 35.535240, mean_q: 22.219655, mean_eps: 0.598499
  446367/2000000: episode: 4202, duration: 2.294s, episode steps: 160, steps per second: 70, episode reward: -33.251, mean reward: -0.208 [-100.000, 11.041], mean action: 1.544 [0.000, 3.000], mean observation: -0.004 [-1.181, 1.000], loss: 1.565692, mean_absolute_error: 35.856933, mean_q: 24.499286, mean_eps: 0.598343
  446506/2000000: episode: 4203, duration: 1.992s, episode steps: 139, steps per second: 70, episode reward: -86.400, mean reward: -0.622 [-100.000, 83.664], mean action: 1.849 [0.000, 3.000], mean observation: 0.061 [-1.310, 1.000], loss: 1.723041, mean_absolute_error: 35.680284, mean_q: 22.376699, mean_eps: 0.598208
  446665/2000000: episode: 4204, duration: 2.282s, episode steps: 159, steps per second: 70, episode reward: -39.452, mean reward: -0.248 [-100.000, 18.307], mean action: 1.616 [0.000, 3.000], mean observation: 0.016 [-1.094, 1.000], loss: 1.724258, mean_absolute_error: 35.456472, mean_q: 20.367675, mean_eps: 0.598073
  447665/2000000: episode: 4205, duration: 16.797s, episode steps: 1000, steps per second: 60, episode reward: 48.008, mean reward: 0.048 [-23.920, 25.955], mean action: 1.507 [0.000, 3.000], mean observation: 0.095 [-0.590, 1.000], loss: 1.664290, mean_absolute_error: 35.519772, mean_q: 20.835790, mean_eps: 0.597551
  447772/2000000: episode: 4206, duration: 1.555s, episode steps: 107, steps per second: 69, episode reward: -125.454, mean reward: -1.172 [-100.000, 10.469], mean action: 1.692 [0.000, 3.000], mean observation: -0.042 [-0.843, 1.000], loss: 1.705969, mean_absolute_error: 36.317778, mean_q: 21.153999, mean_eps: 0.597054
  447907/2000000: episode: 4207, duration: 1.972s, episode steps: 135, steps per second: 68, episode reward: -89.533, mean reward: -0.663 [-100.000, 6.656], mean action: 1.726 [0.000, 3.000], mean observation: 0.143 [-3.177, 1.003], loss: 1.968708, mean_absolute_error: 34.840677, mean_q: 20.862034, mean_eps: 0.596946
  448006/2000000: episode: 4208, duration: 1.459s, episode steps: 99, steps per second: 68, episode reward: -66.024, mean reward: -0.667 [-100.000, 77.284], mean action: 1.727 [0.000, 3.000], mean observation: 0.024 [-1.142, 1.077], loss: 1.543617, mean_absolute_error: 35.338668, mean_q: 22.769644, mean_eps: 0.596840
  449006/2000000: episode: 4209, duration: 15.285s, episode steps: 1000, steps per second: 65, episode reward: 25.433, mean reward: 0.025 [-23.724, 26.711], mean action: 1.564 [0.000, 3.000], mean observation: 0.167 [-1.299, 1.000], loss: 1.795341, mean_absolute_error: 35.315024, mean_q: 20.254190, mean_eps: 0.596345
  449146/2000000: episode: 4210, duration: 2.043s, episode steps: 140, steps per second: 69, episode reward: -68.817, mean reward: -0.492 [-100.000, 11.750], mean action: 1.657 [0.000, 3.000], mean observation: -0.012 [-0.807, 1.000], loss: 2.172672, mean_absolute_error: 35.678891, mean_q: 20.607541, mean_eps: 0.595832
  449242/2000000: episode: 4211, duration: 1.419s, episode steps: 96, steps per second: 68, episode reward: -91.964, mean reward: -0.958 [-100.000, 12.938], mean action: 1.531 [0.000, 3.000], mean observation: 0.066 [-2.521, 1.000], loss: 1.796113, mean_absolute_error: 35.510516, mean_q: 20.172638, mean_eps: 0.595725
  449349/2000000: episode: 4212, duration: 1.549s, episode steps: 107, steps per second: 69, episode reward: -159.597, mean reward: -1.492 [-100.000, 12.689], mean action: 1.607 [0.000, 3.000], mean observation: 0.140 [-3.199, 1.000], loss: 1.658080, mean_absolute_error: 34.634949, mean_q: 19.799254, mean_eps: 0.595634
  449446/2000000: episode: 4213, duration: 1.547s, episode steps: 97, steps per second: 63, episode reward: -121.040, mean reward: -1.248 [-100.000, 17.822], mean action: 1.784 [0.000, 3.000], mean observation: -0.123 [-0.983, 1.000], loss: 1.805575, mean_absolute_error: 36.291811, mean_q: 20.522864, mean_eps: 0.595542
  449636/2000000: episode: 4214, duration: 2.848s, episode steps: 190, steps per second: 67, episode reward: -14.349, mean reward: -0.076 [-100.000, 15.445], mean action: 1.647 [0.000, 3.000], mean observation: 0.052 [-0.625, 1.356], loss: 1.684498, mean_absolute_error: 35.552867, mean_q: 21.326406, mean_eps: 0.595414
  449727/2000000: episode: 4215, duration: 1.317s, episode steps: 91, steps per second: 69, episode reward: -90.358, mean reward: -0.993 [-100.000, 10.837], mean action: 1.758 [0.000, 3.000], mean observation: -0.137 [-2.727, 1.000], loss: 1.576709, mean_absolute_error: 34.632627, mean_q: 22.003886, mean_eps: 0.595288
  449810/2000000: episode: 4216, duration: 1.204s, episode steps: 83, steps per second: 69, episode reward: -93.689, mean reward: -1.129 [-100.000, 29.000], mean action: 1.554 [0.000, 3.000], mean observation: -0.160 [-3.264, 1.000], loss: 2.143839, mean_absolute_error: 35.352640, mean_q: 18.660008, mean_eps: 0.595209
  450049/2000000: episode: 4217, duration: 3.481s, episode steps: 239, steps per second: 69, episode reward: -108.233, mean reward: -0.453 [-100.000, 12.211], mean action: 1.615 [0.000, 3.000], mean observation: 0.166 [-1.164, 1.000], loss: 1.715414, mean_absolute_error: 36.333183, mean_q: 19.409734, mean_eps: 0.595063
  450186/2000000: episode: 4218, duration: 1.977s, episode steps: 137, steps per second: 69, episode reward: -93.684, mean reward: -0.684 [-100.000, 15.487], mean action: 1.584 [0.000, 3.000], mean observation: -0.055 [-0.869, 1.000], loss: 1.644881, mean_absolute_error: 35.617018, mean_q: 21.392760, mean_eps: 0.594894
  450326/2000000: episode: 4219, duration: 2.002s, episode steps: 140, steps per second: 70, episode reward: -62.790, mean reward: -0.449 [-100.000, 15.927], mean action: 1.621 [0.000, 3.000], mean observation: 0.010 [-0.765, 1.221], loss: 1.249089, mean_absolute_error: 35.189494, mean_q: 21.487819, mean_eps: 0.594770
  450433/2000000: episode: 4220, duration: 1.547s, episode steps: 107, steps per second: 69, episode reward: -74.415, mean reward: -0.695 [-100.000, 15.465], mean action: 1.551 [0.000, 3.000], mean observation: 0.109 [-1.276, 1.000], loss: 1.541338, mean_absolute_error: 35.981792, mean_q: 19.455336, mean_eps: 0.594658
  450608/2000000: episode: 4221, duration: 2.554s, episode steps: 175, steps per second: 69, episode reward: -183.874, mean reward: -1.051 [-100.000, 9.467], mean action: 1.771 [0.000, 3.000], mean observation: -0.089 [-1.076, 1.000], loss: 1.734804, mean_absolute_error: 35.585652, mean_q: 21.883518, mean_eps: 0.594532
  450742/2000000: episode: 4222, duration: 1.967s, episode steps: 134, steps per second: 68, episode reward: -89.994, mean reward: -0.672 [-100.000, 8.165], mean action: 1.687 [0.000, 3.000], mean observation: 0.038 [-3.259, 1.012], loss: 2.407358, mean_absolute_error: 36.223267, mean_q: 20.715510, mean_eps: 0.594393
  450846/2000000: episode: 4223, duration: 1.494s, episode steps: 104, steps per second: 70, episode reward: -69.667, mean reward: -0.670 [-100.000, 12.218], mean action: 1.663 [0.000, 3.000], mean observation: 0.024 [-2.838, 1.000], loss: 1.683680, mean_absolute_error: 35.385019, mean_q: 20.542980, mean_eps: 0.594285
  451003/2000000: episode: 4224, duration: 2.229s, episode steps: 157, steps per second: 70, episode reward: -79.584, mean reward: -0.507 [-100.000, 11.170], mean action: 1.745 [0.000, 3.000], mean observation: 0.183 [-2.612, 1.014], loss: 1.663221, mean_absolute_error: 35.733743, mean_q: 21.858481, mean_eps: 0.594168
  452003/2000000: episode: 4225, duration: 15.549s, episode steps: 1000, steps per second: 64, episode reward: 12.827, mean reward: 0.013 [-22.499, 23.641], mean action: 1.477 [0.000, 3.000], mean observation: 0.203 [-0.607, 1.510], loss: 1.694009, mean_absolute_error: 35.731695, mean_q: 20.103652, mean_eps: 0.593648
  452146/2000000: episode: 4226, duration: 2.109s, episode steps: 143, steps per second: 68, episode reward: -83.581, mean reward: -0.584 [-100.000, 9.015], mean action: 1.713 [0.000, 3.000], mean observation: -0.097 [-0.712, 1.000], loss: 1.086666, mean_absolute_error: 35.659272, mean_q: 22.141250, mean_eps: 0.593133
  452300/2000000: episode: 4227, duration: 2.236s, episode steps: 154, steps per second: 69, episode reward: -42.789, mean reward: -0.278 [-100.000, 9.157], mean action: 1.766 [0.000, 3.000], mean observation: 0.096 [-0.757, 2.507], loss: 1.773093, mean_absolute_error: 35.446677, mean_q: 23.066993, mean_eps: 0.593000
  452532/2000000: episode: 4228, duration: 3.413s, episode steps: 232, steps per second: 68, episode reward: -140.753, mean reward: -0.607 [-100.000, 17.475], mean action: 1.784 [0.000, 3.000], mean observation: 0.060 [-1.508, 1.000], loss: 1.522682, mean_absolute_error: 36.204138, mean_q: 20.202077, mean_eps: 0.592827
  452625/2000000: episode: 4229, duration: 1.379s, episode steps: 93, steps per second: 67, episode reward: -135.715, mean reward: -1.459 [-100.000, 7.296], mean action: 1.634 [0.000, 3.000], mean observation: -0.102 [-4.037, 1.000], loss: 1.367951, mean_absolute_error: 35.250200, mean_q: 24.231255, mean_eps: 0.592680
  452761/2000000: episode: 4230, duration: 1.943s, episode steps: 136, steps per second: 70, episode reward: -101.184, mean reward: -0.744 [-100.000, 9.921], mean action: 1.588 [0.000, 3.000], mean observation: 0.022 [-1.066, 1.000], loss: 1.495475, mean_absolute_error: 35.340399, mean_q: 23.579908, mean_eps: 0.592575
  452861/2000000: episode: 4231, duration: 1.427s, episode steps: 100, steps per second: 70, episode reward: -64.843, mean reward: -0.648 [-100.000, 18.116], mean action: 1.700 [0.000, 3.000], mean observation: 0.040 [-0.949, 1.000], loss: 1.403717, mean_absolute_error: 35.545302, mean_q: 23.045144, mean_eps: 0.592469
  453009/2000000: episode: 4232, duration: 2.119s, episode steps: 148, steps per second: 70, episode reward: -127.113, mean reward: -0.859 [-100.000, 10.201], mean action: 1.662 [0.000, 3.000], mean observation: 0.043 [-1.119, 3.131], loss: 2.037687, mean_absolute_error: 35.361588, mean_q: 20.869462, mean_eps: 0.592358
  453173/2000000: episode: 4233, duration: 2.349s, episode steps: 164, steps per second: 70, episode reward: -100.195, mean reward: -0.611 [-100.000, 13.364], mean action: 1.689 [0.000, 3.000], mean observation: 0.024 [-0.525, 1.402], loss: 1.560476, mean_absolute_error: 36.169017, mean_q: 21.122360, mean_eps: 0.592217
  453313/2000000: episode: 4234, duration: 2.017s, episode steps: 140, steps per second: 69, episode reward: -115.827, mean reward: -0.827 [-100.000, 15.825], mean action: 1.621 [0.000, 3.000], mean observation: 0.068 [-3.035, 1.000], loss: 2.080859, mean_absolute_error: 36.378403, mean_q: 20.052240, mean_eps: 0.592080
  453431/2000000: episode: 4235, duration: 1.681s, episode steps: 118, steps per second: 70, episode reward: -87.149, mean reward: -0.739 [-100.000, 9.943], mean action: 1.737 [0.000, 3.000], mean observation: -0.071 [-0.712, 2.479], loss: 2.025554, mean_absolute_error: 35.726117, mean_q: 22.370077, mean_eps: 0.591965
  453634/2000000: episode: 4236, duration: 2.929s, episode steps: 203, steps per second: 69, episode reward: -150.670, mean reward: -0.742 [-100.000, 10.708], mean action: 1.576 [0.000, 3.000], mean observation: 0.063 [-4.085, 1.000], loss: 1.914380, mean_absolute_error: 36.087454, mean_q: 21.432473, mean_eps: 0.591821
  453778/2000000: episode: 4237, duration: 2.070s, episode steps: 144, steps per second: 70, episode reward: -64.438, mean reward: -0.447 [-100.000, 22.832], mean action: 1.778 [0.000, 3.000], mean observation: 0.007 [-0.608, 1.733], loss: 1.435886, mean_absolute_error: 35.088371, mean_q: 19.720367, mean_eps: 0.591665
  453937/2000000: episode: 4238, duration: 2.288s, episode steps: 159, steps per second: 70, episode reward: -123.749, mean reward: -0.778 [-100.000, 33.185], mean action: 1.604 [0.000, 3.000], mean observation: 0.021 [-1.101, 2.812], loss: 1.745695, mean_absolute_error: 36.758479, mean_q: 21.150588, mean_eps: 0.591528
  454063/2000000: episode: 4239, duration: 1.774s, episode steps: 126, steps per second: 71, episode reward: -126.384, mean reward: -1.003 [-100.000, 12.492], mean action: 1.675 [0.000, 3.000], mean observation: -0.040 [-0.934, 1.931], loss: 2.026847, mean_absolute_error: 35.108670, mean_q: 18.227259, mean_eps: 0.591400
  454154/2000000: episode: 4240, duration: 1.315s, episode steps: 91, steps per second: 69, episode reward: -90.479, mean reward: -0.994 [-100.000, 13.474], mean action: 1.604 [0.000, 3.000], mean observation: 0.063 [-0.977, 3.595], loss: 1.421974, mean_absolute_error: 35.560260, mean_q: 20.903235, mean_eps: 0.591303
  454258/2000000: episode: 4241, duration: 1.475s, episode steps: 104, steps per second: 71, episode reward: -138.031, mean reward: -1.327 [-100.000, 10.460], mean action: 1.712 [0.000, 3.000], mean observation: -0.134 [-0.990, 3.112], loss: 2.076373, mean_absolute_error: 35.786789, mean_q: 23.343519, mean_eps: 0.591215
  454354/2000000: episode: 4242, duration: 1.370s, episode steps: 96, steps per second: 70, episode reward: -114.688, mean reward: -1.195 [-100.000, 6.524], mean action: 1.583 [0.000, 3.000], mean observation: 0.166 [-0.959, 3.278], loss: 1.946338, mean_absolute_error: 35.577194, mean_q: 21.942313, mean_eps: 0.591125
  454486/2000000: episode: 4243, duration: 1.881s, episode steps: 132, steps per second: 70, episode reward: -89.562, mean reward: -0.678 [-100.000, 9.634], mean action: 1.742 [0.000, 3.000], mean observation: -0.087 [-0.839, 2.601], loss: 2.036465, mean_absolute_error: 35.752135, mean_q: 20.394506, mean_eps: 0.591022
  454636/2000000: episode: 4244, duration: 2.181s, episode steps: 150, steps per second: 69, episode reward: -110.877, mean reward: -0.739 [-100.000, 19.906], mean action: 1.733 [0.000, 3.000], mean observation: 0.039 [-0.598, 1.000], loss: 1.804626, mean_absolute_error: 34.959498, mean_q: 20.330790, mean_eps: 0.590896
  454772/2000000: episode: 4245, duration: 2.032s, episode steps: 136, steps per second: 67, episode reward: -32.674, mean reward: -0.240 [-100.000, 14.459], mean action: 1.765 [0.000, 3.000], mean observation: -0.074 [-2.075, 1.000], loss: 1.558920, mean_absolute_error: 36.004666, mean_q: 22.847955, mean_eps: 0.590768
  454890/2000000: episode: 4246, duration: 1.744s, episode steps: 118, steps per second: 68, episode reward: -90.872, mean reward: -0.770 [-100.000, 6.224], mean action: 1.822 [0.000, 3.000], mean observation: 0.006 [-0.842, 2.932], loss: 1.280779, mean_absolute_error: 35.716614, mean_q: 21.942103, mean_eps: 0.590653
  455129/2000000: episode: 4247, duration: 3.440s, episode steps: 239, steps per second: 69, episode reward: -52.383, mean reward: -0.219 [-100.000, 13.282], mean action: 1.707 [0.000, 3.000], mean observation: 0.103 [-0.532, 1.023], loss: 1.760855, mean_absolute_error: 35.758326, mean_q: 21.426850, mean_eps: 0.590491
  455215/2000000: episode: 4248, duration: 1.258s, episode steps: 86, steps per second: 68, episode reward: -145.896, mean reward: -1.696 [-100.000, 6.264], mean action: 1.651 [0.000, 3.000], mean observation: 0.032 [-1.079, 1.000], loss: 2.007730, mean_absolute_error: 36.883874, mean_q: 17.227190, mean_eps: 0.590345
  455400/2000000: episode: 4249, duration: 2.690s, episode steps: 185, steps per second: 69, episode reward: -65.745, mean reward: -0.355 [-100.000, 10.659], mean action: 1.659 [0.000, 3.000], mean observation: 0.146 [-1.940, 1.144], loss: 1.802160, mean_absolute_error: 35.819729, mean_q: 20.461381, mean_eps: 0.590225
  455534/2000000: episode: 4250, duration: 1.970s, episode steps: 134, steps per second: 68, episode reward: -108.381, mean reward: -0.809 [-100.000, 8.042], mean action: 1.709 [0.000, 3.000], mean observation: -0.032 [-0.955, 1.084], loss: 1.669340, mean_absolute_error: 35.485578, mean_q: 22.078424, mean_eps: 0.590081
  455623/2000000: episode: 4251, duration: 1.261s, episode steps: 89, steps per second: 71, episode reward: -107.052, mean reward: -1.203 [-100.000, 7.132], mean action: 1.640 [0.000, 3.000], mean observation: -0.032 [-4.235, 1.000], loss: 1.726051, mean_absolute_error: 36.228522, mean_q: 21.254002, mean_eps: 0.589980
  455738/2000000: episode: 4252, duration: 1.675s, episode steps: 115, steps per second: 69, episode reward: -87.394, mean reward: -0.760 [-100.000, 14.630], mean action: 1.704 [0.000, 3.000], mean observation: -0.037 [-0.874, 2.978], loss: 2.304748, mean_absolute_error: 36.407779, mean_q: 21.569202, mean_eps: 0.589888
  455907/2000000: episode: 4253, duration: 2.435s, episode steps: 169, steps per second: 69, episode reward: -126.748, mean reward: -0.750 [-100.000, 8.836], mean action: 1.663 [0.000, 3.000], mean observation: -0.006 [-0.650, 1.273], loss: 1.614828, mean_absolute_error: 35.208315, mean_q: 22.947917, mean_eps: 0.589760
  455989/2000000: episode: 4254, duration: 1.217s, episode steps: 82, steps per second: 67, episode reward: -62.892, mean reward: -0.767 [-100.000, 13.442], mean action: 1.646 [0.000, 3.000], mean observation: 0.075 [-0.916, 2.347], loss: 1.485294, mean_absolute_error: 35.256006, mean_q: 20.423535, mean_eps: 0.589647
  456119/2000000: episode: 4255, duration: 1.862s, episode steps: 130, steps per second: 70, episode reward: -211.135, mean reward: -1.624 [-100.000, 56.384], mean action: 1.715 [0.000, 3.000], mean observation: 0.140 [-0.844, 1.628], loss: 2.604200, mean_absolute_error: 35.799863, mean_q: 19.981866, mean_eps: 0.589551
  456276/2000000: episode: 4256, duration: 2.299s, episode steps: 157, steps per second: 68, episode reward: -82.818, mean reward: -0.528 [-100.000, 10.186], mean action: 1.688 [0.000, 3.000], mean observation: -0.034 [-2.394, 1.000], loss: 2.109361, mean_absolute_error: 36.821681, mean_q: 17.170639, mean_eps: 0.589424
  456364/2000000: episode: 4257, duration: 1.320s, episode steps: 88, steps per second: 67, episode reward: -87.760, mean reward: -0.997 [-100.000, 12.865], mean action: 1.705 [0.000, 3.000], mean observation: -0.013 [-1.085, 3.070], loss: 2.090529, mean_absolute_error: 37.209491, mean_q: 16.929349, mean_eps: 0.589314
  456479/2000000: episode: 4258, duration: 1.650s, episode steps: 115, steps per second: 70, episode reward: -89.101, mean reward: -0.775 [-100.000, 14.366], mean action: 1.565 [0.000, 3.000], mean observation: 0.099 [-0.965, 2.215], loss: 1.313146, mean_absolute_error: 36.279173, mean_q: 22.762620, mean_eps: 0.589222
  456613/2000000: episode: 4259, duration: 1.931s, episode steps: 134, steps per second: 69, episode reward: -53.213, mean reward: -0.397 [-100.000, 13.805], mean action: 1.754 [0.000, 3.000], mean observation: -0.089 [-0.822, 1.781], loss: 1.645327, mean_absolute_error: 36.183041, mean_q: 20.513850, mean_eps: 0.589109
  456721/2000000: episode: 4260, duration: 1.536s, episode steps: 108, steps per second: 70, episode reward: -121.607, mean reward: -1.126 [-100.000, 12.799], mean action: 1.667 [0.000, 3.000], mean observation: -0.092 [-1.026, 2.868], loss: 1.175807, mean_absolute_error: 34.799703, mean_q: 21.809571, mean_eps: 0.588999
  456844/2000000: episode: 4261, duration: 1.781s, episode steps: 123, steps per second: 69, episode reward: -103.885, mean reward: -0.845 [-100.000, 11.337], mean action: 1.675 [0.000, 3.000], mean observation: 0.064 [-1.012, 3.659], loss: 1.671845, mean_absolute_error: 35.381094, mean_q: 19.235320, mean_eps: 0.588896
  457010/2000000: episode: 4262, duration: 2.428s, episode steps: 166, steps per second: 68, episode reward: -28.478, mean reward: -0.172 [-100.000, 26.722], mean action: 1.741 [0.000, 3.000], mean observation: -0.002 [-0.759, 1.021], loss: 1.747134, mean_absolute_error: 36.102931, mean_q: 20.747184, mean_eps: 0.588767
  457162/2000000: episode: 4263, duration: 2.186s, episode steps: 152, steps per second: 70, episode reward: -97.977, mean reward: -0.645 [-100.000, 12.325], mean action: 1.605 [0.000, 3.000], mean observation: 0.002 [-2.974, 1.000], loss: 1.817240, mean_absolute_error: 35.300743, mean_q: 20.688119, mean_eps: 0.588623
  457260/2000000: episode: 4264, duration: 1.429s, episode steps: 98, steps per second: 69, episode reward: -106.965, mean reward: -1.091 [-100.000, 8.418], mean action: 1.612 [0.000, 3.000], mean observation: -0.077 [-1.070, 1.449], loss: 1.677582, mean_absolute_error: 35.844835, mean_q: 18.948699, mean_eps: 0.588511
  457374/2000000: episode: 4265, duration: 1.680s, episode steps: 114, steps per second: 68, episode reward: -136.707, mean reward: -1.199 [-100.000, 12.404], mean action: 1.772 [0.000, 3.000], mean observation: -0.117 [-0.980, 1.529], loss: 1.479588, mean_absolute_error: 35.625161, mean_q: 20.839169, mean_eps: 0.588416
  457562/2000000: episode: 4266, duration: 2.727s, episode steps: 188, steps per second: 69, episode reward: -91.169, mean reward: -0.485 [-100.000, 49.047], mean action: 1.787 [0.000, 3.000], mean observation: -0.053 [-0.745, 1.000], loss: 1.500372, mean_absolute_error: 36.164605, mean_q: 20.296884, mean_eps: 0.588279
  457704/2000000: episode: 4267, duration: 2.221s, episode steps: 142, steps per second: 64, episode reward: -123.266, mean reward: -0.868 [-100.000, 10.467], mean action: 1.704 [0.000, 3.000], mean observation: -0.080 [-2.399, 1.072], loss: 1.887172, mean_absolute_error: 36.029621, mean_q: 19.426185, mean_eps: 0.588131
  457800/2000000: episode: 4268, duration: 1.470s, episode steps: 96, steps per second: 65, episode reward: -71.284, mean reward: -0.743 [-100.000, 16.709], mean action: 1.854 [0.000, 3.000], mean observation: -0.149 [-0.819, 2.275], loss: 2.573817, mean_absolute_error: 36.715734, mean_q: 20.293606, mean_eps: 0.588025
  457895/2000000: episode: 4269, duration: 1.386s, episode steps: 95, steps per second: 69, episode reward: -53.610, mean reward: -0.564 [-100.000, 13.327], mean action: 1.737 [0.000, 3.000], mean observation: -0.014 [-0.957, 1.000], loss: 1.645376, mean_absolute_error: 36.388176, mean_q: 18.213140, mean_eps: 0.587939
  458081/2000000: episode: 4270, duration: 2.682s, episode steps: 186, steps per second: 69, episode reward: -37.549, mean reward: -0.202 [-100.000, 17.651], mean action: 1.737 [0.000, 3.000], mean observation: 0.017 [-0.736, 1.000], loss: 1.636916, mean_absolute_error: 35.738763, mean_q: 21.268774, mean_eps: 0.587811
  458166/2000000: episode: 4271, duration: 1.239s, episode steps: 85, steps per second: 69, episode reward: -93.801, mean reward: -1.104 [-100.000, 9.892], mean action: 1.871 [0.000, 3.000], mean observation: -0.145 [-3.054, 1.000], loss: 1.770209, mean_absolute_error: 35.383347, mean_q: 19.528781, mean_eps: 0.587688
  458313/2000000: episode: 4272, duration: 2.164s, episode steps: 147, steps per second: 68, episode reward: -92.937, mean reward: -0.632 [-100.000, 6.663], mean action: 1.605 [0.000, 3.000], mean observation: -0.027 [-2.588, 1.000], loss: 1.607470, mean_absolute_error: 35.630687, mean_q: 22.297811, mean_eps: 0.587584
  458447/2000000: episode: 4273, duration: 1.895s, episode steps: 134, steps per second: 71, episode reward: -129.695, mean reward: -0.968 [-100.000, 17.780], mean action: 1.746 [0.000, 3.000], mean observation: 0.110 [-0.804, 2.464], loss: 1.684092, mean_absolute_error: 36.042660, mean_q: 19.486504, mean_eps: 0.587458
  458566/2000000: episode: 4274, duration: 1.694s, episode steps: 119, steps per second: 70, episode reward: -89.907, mean reward: -0.756 [-100.000, 12.806], mean action: 1.639 [0.000, 3.000], mean observation: -0.013 [-0.957, 3.188], loss: 1.585517, mean_absolute_error: 36.035732, mean_q: 22.739825, mean_eps: 0.587345
  458694/2000000: episode: 4275, duration: 1.845s, episode steps: 128, steps per second: 69, episode reward: -118.603, mean reward: -0.927 [-100.000, 11.677], mean action: 1.617 [0.000, 3.000], mean observation: 0.119 [-1.037, 1.564], loss: 1.638558, mean_absolute_error: 35.543433, mean_q: 19.168583, mean_eps: 0.587233
  458846/2000000: episode: 4276, duration: 2.194s, episode steps: 152, steps per second: 69, episode reward: -79.834, mean reward: -0.525 [-100.000, 9.672], mean action: 1.730 [0.000, 3.000], mean observation: 0.142 [-0.932, 2.463], loss: 1.359113, mean_absolute_error: 36.270630, mean_q: 22.815936, mean_eps: 0.587107
  459015/2000000: episode: 4277, duration: 2.444s, episode steps: 169, steps per second: 69, episode reward: -68.808, mean reward: -0.407 [-100.000, 26.122], mean action: 1.722 [0.000, 3.000], mean observation: -0.067 [-1.277, 1.110], loss: 2.253371, mean_absolute_error: 36.992227, mean_q: 18.210421, mean_eps: 0.586963
  459201/2000000: episode: 4278, duration: 2.689s, episode steps: 186, steps per second: 69, episode reward: -224.706, mean reward: -1.208 [-100.000, 7.951], mean action: 1.704 [0.000, 3.000], mean observation: -0.078 [-1.003, 1.139], loss: 1.586363, mean_absolute_error: 35.664176, mean_q: 21.118804, mean_eps: 0.586803
  459408/2000000: episode: 4279, duration: 3.001s, episode steps: 207, steps per second: 69, episode reward: -165.113, mean reward: -0.798 [-100.000, 13.980], mean action: 1.821 [0.000, 3.000], mean observation: -0.085 [-1.002, 1.000], loss: 1.465729, mean_absolute_error: 36.014391, mean_q: 21.025355, mean_eps: 0.586626
  459542/2000000: episode: 4280, duration: 1.980s, episode steps: 134, steps per second: 68, episode reward: -90.924, mean reward: -0.679 [-100.000, 12.157], mean action: 1.597 [0.000, 3.000], mean observation: 0.068 [-1.222, 1.000], loss: 1.508319, mean_absolute_error: 35.932925, mean_q: 22.295057, mean_eps: 0.586473
  459634/2000000: episode: 4281, duration: 1.466s, episode steps: 92, steps per second: 63, episode reward: -77.811, mean reward: -0.846 [-100.000, 12.343], mean action: 1.772 [0.000, 3.000], mean observation: 0.031 [-0.933, 1.000], loss: 2.029602, mean_absolute_error: 35.910464, mean_q: 20.062421, mean_eps: 0.586371
  459748/2000000: episode: 4282, duration: 1.753s, episode steps: 114, steps per second: 65, episode reward: -154.126, mean reward: -1.352 [-100.000, 22.632], mean action: 1.763 [0.000, 3.000], mean observation: -0.117 [-1.289, 3.074], loss: 1.328262, mean_absolute_error: 35.860125, mean_q: 18.484348, mean_eps: 0.586279
  459831/2000000: episode: 4283, duration: 1.200s, episode steps: 83, steps per second: 69, episode reward: -125.046, mean reward: -1.507 [-100.000, 8.101], mean action: 1.759 [0.000, 3.000], mean observation: -0.028 [-1.098, 3.796], loss: 1.184784, mean_absolute_error: 36.263210, mean_q: 20.391123, mean_eps: 0.586191
  459951/2000000: episode: 4284, duration: 1.726s, episode steps: 120, steps per second: 70, episode reward: -52.696, mean reward: -0.439 [-100.000, 16.776], mean action: 1.783 [0.000, 3.000], mean observation: -0.096 [-0.718, 1.000], loss: 2.550159, mean_absolute_error: 36.609635, mean_q: 19.312185, mean_eps: 0.586099
  460033/2000000: episode: 4285, duration: 1.308s, episode steps: 82, steps per second: 63, episode reward: -89.761, mean reward: -1.095 [-100.000, 7.547], mean action: 1.695 [0.000, 3.000], mean observation: 0.061 [-3.703, 1.000], loss: 1.527390, mean_absolute_error: 35.649281, mean_q: 21.781810, mean_eps: 0.586007
  460128/2000000: episode: 4286, duration: 1.378s, episode steps: 95, steps per second: 69, episode reward: -136.112, mean reward: -1.433 [-100.000, 9.606], mean action: 1.758 [0.000, 3.000], mean observation: -0.049 [-0.930, 1.344], loss: 1.687858, mean_absolute_error: 36.272113, mean_q: 23.248356, mean_eps: 0.585928
  460299/2000000: episode: 4287, duration: 2.470s, episode steps: 171, steps per second: 69, episode reward: -32.292, mean reward: -0.189 [-100.000, 18.142], mean action: 1.737 [0.000, 3.000], mean observation: -0.055 [-0.700, 1.000], loss: 2.037460, mean_absolute_error: 35.618318, mean_q: 18.986950, mean_eps: 0.585809
  460575/2000000: episode: 4288, duration: 3.983s, episode steps: 276, steps per second: 69, episode reward: -99.412, mean reward: -0.360 [-100.000, 29.294], mean action: 1.754 [0.000, 3.000], mean observation: 0.092 [-0.947, 1.919], loss: 1.485186, mean_absolute_error: 35.200792, mean_q: 20.924796, mean_eps: 0.585608
  460652/2000000: episode: 4289, duration: 1.142s, episode steps: 77, steps per second: 67, episode reward: -119.766, mean reward: -1.555 [-100.000, 8.169], mean action: 1.545 [0.000, 3.000], mean observation: -0.138 [-1.184, 1.000], loss: 1.266061, mean_absolute_error: 36.024658, mean_q: 20.434279, mean_eps: 0.585449
  460811/2000000: episode: 4290, duration: 2.268s, episode steps: 159, steps per second: 70, episode reward: -14.847, mean reward: -0.093 [-100.000, 16.606], mean action: 1.654 [0.000, 3.000], mean observation: 0.146 [-1.057, 1.000], loss: 1.481546, mean_absolute_error: 36.195217, mean_q: 19.465076, mean_eps: 0.585343
  460934/2000000: episode: 4291, duration: 1.758s, episode steps: 123, steps per second: 70, episode reward: -105.750, mean reward: -0.860 [-100.000, 11.589], mean action: 1.691 [0.000, 3.000], mean observation: 0.113 [-0.834, 1.041], loss: 1.746753, mean_absolute_error: 35.481976, mean_q: 22.708641, mean_eps: 0.585215
  461014/2000000: episode: 4292, duration: 1.154s, episode steps: 80, steps per second: 69, episode reward: -84.560, mean reward: -1.057 [-100.000, 15.504], mean action: 1.650 [0.000, 3.000], mean observation: -0.053 [-1.136, 1.000], loss: 1.207673, mean_absolute_error: 36.988789, mean_q: 21.033938, mean_eps: 0.585123
  461193/2000000: episode: 4293, duration: 2.540s, episode steps: 179, steps per second: 70, episode reward: -7.204, mean reward: -0.040 [-100.000, 13.804], mean action: 1.676 [0.000, 3.000], mean observation: 0.002 [-0.954, 1.055], loss: 1.296901, mean_absolute_error: 35.823427, mean_q: 23.330563, mean_eps: 0.585006
  461361/2000000: episode: 4294, duration: 2.367s, episode steps: 168, steps per second: 71, episode reward: -55.506, mean reward: -0.330 [-100.000, 73.863], mean action: 1.536 [0.000, 3.000], mean observation: 0.042 [-1.454, 1.000], loss: 2.231950, mean_absolute_error: 36.666413, mean_q: 18.743872, mean_eps: 0.584850
  461470/2000000: episode: 4295, duration: 1.538s, episode steps: 109, steps per second: 71, episode reward: -253.219, mean reward: -2.323 [-100.000, 47.603], mean action: 1.670 [0.000, 3.000], mean observation: 0.174 [-1.031, 2.000], loss: 1.691489, mean_absolute_error: 36.563568, mean_q: 22.088940, mean_eps: 0.584726
  461547/2000000: episode: 4296, duration: 1.105s, episode steps: 77, steps per second: 70, episode reward: -148.013, mean reward: -1.922 [-100.000, 11.138], mean action: 1.935 [0.000, 3.000], mean observation: -0.167 [-1.129, 3.889], loss: 2.160812, mean_absolute_error: 36.234408, mean_q: 20.678644, mean_eps: 0.584643
  461665/2000000: episode: 4297, duration: 1.703s, episode steps: 118, steps per second: 69, episode reward: -81.182, mean reward: -0.688 [-100.000, 9.830], mean action: 1.763 [0.000, 3.000], mean observation: 0.137 [-0.715, 2.399], loss: 1.690854, mean_absolute_error: 36.066625, mean_q: 21.320687, mean_eps: 0.584555
  461825/2000000: episode: 4298, duration: 2.286s, episode steps: 160, steps per second: 70, episode reward: -58.113, mean reward: -0.363 [-100.000, 18.958], mean action: 1.725 [0.000, 3.000], mean observation: 0.002 [-0.735, 2.285], loss: 1.721592, mean_absolute_error: 35.251579, mean_q: 21.678280, mean_eps: 0.584429
  461965/2000000: episode: 4299, duration: 1.979s, episode steps: 140, steps per second: 71, episode reward: -114.290, mean reward: -0.816 [-100.000, 10.207], mean action: 1.707 [0.000, 3.000], mean observation: 0.019 [-0.921, 2.439], loss: 1.701398, mean_absolute_error: 36.162237, mean_q: 21.003794, mean_eps: 0.584294
  462065/2000000: episode: 4300, duration: 1.440s, episode steps: 100, steps per second: 69, episode reward: -106.373, mean reward: -1.064 [-100.000, 14.706], mean action: 1.650 [0.000, 3.000], mean observation: 0.159 [-1.009, 1.000], loss: 1.596691, mean_absolute_error: 36.065264, mean_q: 21.536739, mean_eps: 0.584186
  462426/2000000: episode: 4301, duration: 5.256s, episode steps: 361, steps per second: 69, episode reward: -96.883, mean reward: -0.268 [-100.000, 44.649], mean action: 1.643 [0.000, 3.000], mean observation: 0.176 [-0.917, 1.000], loss: 2.026043, mean_absolute_error: 35.848903, mean_q: 21.484229, mean_eps: 0.583979
  462547/2000000: episode: 4302, duration: 1.759s, episode steps: 121, steps per second: 69, episode reward: -109.379, mean reward: -0.904 [-100.000, 8.336], mean action: 1.653 [0.000, 3.000], mean observation: 0.014 [-0.931, 2.570], loss: 1.829148, mean_absolute_error: 35.396409, mean_q: 21.904227, mean_eps: 0.583763
  462652/2000000: episode: 4303, duration: 1.547s, episode steps: 105, steps per second: 68, episode reward: -124.282, mean reward: -1.184 [-100.000, 12.066], mean action: 1.600 [0.000, 3.000], mean observation: -0.116 [-1.322, 3.489], loss: 1.379545, mean_absolute_error: 35.378990, mean_q: 19.751123, mean_eps: 0.583662
  462771/2000000: episode: 4304, duration: 1.711s, episode steps: 119, steps per second: 70, episode reward: -62.367, mean reward: -0.524 [-100.000, 13.658], mean action: 1.622 [0.000, 3.000], mean observation: 0.074 [-1.056, 1.000], loss: 1.460995, mean_absolute_error: 35.072755, mean_q: 21.935719, mean_eps: 0.583561
  462916/2000000: episode: 4305, duration: 2.120s, episode steps: 145, steps per second: 68, episode reward: -75.167, mean reward: -0.518 [-100.000, 14.389], mean action: 1.614 [0.000, 3.000], mean observation: -0.005 [-1.253, 1.147], loss: 1.334302, mean_absolute_error: 35.191764, mean_q: 20.222071, mean_eps: 0.583442
  463008/2000000: episode: 4306, duration: 1.383s, episode steps: 92, steps per second: 67, episode reward: -110.276, mean reward: -1.199 [-100.000, 10.882], mean action: 1.739 [0.000, 3.000], mean observation: 0.083 [-3.356, 1.000], loss: 2.110724, mean_absolute_error: 37.059544, mean_q: 21.935317, mean_eps: 0.583336
  463251/2000000: episode: 4307, duration: 3.536s, episode steps: 243, steps per second: 69, episode reward: -38.904, mean reward: -0.160 [-100.000, 14.052], mean action: 1.675 [0.000, 3.000], mean observation: 0.064 [-0.985, 1.043], loss: 1.617518, mean_absolute_error: 35.939799, mean_q: 20.751985, mean_eps: 0.583185
  463375/2000000: episode: 4308, duration: 1.777s, episode steps: 124, steps per second: 70, episode reward: -125.085, mean reward: -1.009 [-100.000, 7.314], mean action: 1.629 [0.000, 3.000], mean observation: 0.101 [-2.710, 1.000], loss: 1.853011, mean_absolute_error: 36.418995, mean_q: 18.606821, mean_eps: 0.583019
  463524/2000000: episode: 4309, duration: 2.185s, episode steps: 149, steps per second: 68, episode reward: -85.446, mean reward: -0.573 [-100.000, 8.173], mean action: 1.597 [0.000, 3.000], mean observation: 0.041 [-0.842, 2.876], loss: 1.371300, mean_absolute_error: 35.485517, mean_q: 24.624365, mean_eps: 0.582897
  463668/2000000: episode: 4310, duration: 2.143s, episode steps: 144, steps per second: 67, episode reward: -73.175, mean reward: -0.508 [-100.000, 12.633], mean action: 1.653 [0.000, 3.000], mean observation: -0.039 [-1.199, 1.000], loss: 1.647669, mean_absolute_error: 36.394926, mean_q: 20.334834, mean_eps: 0.582765
  463745/2000000: episode: 4311, duration: 1.170s, episode steps: 77, steps per second: 66, episode reward: -98.116, mean reward: -1.274 [-100.000, 17.199], mean action: 1.610 [0.000, 3.000], mean observation: 0.066 [-1.138, 1.000], loss: 2.640998, mean_absolute_error: 37.450202, mean_q: 18.057531, mean_eps: 0.582665
  463998/2000000: episode: 4312, duration: 3.622s, episode steps: 253, steps per second: 70, episode reward: -104.203, mean reward: -0.412 [-100.000, 12.953], mean action: 1.708 [0.000, 3.000], mean observation: 0.123 [-3.134, 1.001], loss: 1.726314, mean_absolute_error: 37.032650, mean_q: 20.176574, mean_eps: 0.582515
  464067/2000000: episode: 4313, duration: 0.991s, episode steps: 69, steps per second: 70, episode reward: -93.665, mean reward: -1.357 [-100.000, 11.909], mean action: 1.609 [0.000, 3.000], mean observation: 0.083 [-1.120, 3.989], loss: 2.803409, mean_absolute_error: 35.509207, mean_q: 17.290209, mean_eps: 0.582371
  464193/2000000: episode: 4314, duration: 1.827s, episode steps: 126, steps per second: 69, episode reward: -78.633, mean reward: -0.624 [-100.000, 10.679], mean action: 1.754 [0.000, 3.000], mean observation: 0.036 [-1.082, 3.207], loss: 1.315429, mean_absolute_error: 36.288577, mean_q: 21.165593, mean_eps: 0.582283
  464366/2000000: episode: 4315, duration: 2.480s, episode steps: 173, steps per second: 70, episode reward: -54.485, mean reward: -0.315 [-100.000, 23.057], mean action: 1.566 [0.000, 3.000], mean observation: -0.014 [-0.806, 1.004], loss: 2.477958, mean_absolute_error: 35.897947, mean_q: 20.629765, mean_eps: 0.582148
  464450/2000000: episode: 4316, duration: 1.217s, episode steps: 84, steps per second: 69, episode reward: -94.914, mean reward: -1.130 [-100.000, 7.318], mean action: 1.738 [0.000, 3.000], mean observation: -0.056 [-3.416, 1.000], loss: 2.095947, mean_absolute_error: 36.138035, mean_q: 20.489708, mean_eps: 0.582033
  464624/2000000: episode: 4317, duration: 2.511s, episode steps: 174, steps per second: 69, episode reward: -58.554, mean reward: -0.337 [-100.000, 16.943], mean action: 1.575 [0.000, 3.000], mean observation: -0.002 [-0.696, 1.432], loss: 2.424535, mean_absolute_error: 35.772587, mean_q: 21.473611, mean_eps: 0.581918
  464719/2000000: episode: 4318, duration: 1.412s, episode steps: 95, steps per second: 67, episode reward: -86.961, mean reward: -0.915 [-100.000, 15.600], mean action: 1.779 [0.000, 3.000], mean observation: -0.135 [-0.923, 1.000], loss: 1.803586, mean_absolute_error: 36.046706, mean_q: 17.362729, mean_eps: 0.581797
  464874/2000000: episode: 4319, duration: 2.258s, episode steps: 155, steps per second: 69, episode reward: -51.903, mean reward: -0.335 [-100.000, 29.586], mean action: 1.703 [0.000, 3.000], mean observation: -0.025 [-2.023, 1.000], loss: 2.172859, mean_absolute_error: 35.763893, mean_q: 19.243074, mean_eps: 0.581684
  464986/2000000: episode: 4320, duration: 1.634s, episode steps: 112, steps per second: 69, episode reward: -55.569, mean reward: -0.496 [-100.000, 17.134], mean action: 1.652 [0.000, 3.000], mean observation: 0.020 [-1.026, 1.000], loss: 1.358643, mean_absolute_error: 35.579849, mean_q: 21.630390, mean_eps: 0.581563
  465088/2000000: episode: 4321, duration: 1.474s, episode steps: 102, steps per second: 69, episode reward: -76.370, mean reward: -0.749 [-100.000, 6.685], mean action: 1.824 [0.000, 3.000], mean observation: -0.046 [-0.901, 2.999], loss: 2.015822, mean_absolute_error: 35.749332, mean_q: 25.158171, mean_eps: 0.581468
  466088/2000000: episode: 4322, duration: 15.555s, episode steps: 1000, steps per second: 64, episode reward: 51.450, mean reward: 0.051 [-25.039, 24.399], mean action: 1.596 [0.000, 3.000], mean observation: 0.114 [-0.723, 1.000], loss: 1.807172, mean_absolute_error: 36.130300, mean_q: 20.311740, mean_eps: 0.580973
  466200/2000000: episode: 4323, duration: 1.632s, episode steps: 112, steps per second: 69, episode reward: -73.000, mean reward: -0.652 [-100.000, 20.536], mean action: 1.562 [0.000, 3.000], mean observation: 0.126 [-0.690, 1.000], loss: 2.155687, mean_absolute_error: 35.495342, mean_q: 22.412824, mean_eps: 0.580472
  466292/2000000: episode: 4324, duration: 1.384s, episode steps: 92, steps per second: 66, episode reward: -124.599, mean reward: -1.354 [-100.000, 9.344], mean action: 1.663 [0.000, 3.000], mean observation: -0.050 [-3.642, 1.000], loss: 1.683890, mean_absolute_error: 36.425515, mean_q: 18.490208, mean_eps: 0.580380
  466400/2000000: episode: 4325, duration: 1.601s, episode steps: 108, steps per second: 67, episode reward: -61.722, mean reward: -0.572 [-100.000, 10.380], mean action: 1.741 [0.000, 3.000], mean observation: 0.094 [-0.720, 1.456], loss: 2.372163, mean_absolute_error: 37.879641, mean_q: 16.450137, mean_eps: 0.580290
  466515/2000000: episode: 4326, duration: 1.677s, episode steps: 115, steps per second: 69, episode reward: -69.524, mean reward: -0.605 [-100.000, 11.270], mean action: 1.687 [0.000, 3.000], mean observation: 0.006 [-0.990, 1.531], loss: 1.368087, mean_absolute_error: 34.814942, mean_q: 20.874816, mean_eps: 0.580190
  466616/2000000: episode: 4327, duration: 1.509s, episode steps: 101, steps per second: 67, episode reward: -112.421, mean reward: -1.113 [-100.000, 8.968], mean action: 1.644 [0.000, 3.000], mean observation: 0.095 [-1.834, 1.000], loss: 1.356457, mean_absolute_error: 34.512152, mean_q: 22.206147, mean_eps: 0.580092
  466740/2000000: episode: 4328, duration: 1.828s, episode steps: 124, steps per second: 68, episode reward: -64.115, mean reward: -0.517 [-100.000, 32.403], mean action: 1.621 [0.000, 3.000], mean observation: 0.109 [-1.351, 1.000], loss: 2.488608, mean_absolute_error: 36.502631, mean_q: 19.190776, mean_eps: 0.579992
  466828/2000000: episode: 4329, duration: 1.334s, episode steps: 88, steps per second: 66, episode reward: -91.116, mean reward: -1.035 [-100.000, 14.439], mean action: 1.659 [0.000, 3.000], mean observation: 0.033 [-3.730, 1.000], loss: 1.874603, mean_absolute_error: 36.160330, mean_q: 20.473157, mean_eps: 0.579896
  466963/2000000: episode: 4330, duration: 1.966s, episode steps: 135, steps per second: 69, episode reward: -33.154, mean reward: -0.246 [-100.000, 11.568], mean action: 1.659 [0.000, 3.000], mean observation: 0.096 [-1.007, 1.005], loss: 2.375006, mean_absolute_error: 35.588206, mean_q: 19.767015, mean_eps: 0.579795
  467114/2000000: episode: 4331, duration: 2.171s, episode steps: 151, steps per second: 70, episode reward: -93.026, mean reward: -0.616 [-100.000, 17.300], mean action: 1.762 [0.000, 3.000], mean observation: -0.009 [-0.801, 2.515], loss: 1.401959, mean_absolute_error: 35.504639, mean_q: 21.209837, mean_eps: 0.579666
  467212/2000000: episode: 4332, duration: 1.423s, episode steps: 98, steps per second: 69, episode reward: -144.047, mean reward: -1.470 [-100.000, 11.742], mean action: 1.816 [0.000, 3.000], mean observation: -0.027 [-0.995, 3.688], loss: 1.498076, mean_absolute_error: 36.392084, mean_q: 19.352810, mean_eps: 0.579554
  467371/2000000: episode: 4333, duration: 2.271s, episode steps: 159, steps per second: 70, episode reward: -58.202, mean reward: -0.366 [-100.000, 18.442], mean action: 1.597 [0.000, 3.000], mean observation: 0.110 [-0.883, 1.807], loss: 1.409578, mean_absolute_error: 35.171953, mean_q: 21.554709, mean_eps: 0.579439
  467510/2000000: episode: 4334, duration: 1.997s, episode steps: 139, steps per second: 70, episode reward: -128.069, mean reward: -0.921 [-100.000, 8.469], mean action: 1.633 [0.000, 3.000], mean observation: -0.068 [-2.713, 1.000], loss: 1.647145, mean_absolute_error: 36.004058, mean_q: 20.171406, mean_eps: 0.579304
  467587/2000000: episode: 4335, duration: 1.112s, episode steps: 77, steps per second: 69, episode reward: -52.475, mean reward: -0.681 [-100.000, 20.896], mean action: 1.727 [0.000, 3.000], mean observation: 0.068 [-0.904, 3.105], loss: 1.386295, mean_absolute_error: 34.124417, mean_q: 20.259083, mean_eps: 0.579207
  467727/2000000: episode: 4336, duration: 2.040s, episode steps: 140, steps per second: 69, episode reward: -36.802, mean reward: -0.263 [-100.000, 19.270], mean action: 1.750 [0.000, 3.000], mean observation: -0.016 [-0.693, 1.071], loss: 2.102116, mean_absolute_error: 36.124606, mean_q: 19.724800, mean_eps: 0.579110
  467927/2000000: episode: 4337, duration: 2.906s, episode steps: 200, steps per second: 69, episode reward: -103.207, mean reward: -0.516 [-100.000, 21.305], mean action: 1.700 [0.000, 3.000], mean observation: 0.070 [-1.928, 1.000], loss: 1.634797, mean_absolute_error: 36.042736, mean_q: 21.670359, mean_eps: 0.578957
  468058/2000000: episode: 4338, duration: 1.891s, episode steps: 131, steps per second: 69, episode reward: -106.321, mean reward: -0.812 [-100.000, 17.106], mean action: 1.641 [0.000, 3.000], mean observation: 0.063 [-3.993, 1.000], loss: 1.676716, mean_absolute_error: 35.879347, mean_q: 20.947684, mean_eps: 0.578807
  468133/2000000: episode: 4339, duration: 1.121s, episode steps: 75, steps per second: 67, episode reward: -87.965, mean reward: -1.173 [-100.000, 15.154], mean action: 1.880 [0.000, 3.000], mean observation: -0.116 [-3.301, 1.000], loss: 1.399955, mean_absolute_error: 35.757240, mean_q: 20.407967, mean_eps: 0.578714
  468262/2000000: episode: 4340, duration: 1.827s, episode steps: 129, steps per second: 71, episode reward: -97.477, mean reward: -0.756 [-100.000, 9.310], mean action: 1.605 [0.000, 3.000], mean observation: -0.097 [-0.763, 1.000], loss: 1.869851, mean_absolute_error: 34.360849, mean_q: 19.603733, mean_eps: 0.578622
  469262/2000000: episode: 4341, duration: 16.048s, episode steps: 1000, steps per second: 62, episode reward: 72.746, mean reward: 0.073 [-21.129, 23.834], mean action: 1.893 [0.000, 3.000], mean observation: 0.164 [-0.607, 1.000], loss: 1.895991, mean_absolute_error: 36.033258, mean_q: 20.711817, mean_eps: 0.578114
  469360/2000000: episode: 4342, duration: 1.447s, episode steps: 98, steps per second: 68, episode reward: -90.992, mean reward: -0.928 [-100.000, 12.871], mean action: 1.684 [0.000, 3.000], mean observation: -0.085 [-0.901, 1.000], loss: 1.960338, mean_absolute_error: 35.023131, mean_q: 16.292182, mean_eps: 0.577621
  469495/2000000: episode: 4343, duration: 1.960s, episode steps: 135, steps per second: 69, episode reward: -101.858, mean reward: -0.755 [-100.000, 33.752], mean action: 1.652 [0.000, 3.000], mean observation: -0.121 [-1.055, 2.228], loss: 1.588121, mean_absolute_error: 36.233063, mean_q: 20.603663, mean_eps: 0.577517
  469642/2000000: episode: 4344, duration: 2.167s, episode steps: 147, steps per second: 68, episode reward: -49.609, mean reward: -0.337 [-100.000, 19.246], mean action: 1.816 [0.000, 3.000], mean observation: 0.053 [-0.670, 2.221], loss: 2.442360, mean_absolute_error: 37.212025, mean_q: 18.472221, mean_eps: 0.577389
  469762/2000000: episode: 4345, duration: 1.756s, episode steps: 120, steps per second: 68, episode reward: -119.040, mean reward: -0.992 [-100.000, 6.210], mean action: 1.633 [0.000, 3.000], mean observation: 0.049 [-3.563, 1.000], loss: 1.555330, mean_absolute_error: 36.934013, mean_q: 18.775788, mean_eps: 0.577268
  469918/2000000: episode: 4346, duration: 2.241s, episode steps: 156, steps per second: 70, episode reward: -157.310, mean reward: -1.008 [-100.000, 7.667], mean action: 1.679 [0.000, 3.000], mean observation: 0.047 [-4.078, 1.000], loss: 1.698143, mean_absolute_error: 35.467107, mean_q: 21.552031, mean_eps: 0.577144
  470036/2000000: episode: 4347, duration: 1.718s, episode steps: 118, steps per second: 69, episode reward: -106.216, mean reward: -0.900 [-100.000, 11.177], mean action: 1.712 [0.000, 3.000], mean observation: -0.123 [-1.507, 1.000], loss: 1.929149, mean_absolute_error: 34.202168, mean_q: 21.774188, mean_eps: 0.577022
  470144/2000000: episode: 4348, duration: 1.585s, episode steps: 108, steps per second: 68, episode reward: -111.490, mean reward: -1.032 [-100.000, 9.033], mean action: 1.630 [0.000, 3.000], mean observation: -0.009 [-0.912, 3.284], loss: 2.396481, mean_absolute_error: 37.110629, mean_q: 20.721574, mean_eps: 0.576921
  470253/2000000: episode: 4349, duration: 1.615s, episode steps: 109, steps per second: 67, episode reward: -91.588, mean reward: -0.840 [-100.000, 14.636], mean action: 1.725 [0.000, 3.000], mean observation: 0.027 [-0.970, 2.869], loss: 2.112728, mean_absolute_error: 37.260522, mean_q: 18.428099, mean_eps: 0.576822
  470408/2000000: episode: 4350, duration: 2.236s, episode steps: 155, steps per second: 69, episode reward: -118.549, mean reward: -0.765 [-100.000, 15.223], mean action: 1.581 [0.000, 3.000], mean observation: 0.048 [-0.745, 1.000], loss: 2.533257, mean_absolute_error: 37.213875, mean_q: 16.144421, mean_eps: 0.576703
  470605/2000000: episode: 4351, duration: 2.863s, episode steps: 197, steps per second: 69, episode reward: -106.276, mean reward: -0.539 [-100.000, 15.027], mean action: 1.695 [0.000, 3.000], mean observation: -0.069 [-1.491, 1.000], loss: 2.016855, mean_absolute_error: 36.993805, mean_q: 16.019237, mean_eps: 0.576545
  470695/2000000: episode: 4352, duration: 1.275s, episode steps: 90, steps per second: 71, episode reward: -35.550, mean reward: -0.395 [-100.000, 14.738], mean action: 1.689 [0.000, 3.000], mean observation: 0.010 [-0.887, 2.551], loss: 1.669621, mean_absolute_error: 36.435199, mean_q: 18.465148, mean_eps: 0.576415
  470828/2000000: episode: 4353, duration: 1.915s, episode steps: 133, steps per second: 69, episode reward: -87.159, mean reward: -0.655 [-100.000, 18.832], mean action: 1.699 [0.000, 3.000], mean observation: -0.083 [-0.779, 2.545], loss: 1.981417, mean_absolute_error: 36.160585, mean_q: 19.731319, mean_eps: 0.576316
  471004/2000000: episode: 4354, duration: 2.589s, episode steps: 176, steps per second: 68, episode reward: -101.086, mean reward: -0.574 [-100.000, 9.120], mean action: 1.653 [0.000, 3.000], mean observation: 0.013 [-2.652, 1.000], loss: 1.511695, mean_absolute_error: 35.136054, mean_q: 21.376341, mean_eps: 0.576177
  471123/2000000: episode: 4355, duration: 1.734s, episode steps: 119, steps per second: 69, episode reward: -62.863, mean reward: -0.528 [-100.000, 22.441], mean action: 1.664 [0.000, 3.000], mean observation: 0.089 [-0.960, 1.357], loss: 1.372906, mean_absolute_error: 35.150594, mean_q: 21.858584, mean_eps: 0.576044
  471266/2000000: episode: 4356, duration: 2.056s, episode steps: 143, steps per second: 70, episode reward: -205.293, mean reward: -1.436 [-100.000, 40.407], mean action: 1.741 [0.000, 3.000], mean observation: 0.157 [-0.742, 2.141], loss: 1.839472, mean_absolute_error: 35.811522, mean_q: 22.737810, mean_eps: 0.575925
  471348/2000000: episode: 4357, duration: 1.210s, episode steps: 82, steps per second: 68, episode reward: -132.318, mean reward: -1.614 [-100.000, 7.046], mean action: 1.744 [0.000, 3.000], mean observation: 0.077 [-0.982, 3.474], loss: 1.345527, mean_absolute_error: 36.489893, mean_q: 19.789119, mean_eps: 0.575825
  471451/2000000: episode: 4358, duration: 1.496s, episode steps: 103, steps per second: 69, episode reward: -70.101, mean reward: -0.681 [-100.000, 10.769], mean action: 1.650 [0.000, 3.000], mean observation: 0.059 [-2.452, 1.000], loss: 1.568226, mean_absolute_error: 35.820201, mean_q: 20.390495, mean_eps: 0.575742
  471571/2000000: episode: 4359, duration: 1.740s, episode steps: 120, steps per second: 69, episode reward: -4.134, mean reward: -0.034 [-100.000, 27.217], mean action: 1.717 [0.000, 3.000], mean observation: 0.023 [-0.852, 1.000], loss: 1.390161, mean_absolute_error: 36.256381, mean_q: 18.447195, mean_eps: 0.575641
  471676/2000000: episode: 4360, duration: 1.551s, episode steps: 105, steps per second: 68, episode reward: -64.235, mean reward: -0.612 [-100.000, 18.655], mean action: 1.638 [0.000, 3.000], mean observation: 0.010 [-0.948, 1.000], loss: 1.475571, mean_absolute_error: 37.184478, mean_q: 20.106949, mean_eps: 0.575540
  471808/2000000: episode: 4361, duration: 1.940s, episode steps: 132, steps per second: 68, episode reward: -113.556, mean reward: -0.860 [-100.000, 6.808], mean action: 1.629 [0.000, 3.000], mean observation: -0.056 [-0.845, 2.756], loss: 1.696445, mean_absolute_error: 36.191844, mean_q: 20.367783, mean_eps: 0.575434
  472128/2000000: episode: 4362, duration: 4.717s, episode steps: 320, steps per second: 68, episode reward: -76.049, mean reward: -0.238 [-100.000, 8.507], mean action: 1.728 [0.000, 3.000], mean observation: 0.034 [-0.724, 1.661], loss: 1.566527, mean_absolute_error: 36.006533, mean_q: 20.303076, mean_eps: 0.575231
  473128/2000000: episode: 4363, duration: 16.197s, episode steps: 1000, steps per second: 62, episode reward: 41.306, mean reward: 0.041 [-24.525, 55.513], mean action: 1.572 [0.000, 3.000], mean observation: 0.226 [-0.712, 1.249], loss: 1.680485, mean_absolute_error: 36.052300, mean_q: 20.794810, mean_eps: 0.574637
  473223/2000000: episode: 4364, duration: 1.668s, episode steps: 95, steps per second: 57, episode reward: -44.623, mean reward: -0.470 [-100.000, 28.119], mean action: 1.716 [0.000, 3.000], mean observation: 0.012 [-0.890, 2.847], loss: 2.192272, mean_absolute_error: 37.837421, mean_q: 19.981728, mean_eps: 0.574143
  473354/2000000: episode: 4365, duration: 2.046s, episode steps: 131, steps per second: 64, episode reward: -217.349, mean reward: -1.659 [-100.000, 45.078], mean action: 1.779 [0.000, 3.000], mean observation: -0.180 [-1.865, 1.000], loss: 1.895926, mean_absolute_error: 36.851931, mean_q: 19.152024, mean_eps: 0.574041
  473455/2000000: episode: 4366, duration: 1.549s, episode steps: 101, steps per second: 65, episode reward: -72.828, mean reward: -0.721 [-100.000, 14.321], mean action: 1.762 [0.000, 3.000], mean observation: 0.076 [-0.940, 1.000], loss: 1.533223, mean_absolute_error: 35.217437, mean_q: 22.491701, mean_eps: 0.573936
  474455/2000000: episode: 4367, duration: 15.807s, episode steps: 1000, steps per second: 63, episode reward: 14.443, mean reward: 0.014 [-23.988, 63.539], mean action: 1.282 [0.000, 3.000], mean observation: 0.036 [-0.875, 1.000], loss: 1.708891, mean_absolute_error: 35.677984, mean_q: 20.247843, mean_eps: 0.573441
  474564/2000000: episode: 4368, duration: 1.632s, episode steps: 109, steps per second: 67, episode reward: -85.342, mean reward: -0.783 [-100.000, 11.563], mean action: 1.615 [0.000, 3.000], mean observation: 0.113 [-2.877, 1.000], loss: 1.786658, mean_absolute_error: 34.613307, mean_q: 19.536169, mean_eps: 0.572943
  474733/2000000: episode: 4369, duration: 2.455s, episode steps: 169, steps per second: 69, episode reward: -67.154, mean reward: -0.397 [-100.000, 10.919], mean action: 1.609 [0.000, 3.000], mean observation: 0.020 [-2.411, 1.000], loss: 2.398484, mean_absolute_error: 36.902046, mean_q: 20.298197, mean_eps: 0.572817
  474813/2000000: episode: 4370, duration: 1.168s, episode steps: 80, steps per second: 68, episode reward: -73.613, mean reward: -0.920 [-100.000, 7.872], mean action: 1.725 [0.000, 3.000], mean observation: -0.034 [-0.954, 3.119], loss: 1.834540, mean_absolute_error: 36.573558, mean_q: 15.393869, mean_eps: 0.572703
  475062/2000000: episode: 4371, duration: 3.575s, episode steps: 249, steps per second: 70, episode reward: -44.630, mean reward: -0.179 [-100.000, 15.292], mean action: 1.735 [0.000, 3.000], mean observation: 0.050 [-1.339, 1.000], loss: 1.390712, mean_absolute_error: 36.345383, mean_q: 19.873953, mean_eps: 0.572556
  475193/2000000: episode: 4372, duration: 1.888s, episode steps: 131, steps per second: 69, episode reward: -35.523, mean reward: -0.271 [-100.000, 18.623], mean action: 1.550 [0.000, 3.000], mean observation: -0.013 [-0.707, 1.441], loss: 1.598286, mean_absolute_error: 35.135731, mean_q: 21.438733, mean_eps: 0.572385
  475308/2000000: episode: 4373, duration: 1.651s, episode steps: 115, steps per second: 70, episode reward: -90.485, mean reward: -0.787 [-100.000, 11.228], mean action: 1.696 [0.000, 3.000], mean observation: -0.043 [-0.791, 2.360], loss: 1.493771, mean_absolute_error: 36.363757, mean_q: 19.776961, mean_eps: 0.572275
  476308/2000000: episode: 4374, duration: 15.498s, episode steps: 1000, steps per second: 65, episode reward: 29.737, mean reward: 0.030 [-24.262, 23.235], mean action: 1.518 [0.000, 3.000], mean observation: 0.171 [-0.634, 1.183], loss: 1.828249, mean_absolute_error: 35.972798, mean_q: 20.046261, mean_eps: 0.571775
  476476/2000000: episode: 4375, duration: 2.492s, episode steps: 168, steps per second: 67, episode reward: -120.101, mean reward: -0.715 [-100.000, 14.172], mean action: 1.690 [0.000, 3.000], mean observation: 0.052 [-0.827, 1.563], loss: 1.679033, mean_absolute_error: 37.081846, mean_q: 19.664795, mean_eps: 0.571249
  476645/2000000: episode: 4376, duration: 2.459s, episode steps: 169, steps per second: 69, episode reward: -52.432, mean reward: -0.310 [-100.000, 15.271], mean action: 1.675 [0.000, 3.000], mean observation: -0.037 [-0.872, 1.230], loss: 1.478308, mean_absolute_error: 36.864871, mean_q: 21.505268, mean_eps: 0.571096
  476885/2000000: episode: 4377, duration: 3.466s, episode steps: 240, steps per second: 69, episode reward: -122.871, mean reward: -0.512 [-100.000, 9.188], mean action: 1.696 [0.000, 3.000], mean observation: 0.004 [-0.730, 2.738], loss: 1.818807, mean_absolute_error: 36.656293, mean_q: 18.908961, mean_eps: 0.570911
  477032/2000000: episode: 4378, duration: 2.132s, episode steps: 147, steps per second: 69, episode reward: -76.149, mean reward: -0.518 [-100.000, 18.307], mean action: 1.571 [0.000, 3.000], mean observation: 0.122 [-1.226, 1.000], loss: 1.443751, mean_absolute_error: 35.984525, mean_q: 19.514215, mean_eps: 0.570738
  477180/2000000: episode: 4379, duration: 2.167s, episode steps: 148, steps per second: 68, episode reward: -54.021, mean reward: -0.365 [-100.000, 15.072], mean action: 1.723 [0.000, 3.000], mean observation: -0.083 [-0.678, 1.115], loss: 1.506560, mean_absolute_error: 35.920791, mean_q: 20.031821, mean_eps: 0.570606
  477329/2000000: episode: 4380, duration: 2.172s, episode steps: 149, steps per second: 69, episode reward: -47.272, mean reward: -0.317 [-100.000, 13.440], mean action: 1.792 [0.000, 3.000], mean observation: 0.002 [-0.829, 1.017], loss: 2.259271, mean_absolute_error: 36.558962, mean_q: 18.957397, mean_eps: 0.570471
  477482/2000000: episode: 4381, duration: 2.188s, episode steps: 153, steps per second: 70, episode reward: -61.407, mean reward: -0.401 [-100.000, 32.221], mean action: 1.712 [0.000, 3.000], mean observation: -0.115 [-1.002, 2.180], loss: 1.425731, mean_absolute_error: 35.409102, mean_q: 22.250091, mean_eps: 0.570335
  477644/2000000: episode: 4382, duration: 2.410s, episode steps: 162, steps per second: 67, episode reward: -37.301, mean reward: -0.230 [-100.000, 12.139], mean action: 1.667 [0.000, 3.000], mean observation: -0.003 [-0.672, 1.000], loss: 1.691361, mean_absolute_error: 35.211935, mean_q: 21.474152, mean_eps: 0.570194
  477727/2000000: episode: 4383, duration: 1.221s, episode steps: 83, steps per second: 68, episode reward: -80.020, mean reward: -0.964 [-100.000, 14.398], mean action: 1.627 [0.000, 3.000], mean observation: -0.062 [-1.095, 2.430], loss: 1.328442, mean_absolute_error: 34.866202, mean_q: 20.433471, mean_eps: 0.570084
  477940/2000000: episode: 4384, duration: 3.176s, episode steps: 213, steps per second: 67, episode reward: -43.778, mean reward: -0.206 [-100.000, 17.233], mean action: 1.746 [0.000, 3.000], mean observation: 0.019 [-0.490, 1.194], loss: 1.655811, mean_absolute_error: 36.202350, mean_q: 20.250201, mean_eps: 0.569951
  478088/2000000: episode: 4385, duration: 2.182s, episode steps: 148, steps per second: 68, episode reward: -75.679, mean reward: -0.511 [-100.000, 17.098], mean action: 1.764 [0.000, 3.000], mean observation: -0.105 [-0.750, 2.901], loss: 1.667063, mean_absolute_error: 35.276164, mean_q: 21.856986, mean_eps: 0.569789
  478227/2000000: episode: 4386, duration: 2.025s, episode steps: 139, steps per second: 69, episode reward: -55.273, mean reward: -0.398 [-100.000, 17.952], mean action: 1.712 [0.000, 3.000], mean observation: 0.061 [-0.997, 1.005], loss: 1.667930, mean_absolute_error: 35.671945, mean_q: 19.255098, mean_eps: 0.569660
  478312/2000000: episode: 4387, duration: 1.281s, episode steps: 85, steps per second: 66, episode reward: -54.976, mean reward: -0.647 [-100.000, 28.787], mean action: 1.753 [0.000, 3.000], mean observation: -0.030 [-1.072, 1.000], loss: 1.851865, mean_absolute_error: 37.065427, mean_q: 21.802690, mean_eps: 0.569559
  478428/2000000: episode: 4388, duration: 1.714s, episode steps: 116, steps per second: 68, episode reward: -72.179, mean reward: -0.622 [-100.000, 15.894], mean action: 1.560 [0.000, 3.000], mean observation: 0.072 [-1.825, 1.000], loss: 1.418844, mean_absolute_error: 35.738700, mean_q: 20.600117, mean_eps: 0.569469
  478558/2000000: episode: 4389, duration: 1.887s, episode steps: 130, steps per second: 69, episode reward: -62.958, mean reward: -0.484 [-100.000, 14.233], mean action: 1.646 [0.000, 3.000], mean observation: 0.025 [-0.912, 1.000], loss: 1.375940, mean_absolute_error: 34.931290, mean_q: 20.651567, mean_eps: 0.569357
  478755/2000000: episode: 4390, duration: 2.831s, episode steps: 197, steps per second: 70, episode reward: -34.543, mean reward: -0.175 [-100.000, 15.152], mean action: 1.640 [0.000, 3.000], mean observation: 0.115 [-0.714, 2.747], loss: 1.674789, mean_absolute_error: 35.130571, mean_q: 20.289214, mean_eps: 0.569210
  478901/2000000: episode: 4391, duration: 2.142s, episode steps: 146, steps per second: 68, episode reward: -161.686, mean reward: -1.107 [-100.000, 15.703], mean action: 1.815 [0.000, 3.000], mean observation: -0.118 [-1.030, 1.022], loss: 1.260548, mean_absolute_error: 35.954157, mean_q: 17.768259, mean_eps: 0.569055
  479037/2000000: episode: 4392, duration: 1.970s, episode steps: 136, steps per second: 69, episode reward: -65.985, mean reward: -0.485 [-100.000, 13.071], mean action: 1.662 [0.000, 3.000], mean observation: 0.092 [-0.750, 2.246], loss: 1.321350, mean_absolute_error: 34.559813, mean_q: 23.591425, mean_eps: 0.568927
  479180/2000000: episode: 4393, duration: 2.043s, episode steps: 143, steps per second: 70, episode reward: -145.401, mean reward: -1.017 [-100.000, 5.891], mean action: 1.741 [0.000, 3.000], mean observation: 0.048 [-0.927, 2.672], loss: 1.833239, mean_absolute_error: 36.213387, mean_q: 19.161205, mean_eps: 0.568803
  479266/2000000: episode: 4394, duration: 1.260s, episode steps: 86, steps per second: 68, episode reward: -89.894, mean reward: -1.045 [-100.000, 10.872], mean action: 1.779 [0.000, 3.000], mean observation: -0.050 [-0.925, 2.923], loss: 2.028432, mean_absolute_error: 36.930581, mean_q: 18.623862, mean_eps: 0.568700
  479421/2000000: episode: 4395, duration: 2.237s, episode steps: 155, steps per second: 69, episode reward: -100.886, mean reward: -0.651 [-100.000, 13.218], mean action: 1.729 [0.000, 3.000], mean observation: -0.077 [-1.067, 3.249], loss: 2.053029, mean_absolute_error: 36.238686, mean_q: 20.433216, mean_eps: 0.568590
  479622/2000000: episode: 4396, duration: 2.908s, episode steps: 201, steps per second: 69, episode reward: -55.265, mean reward: -0.275 [-100.000, 20.871], mean action: 1.682 [0.000, 3.000], mean observation: -0.032 [-0.763, 1.000], loss: 2.106240, mean_absolute_error: 36.917105, mean_q: 19.009244, mean_eps: 0.568430
  479710/2000000: episode: 4397, duration: 1.265s, episode steps: 88, steps per second: 70, episode reward: -100.293, mean reward: -1.140 [-100.000, 8.802], mean action: 1.761 [0.000, 3.000], mean observation: 0.109 [-0.744, 2.722], loss: 2.207680, mean_absolute_error: 36.528661, mean_q: 22.253026, mean_eps: 0.568301
  479810/2000000: episode: 4398, duration: 1.427s, episode steps: 100, steps per second: 70, episode reward: -94.211, mean reward: -0.942 [-100.000, 8.724], mean action: 1.540 [0.000, 3.000], mean observation: 0.088 [-3.108, 1.000], loss: 2.086411, mean_absolute_error: 35.384389, mean_q: 22.933381, mean_eps: 0.568216
  479939/2000000: episode: 4399, duration: 1.825s, episode steps: 129, steps per second: 71, episode reward: -51.024, mean reward: -0.396 [-100.000, 11.900], mean action: 1.651 [0.000, 3.000], mean observation: 0.137 [-0.640, 1.016], loss: 1.700605, mean_absolute_error: 35.235042, mean_q: 19.416305, mean_eps: 0.568113
  480034/2000000: episode: 4400, duration: 1.391s, episode steps: 95, steps per second: 68, episode reward: -24.661, mean reward: -0.260 [-100.000, 17.218], mean action: 1.768 [0.000, 3.000], mean observation: 0.029 [-1.009, 1.000], loss: 2.237076, mean_absolute_error: 36.542611, mean_q: 21.678927, mean_eps: 0.568013
  480175/2000000: episode: 4401, duration: 2.004s, episode steps: 141, steps per second: 70, episode reward: -98.054, mean reward: -0.695 [-100.000, 13.849], mean action: 1.773 [0.000, 3.000], mean observation: 0.120 [-1.061, 1.050], loss: 1.761876, mean_absolute_error: 36.935382, mean_q: 17.981640, mean_eps: 0.567906
  480301/2000000: episode: 4402, duration: 1.855s, episode steps: 126, steps per second: 68, episode reward: -44.613, mean reward: -0.354 [-100.000, 18.365], mean action: 1.571 [0.000, 3.000], mean observation: -0.001 [-0.687, 1.254], loss: 1.599225, mean_absolute_error: 35.996843, mean_q: 18.937711, mean_eps: 0.567786
  480527/2000000: episode: 4403, duration: 3.244s, episode steps: 226, steps per second: 70, episode reward: -86.671, mean reward: -0.383 [-100.000, 11.676], mean action: 1.695 [0.000, 3.000], mean observation: -0.013 [-1.646, 1.000], loss: 2.476839, mean_absolute_error: 36.635588, mean_q: 20.588207, mean_eps: 0.567627
  480642/2000000: episode: 4404, duration: 1.653s, episode steps: 115, steps per second: 70, episode reward: -85.632, mean reward: -0.745 [-100.000, 19.091], mean action: 1.661 [0.000, 3.000], mean observation: -0.003 [-1.035, 3.268], loss: 1.247210, mean_absolute_error: 35.039288, mean_q: 17.849304, mean_eps: 0.567474
  480837/2000000: episode: 4405, duration: 2.810s, episode steps: 195, steps per second: 69, episode reward: -113.497, mean reward: -0.582 [-100.000, 14.747], mean action: 1.605 [0.000, 3.000], mean observation: 0.008 [-1.214, 1.000], loss: 1.961778, mean_absolute_error: 36.731041, mean_q: 17.075544, mean_eps: 0.567334
  481001/2000000: episode: 4406, duration: 2.364s, episode steps: 164, steps per second: 69, episode reward: -96.278, mean reward: -0.587 [-100.000, 14.177], mean action: 1.695 [0.000, 3.000], mean observation: -0.038 [-2.000, 1.000], loss: 2.106146, mean_absolute_error: 36.500513, mean_q: 17.204366, mean_eps: 0.567172
  481123/2000000: episode: 4407, duration: 1.747s, episode steps: 122, steps per second: 70, episode reward: -136.620, mean reward: -1.120 [-100.000, 9.055], mean action: 1.746 [0.000, 3.000], mean observation: -0.113 [-0.997, 3.378], loss: 1.658462, mean_absolute_error: 35.543348, mean_q: 20.268184, mean_eps: 0.567044
  481271/2000000: episode: 4408, duration: 2.109s, episode steps: 148, steps per second: 70, episode reward: -100.248, mean reward: -0.677 [-100.000, 9.420], mean action: 1.628 [0.000, 3.000], mean observation: 0.153 [-0.904, 1.902], loss: 2.072394, mean_absolute_error: 36.431819, mean_q: 19.260156, mean_eps: 0.566924
  481359/2000000: episode: 4409, duration: 1.278s, episode steps: 88, steps per second: 69, episode reward: -100.903, mean reward: -1.147 [-100.000, 7.632], mean action: 1.795 [0.000, 3.000], mean observation: -0.158 [-0.969, 3.049], loss: 2.275191, mean_absolute_error: 36.459977, mean_q: 17.217855, mean_eps: 0.566817
  481489/2000000: episode: 4410, duration: 1.875s, episode steps: 130, steps per second: 69, episode reward: -132.478, mean reward: -1.019 [-100.000, 10.218], mean action: 1.562 [0.000, 3.000], mean observation: 0.006 [-1.315, 4.535], loss: 1.452346, mean_absolute_error: 35.337307, mean_q: 20.820501, mean_eps: 0.566718
  481595/2000000: episode: 4411, duration: 1.502s, episode steps: 106, steps per second: 71, episode reward: -73.477, mean reward: -0.693 [-100.000, 17.486], mean action: 1.698 [0.000, 3.000], mean observation: 0.049 [-2.464, 1.000], loss: 1.804665, mean_absolute_error: 34.689752, mean_q: 18.692049, mean_eps: 0.566612
  481803/2000000: episode: 4412, duration: 3.011s, episode steps: 208, steps per second: 69, episode reward: -250.453, mean reward: -1.204 [-100.000, 38.598], mean action: 1.712 [0.000, 3.000], mean observation: 0.168 [-0.559, 1.782], loss: 2.359531, mean_absolute_error: 36.031220, mean_q: 20.404630, mean_eps: 0.566472
  481911/2000000: episode: 4413, duration: 1.734s, episode steps: 108, steps per second: 62, episode reward: -51.233, mean reward: -0.474 [-100.000, 12.977], mean action: 1.611 [0.000, 3.000], mean observation: 0.012 [-0.900, 1.000], loss: 1.585570, mean_absolute_error: 36.400401, mean_q: 19.030647, mean_eps: 0.566330
  482079/2000000: episode: 4414, duration: 2.443s, episode steps: 168, steps per second: 69, episode reward: -107.863, mean reward: -0.642 [-100.000, 15.465], mean action: 1.810 [0.000, 3.000], mean observation: 0.097 [-0.687, 1.676], loss: 1.900103, mean_absolute_error: 36.725518, mean_q: 17.054304, mean_eps: 0.566205
  482186/2000000: episode: 4415, duration: 1.553s, episode steps: 107, steps per second: 69, episode reward: -85.468, mean reward: -0.799 [-100.000, 17.393], mean action: 1.664 [0.000, 3.000], mean observation: -0.015 [-2.811, 1.000], loss: 1.420618, mean_absolute_error: 36.684797, mean_q: 19.482342, mean_eps: 0.566081
  483186/2000000: episode: 4416, duration: 15.036s, episode steps: 1000, steps per second: 67, episode reward: 6.318, mean reward: 0.006 [-23.551, 32.814], mean action: 1.931 [0.000, 3.000], mean observation: 0.077 [-0.782, 1.007], loss: 1.962491, mean_absolute_error: 36.117126, mean_q: 19.334349, mean_eps: 0.565583
  483286/2000000: episode: 4417, duration: 1.439s, episode steps: 100, steps per second: 69, episode reward: -103.836, mean reward: -1.038 [-100.000, 11.112], mean action: 1.670 [0.000, 3.000], mean observation: -0.127 [-0.875, 2.945], loss: 1.604545, mean_absolute_error: 35.123488, mean_q: 19.932084, mean_eps: 0.565088
  483416/2000000: episode: 4418, duration: 1.872s, episode steps: 130, steps per second: 69, episode reward: -28.616, mean reward: -0.220 [-100.000, 14.360], mean action: 1.608 [0.000, 3.000], mean observation: -0.027 [-0.648, 1.000], loss: 2.001073, mean_absolute_error: 35.971735, mean_q: 18.870533, mean_eps: 0.564985
  483553/2000000: episode: 4419, duration: 2.013s, episode steps: 137, steps per second: 68, episode reward: -47.656, mean reward: -0.348 [-100.000, 9.932], mean action: 1.752 [0.000, 3.000], mean observation: 0.119 [-0.640, 2.291], loss: 1.441810, mean_absolute_error: 35.392949, mean_q: 19.875327, mean_eps: 0.564864
  483679/2000000: episode: 4420, duration: 2.060s, episode steps: 126, steps per second: 61, episode reward: -90.545, mean reward: -0.719 [-100.000, 14.527], mean action: 1.651 [0.000, 3.000], mean observation: 0.013 [-0.778, 2.367], loss: 1.622346, mean_absolute_error: 35.681348, mean_q: 19.771952, mean_eps: 0.564746
  484679/2000000: episode: 4421, duration: 15.566s, episode steps: 1000, steps per second: 64, episode reward: 34.479, mean reward: 0.034 [-21.939, 64.556], mean action: 1.442 [0.000, 3.000], mean observation: 0.050 [-1.019, 1.000], loss: 1.701365, mean_absolute_error: 35.993948, mean_q: 20.075838, mean_eps: 0.564240
  484752/2000000: episode: 4422, duration: 1.087s, episode steps: 73, steps per second: 67, episode reward: -116.561, mean reward: -1.597 [-100.000, 19.447], mean action: 1.795 [0.000, 3.000], mean observation: -0.156 [-1.318, 3.692], loss: 2.002612, mean_absolute_error: 35.964368, mean_q: 15.745175, mean_eps: 0.563757
  484917/2000000: episode: 4423, duration: 2.407s, episode steps: 165, steps per second: 69, episode reward: -54.010, mean reward: -0.327 [-100.000, 12.630], mean action: 1.697 [0.000, 3.000], mean observation: 0.027 [-0.587, 1.000], loss: 1.642627, mean_absolute_error: 35.975733, mean_q: 20.258028, mean_eps: 0.563649
  485041/2000000: episode: 4424, duration: 1.807s, episode steps: 124, steps per second: 69, episode reward: -51.267, mean reward: -0.413 [-100.000, 24.102], mean action: 1.831 [0.000, 3.000], mean observation: -0.101 [-0.800, 2.699], loss: 1.518219, mean_absolute_error: 35.394001, mean_q: 19.532454, mean_eps: 0.563518
  485225/2000000: episode: 4425, duration: 2.669s, episode steps: 184, steps per second: 69, episode reward: -133.402, mean reward: -0.725 [-100.000, 18.702], mean action: 1.625 [0.000, 3.000], mean observation: 0.087 [-1.076, 3.966], loss: 1.653929, mean_absolute_error: 35.774314, mean_q: 20.293554, mean_eps: 0.563379
  486225/2000000: episode: 4426, duration: 15.504s, episode steps: 1000, steps per second: 64, episode reward: 21.115, mean reward: 0.021 [-21.550, 25.604], mean action: 1.437 [0.000, 3.000], mean observation: 0.127 [-0.707, 1.175], loss: 1.626416, mean_absolute_error: 35.850765, mean_q: 19.364815, mean_eps: 0.562847
  486368/2000000: episode: 4427, duration: 2.084s, episode steps: 143, steps per second: 69, episode reward: -110.850, mean reward: -0.775 [-100.000, 19.709], mean action: 1.538 [0.000, 3.000], mean observation: 0.156 [-1.614, 1.000], loss: 1.910729, mean_absolute_error: 35.626944, mean_q: 16.932409, mean_eps: 0.562334
  486484/2000000: episode: 4428, duration: 1.719s, episode steps: 116, steps per second: 67, episode reward: -45.172, mean reward: -0.389 [-100.000, 20.013], mean action: 1.629 [0.000, 3.000], mean observation: -0.084 [-0.754, 1.231], loss: 1.351034, mean_absolute_error: 35.403108, mean_q: 18.606848, mean_eps: 0.562218
  487484/2000000: episode: 4429, duration: 16.193s, episode steps: 1000, steps per second: 62, episode reward: 33.782, mean reward: 0.034 [-22.646, 75.744], mean action: 1.408 [0.000, 3.000], mean observation: 0.193 [-1.267, 1.000], loss: 1.624375, mean_absolute_error: 35.695987, mean_q: 19.932362, mean_eps: 0.561716
  487657/2000000: episode: 4430, duration: 2.560s, episode steps: 173, steps per second: 68, episode reward: -84.438, mean reward: -0.488 [-100.000, 13.114], mean action: 1.682 [0.000, 3.000], mean observation: 0.130 [-0.733, 2.417], loss: 1.262848, mean_absolute_error: 35.682708, mean_q: 19.999441, mean_eps: 0.561187
  487850/2000000: episode: 4431, duration: 2.815s, episode steps: 193, steps per second: 69, episode reward: -83.106, mean reward: -0.431 [-100.000, 7.729], mean action: 1.637 [0.000, 3.000], mean observation: 0.013 [-2.892, 1.000], loss: 1.735155, mean_absolute_error: 36.672348, mean_q: 19.296470, mean_eps: 0.561021
  488052/2000000: episode: 4432, duration: 2.902s, episode steps: 202, steps per second: 70, episode reward: -123.678, mean reward: -0.612 [-100.000, 4.153], mean action: 1.653 [0.000, 3.000], mean observation: 0.188 [-0.457, 1.011], loss: 1.568595, mean_absolute_error: 35.579995, mean_q: 18.798450, mean_eps: 0.560845
  488163/2000000: episode: 4433, duration: 1.620s, episode steps: 111, steps per second: 69, episode reward: -63.672, mean reward: -0.574 [-100.000, 16.975], mean action: 1.739 [0.000, 3.000], mean observation: -0.020 [-0.844, 2.590], loss: 1.586983, mean_absolute_error: 35.785934, mean_q: 19.431372, mean_eps: 0.560705
  488286/2000000: episode: 4434, duration: 1.793s, episode steps: 123, steps per second: 69, episode reward: -152.534, mean reward: -1.240 [-100.000, 11.307], mean action: 1.724 [0.000, 3.000], mean observation: 0.102 [-0.732, 1.127], loss: 2.338646, mean_absolute_error: 36.509711, mean_q: 16.448091, mean_eps: 0.560598
  488461/2000000: episode: 4435, duration: 2.496s, episode steps: 175, steps per second: 70, episode reward: -67.404, mean reward: -0.385 [-100.000, 13.051], mean action: 1.731 [0.000, 3.000], mean observation: 0.108 [-0.679, 1.003], loss: 2.062797, mean_absolute_error: 36.677911, mean_q: 21.823478, mean_eps: 0.560463
  489461/2000000: episode: 4436, duration: 15.405s, episode steps: 1000, steps per second: 65, episode reward: 27.670, mean reward: 0.028 [-24.650, 32.409], mean action: 1.186 [0.000, 3.000], mean observation: 0.059 [-0.784, 1.271], loss: 1.648245, mean_absolute_error: 35.231860, mean_q: 20.224773, mean_eps: 0.559934
  489616/2000000: episode: 4437, duration: 2.277s, episode steps: 155, steps per second: 68, episode reward: -29.764, mean reward: -0.192 [-100.000, 17.327], mean action: 1.755 [0.000, 3.000], mean observation: 0.122 [-0.645, 1.000], loss: 1.505197, mean_absolute_error: 35.957066, mean_q: 20.549951, mean_eps: 0.559416
  490616/2000000: episode: 4438, duration: 15.525s, episode steps: 1000, steps per second: 64, episode reward: 23.946, mean reward: 0.024 [-23.292, 23.299], mean action: 1.803 [0.000, 3.000], mean observation: 0.224 [-1.550, 1.000], loss: 1.863752, mean_absolute_error: 36.007490, mean_q: 20.930489, mean_eps: 0.558897
  490744/2000000: episode: 4439, duration: 1.877s, episode steps: 128, steps per second: 68, episode reward: -54.166, mean reward: -0.423 [-100.000, 19.087], mean action: 1.680 [0.000, 3.000], mean observation: 0.095 [-2.394, 1.000], loss: 1.905841, mean_absolute_error: 35.751555, mean_q: 20.529138, mean_eps: 0.558390
  490846/2000000: episode: 4440, duration: 1.476s, episode steps: 102, steps per second: 69, episode reward: -79.297, mean reward: -0.777 [-100.000, 18.808], mean action: 1.657 [0.000, 3.000], mean observation: -0.105 [-0.957, 3.803], loss: 1.718602, mean_absolute_error: 36.757014, mean_q: 16.205960, mean_eps: 0.558285
  491020/2000000: episode: 4441, duration: 2.522s, episode steps: 174, steps per second: 69, episode reward: -44.123, mean reward: -0.254 [-100.000, 9.151], mean action: 1.730 [0.000, 3.000], mean observation: 0.021 [-2.533, 1.000], loss: 2.889620, mean_absolute_error: 38.548513, mean_q: 15.622688, mean_eps: 0.558161
  491155/2000000: episode: 4442, duration: 1.946s, episode steps: 135, steps per second: 69, episode reward: -61.621, mean reward: -0.456 [-100.000, 12.146], mean action: 1.644 [0.000, 3.000], mean observation: 0.052 [-1.220, 1.000], loss: 1.680613, mean_absolute_error: 36.650822, mean_q: 22.701673, mean_eps: 0.558023
  491275/2000000: episode: 4443, duration: 1.712s, episode steps: 120, steps per second: 70, episode reward: -61.305, mean reward: -0.511 [-100.000, 18.805], mean action: 1.808 [0.000, 3.000], mean observation: -0.074 [-1.908, 1.000], loss: 1.426984, mean_absolute_error: 36.982533, mean_q: 19.180900, mean_eps: 0.557907
  491365/2000000: episode: 4444, duration: 1.319s, episode steps: 90, steps per second: 68, episode reward: -79.136, mean reward: -0.879 [-100.000, 14.575], mean action: 1.656 [0.000, 3.000], mean observation: -0.033 [-0.965, 1.000], loss: 1.940178, mean_absolute_error: 37.161927, mean_q: 18.295051, mean_eps: 0.557812
  491497/2000000: episode: 4445, duration: 1.896s, episode steps: 132, steps per second: 70, episode reward: -65.502, mean reward: -0.496 [-100.000, 18.285], mean action: 1.682 [0.000, 3.000], mean observation: 0.014 [-1.069, 1.000], loss: 1.941778, mean_absolute_error: 35.565561, mean_q: 22.859144, mean_eps: 0.557711
  491663/2000000: episode: 4446, duration: 2.376s, episode steps: 166, steps per second: 70, episode reward: -88.366, mean reward: -0.532 [-100.000, 12.579], mean action: 1.717 [0.000, 3.000], mean observation: 0.146 [-0.582, 1.339], loss: 1.911179, mean_absolute_error: 36.351315, mean_q: 18.770400, mean_eps: 0.557578
  492663/2000000: episode: 4447, duration: 15.442s, episode steps: 1000, steps per second: 65, episode reward: 34.952, mean reward: 0.035 [-24.182, 23.273], mean action: 1.564 [0.000, 3.000], mean observation: 0.162 [-1.211, 1.000], loss: 1.936428, mean_absolute_error: 36.550437, mean_q: 20.868376, mean_eps: 0.557054
  492791/2000000: episode: 4448, duration: 1.845s, episode steps: 128, steps per second: 69, episode reward: -103.051, mean reward: -0.805 [-100.000, 10.269], mean action: 1.828 [0.000, 3.000], mean observation: -0.072 [-0.949, 2.246], loss: 1.490196, mean_absolute_error: 37.012705, mean_q: 22.110113, mean_eps: 0.556547
  492905/2000000: episode: 4449, duration: 1.648s, episode steps: 114, steps per second: 69, episode reward: -89.154, mean reward: -0.782 [-100.000, 9.413], mean action: 1.684 [0.000, 3.000], mean observation: -0.089 [-2.481, 1.000], loss: 1.619480, mean_absolute_error: 36.266986, mean_q: 18.753789, mean_eps: 0.556437
  492998/2000000: episode: 4450, duration: 1.326s, episode steps: 93, steps per second: 70, episode reward: -61.344, mean reward: -0.660 [-100.000, 16.435], mean action: 1.634 [0.000, 3.000], mean observation: 0.009 [-1.011, 1.000], loss: 1.600972, mean_absolute_error: 35.911486, mean_q: 20.234132, mean_eps: 0.556343
  493071/2000000: episode: 4451, duration: 1.046s, episode steps: 73, steps per second: 70, episode reward: -72.610, mean reward: -0.995 [-100.000, 21.029], mean action: 1.534 [0.000, 3.000], mean observation: -0.116 [-1.040, 1.000], loss: 1.316252, mean_absolute_error: 36.132720, mean_q: 19.947516, mean_eps: 0.556269
  493182/2000000: episode: 4452, duration: 1.577s, episode steps: 111, steps per second: 70, episode reward: -79.270, mean reward: -0.714 [-100.000, 11.262], mean action: 1.595 [0.000, 3.000], mean observation: -0.084 [-0.901, 2.894], loss: 2.520641, mean_absolute_error: 37.034313, mean_q: 19.428081, mean_eps: 0.556187
  493331/2000000: episode: 4453, duration: 2.116s, episode steps: 149, steps per second: 70, episode reward: -99.235, mean reward: -0.666 [-100.000, 12.433], mean action: 1.664 [0.000, 3.000], mean observation: -0.076 [-0.577, 1.000], loss: 1.551164, mean_absolute_error: 37.451750, mean_q: 20.769960, mean_eps: 0.556070
  493450/2000000: episode: 4454, duration: 1.688s, episode steps: 119, steps per second: 71, episode reward: -104.180, mean reward: -0.875 [-100.000, 9.285], mean action: 1.655 [0.000, 3.000], mean observation: -0.063 [-0.697, 2.554], loss: 2.149734, mean_absolute_error: 36.962123, mean_q: 21.471632, mean_eps: 0.555949
  493548/2000000: episode: 4455, duration: 1.435s, episode steps: 98, steps per second: 68, episode reward: -78.683, mean reward: -0.803 [-100.000, 18.055], mean action: 1.755 [0.000, 3.000], mean observation: -0.026 [-0.903, 3.002], loss: 1.459591, mean_absolute_error: 35.788993, mean_q: 19.201315, mean_eps: 0.555852
  494548/2000000: episode: 4456, duration: 15.650s, episode steps: 1000, steps per second: 64, episode reward: 12.127, mean reward: 0.012 [-22.889, 23.008], mean action: 1.660 [0.000, 3.000], mean observation: 0.174 [-1.125, 1.000], loss: 1.765152, mean_absolute_error: 36.244292, mean_q: 21.290475, mean_eps: 0.555359
  494633/2000000: episode: 4457, duration: 1.265s, episode steps: 85, steps per second: 67, episode reward: -104.598, mean reward: -1.231 [-100.000, 9.178], mean action: 1.859 [0.000, 3.000], mean observation: -0.080 [-0.987, 3.107], loss: 2.457249, mean_absolute_error: 37.594675, mean_q: 18.486029, mean_eps: 0.554869
  494828/2000000: episode: 4458, duration: 2.800s, episode steps: 195, steps per second: 70, episode reward: -13.011, mean reward: -0.067 [-100.000, 19.526], mean action: 1.800 [0.000, 3.000], mean observation: 0.025 [-1.315, 1.051], loss: 1.856470, mean_absolute_error: 36.101301, mean_q: 21.046699, mean_eps: 0.554743
  495061/2000000: episode: 4459, duration: 3.414s, episode steps: 233, steps per second: 68, episode reward: -45.495, mean reward: -0.195 [-100.000, 18.791], mean action: 1.725 [0.000, 3.000], mean observation: 0.048 [-0.809, 1.000], loss: 1.437016, mean_absolute_error: 35.982488, mean_q: 20.768506, mean_eps: 0.554550
  495190/2000000: episode: 4460, duration: 1.828s, episode steps: 129, steps per second: 71, episode reward: -110.668, mean reward: -0.858 [-100.000, 10.093], mean action: 1.729 [0.000, 3.000], mean observation: 0.113 [-0.848, 2.811], loss: 2.524192, mean_absolute_error: 37.235392, mean_q: 20.290877, mean_eps: 0.554387
  495314/2000000: episode: 4461, duration: 1.749s, episode steps: 124, steps per second: 71, episode reward: -82.621, mean reward: -0.666 [-100.000, 11.649], mean action: 1.653 [0.000, 3.000], mean observation: 0.103 [-3.357, 1.000], loss: 1.602796, mean_absolute_error: 37.168163, mean_q: 16.836442, mean_eps: 0.554273
  495465/2000000: episode: 4462, duration: 2.185s, episode steps: 151, steps per second: 69, episode reward: -146.290, mean reward: -0.969 [-100.000, 8.007], mean action: 1.682 [0.000, 3.000], mean observation: -0.107 [-0.958, 3.677], loss: 1.386798, mean_absolute_error: 37.165150, mean_q: 19.317575, mean_eps: 0.554149
  495583/2000000: episode: 4463, duration: 1.671s, episode steps: 118, steps per second: 71, episode reward: -133.891, mean reward: -1.135 [-100.000, 15.107], mean action: 1.695 [0.000, 3.000], mean observation: -0.105 [-0.967, 1.000], loss: 1.815160, mean_absolute_error: 36.099251, mean_q: 20.875286, mean_eps: 0.554028
  495673/2000000: episode: 4464, duration: 1.311s, episode steps: 90, steps per second: 69, episode reward: -127.263, mean reward: -1.414 [-100.000, 7.159], mean action: 1.667 [0.000, 3.000], mean observation: 0.139 [-1.017, 3.985], loss: 1.454383, mean_absolute_error: 36.241575, mean_q: 23.712027, mean_eps: 0.553935
  495828/2000000: episode: 4465, duration: 2.215s, episode steps: 155, steps per second: 70, episode reward: -90.732, mean reward: -0.585 [-100.000, 13.987], mean action: 1.716 [0.000, 3.000], mean observation: -0.116 [-1.006, 1.039], loss: 1.362574, mean_absolute_error: 36.234568, mean_q: 22.091081, mean_eps: 0.553825
  496047/2000000: episode: 4466, duration: 3.121s, episode steps: 219, steps per second: 70, episode reward: -30.308, mean reward: -0.138 [-100.000, 12.704], mean action: 1.639 [0.000, 3.000], mean observation: 0.068 [-0.609, 1.007], loss: 1.542351, mean_absolute_error: 35.836125, mean_q: 21.455451, mean_eps: 0.553658
  496133/2000000: episode: 4467, duration: 1.252s, episode steps: 86, steps per second: 69, episode reward: -147.800, mean reward: -1.719 [-100.000, 16.575], mean action: 1.523 [0.000, 3.000], mean observation: 0.103 [-2.919, 1.017], loss: 1.411228, mean_absolute_error: 34.751504, mean_q: 22.039878, mean_eps: 0.553519
  496263/2000000: episode: 4468, duration: 1.852s, episode steps: 130, steps per second: 70, episode reward: -5.258, mean reward: -0.040 [-100.000, 21.234], mean action: 1.785 [0.000, 3.000], mean observation: 0.082 [-0.882, 1.503], loss: 1.808958, mean_absolute_error: 35.548871, mean_q: 18.139161, mean_eps: 0.553422
  496392/2000000: episode: 4469, duration: 1.894s, episode steps: 129, steps per second: 68, episode reward: -77.610, mean reward: -0.602 [-100.000, 14.604], mean action: 1.752 [0.000, 3.000], mean observation: -0.093 [-0.785, 1.000], loss: 2.244448, mean_absolute_error: 37.073892, mean_q: 20.315737, mean_eps: 0.553307
  497392/2000000: episode: 4470, duration: 15.019s, episode steps: 1000, steps per second: 67, episode reward: 27.381, mean reward: 0.027 [-23.536, 52.336], mean action: 1.325 [0.000, 3.000], mean observation: 0.131 [-1.464, 1.000], loss: 1.699602, mean_absolute_error: 36.374182, mean_q: 21.140046, mean_eps: 0.552799
  497515/2000000: episode: 4471, duration: 1.783s, episode steps: 123, steps per second: 69, episode reward: -104.630, mean reward: -0.851 [-100.000, 16.066], mean action: 1.707 [0.000, 3.000], mean observation: 0.030 [-2.292, 1.000], loss: 2.366453, mean_absolute_error: 37.209384, mean_q: 23.242417, mean_eps: 0.552293
  497691/2000000: episode: 4472, duration: 2.507s, episode steps: 176, steps per second: 70, episode reward: -67.210, mean reward: -0.382 [-100.000, 16.987], mean action: 1.642 [0.000, 3.000], mean observation: 0.119 [-1.568, 1.000], loss: 2.200272, mean_absolute_error: 36.579244, mean_q: 19.124352, mean_eps: 0.552158
  497831/2000000: episode: 4473, duration: 2.229s, episode steps: 140, steps per second: 63, episode reward: -59.669, mean reward: -0.426 [-100.000, 14.280], mean action: 1.607 [0.000, 3.000], mean observation: -0.064 [-0.717, 2.697], loss: 1.425973, mean_absolute_error: 35.737961, mean_q: 20.696925, mean_eps: 0.552016
  498831/2000000: episode: 4474, duration: 15.377s, episode steps: 1000, steps per second: 65, episode reward: -24.524, mean reward: -0.025 [-24.901, 29.739], mean action: 1.940 [0.000, 3.000], mean observation: 0.254 [-0.780, 1.109], loss: 1.946575, mean_absolute_error: 36.169141, mean_q: 20.687399, mean_eps: 0.551503
  499050/2000000: episode: 4475, duration: 3.180s, episode steps: 219, steps per second: 69, episode reward: -90.797, mean reward: -0.415 [-100.000, 14.300], mean action: 1.644 [0.000, 3.000], mean observation: 0.038 [-0.664, 1.103], loss: 1.550095, mean_absolute_error: 35.971631, mean_q: 20.306322, mean_eps: 0.550954
  499267/2000000: episode: 4476, duration: 3.089s, episode steps: 217, steps per second: 70, episode reward: -78.867, mean reward: -0.363 [-100.000, 10.993], mean action: 1.783 [0.000, 3.000], mean observation: 0.075 [-1.551, 1.000], loss: 2.031846, mean_absolute_error: 36.166448, mean_q: 21.692821, mean_eps: 0.550758
  499495/2000000: episode: 4477, duration: 3.265s, episode steps: 228, steps per second: 70, episode reward: -120.903, mean reward: -0.530 [-100.000, 22.976], mean action: 1.654 [0.000, 3.000], mean observation: 0.142 [-0.819, 2.944], loss: 1.483125, mean_absolute_error: 35.053332, mean_q: 21.498681, mean_eps: 0.550558
  499603/2000000: episode: 4478, duration: 1.559s, episode steps: 108, steps per second: 69, episode reward: -125.148, mean reward: -1.159 [-100.000, 13.454], mean action: 1.731 [0.000, 3.000], mean observation: -0.088 [-1.086, 1.577], loss: 2.287222, mean_absolute_error: 35.591744, mean_q: 23.498819, mean_eps: 0.550407
  499727/2000000: episode: 4479, duration: 1.763s, episode steps: 124, steps per second: 70, episode reward: -12.359, mean reward: -0.100 [-100.000, 15.586], mean action: 1.766 [0.000, 3.000], mean observation: 0.006 [-0.801, 1.245], loss: 1.568274, mean_absolute_error: 36.052159, mean_q: 19.294430, mean_eps: 0.550302
  499944/2000000: episode: 4480, duration: 3.388s, episode steps: 217, steps per second: 64, episode reward: -79.523, mean reward: -0.366 [-100.000, 14.113], mean action: 1.765 [0.000, 3.000], mean observation: 0.065 [-0.655, 1.418], loss: 1.674024, mean_absolute_error: 35.742123, mean_q: 20.863777, mean_eps: 0.550149
  500064/2000000: episode: 4481, duration: 1.759s, episode steps: 120, steps per second: 68, episode reward: -40.886, mean reward: -0.341 [-100.000, 12.382], mean action: 1.808 [0.000, 3.000], mean observation: -0.000 [-0.689, 1.000], loss: 1.298384, mean_absolute_error: 36.855602, mean_q: 20.823770, mean_eps: 0.549998
  501064/2000000: episode: 4482, duration: 15.685s, episode steps: 1000, steps per second: 64, episode reward: 41.843, mean reward: 0.042 [-22.005, 25.289], mean action: 1.568 [0.000, 3.000], mean observation: 0.123 [-0.613, 1.000], loss: 1.922982, mean_absolute_error: 36.368015, mean_q: 21.594526, mean_eps: 0.549494
  501167/2000000: episode: 4483, duration: 1.519s, episode steps: 103, steps per second: 68, episode reward: -75.166, mean reward: -0.730 [-100.000, 18.314], mean action: 1.709 [0.000, 3.000], mean observation: -0.026 [-0.899, 2.414], loss: 2.553391, mean_absolute_error: 36.991476, mean_q: 21.574750, mean_eps: 0.548997
  501318/2000000: episode: 4484, duration: 2.158s, episode steps: 151, steps per second: 70, episode reward: -149.813, mean reward: -0.992 [-100.000, 18.111], mean action: 1.742 [0.000, 3.000], mean observation: -0.015 [-1.124, 3.905], loss: 1.722207, mean_absolute_error: 35.471117, mean_q: 22.574922, mean_eps: 0.548882
  501443/2000000: episode: 4485, duration: 1.778s, episode steps: 125, steps per second: 70, episode reward: -58.625, mean reward: -0.469 [-100.000, 23.331], mean action: 1.680 [0.000, 3.000], mean observation: -0.048 [-0.773, 1.634], loss: 2.565054, mean_absolute_error: 36.679821, mean_q: 21.427356, mean_eps: 0.548758
  501646/2000000: episode: 4486, duration: 2.907s, episode steps: 203, steps per second: 70, episode reward: -89.501, mean reward: -0.441 [-100.000, 7.545], mean action: 1.773 [0.000, 3.000], mean observation: -0.021 [-0.806, 2.428], loss: 1.831363, mean_absolute_error: 36.272390, mean_q: 22.753440, mean_eps: 0.548610
  501847/2000000: episode: 4487, duration: 2.875s, episode steps: 201, steps per second: 70, episode reward: 16.012, mean reward: 0.080 [-100.000, 22.580], mean action: 1.701 [0.000, 3.000], mean observation: 0.036 [-1.554, 1.029], loss: 2.087884, mean_absolute_error: 36.077550, mean_q: 21.640425, mean_eps: 0.548429
  501962/2000000: episode: 4488, duration: 1.665s, episode steps: 115, steps per second: 69, episode reward: -112.712, mean reward: -0.980 [-100.000, 19.716], mean action: 1.783 [0.000, 3.000], mean observation: -0.007 [-0.957, 3.257], loss: 1.767976, mean_absolute_error: 35.114371, mean_q: 22.995809, mean_eps: 0.548286
  502055/2000000: episode: 4489, duration: 1.329s, episode steps: 93, steps per second: 70, episode reward: -47.161, mean reward: -0.507 [-100.000, 36.740], mean action: 1.710 [0.000, 3.000], mean observation: 0.031 [-0.751, 1.827], loss: 1.692536, mean_absolute_error: 37.635711, mean_q: 21.322137, mean_eps: 0.548193
  502189/2000000: episode: 4490, duration: 1.937s, episode steps: 134, steps per second: 69, episode reward: -52.716, mean reward: -0.393 [-100.000, 19.102], mean action: 1.746 [0.000, 3.000], mean observation: -0.064 [-0.769, 1.000], loss: 1.670937, mean_absolute_error: 36.060520, mean_q: 21.055378, mean_eps: 0.548090
  502396/2000000: episode: 4491, duration: 3.001s, episode steps: 207, steps per second: 69, episode reward: -46.757, mean reward: -0.226 [-100.000, 14.798], mean action: 1.628 [0.000, 3.000], mean observation: 0.028 [-0.631, 1.000], loss: 2.227660, mean_absolute_error: 37.541356, mean_q: 20.084520, mean_eps: 0.547937
  502523/2000000: episode: 4492, duration: 1.820s, episode steps: 127, steps per second: 70, episode reward: -43.969, mean reward: -0.346 [-100.000, 14.221], mean action: 1.732 [0.000, 3.000], mean observation: 0.052 [-0.786, 1.000], loss: 1.386352, mean_absolute_error: 35.780468, mean_q: 24.186654, mean_eps: 0.547788
  502643/2000000: episode: 4493, duration: 1.725s, episode steps: 120, steps per second: 70, episode reward: -10.183, mean reward: -0.085 [-100.000, 13.672], mean action: 1.792 [0.000, 3.000], mean observation: 0.016 [-0.640, 1.000], loss: 2.533480, mean_absolute_error: 37.214099, mean_q: 20.387695, mean_eps: 0.547676
  502774/2000000: episode: 4494, duration: 1.891s, episode steps: 131, steps per second: 69, episode reward: -94.366, mean reward: -0.720 [-100.000, 10.094], mean action: 1.664 [0.000, 3.000], mean observation: -0.015 [-0.762, 2.554], loss: 2.084515, mean_absolute_error: 36.558230, mean_q: 22.921746, mean_eps: 0.547563
  502871/2000000: episode: 4495, duration: 1.372s, episode steps: 97, steps per second: 71, episode reward: -118.682, mean reward: -1.224 [-100.000, 17.871], mean action: 1.732 [0.000, 3.000], mean observation: -0.044 [-0.924, 1.000], loss: 2.509959, mean_absolute_error: 36.240782, mean_q: 21.858275, mean_eps: 0.547460
  502972/2000000: episode: 4496, duration: 1.515s, episode steps: 101, steps per second: 67, episode reward: -61.279, mean reward: -0.607 [-100.000, 47.623], mean action: 1.723 [0.000, 3.000], mean observation: 0.102 [-0.962, 1.802], loss: 1.360353, mean_absolute_error: 36.556843, mean_q: 22.653582, mean_eps: 0.547372
  503073/2000000: episode: 4497, duration: 1.489s, episode steps: 101, steps per second: 68, episode reward: -51.808, mean reward: -0.513 [-100.000, 6.756], mean action: 1.713 [0.000, 3.000], mean observation: 0.003 [-2.390, 1.000], loss: 1.887287, mean_absolute_error: 35.984889, mean_q: 22.229870, mean_eps: 0.547280
  503329/2000000: episode: 4498, duration: 3.695s, episode steps: 256, steps per second: 69, episode reward: -70.513, mean reward: -0.275 [-100.000, 22.752], mean action: 1.711 [0.000, 3.000], mean observation: 0.083 [-0.471, 1.000], loss: 1.651625, mean_absolute_error: 37.125251, mean_q: 21.966678, mean_eps: 0.547118
  503509/2000000: episode: 4499, duration: 2.588s, episode steps: 180, steps per second: 70, episode reward: -97.205, mean reward: -0.540 [-100.000, 11.115], mean action: 1.761 [0.000, 3.000], mean observation: 0.008 [-2.401, 1.000], loss: 1.702593, mean_absolute_error: 37.211674, mean_q: 21.693727, mean_eps: 0.546922
  503659/2000000: episode: 4500, duration: 2.165s, episode steps: 150, steps per second: 69, episode reward: -98.379, mean reward: -0.656 [-100.000, 10.932], mean action: 1.740 [0.000, 3.000], mean observation: 0.007 [-0.790, 2.947], loss: 1.760339, mean_absolute_error: 36.198021, mean_q: 22.668421, mean_eps: 0.546774
  503770/2000000: episode: 4501, duration: 1.619s, episode steps: 111, steps per second: 69, episode reward: -1.966, mean reward: -0.018 [-100.000, 21.553], mean action: 1.622 [0.000, 3.000], mean observation: 0.009 [-0.867, 1.000], loss: 2.204291, mean_absolute_error: 38.636605, mean_q: 20.387442, mean_eps: 0.546657
  503866/2000000: episode: 4502, duration: 1.389s, episode steps: 96, steps per second: 69, episode reward: -62.794, mean reward: -0.654 [-100.000, 12.932], mean action: 1.771 [0.000, 3.000], mean observation: 0.058 [-0.781, 1.230], loss: 1.760617, mean_absolute_error: 36.953679, mean_q: 20.879821, mean_eps: 0.546564
  504089/2000000: episode: 4503, duration: 3.212s, episode steps: 223, steps per second: 69, episode reward: -84.754, mean reward: -0.380 [-100.000, 10.506], mean action: 1.637 [0.000, 3.000], mean observation: -0.042 [-0.925, 1.220], loss: 1.696916, mean_absolute_error: 35.989035, mean_q: 22.644579, mean_eps: 0.546420
  504158/2000000: episode: 4504, duration: 0.983s, episode steps: 69, steps per second: 70, episode reward: -114.547, mean reward: -1.660 [-100.000, 12.241], mean action: 1.493 [0.000, 3.000], mean observation: -0.101 [-1.271, 4.288], loss: 2.422166, mean_absolute_error: 37.742617, mean_q: 17.085123, mean_eps: 0.546288
  504241/2000000: episode: 4505, duration: 1.185s, episode steps: 83, steps per second: 70, episode reward: -89.355, mean reward: -1.077 [-100.000, 10.117], mean action: 1.675 [0.000, 3.000], mean observation: 0.042 [-2.942, 1.000], loss: 1.599174, mean_absolute_error: 35.936684, mean_q: 24.384784, mean_eps: 0.546220
  504369/2000000: episode: 4506, duration: 1.865s, episode steps: 128, steps per second: 69, episode reward: -59.803, mean reward: -0.467 [-100.000, 10.549], mean action: 1.750 [0.000, 3.000], mean observation: 0.027 [-1.400, 1.000], loss: 1.529409, mean_absolute_error: 36.281922, mean_q: 23.026072, mean_eps: 0.546125
  504483/2000000: episode: 4507, duration: 1.619s, episode steps: 114, steps per second: 70, episode reward: -212.603, mean reward: -1.865 [-100.000, 45.016], mean action: 1.649 [0.000, 3.000], mean observation: -0.201 [-1.922, 1.000], loss: 1.996254, mean_absolute_error: 36.161145, mean_q: 22.666201, mean_eps: 0.546017
  504585/2000000: episode: 4508, duration: 1.492s, episode steps: 102, steps per second: 68, episode reward: -243.326, mean reward: -2.386 [-100.000, 54.182], mean action: 1.745 [0.000, 3.000], mean observation: 0.177 [-1.086, 2.276], loss: 2.366800, mean_absolute_error: 37.409500, mean_q: 19.567435, mean_eps: 0.545919
  504711/2000000: episode: 4509, duration: 1.764s, episode steps: 126, steps per second: 71, episode reward: -40.100, mean reward: -0.318 [-100.000, 17.690], mean action: 1.659 [0.000, 3.000], mean observation: 0.054 [-1.351, 1.000], loss: 2.041707, mean_absolute_error: 35.526798, mean_q: 23.116483, mean_eps: 0.545817
  504852/2000000: episode: 4510, duration: 2.046s, episode steps: 141, steps per second: 69, episode reward: -138.323, mean reward: -0.981 [-100.000, 13.774], mean action: 1.844 [0.000, 3.000], mean observation: -0.045 [-1.144, 3.634], loss: 2.320459, mean_absolute_error: 36.917400, mean_q: 21.959873, mean_eps: 0.545698
  504952/2000000: episode: 4511, duration: 1.487s, episode steps: 100, steps per second: 67, episode reward: -100.406, mean reward: -1.004 [-100.000, 8.740], mean action: 1.670 [0.000, 3.000], mean observation: -0.065 [-0.940, 3.316], loss: 1.070924, mean_absolute_error: 37.253131, mean_q: 24.117911, mean_eps: 0.545590
  505034/2000000: episode: 4512, duration: 1.225s, episode steps: 82, steps per second: 67, episode reward: -114.939, mean reward: -1.402 [-100.000, 9.846], mean action: 1.683 [0.000, 3.000], mean observation: -0.077 [-1.049, 3.244], loss: 1.598971, mean_absolute_error: 36.549249, mean_q: 15.481747, mean_eps: 0.545507
  505173/2000000: episode: 4513, duration: 2.011s, episode steps: 139, steps per second: 69, episode reward: -143.932, mean reward: -1.035 [-100.000, 20.705], mean action: 1.655 [0.000, 3.000], mean observation: -0.052 [-1.345, 3.672], loss: 2.079195, mean_absolute_error: 37.783935, mean_q: 19.341106, mean_eps: 0.545406
  505278/2000000: episode: 4514, duration: 1.502s, episode steps: 105, steps per second: 70, episode reward: -67.102, mean reward: -0.639 [-100.000, 11.226], mean action: 1.695 [0.000, 3.000], mean observation: -0.089 [-2.603, 1.000], loss: 1.412811, mean_absolute_error: 37.128597, mean_q: 24.462061, mean_eps: 0.545297
  505403/2000000: episode: 4515, duration: 1.766s, episode steps: 125, steps per second: 71, episode reward: -88.650, mean reward: -0.709 [-100.000, 11.330], mean action: 1.664 [0.000, 3.000], mean observation: -0.098 [-1.052, 1.000], loss: 1.894121, mean_absolute_error: 36.921502, mean_q: 25.287812, mean_eps: 0.545194
  505552/2000000: episode: 4516, duration: 2.172s, episode steps: 149, steps per second: 69, episode reward: -75.241, mean reward: -0.505 [-100.000, 12.916], mean action: 1.557 [0.000, 3.000], mean observation: -0.054 [-0.950, 1.000], loss: 1.666104, mean_absolute_error: 37.251810, mean_q: 19.499874, mean_eps: 0.545072
  505652/2000000: episode: 4517, duration: 1.504s, episode steps: 100, steps per second: 66, episode reward: -119.435, mean reward: -1.194 [-100.000, 10.902], mean action: 1.730 [0.000, 3.000], mean observation: 0.089 [-1.903, 1.000], loss: 1.956406, mean_absolute_error: 36.218289, mean_q: 21.270211, mean_eps: 0.544960
  505808/2000000: episode: 4518, duration: 2.295s, episode steps: 156, steps per second: 68, episode reward: -108.946, mean reward: -0.698 [-100.000, 19.146], mean action: 1.609 [0.000, 3.000], mean observation: -0.103 [-1.014, 1.000], loss: 1.986150, mean_absolute_error: 36.144950, mean_q: 23.289461, mean_eps: 0.544845
  505935/2000000: episode: 4519, duration: 1.904s, episode steps: 127, steps per second: 67, episode reward: -48.701, mean reward: -0.383 [-100.000, 74.206], mean action: 1.606 [0.000, 3.000], mean observation: -0.048 [-1.132, 1.356], loss: 1.962170, mean_absolute_error: 37.449091, mean_q: 21.346369, mean_eps: 0.544717
  506087/2000000: episode: 4520, duration: 2.270s, episode steps: 152, steps per second: 67, episode reward: -82.747, mean reward: -0.544 [-100.000, 19.920], mean action: 1.730 [0.000, 3.000], mean observation: -0.112 [-0.771, 1.932], loss: 1.568717, mean_absolute_error: 36.045198, mean_q: 21.780762, mean_eps: 0.544591
  506237/2000000: episode: 4521, duration: 2.197s, episode steps: 150, steps per second: 68, episode reward: -73.820, mean reward: -0.492 [-100.000, 18.413], mean action: 1.740 [0.000, 3.000], mean observation: -0.008 [-0.809, 2.876], loss: 1.677164, mean_absolute_error: 36.027687, mean_q: 19.896736, mean_eps: 0.544454
  506377/2000000: episode: 4522, duration: 2.014s, episode steps: 140, steps per second: 69, episode reward: -63.895, mean reward: -0.456 [-100.000, 15.969], mean action: 1.721 [0.000, 3.000], mean observation: 0.015 [-0.784, 1.694], loss: 1.724606, mean_absolute_error: 35.851527, mean_q: 23.336818, mean_eps: 0.544323
  506473/2000000: episode: 4523, duration: 1.389s, episode steps: 96, steps per second: 69, episode reward: -96.856, mean reward: -1.009 [-100.000, 9.443], mean action: 1.875 [0.000, 3.000], mean observation: -0.084 [-2.461, 1.000], loss: 1.636565, mean_absolute_error: 36.582168, mean_q: 20.879248, mean_eps: 0.544217
  506673/2000000: episode: 4524, duration: 2.902s, episode steps: 200, steps per second: 69, episode reward: -168.186, mean reward: -0.841 [-100.000, 38.305], mean action: 1.710 [0.000, 3.000], mean observation: -0.079 [-0.933, 1.131], loss: 1.825811, mean_absolute_error: 36.076577, mean_q: 19.181066, mean_eps: 0.544083
  507673/2000000: episode: 4525, duration: 15.684s, episode steps: 1000, steps per second: 64, episode reward: 36.795, mean reward: 0.037 [-24.124, 24.055], mean action: 1.441 [0.000, 3.000], mean observation: 0.161 [-1.474, 1.000], loss: 1.744144, mean_absolute_error: 36.565044, mean_q: 21.415030, mean_eps: 0.543543
  507781/2000000: episode: 4526, duration: 1.600s, episode steps: 108, steps per second: 68, episode reward: -220.921, mean reward: -2.046 [-100.000, 15.821], mean action: 1.870 [0.000, 3.000], mean observation: -0.192 [-2.414, 1.000], loss: 1.662376, mean_absolute_error: 36.114375, mean_q: 19.927910, mean_eps: 0.543045
  507903/2000000: episode: 4527, duration: 1.742s, episode steps: 122, steps per second: 70, episode reward: -68.272, mean reward: -0.560 [-100.000, 13.997], mean action: 1.664 [0.000, 3.000], mean observation: 0.123 [-0.792, 1.661], loss: 1.640128, mean_absolute_error: 36.807523, mean_q: 20.385926, mean_eps: 0.542942
  508010/2000000: episode: 4528, duration: 1.550s, episode steps: 107, steps per second: 69, episode reward: -66.098, mean reward: -0.618 [-100.000, 11.732], mean action: 1.729 [0.000, 3.000], mean observation: -0.032 [-0.754, 2.345], loss: 2.029062, mean_absolute_error: 36.317153, mean_q: 21.282092, mean_eps: 0.542840
  508201/2000000: episode: 4529, duration: 2.753s, episode steps: 191, steps per second: 69, episode reward: -163.477, mean reward: -0.856 [-100.000, 4.279], mean action: 1.738 [0.000, 3.000], mean observation: -0.110 [-1.002, 0.946], loss: 1.949303, mean_absolute_error: 36.018241, mean_q: 21.774272, mean_eps: 0.542705
  508286/2000000: episode: 4530, duration: 1.211s, episode steps: 85, steps per second: 70, episode reward: -71.356, mean reward: -0.839 [-100.000, 8.843], mean action: 1.741 [0.000, 3.000], mean observation: -0.123 [-0.854, 2.724], loss: 2.027805, mean_absolute_error: 38.258416, mean_q: 22.320500, mean_eps: 0.542580
  508392/2000000: episode: 4531, duration: 1.556s, episode steps: 106, steps per second: 68, episode reward: -121.307, mean reward: -1.144 [-100.000, 11.488], mean action: 1.858 [0.000, 3.000], mean observation: 0.098 [-4.382, 1.000], loss: 1.521508, mean_absolute_error: 36.416543, mean_q: 22.120041, mean_eps: 0.542496
  508473/2000000: episode: 4532, duration: 1.216s, episode steps: 81, steps per second: 67, episode reward: -90.687, mean reward: -1.120 [-100.000, 12.345], mean action: 1.889 [0.000, 3.000], mean observation: -0.177 [-3.904, 1.000], loss: 1.636878, mean_absolute_error: 36.516066, mean_q: 19.869227, mean_eps: 0.542411
  508569/2000000: episode: 4533, duration: 1.395s, episode steps: 96, steps per second: 69, episode reward: -17.755, mean reward: -0.185 [-100.000, 43.583], mean action: 1.656 [0.000, 3.000], mean observation: 0.124 [-1.589, 1.000], loss: 1.639631, mean_absolute_error: 36.614861, mean_q: 23.103277, mean_eps: 0.542330
  508628/2000000: episode: 4534, duration: 0.870s, episode steps: 59, steps per second: 68, episode reward: -105.668, mean reward: -1.791 [-100.000, 8.191], mean action: 1.797 [0.000, 3.000], mean observation: -0.169 [-4.769, 1.000], loss: 1.364282, mean_absolute_error: 36.510358, mean_q: 23.627964, mean_eps: 0.542262
  508875/2000000: episode: 4535, duration: 3.634s, episode steps: 247, steps per second: 68, episode reward: -65.289, mean reward: -0.264 [-100.000, 15.958], mean action: 1.628 [0.000, 3.000], mean observation: 0.009 [-1.637, 1.000], loss: 1.829823, mean_absolute_error: 36.524297, mean_q: 21.479091, mean_eps: 0.542125
  509004/2000000: episode: 4536, duration: 1.901s, episode steps: 129, steps per second: 68, episode reward: -79.269, mean reward: -0.614 [-100.000, 12.403], mean action: 1.698 [0.000, 3.000], mean observation: -0.058 [-0.712, 2.162], loss: 2.146341, mean_absolute_error: 35.759222, mean_q: 23.313967, mean_eps: 0.541956
  510004/2000000: episode: 4537, duration: 15.763s, episode steps: 1000, steps per second: 63, episode reward: -24.224, mean reward: -0.024 [-24.613, 24.004], mean action: 1.239 [0.000, 3.000], mean observation: 0.075 [-1.121, 1.000], loss: 1.774959, mean_absolute_error: 36.683493, mean_q: 21.716896, mean_eps: 0.541448
  510209/2000000: episode: 4538, duration: 3.053s, episode steps: 205, steps per second: 67, episode reward: -162.853, mean reward: -0.794 [-100.000, 7.569], mean action: 1.805 [0.000, 3.000], mean observation: -0.104 [-0.970, 1.631], loss: 1.611896, mean_absolute_error: 37.059901, mean_q: 23.597048, mean_eps: 0.540905
  510347/2000000: episode: 4539, duration: 1.974s, episode steps: 138, steps per second: 70, episode reward: -86.441, mean reward: -0.626 [-100.000, 10.042], mean action: 1.609 [0.000, 3.000], mean observation: -0.062 [-0.802, 2.682], loss: 1.789899, mean_absolute_error: 35.878055, mean_q: 25.208017, mean_eps: 0.540750
  510437/2000000: episode: 4540, duration: 1.336s, episode steps: 90, steps per second: 67, episode reward: -56.132, mean reward: -0.624 [-100.000, 18.925], mean action: 1.678 [0.000, 3.000], mean observation: -0.026 [-1.827, 1.000], loss: 1.611866, mean_absolute_error: 35.323474, mean_q: 22.828475, mean_eps: 0.540647
  510597/2000000: episode: 4541, duration: 2.318s, episode steps: 160, steps per second: 69, episode reward: -49.600, mean reward: -0.310 [-100.000, 14.974], mean action: 1.700 [0.000, 3.000], mean observation: 0.015 [-0.783, 1.000], loss: 1.682980, mean_absolute_error: 36.469019, mean_q: 21.632561, mean_eps: 0.540534
  510675/2000000: episode: 4542, duration: 1.119s, episode steps: 78, steps per second: 70, episode reward: -16.210, mean reward: -0.208 [-100.000, 40.607], mean action: 1.705 [0.000, 3.000], mean observation: -0.095 [-2.246, 1.000], loss: 2.044814, mean_absolute_error: 35.683520, mean_q: 21.151399, mean_eps: 0.540428
  511675/2000000: episode: 4543, duration: 15.724s, episode steps: 1000, steps per second: 64, episode reward: 46.806, mean reward: 0.047 [-24.132, 35.314], mean action: 1.174 [0.000, 3.000], mean observation: 0.076 [-0.760, 1.000], loss: 1.691771, mean_absolute_error: 36.280976, mean_q: 23.359412, mean_eps: 0.539943
  511786/2000000: episode: 4544, duration: 1.647s, episode steps: 111, steps per second: 67, episode reward: -102.426, mean reward: -0.923 [-100.000, 10.063], mean action: 1.730 [0.000, 3.000], mean observation: -0.160 [-2.912, 1.000], loss: 1.598382, mean_absolute_error: 35.466866, mean_q: 19.837226, mean_eps: 0.539443
  511908/2000000: episode: 4545, duration: 1.788s, episode steps: 122, steps per second: 68, episode reward: -73.362, mean reward: -0.601 [-100.000, 17.076], mean action: 1.721 [0.000, 3.000], mean observation: 0.078 [-0.690, 2.148], loss: 1.488508, mean_absolute_error: 35.560277, mean_q: 22.404102, mean_eps: 0.539339
  512049/2000000: episode: 4546, duration: 2.095s, episode steps: 141, steps per second: 67, episode reward: -63.866, mean reward: -0.453 [-100.000, 7.227], mean action: 1.766 [0.000, 3.000], mean observation: 0.082 [-1.565, 1.000], loss: 1.640314, mean_absolute_error: 35.377601, mean_q: 22.635644, mean_eps: 0.539220
  512157/2000000: episode: 4547, duration: 1.581s, episode steps: 108, steps per second: 68, episode reward: -110.145, mean reward: -1.020 [-100.000, 13.485], mean action: 1.620 [0.000, 3.000], mean observation: 0.015 [-2.846, 1.000], loss: 2.055458, mean_absolute_error: 36.877255, mean_q: 18.716752, mean_eps: 0.539106
  512287/2000000: episode: 4548, duration: 1.847s, episode steps: 130, steps per second: 70, episode reward: -87.972, mean reward: -0.677 [-100.000, 10.857], mean action: 1.715 [0.000, 3.000], mean observation: 0.111 [-3.351, 1.000], loss: 1.730355, mean_absolute_error: 37.156379, mean_q: 24.734455, mean_eps: 0.539000
  513287/2000000: episode: 4549, duration: 15.502s, episode steps: 1000, steps per second: 65, episode reward: 1.509, mean reward: 0.002 [-23.671, 24.066], mean action: 1.509 [0.000, 3.000], mean observation: 0.085 [-1.027, 1.091], loss: 1.934242, mean_absolute_error: 36.300408, mean_q: 23.864220, mean_eps: 0.538493
  513442/2000000: episode: 4550, duration: 2.263s, episode steps: 155, steps per second: 68, episode reward: -37.158, mean reward: -0.240 [-100.000, 17.676], mean action: 1.806 [0.000, 3.000], mean observation: 0.091 [-0.691, 1.702], loss: 1.862686, mean_absolute_error: 36.681523, mean_q: 23.010624, mean_eps: 0.537972
  513597/2000000: episode: 4551, duration: 2.229s, episode steps: 155, steps per second: 70, episode reward: -76.724, mean reward: -0.495 [-100.000, 8.190], mean action: 1.845 [0.000, 3.000], mean observation: 0.071 [-0.901, 3.172], loss: 2.057369, mean_absolute_error: 35.916730, mean_q: 21.810502, mean_eps: 0.537832
  513689/2000000: episode: 4552, duration: 1.322s, episode steps: 92, steps per second: 70, episode reward: -93.907, mean reward: -1.021 [-100.000, 7.874], mean action: 1.761 [0.000, 3.000], mean observation: 0.059 [-3.450, 1.000], loss: 1.887937, mean_absolute_error: 36.202161, mean_q: 21.739593, mean_eps: 0.537720
  513802/2000000: episode: 4553, duration: 1.620s, episode steps: 113, steps per second: 70, episode reward: -41.777, mean reward: -0.370 [-100.000, 19.791], mean action: 1.664 [0.000, 3.000], mean observation: -0.037 [-0.701, 1.930], loss: 1.589949, mean_absolute_error: 36.090985, mean_q: 24.630855, mean_eps: 0.537629
  514006/2000000: episode: 4554, duration: 3.117s, episode steps: 204, steps per second: 65, episode reward: -223.452, mean reward: -1.095 [-100.000, 30.945], mean action: 1.794 [0.000, 3.000], mean observation: 0.028 [-0.993, 1.704], loss: 1.574829, mean_absolute_error: 35.882034, mean_q: 24.435103, mean_eps: 0.537486
  514100/2000000: episode: 4555, duration: 1.397s, episode steps: 94, steps per second: 67, episode reward: -121.412, mean reward: -1.292 [-100.000, 6.078], mean action: 1.819 [0.000, 3.000], mean observation: -0.122 [-0.926, 1.722], loss: 1.533047, mean_absolute_error: 37.546522, mean_q: 22.059500, mean_eps: 0.537353
  514204/2000000: episode: 4556, duration: 1.562s, episode steps: 104, steps per second: 67, episode reward: -45.326, mean reward: -0.436 [-100.000, 8.101], mean action: 1.750 [0.000, 3.000], mean observation: -0.080 [-1.514, 1.000], loss: 1.920604, mean_absolute_error: 36.319868, mean_q: 21.528408, mean_eps: 0.537265
  514429/2000000: episode: 4557, duration: 3.292s, episode steps: 225, steps per second: 68, episode reward: -68.932, mean reward: -0.306 [-100.000, 14.500], mean action: 1.738 [0.000, 3.000], mean observation: 0.089 [-0.669, 1.016], loss: 1.920577, mean_absolute_error: 36.664934, mean_q: 24.259379, mean_eps: 0.537116
  515429/2000000: episode: 4558, duration: 15.440s, episode steps: 1000, steps per second: 65, episode reward: -13.963, mean reward: -0.014 [-21.626, 22.365], mean action: 1.463 [0.000, 3.000], mean observation: 0.233 [-0.619, 1.000], loss: 1.634500, mean_absolute_error: 36.601777, mean_q: 22.734509, mean_eps: 0.536563
  515520/2000000: episode: 4559, duration: 1.348s, episode steps: 91, steps per second: 67, episode reward: -64.960, mean reward: -0.714 [-100.000, 9.772], mean action: 1.791 [0.000, 3.000], mean observation: -0.019 [-0.863, 2.649], loss: 1.322201, mean_absolute_error: 35.091756, mean_q: 25.686471, mean_eps: 0.536073
  515682/2000000: episode: 4560, duration: 2.376s, episode steps: 162, steps per second: 68, episode reward: -82.231, mean reward: -0.508 [-100.000, 20.515], mean action: 1.735 [0.000, 3.000], mean observation: -0.035 [-1.299, 1.000], loss: 1.807753, mean_absolute_error: 36.125179, mean_q: 19.095844, mean_eps: 0.535960
  515800/2000000: episode: 4561, duration: 1.764s, episode steps: 118, steps per second: 67, episode reward: -87.410, mean reward: -0.741 [-100.000, 8.691], mean action: 1.729 [0.000, 3.000], mean observation: 0.113 [-0.618, 1.921], loss: 1.648509, mean_absolute_error: 36.405530, mean_q: 19.495526, mean_eps: 0.535834
  515927/2000000: episode: 4562, duration: 1.841s, episode steps: 127, steps per second: 69, episode reward: -86.408, mean reward: -0.680 [-100.000, 41.463], mean action: 1.701 [0.000, 3.000], mean observation: 0.149 [-3.343, 1.000], loss: 1.783522, mean_absolute_error: 37.087739, mean_q: 24.110743, mean_eps: 0.535724
  516069/2000000: episode: 4563, duration: 2.056s, episode steps: 142, steps per second: 69, episode reward: -228.566, mean reward: -1.610 [-100.000, 41.772], mean action: 1.613 [0.000, 3.000], mean observation: 0.158 [-0.695, 1.948], loss: 2.118360, mean_absolute_error: 37.223654, mean_q: 21.508145, mean_eps: 0.535602
  516232/2000000: episode: 4564, duration: 2.336s, episode steps: 163, steps per second: 70, episode reward: -106.570, mean reward: -0.654 [-100.000, 15.958], mean action: 1.644 [0.000, 3.000], mean observation: -0.002 [-0.798, 2.726], loss: 1.919343, mean_absolute_error: 36.372571, mean_q: 24.308040, mean_eps: 0.535465
  516407/2000000: episode: 4565, duration: 2.552s, episode steps: 175, steps per second: 69, episode reward: -121.273, mean reward: -0.693 [-100.000, 7.112], mean action: 1.640 [0.000, 3.000], mean observation: 0.010 [-3.185, 1.000], loss: 1.689799, mean_absolute_error: 36.757955, mean_q: 24.085744, mean_eps: 0.535314
  516498/2000000: episode: 4566, duration: 1.336s, episode steps: 91, steps per second: 68, episode reward: -81.770, mean reward: -0.899 [-100.000, 11.299], mean action: 1.703 [0.000, 3.000], mean observation: 0.055 [-0.955, 1.000], loss: 2.231295, mean_absolute_error: 37.094246, mean_q: 22.990658, mean_eps: 0.535193
  516604/2000000: episode: 4567, duration: 1.547s, episode steps: 106, steps per second: 69, episode reward: -19.041, mean reward: -0.180 [-100.000, 15.404], mean action: 1.632 [0.000, 3.000], mean observation: 0.001 [-0.789, 1.000], loss: 1.406651, mean_absolute_error: 35.672621, mean_q: 22.700726, mean_eps: 0.535105
  517604/2000000: episode: 4568, duration: 15.522s, episode steps: 1000, steps per second: 64, episode reward: 74.372, mean reward: 0.074 [-22.192, 22.566], mean action: 1.514 [0.000, 3.000], mean observation: 0.160 [-0.600, 1.000], loss: 1.680055, mean_absolute_error: 36.313741, mean_q: 22.699996, mean_eps: 0.534608
  517741/2000000: episode: 4569, duration: 2.010s, episode steps: 137, steps per second: 68, episode reward: -52.240, mean reward: -0.381 [-100.000, 36.416], mean action: 1.810 [0.000, 3.000], mean observation: -0.109 [-0.775, 1.285], loss: 1.626522, mean_absolute_error: 36.288906, mean_q: 21.390468, mean_eps: 0.534095
  518741/2000000: episode: 4570, duration: 15.640s, episode steps: 1000, steps per second: 64, episode reward: 28.934, mean reward: 0.029 [-21.452, 23.209], mean action: 1.704 [0.000, 3.000], mean observation: 0.176 [-0.892, 1.000], loss: 1.727131, mean_absolute_error: 36.020837, mean_q: 23.414688, mean_eps: 0.533582
  518868/2000000: episode: 4571, duration: 1.855s, episode steps: 127, steps per second: 68, episode reward: -105.925, mean reward: -0.834 [-100.000, 8.573], mean action: 1.724 [0.000, 3.000], mean observation: -0.027 [-0.863, 2.829], loss: 2.024274, mean_absolute_error: 35.617310, mean_q: 19.453236, mean_eps: 0.533076
  519000/2000000: episode: 4572, duration: 1.951s, episode steps: 132, steps per second: 68, episode reward: -67.144, mean reward: -0.509 [-100.000, 15.451], mean action: 1.682 [0.000, 3.000], mean observation: 0.124 [-1.263, 1.000], loss: 2.710308, mean_absolute_error: 36.972883, mean_q: 23.876723, mean_eps: 0.532961
  519257/2000000: episode: 4573, duration: 3.789s, episode steps: 257, steps per second: 68, episode reward: -93.256, mean reward: -0.363 [-100.000, 8.143], mean action: 1.747 [0.000, 3.000], mean observation: -0.014 [-0.635, 1.356], loss: 1.787791, mean_absolute_error: 36.776056, mean_q: 21.707345, mean_eps: 0.532785
  519412/2000000: episode: 4574, duration: 2.249s, episode steps: 155, steps per second: 69, episode reward: -72.371, mean reward: -0.467 [-100.000, 13.642], mean action: 1.555 [0.000, 3.000], mean observation: -0.041 [-1.042, 1.000], loss: 2.001599, mean_absolute_error: 37.058177, mean_q: 18.656706, mean_eps: 0.532599
  519507/2000000: episode: 4575, duration: 1.397s, episode steps: 95, steps per second: 68, episode reward: -135.468, mean reward: -1.426 [-100.000, 29.713], mean action: 1.789 [0.000, 3.000], mean observation: -0.127 [-1.120, 3.158], loss: 1.662671, mean_absolute_error: 35.798745, mean_q: 24.879656, mean_eps: 0.532488
  519626/2000000: episode: 4576, duration: 1.730s, episode steps: 119, steps per second: 69, episode reward: -64.883, mean reward: -0.545 [-100.000, 14.842], mean action: 1.731 [0.000, 3.000], mean observation: 0.005 [-0.830, 1.000], loss: 2.392002, mean_absolute_error: 36.619267, mean_q: 20.855925, mean_eps: 0.532391
  519804/2000000: episode: 4577, duration: 2.638s, episode steps: 178, steps per second: 67, episode reward: -52.755, mean reward: -0.296 [-100.000, 11.834], mean action: 1.708 [0.000, 3.000], mean observation: 0.068 [-1.581, 1.000], loss: 1.758145, mean_absolute_error: 35.781222, mean_q: 23.192530, mean_eps: 0.532257
  519895/2000000: episode: 4578, duration: 1.331s, episode steps: 91, steps per second: 68, episode reward: -94.995, mean reward: -1.044 [-100.000, 9.867], mean action: 1.703 [0.000, 3.000], mean observation: -0.152 [-0.918, 2.687], loss: 1.483215, mean_absolute_error: 35.980850, mean_q: 25.225013, mean_eps: 0.532137
  520003/2000000: episode: 4579, duration: 1.569s, episode steps: 108, steps per second: 69, episode reward: -67.984, mean reward: -0.629 [-100.000, 21.122], mean action: 1.713 [0.000, 3.000], mean observation: 0.124 [-0.797, 2.496], loss: 1.900642, mean_absolute_error: 36.266519, mean_q: 24.530540, mean_eps: 0.532047
  520204/2000000: episode: 4580, duration: 2.907s, episode steps: 201, steps per second: 69, episode reward: -30.987, mean reward: -0.154 [-100.000, 13.939], mean action: 1.711 [0.000, 3.000], mean observation: -0.029 [-0.865, 1.000], loss: 1.533478, mean_absolute_error: 35.155946, mean_q: 23.949605, mean_eps: 0.531908
  520381/2000000: episode: 4581, duration: 2.733s, episode steps: 177, steps per second: 65, episode reward: -87.065, mean reward: -0.492 [-100.000, 13.047], mean action: 1.684 [0.000, 3.000], mean observation: 0.000 [-1.092, 1.000], loss: 1.486198, mean_absolute_error: 36.944433, mean_q: 24.914833, mean_eps: 0.531737
  520555/2000000: episode: 4582, duration: 2.507s, episode steps: 174, steps per second: 69, episode reward: -100.734, mean reward: -0.579 [-100.000, 9.468], mean action: 1.661 [0.000, 3.000], mean observation: -0.049 [-0.988, 1.000], loss: 1.604387, mean_absolute_error: 36.528170, mean_q: 23.106960, mean_eps: 0.531579
  520657/2000000: episode: 4583, duration: 1.499s, episode steps: 102, steps per second: 68, episode reward: -93.650, mean reward: -0.918 [-100.000, 11.501], mean action: 1.716 [0.000, 3.000], mean observation: 0.056 [-0.933, 2.925], loss: 1.438244, mean_absolute_error: 36.948636, mean_q: 24.608152, mean_eps: 0.531455
  520804/2000000: episode: 4584, duration: 2.140s, episode steps: 147, steps per second: 69, episode reward: -37.609, mean reward: -0.256 [-100.000, 19.325], mean action: 1.748 [0.000, 3.000], mean observation: -0.044 [-2.082, 1.000], loss: 2.408248, mean_absolute_error: 37.515796, mean_q: 21.288745, mean_eps: 0.531343
  520920/2000000: episode: 4585, duration: 1.734s, episode steps: 116, steps per second: 67, episode reward: -95.359, mean reward: -0.822 [-100.000, 13.797], mean action: 1.793 [0.000, 3.000], mean observation: -0.014 [-2.504, 1.000], loss: 2.154623, mean_absolute_error: 35.977686, mean_q: 23.539419, mean_eps: 0.531226
  521035/2000000: episode: 4586, duration: 1.672s, episode steps: 115, steps per second: 69, episode reward: -89.521, mean reward: -0.778 [-100.000, 17.469], mean action: 1.765 [0.000, 3.000], mean observation: 0.033 [-2.790, 1.000], loss: 2.792920, mean_absolute_error: 36.998615, mean_q: 22.305382, mean_eps: 0.531122
  522035/2000000: episode: 4587, duration: 15.738s, episode steps: 1000, steps per second: 64, episode reward: 74.115, mean reward: 0.074 [-23.368, 29.344], mean action: 1.391 [0.000, 3.000], mean observation: 0.186 [-0.554, 1.000], loss: 1.843647, mean_absolute_error: 36.329480, mean_q: 23.655222, mean_eps: 0.530619
  522258/2000000: episode: 4588, duration: 3.256s, episode steps: 223, steps per second: 68, episode reward: -33.945, mean reward: -0.152 [-100.000, 16.403], mean action: 1.628 [0.000, 3.000], mean observation: 0.090 [-1.391, 1.020], loss: 1.739240, mean_absolute_error: 36.020931, mean_q: 25.044515, mean_eps: 0.530069
  523258/2000000: episode: 4589, duration: 15.611s, episode steps: 1000, steps per second: 64, episode reward: 64.345, mean reward: 0.064 [-22.851, 32.334], mean action: 1.588 [0.000, 3.000], mean observation: 0.146 [-1.149, 1.000], loss: 1.816045, mean_absolute_error: 36.643022, mean_q: 23.071370, mean_eps: 0.529518
  524258/2000000: episode: 4590, duration: 15.545s, episode steps: 1000, steps per second: 64, episode reward: 11.289, mean reward: 0.011 [-24.297, 23.542], mean action: 2.019 [0.000, 3.000], mean observation: 0.238 [-0.635, 1.024], loss: 1.740851, mean_absolute_error: 36.072173, mean_q: 23.819012, mean_eps: 0.528618
  524492/2000000: episode: 4591, duration: 3.434s, episode steps: 234, steps per second: 68, episode reward: -56.302, mean reward: -0.241 [-100.000, 16.959], mean action: 1.662 [0.000, 3.000], mean observation: 0.044 [-0.607, 1.116], loss: 1.701236, mean_absolute_error: 35.641068, mean_q: 24.491224, mean_eps: 0.528063
  524612/2000000: episode: 4592, duration: 1.787s, episode steps: 120, steps per second: 67, episode reward: -61.889, mean reward: -0.516 [-100.000, 30.948], mean action: 1.667 [0.000, 3.000], mean observation: 0.035 [-0.684, 1.800], loss: 2.463401, mean_absolute_error: 36.612083, mean_q: 22.999106, mean_eps: 0.527905
  525612/2000000: episode: 4593, duration: 16.021s, episode steps: 1000, steps per second: 62, episode reward: 41.910, mean reward: 0.042 [-23.669, 23.852], mean action: 1.601 [0.000, 3.000], mean observation: 0.233 [-0.721, 1.000], loss: 1.739398, mean_absolute_error: 35.547779, mean_q: 22.587948, mean_eps: 0.527401
  525773/2000000: episode: 4594, duration: 2.379s, episode steps: 161, steps per second: 68, episode reward: -8.279, mean reward: -0.051 [-100.000, 18.218], mean action: 1.783 [0.000, 3.000], mean observation: 0.084 [-0.814, 1.003], loss: 1.286118, mean_absolute_error: 35.393215, mean_q: 24.266576, mean_eps: 0.526877
  525875/2000000: episode: 4595, duration: 1.463s, episode steps: 102, steps per second: 70, episode reward: -104.430, mean reward: -1.024 [-100.000, 7.941], mean action: 1.735 [0.000, 3.000], mean observation: -0.017 [-0.915, 3.316], loss: 3.394149, mean_absolute_error: 37.298152, mean_q: 24.119649, mean_eps: 0.526758
  526103/2000000: episode: 4596, duration: 3.340s, episode steps: 228, steps per second: 68, episode reward: -243.114, mean reward: -1.066 [-100.000, 31.143], mean action: 1.851 [0.000, 3.000], mean observation: -0.023 [-0.847, 1.531], loss: 1.994536, mean_absolute_error: 35.923077, mean_q: 23.015866, mean_eps: 0.526611
  526318/2000000: episode: 4597, duration: 3.155s, episode steps: 215, steps per second: 68, episode reward: -225.214, mean reward: -1.048 [-100.000, 37.028], mean action: 1.660 [0.000, 3.000], mean observation: 0.065 [-1.784, 1.000], loss: 1.505676, mean_absolute_error: 35.586855, mean_q: 22.241125, mean_eps: 0.526411
  526475/2000000: episode: 4598, duration: 2.272s, episode steps: 157, steps per second: 69, episode reward: -73.303, mean reward: -0.467 [-100.000, 38.687], mean action: 1.803 [0.000, 3.000], mean observation: 0.042 [-0.956, 2.773], loss: 1.679561, mean_absolute_error: 35.536164, mean_q: 24.792256, mean_eps: 0.526244
  527475/2000000: episode: 4599, duration: 16.103s, episode steps: 1000, steps per second: 62, episode reward: 16.391, mean reward: 0.016 [-23.482, 23.711], mean action: 1.233 [0.000, 3.000], mean observation: 0.133 [-0.572, 1.084], loss: 1.740141, mean_absolute_error: 35.666162, mean_q: 23.610933, mean_eps: 0.525723
  527630/2000000: episode: 4600, duration: 2.258s, episode steps: 155, steps per second: 69, episode reward: -100.005, mean reward: -0.645 [-100.000, 13.020], mean action: 1.781 [0.000, 3.000], mean observation: 0.055 [-0.644, 1.000], loss: 1.844509, mean_absolute_error: 35.973097, mean_q: 24.919174, mean_eps: 0.525203
  527798/2000000: episode: 4601, duration: 2.437s, episode steps: 168, steps per second: 69, episode reward: -5.053, mean reward: -0.030 [-100.000, 15.770], mean action: 1.839 [0.000, 3.000], mean observation: -0.007 [-1.416, 1.000], loss: 2.433724, mean_absolute_error: 36.587934, mean_q: 22.287896, mean_eps: 0.525057
  527917/2000000: episode: 4602, duration: 1.744s, episode steps: 119, steps per second: 68, episode reward: -98.791, mean reward: -0.830 [-100.000, 11.296], mean action: 1.714 [0.000, 3.000], mean observation: 0.011 [-0.968, 2.867], loss: 1.529242, mean_absolute_error: 34.573814, mean_q: 23.741141, mean_eps: 0.524928
  528075/2000000: episode: 4603, duration: 2.285s, episode steps: 158, steps per second: 69, episode reward: -60.740, mean reward: -0.384 [-100.000, 18.610], mean action: 1.728 [0.000, 3.000], mean observation: 0.063 [-2.207, 1.000], loss: 1.505474, mean_absolute_error: 36.306633, mean_q: 22.043243, mean_eps: 0.524804
  528234/2000000: episode: 4604, duration: 2.348s, episode steps: 159, steps per second: 68, episode reward: -121.163, mean reward: -0.762 [-100.000, 8.116], mean action: 1.717 [0.000, 3.000], mean observation: 0.019 [-0.840, 1.000], loss: 1.764136, mean_absolute_error: 36.554686, mean_q: 24.231267, mean_eps: 0.524661
  528348/2000000: episode: 4605, duration: 1.659s, episode steps: 114, steps per second: 69, episode reward: -61.759, mean reward: -0.542 [-100.000, 26.593], mean action: 1.728 [0.000, 3.000], mean observation: -0.114 [-0.846, 2.132], loss: 1.364978, mean_absolute_error: 35.595264, mean_q: 26.748417, mean_eps: 0.524539
  528511/2000000: episode: 4606, duration: 2.425s, episode steps: 163, steps per second: 67, episode reward: -128.943, mean reward: -0.791 [-100.000, 16.630], mean action: 1.742 [0.000, 3.000], mean observation: -0.006 [-0.896, 3.035], loss: 1.765477, mean_absolute_error: 35.668658, mean_q: 22.644905, mean_eps: 0.524415
  528691/2000000: episode: 4607, duration: 2.605s, episode steps: 180, steps per second: 69, episode reward: -85.723, mean reward: -0.476 [-100.000, 20.245], mean action: 1.833 [0.000, 3.000], mean observation: -0.098 [-0.772, 1.792], loss: 1.622617, mean_absolute_error: 36.557846, mean_q: 23.568511, mean_eps: 0.524260
  529691/2000000: episode: 4608, duration: 16.940s, episode steps: 1000, steps per second: 59, episode reward: -1.141, mean reward: -0.001 [-22.114, 23.343], mean action: 1.450 [0.000, 3.000], mean observation: 0.189 [-1.042, 1.000], loss: 1.945480, mean_absolute_error: 36.157173, mean_q: 22.958010, mean_eps: 0.523729
  529812/2000000: episode: 4609, duration: 1.815s, episode steps: 121, steps per second: 67, episode reward: -101.834, mean reward: -0.842 [-100.000, 15.537], mean action: 1.694 [0.000, 3.000], mean observation: -0.001 [-0.934, 1.000], loss: 1.844917, mean_absolute_error: 36.045814, mean_q: 19.977197, mean_eps: 0.523225
  529943/2000000: episode: 4610, duration: 1.918s, episode steps: 131, steps per second: 68, episode reward: -31.306, mean reward: -0.239 [-100.000, 17.824], mean action: 1.695 [0.000, 3.000], mean observation: 0.013 [-0.687, 1.001], loss: 1.930957, mean_absolute_error: 36.079735, mean_q: 24.953996, mean_eps: 0.523112
  530109/2000000: episode: 4611, duration: 2.424s, episode steps: 166, steps per second: 68, episode reward: -35.385, mean reward: -0.213 [-100.000, 19.537], mean action: 1.693 [0.000, 3.000], mean observation: 0.027 [-1.001, 1.000], loss: 2.017402, mean_absolute_error: 36.310160, mean_q: 23.730602, mean_eps: 0.522977
  530238/2000000: episode: 4612, duration: 1.855s, episode steps: 129, steps per second: 70, episode reward: -90.357, mean reward: -0.700 [-100.000, 10.801], mean action: 1.798 [0.000, 3.000], mean observation: 0.098 [-2.978, 1.000], loss: 1.658413, mean_absolute_error: 35.790777, mean_q: 23.264268, mean_eps: 0.522843
  530336/2000000: episode: 4613, duration: 1.452s, episode steps: 98, steps per second: 67, episode reward: -56.238, mean reward: -0.574 [-100.000, 10.267], mean action: 1.480 [0.000, 3.000], mean observation: 0.040 [-2.139, 1.000], loss: 2.119646, mean_absolute_error: 37.059878, mean_q: 21.538160, mean_eps: 0.522743
  530512/2000000: episode: 4614, duration: 2.588s, episode steps: 176, steps per second: 68, episode reward: -37.374, mean reward: -0.212 [-100.000, 13.992], mean action: 1.750 [0.000, 3.000], mean observation: 0.061 [-1.187, 1.000], loss: 2.203753, mean_absolute_error: 35.935750, mean_q: 21.852215, mean_eps: 0.522620
  530661/2000000: episode: 4615, duration: 2.211s, episode steps: 149, steps per second: 67, episode reward: -38.448, mean reward: -0.258 [-100.000, 10.753], mean action: 1.745 [0.000, 3.000], mean observation: 0.135 [-2.531, 1.036], loss: 1.778200, mean_absolute_error: 36.139695, mean_q: 26.253485, mean_eps: 0.522473
  530757/2000000: episode: 4616, duration: 1.399s, episode steps: 96, steps per second: 69, episode reward: -85.006, mean reward: -0.885 [-100.000, 11.592], mean action: 1.698 [0.000, 3.000], mean observation: 0.133 [-1.655, 1.000], loss: 1.610649, mean_absolute_error: 34.732692, mean_q: 22.992817, mean_eps: 0.522361
  530885/2000000: episode: 4617, duration: 1.853s, episode steps: 128, steps per second: 69, episode reward: -62.311, mean reward: -0.487 [-100.000, 13.229], mean action: 1.547 [0.000, 3.000], mean observation: -0.029 [-0.664, 1.000], loss: 2.320552, mean_absolute_error: 37.316873, mean_q: 24.004660, mean_eps: 0.522260
  531011/2000000: episode: 4618, duration: 1.811s, episode steps: 126, steps per second: 70, episode reward: -230.326, mean reward: -1.828 [-100.000, 27.188], mean action: 1.706 [0.000, 3.000], mean observation: 0.156 [-0.657, 1.886], loss: 2.106293, mean_absolute_error: 35.907900, mean_q: 22.692194, mean_eps: 0.522147
  531158/2000000: episode: 4619, duration: 2.138s, episode steps: 147, steps per second: 69, episode reward: -55.490, mean reward: -0.377 [-100.000, 13.898], mean action: 1.741 [0.000, 3.000], mean observation: 0.071 [-0.609, 1.451], loss: 2.491027, mean_absolute_error: 36.404435, mean_q: 24.576715, mean_eps: 0.522024
  531262/2000000: episode: 4620, duration: 1.494s, episode steps: 104, steps per second: 70, episode reward: -166.372, mean reward: -1.600 [-100.000, 21.805], mean action: 1.644 [0.000, 3.000], mean observation: -0.012 [-3.455, 1.000], loss: 2.149998, mean_absolute_error: 37.609618, mean_q: 18.799209, mean_eps: 0.521911
  531420/2000000: episode: 4621, duration: 2.346s, episode steps: 158, steps per second: 67, episode reward: -97.048, mean reward: -0.614 [-100.000, 18.912], mean action: 1.854 [0.000, 3.000], mean observation: 0.003 [-0.789, 2.605], loss: 1.469878, mean_absolute_error: 35.642657, mean_q: 24.554096, mean_eps: 0.521794
  531573/2000000: episode: 4622, duration: 2.255s, episode steps: 153, steps per second: 68, episode reward: -115.644, mean reward: -0.756 [-100.000, 12.465], mean action: 1.686 [0.000, 3.000], mean observation: -0.036 [-1.360, 4.720], loss: 1.603420, mean_absolute_error: 35.760656, mean_q: 24.332535, mean_eps: 0.521654
  531718/2000000: episode: 4623, duration: 2.106s, episode steps: 145, steps per second: 69, episode reward: -37.891, mean reward: -0.261 [-100.000, 14.155], mean action: 1.669 [0.000, 3.000], mean observation: 0.029 [-0.699, 1.000], loss: 1.675938, mean_absolute_error: 35.961323, mean_q: 23.265739, mean_eps: 0.521519
  532718/2000000: episode: 4624, duration: 15.598s, episode steps: 1000, steps per second: 64, episode reward: 55.719, mean reward: 0.056 [-24.495, 23.354], mean action: 1.613 [0.000, 3.000], mean observation: 0.184 [-0.639, 1.173], loss: 1.724179, mean_absolute_error: 36.102791, mean_q: 23.178069, mean_eps: 0.521004
  532836/2000000: episode: 4625, duration: 1.771s, episode steps: 118, steps per second: 67, episode reward: -140.869, mean reward: -1.194 [-100.000, 9.528], mean action: 1.814 [0.000, 3.000], mean observation: -0.133 [-1.002, 1.395], loss: 1.890013, mean_absolute_error: 36.038566, mean_q: 20.760547, mean_eps: 0.520502
  532988/2000000: episode: 4626, duration: 2.276s, episode steps: 152, steps per second: 67, episode reward: -40.112, mean reward: -0.264 [-100.000, 17.754], mean action: 1.809 [0.000, 3.000], mean observation: -0.002 [-1.640, 1.000], loss: 2.126913, mean_absolute_error: 35.920528, mean_q: 24.145192, mean_eps: 0.520381
  533142/2000000: episode: 4627, duration: 2.260s, episode steps: 154, steps per second: 68, episode reward: -118.267, mean reward: -0.768 [-100.000, 69.087], mean action: 1.721 [0.000, 3.000], mean observation: -0.082 [-1.240, 1.345], loss: 1.707203, mean_absolute_error: 35.899182, mean_q: 25.554087, mean_eps: 0.520242
  533243/2000000: episode: 4628, duration: 1.565s, episode steps: 101, steps per second: 65, episode reward: -118.908, mean reward: -1.177 [-100.000, 10.824], mean action: 1.554 [0.000, 3.000], mean observation: -0.071 [-1.039, 3.693], loss: 1.925670, mean_absolute_error: 36.266694, mean_q: 23.715084, mean_eps: 0.520127
  533488/2000000: episode: 4629, duration: 3.663s, episode steps: 245, steps per second: 67, episode reward: -41.452, mean reward: -0.169 [-100.000, 12.386], mean action: 1.645 [0.000, 3.000], mean observation: 0.059 [-1.408, 1.023], loss: 1.921139, mean_absolute_error: 34.733607, mean_q: 24.628563, mean_eps: 0.519972
  533637/2000000: episode: 4630, duration: 2.196s, episode steps: 149, steps per second: 68, episode reward: -11.537, mean reward: -0.077 [-100.000, 20.287], mean action: 1.691 [0.000, 3.000], mean observation: 0.032 [-1.017, 1.000], loss: 2.168572, mean_absolute_error: 36.192771, mean_q: 23.407641, mean_eps: 0.519794
  533753/2000000: episode: 4631, duration: 1.691s, episode steps: 116, steps per second: 69, episode reward: -107.479, mean reward: -0.927 [-100.000, 4.626], mean action: 1.914 [0.000, 3.000], mean observation: -0.156 [-1.834, 1.000], loss: 1.852345, mean_absolute_error: 37.167593, mean_q: 24.812229, mean_eps: 0.519674
  533946/2000000: episode: 4632, duration: 2.794s, episode steps: 193, steps per second: 69, episode reward: -118.133, mean reward: -0.612 [-100.000, 11.390], mean action: 1.699 [0.000, 3.000], mean observation: 0.144 [-0.641, 1.000], loss: 1.563910, mean_absolute_error: 35.723759, mean_q: 23.675155, mean_eps: 0.519535
  534082/2000000: episode: 4633, duration: 1.992s, episode steps: 136, steps per second: 68, episode reward: -78.417, mean reward: -0.577 [-100.000, 22.572], mean action: 1.691 [0.000, 3.000], mean observation: 0.042 [-0.632, 1.866], loss: 2.424880, mean_absolute_error: 35.711825, mean_q: 20.663004, mean_eps: 0.519387
  534304/2000000: episode: 4634, duration: 3.260s, episode steps: 222, steps per second: 68, episode reward: -214.966, mean reward: -0.968 [-100.000, 37.507], mean action: 1.833 [0.000, 3.000], mean observation: 0.050 [-0.648, 1.664], loss: 1.733962, mean_absolute_error: 35.789117, mean_q: 24.367151, mean_eps: 0.519227
  534419/2000000: episode: 4635, duration: 1.707s, episode steps: 115, steps per second: 67, episode reward: -122.287, mean reward: -1.063 [-100.000, 10.124], mean action: 1.557 [0.000, 3.000], mean observation: -0.041 [-1.111, 4.227], loss: 2.167173, mean_absolute_error: 36.467225, mean_q: 21.931781, mean_eps: 0.519076
  534585/2000000: episode: 4636, duration: 2.409s, episode steps: 166, steps per second: 69, episode reward: -76.887, mean reward: -0.463 [-100.000, 10.912], mean action: 1.783 [0.000, 3.000], mean observation: -0.041 [-0.961, 1.000], loss: 1.915292, mean_absolute_error: 36.824815, mean_q: 24.371075, mean_eps: 0.518948
  534688/2000000: episode: 4637, duration: 1.539s, episode steps: 103, steps per second: 67, episode reward: -76.153, mean reward: -0.739 [-100.000, 29.629], mean action: 1.699 [0.000, 3.000], mean observation: 0.007 [-1.061, 3.098], loss: 2.419292, mean_absolute_error: 36.222157, mean_q: 20.698630, mean_eps: 0.518828
  534778/2000000: episode: 4638, duration: 1.313s, episode steps: 90, steps per second: 69, episode reward: -72.754, mean reward: -0.808 [-100.000, 17.150], mean action: 1.578 [0.000, 3.000], mean observation: -0.035 [-2.630, 1.000], loss: 1.237926, mean_absolute_error: 35.865566, mean_q: 26.304906, mean_eps: 0.518741
  534920/2000000: episode: 4639, duration: 2.092s, episode steps: 142, steps per second: 68, episode reward: -100.665, mean reward: -0.709 [-100.000, 15.528], mean action: 1.725 [0.000, 3.000], mean observation: -0.120 [-1.134, 1.661], loss: 1.359538, mean_absolute_error: 35.129941, mean_q: 24.248447, mean_eps: 0.518637
  535044/2000000: episode: 4640, duration: 1.857s, episode steps: 124, steps per second: 67, episode reward: -116.069, mean reward: -0.936 [-100.000, 7.373], mean action: 1.653 [0.000, 3.000], mean observation: 0.012 [-0.823, 2.881], loss: 2.006041, mean_absolute_error: 36.251926, mean_q: 23.359383, mean_eps: 0.518518
  535371/2000000: episode: 4641, duration: 4.872s, episode steps: 327, steps per second: 67, episode reward: -96.824, mean reward: -0.296 [-100.000, 12.311], mean action: 1.786 [0.000, 3.000], mean observation: -0.014 [-0.980, 1.661], loss: 1.763957, mean_absolute_error: 35.964486, mean_q: 23.848403, mean_eps: 0.518315
  536371/2000000: episode: 4642, duration: 15.499s, episode steps: 1000, steps per second: 65, episode reward: 39.330, mean reward: 0.039 [-22.029, 49.149], mean action: 1.218 [0.000, 3.000], mean observation: 0.085 [-0.791, 1.284], loss: 1.761690, mean_absolute_error: 36.140495, mean_q: 24.301214, mean_eps: 0.517717
  536454/2000000: episode: 4643, duration: 1.250s, episode steps: 83, steps per second: 66, episode reward: -63.511, mean reward: -0.765 [-100.000, 6.639], mean action: 1.699 [0.000, 3.000], mean observation: -0.129 [-0.956, 2.938], loss: 1.880923, mean_absolute_error: 36.227774, mean_q: 25.193879, mean_eps: 0.517229
  536559/2000000: episode: 4644, duration: 1.524s, episode steps: 105, steps per second: 69, episode reward: -119.663, mean reward: -1.140 [-100.000, 18.809], mean action: 1.733 [0.000, 3.000], mean observation: -0.099 [-1.035, 1.000], loss: 1.490674, mean_absolute_error: 35.712769, mean_q: 23.958042, mean_eps: 0.517145
  536748/2000000: episode: 4645, duration: 2.795s, episode steps: 189, steps per second: 68, episode reward: -99.875, mean reward: -0.528 [-100.000, 15.054], mean action: 1.656 [0.000, 3.000], mean observation: 0.104 [-1.484, 1.000], loss: 1.871570, mean_absolute_error: 36.951073, mean_q: 21.545150, mean_eps: 0.517013
  536863/2000000: episode: 4646, duration: 1.736s, episode steps: 115, steps per second: 66, episode reward: -57.742, mean reward: -0.502 [-100.000, 12.696], mean action: 1.791 [0.000, 3.000], mean observation: -0.055 [-0.704, 1.985], loss: 1.753812, mean_absolute_error: 36.538551, mean_q: 24.660202, mean_eps: 0.516876
  537054/2000000: episode: 4647, duration: 2.801s, episode steps: 191, steps per second: 68, episode reward: -49.664, mean reward: -0.260 [-100.000, 17.107], mean action: 1.723 [0.000, 3.000], mean observation: 0.115 [-0.976, 1.000], loss: 1.697413, mean_absolute_error: 36.788300, mean_q: 23.720694, mean_eps: 0.516738
  537141/2000000: episode: 4648, duration: 1.290s, episode steps: 87, steps per second: 67, episode reward: -46.445, mean reward: -0.534 [-100.000, 22.560], mean action: 1.701 [0.000, 3.000], mean observation: 0.015 [-0.949, 2.646], loss: 1.954030, mean_absolute_error: 35.165041, mean_q: 23.817721, mean_eps: 0.516612
  537277/2000000: episode: 4649, duration: 1.970s, episode steps: 136, steps per second: 69, episode reward: -70.387, mean reward: -0.518 [-100.000, 20.427], mean action: 1.713 [0.000, 3.000], mean observation: 0.071 [-0.913, 1.000], loss: 2.481242, mean_absolute_error: 36.185759, mean_q: 24.114552, mean_eps: 0.516511
  537407/2000000: episode: 4650, duration: 1.895s, episode steps: 130, steps per second: 69, episode reward: -86.737, mean reward: -0.667 [-100.000, 6.760], mean action: 1.685 [0.000, 3.000], mean observation: 0.087 [-2.165, 1.000], loss: 1.404353, mean_absolute_error: 36.123707, mean_q: 22.518348, mean_eps: 0.516392
  537493/2000000: episode: 4651, duration: 1.285s, episode steps: 86, steps per second: 67, episode reward: -139.573, mean reward: -1.623 [-100.000, 6.831], mean action: 1.698 [0.000, 3.000], mean observation: 0.038 [-0.973, 1.106], loss: 2.280459, mean_absolute_error: 36.587917, mean_q: 24.508342, mean_eps: 0.516295
  538493/2000000: episode: 4652, duration: 16.261s, episode steps: 1000, steps per second: 61, episode reward: 59.432, mean reward: 0.059 [-22.411, 23.480], mean action: 1.389 [0.000, 3.000], mean observation: 0.153 [-1.053, 1.000], loss: 1.763684, mean_absolute_error: 35.857025, mean_q: 24.351152, mean_eps: 0.515805
  538561/2000000: episode: 4653, duration: 1.014s, episode steps: 68, steps per second: 67, episode reward: -146.371, mean reward: -2.153 [-100.000, 8.641], mean action: 1.574 [0.000, 3.000], mean observation: -0.142 [-4.062, 1.000], loss: 2.387580, mean_absolute_error: 37.409895, mean_q: 26.184674, mean_eps: 0.515325
  538690/2000000: episode: 4654, duration: 1.868s, episode steps: 129, steps per second: 69, episode reward: -30.653, mean reward: -0.238 [-100.000, 18.287], mean action: 1.752 [0.000, 3.000], mean observation: 0.095 [-0.846, 1.000], loss: 1.832219, mean_absolute_error: 36.894069, mean_q: 24.409059, mean_eps: 0.515237
  538878/2000000: episode: 4655, duration: 2.744s, episode steps: 188, steps per second: 69, episode reward: -80.198, mean reward: -0.427 [-100.000, 30.077], mean action: 1.617 [0.000, 3.000], mean observation: -0.089 [-1.768, 1.000], loss: 1.863786, mean_absolute_error: 35.361255, mean_q: 24.678398, mean_eps: 0.515094
  539070/2000000: episode: 4656, duration: 2.816s, episode steps: 192, steps per second: 68, episode reward: -37.866, mean reward: -0.197 [-100.000, 21.244], mean action: 1.729 [0.000, 3.000], mean observation: 0.054 [-1.329, 1.000], loss: 1.664773, mean_absolute_error: 35.279343, mean_q: 25.081131, mean_eps: 0.514923
  539208/2000000: episode: 4657, duration: 2.047s, episode steps: 138, steps per second: 67, episode reward: -109.989, mean reward: -0.797 [-100.000, 9.822], mean action: 1.725 [0.000, 3.000], mean observation: 0.013 [-1.087, 3.477], loss: 1.277752, mean_absolute_error: 34.975153, mean_q: 25.454284, mean_eps: 0.514776
  539482/2000000: episode: 4658, duration: 4.078s, episode steps: 274, steps per second: 67, episode reward: -135.143, mean reward: -0.493 [-100.000, 15.565], mean action: 1.672 [0.000, 3.000], mean observation: 0.137 [-2.544, 1.041], loss: 1.512976, mean_absolute_error: 35.965317, mean_q: 24.673275, mean_eps: 0.514590
  539589/2000000: episode: 4659, duration: 1.626s, episode steps: 107, steps per second: 66, episode reward: -94.549, mean reward: -0.884 [-100.000, 10.465], mean action: 1.682 [0.000, 3.000], mean observation: -0.007 [-0.914, 3.039], loss: 1.511316, mean_absolute_error: 35.167689, mean_q: 23.930614, mean_eps: 0.514418
  540589/2000000: episode: 4660, duration: 15.492s, episode steps: 1000, steps per second: 65, episode reward: -18.131, mean reward: -0.018 [-24.599, 24.010], mean action: 1.307 [0.000, 3.000], mean observation: 0.228 [-0.728, 1.000], loss: 1.711020, mean_absolute_error: 35.231808, mean_q: 23.389670, mean_eps: 0.513919
  541239/2000000: episode: 4661, duration: 10.546s, episode steps: 650, steps per second: 62, episode reward: -210.387, mean reward: -0.324 [-100.000, 68.143], mean action: 1.392 [0.000, 3.000], mean observation: 0.169 [-1.276, 1.000], loss: 1.782321, mean_absolute_error: 35.844952, mean_q: 25.018559, mean_eps: 0.513177
  541326/2000000: episode: 4662, duration: 1.300s, episode steps: 87, steps per second: 67, episode reward: -65.003, mean reward: -0.747 [-100.000, 18.438], mean action: 1.690 [0.000, 3.000], mean observation: 0.032 [-2.695, 1.000], loss: 1.610763, mean_absolute_error: 36.156643, mean_q: 21.264710, mean_eps: 0.512846
  541454/2000000: episode: 4663, duration: 1.855s, episode steps: 128, steps per second: 69, episode reward: -88.673, mean reward: -0.693 [-100.000, 11.752], mean action: 1.688 [0.000, 3.000], mean observation: 0.062 [-0.972, 1.603], loss: 1.640645, mean_absolute_error: 35.619904, mean_q: 23.527637, mean_eps: 0.512749
  542454/2000000: episode: 4664, duration: 15.430s, episode steps: 1000, steps per second: 65, episode reward: 36.434, mean reward: 0.036 [-23.721, 26.509], mean action: 1.173 [0.000, 3.000], mean observation: 0.106 [-0.617, 1.016], loss: 1.648146, mean_absolute_error: 35.234508, mean_q: 23.827905, mean_eps: 0.512241
  542712/2000000: episode: 4665, duration: 3.822s, episode steps: 258, steps per second: 68, episode reward: -61.485, mean reward: -0.238 [-100.000, 12.062], mean action: 1.814 [0.000, 3.000], mean observation: 0.119 [-0.580, 1.613], loss: 1.845054, mean_absolute_error: 35.652991, mean_q: 23.471380, mean_eps: 0.511676
  543712/2000000: episode: 4666, duration: 15.396s, episode steps: 1000, steps per second: 65, episode reward: 17.562, mean reward: 0.018 [-23.879, 23.014], mean action: 1.138 [0.000, 3.000], mean observation: 0.053 [-1.118, 1.000], loss: 1.670558, mean_absolute_error: 35.568611, mean_q: 24.700932, mean_eps: 0.511111
  543791/2000000: episode: 4667, duration: 1.181s, episode steps: 79, steps per second: 67, episode reward: -88.075, mean reward: -1.115 [-100.000, 10.929], mean action: 1.924 [0.000, 3.000], mean observation: -0.149 [-3.023, 1.000], loss: 1.624896, mean_absolute_error: 35.948476, mean_q: 25.549551, mean_eps: 0.510625
  543923/2000000: episode: 4668, duration: 1.935s, episode steps: 132, steps per second: 68, episode reward: -51.224, mean reward: -0.388 [-100.000, 18.579], mean action: 1.720 [0.000, 3.000], mean observation: 0.032 [-1.681, 1.000], loss: 1.639801, mean_absolute_error: 35.575943, mean_q: 24.830392, mean_eps: 0.510530
  544085/2000000: episode: 4669, duration: 2.412s, episode steps: 162, steps per second: 67, episode reward: -90.297, mean reward: -0.557 [-100.000, 11.083], mean action: 1.790 [0.000, 3.000], mean observation: -0.063 [-0.747, 2.794], loss: 1.539306, mean_absolute_error: 35.384356, mean_q: 24.899712, mean_eps: 0.510396
  544282/2000000: episode: 4670, duration: 2.878s, episode steps: 197, steps per second: 68, episode reward: -60.381, mean reward: -0.307 [-100.000, 14.764], mean action: 1.635 [0.000, 3.000], mean observation: 0.029 [-1.384, 1.000], loss: 1.621790, mean_absolute_error: 35.787209, mean_q: 23.452731, mean_eps: 0.510234
  544441/2000000: episode: 4671, duration: 2.338s, episode steps: 159, steps per second: 68, episode reward: -39.542, mean reward: -0.249 [-100.000, 10.698], mean action: 1.786 [0.000, 3.000], mean observation: -0.010 [-0.586, 1.250], loss: 1.613311, mean_absolute_error: 36.070130, mean_q: 27.155007, mean_eps: 0.510074
  544584/2000000: episode: 4672, duration: 2.105s, episode steps: 143, steps per second: 68, episode reward: -23.812, mean reward: -0.167 [-100.000, 13.109], mean action: 1.678 [0.000, 3.000], mean observation: 0.025 [-0.723, 1.000], loss: 1.491505, mean_absolute_error: 35.344584, mean_q: 24.835557, mean_eps: 0.509939
  544774/2000000: episode: 4673, duration: 2.825s, episode steps: 190, steps per second: 67, episode reward: -45.572, mean reward: -0.240 [-100.000, 21.835], mean action: 1.695 [0.000, 3.000], mean observation: 0.010 [-0.716, 1.000], loss: 1.512205, mean_absolute_error: 34.966476, mean_q: 25.283843, mean_eps: 0.509790
  544892/2000000: episode: 4674, duration: 1.771s, episode steps: 118, steps per second: 67, episode reward: -74.687, mean reward: -0.633 [-100.000, 13.635], mean action: 1.754 [0.000, 3.000], mean observation: -0.111 [-1.640, 1.000], loss: 2.282620, mean_absolute_error: 34.802418, mean_q: 25.636115, mean_eps: 0.509651
  545041/2000000: episode: 4675, duration: 2.200s, episode steps: 149, steps per second: 68, episode reward: -66.744, mean reward: -0.448 [-100.000, 9.366], mean action: 1.732 [0.000, 3.000], mean observation: -0.020 [-0.654, 1.166], loss: 1.523993, mean_absolute_error: 35.893756, mean_q: 25.099439, mean_eps: 0.509531
  546041/2000000: episode: 4676, duration: 15.977s, episode steps: 1000, steps per second: 63, episode reward: 22.435, mean reward: 0.022 [-25.133, 22.690], mean action: 1.260 [0.000, 3.000], mean observation: 0.104 [-1.372, 1.000], loss: 1.836011, mean_absolute_error: 35.147192, mean_q: 24.324677, mean_eps: 0.509012
  547041/2000000: episode: 4677, duration: 16.207s, episode steps: 1000, steps per second: 62, episode reward: 33.596, mean reward: 0.034 [-21.703, 22.790], mean action: 1.325 [0.000, 3.000], mean observation: 0.099 [-1.317, 1.000], loss: 1.721650, mean_absolute_error: 35.512090, mean_q: 24.787136, mean_eps: 0.508112
  547263/2000000: episode: 4678, duration: 3.266s, episode steps: 222, steps per second: 68, episode reward: -37.656, mean reward: -0.170 [-100.000, 19.916], mean action: 1.707 [0.000, 3.000], mean observation: 0.083 [-0.566, 1.102], loss: 1.580126, mean_absolute_error: 35.531936, mean_q: 26.371002, mean_eps: 0.507563
  547450/2000000: episode: 4679, duration: 2.715s, episode steps: 187, steps per second: 69, episode reward: -66.361, mean reward: -0.355 [-100.000, 7.782], mean action: 1.561 [0.000, 3.000], mean observation: 0.054 [-1.636, 1.038], loss: 1.891145, mean_absolute_error: 35.322336, mean_q: 24.280705, mean_eps: 0.507380
  548450/2000000: episode: 4680, duration: 16.638s, episode steps: 1000, steps per second: 60, episode reward: 51.632, mean reward: 0.052 [-23.381, 23.462], mean action: 0.999 [0.000, 3.000], mean observation: 0.141 [-1.328, 1.000], loss: 2.036281, mean_absolute_error: 35.527700, mean_q: 25.009075, mean_eps: 0.506845
  548652/2000000: episode: 4681, duration: 3.007s, episode steps: 202, steps per second: 67, episode reward: -42.339, mean reward: -0.210 [-100.000, 12.125], mean action: 1.743 [0.000, 3.000], mean observation: 0.009 [-0.600, 1.000], loss: 1.489580, mean_absolute_error: 35.719753, mean_q: 27.130054, mean_eps: 0.506305
  548785/2000000: episode: 4682, duration: 1.999s, episode steps: 133, steps per second: 67, episode reward: -87.048, mean reward: -0.654 [-100.000, 11.028], mean action: 1.669 [0.000, 3.000], mean observation: 0.110 [-0.658, 1.802], loss: 1.595726, mean_absolute_error: 36.140685, mean_q: 22.541521, mean_eps: 0.506154
  548865/2000000: episode: 4683, duration: 1.163s, episode steps: 80, steps per second: 69, episode reward: -61.180, mean reward: -0.765 [-100.000, 13.682], mean action: 1.738 [0.000, 3.000], mean observation: 0.015 [-3.005, 1.000], loss: 1.233039, mean_absolute_error: 35.400015, mean_q: 24.824109, mean_eps: 0.506057
  549029/2000000: episode: 4684, duration: 2.457s, episode steps: 164, steps per second: 67, episode reward: -34.447, mean reward: -0.210 [-100.000, 20.299], mean action: 1.585 [0.000, 3.000], mean observation: 0.114 [-1.642, 1.000], loss: 1.687895, mean_absolute_error: 35.349024, mean_q: 23.856938, mean_eps: 0.505947
  549165/2000000: episode: 4685, duration: 2.195s, episode steps: 136, steps per second: 62, episode reward: -108.172, mean reward: -0.795 [-100.000, 15.409], mean action: 1.838 [0.000, 3.000], mean observation: -0.063 [-0.755, 1.860], loss: 2.070069, mean_absolute_error: 35.691716, mean_q: 21.977358, mean_eps: 0.505812
  549425/2000000: episode: 4686, duration: 3.863s, episode steps: 260, steps per second: 67, episode reward: -61.205, mean reward: -0.235 [-100.000, 18.188], mean action: 1.723 [0.000, 3.000], mean observation: -0.003 [-0.529, 1.000], loss: 1.581416, mean_absolute_error: 34.748074, mean_q: 26.832955, mean_eps: 0.505634
  550425/2000000: episode: 4687, duration: 16.146s, episode steps: 1000, steps per second: 62, episode reward: -44.799, mean reward: -0.045 [-24.785, 23.219], mean action: 1.159 [0.000, 3.000], mean observation: 0.092 [-0.730, 1.000], loss: 1.768071, mean_absolute_error: 35.326718, mean_q: 24.701968, mean_eps: 0.505067
  550535/2000000: episode: 4688, duration: 1.586s, episode steps: 110, steps per second: 69, episode reward: -29.201, mean reward: -0.265 [-100.000, 24.491], mean action: 1.855 [0.000, 3.000], mean observation: -0.121 [-0.817, 1.507], loss: 1.890964, mean_absolute_error: 33.912947, mean_q: 23.932372, mean_eps: 0.504568
  551535/2000000: episode: 4689, duration: 15.759s, episode steps: 1000, steps per second: 63, episode reward: 8.792, mean reward: 0.009 [-24.170, 23.336], mean action: 1.505 [0.000, 3.000], mean observation: 0.230 [-1.689, 1.000], loss: 1.878258, mean_absolute_error: 35.541365, mean_q: 24.869102, mean_eps: 0.504069
  551741/2000000: episode: 4690, duration: 3.080s, episode steps: 206, steps per second: 67, episode reward: -66.283, mean reward: -0.322 [-100.000, 9.838], mean action: 1.660 [0.000, 3.000], mean observation: 0.073 [-0.900, 2.457], loss: 1.462128, mean_absolute_error: 34.200623, mean_q: 24.960469, mean_eps: 0.503526
  551831/2000000: episode: 4691, duration: 1.295s, episode steps: 90, steps per second: 69, episode reward: -112.511, mean reward: -1.250 [-100.000, 12.303], mean action: 1.611 [0.000, 3.000], mean observation: 0.120 [-1.044, 1.190], loss: 1.550799, mean_absolute_error: 35.783051, mean_q: 21.004983, mean_eps: 0.503393
  552831/2000000: episode: 4692, duration: 15.348s, episode steps: 1000, steps per second: 65, episode reward: 75.678, mean reward: 0.076 [-24.484, 23.151], mean action: 1.239 [0.000, 3.000], mean observation: 0.182 [-0.619, 1.000], loss: 1.793795, mean_absolute_error: 35.248512, mean_q: 24.115830, mean_eps: 0.502903
  552964/2000000: episode: 4693, duration: 2.146s, episode steps: 133, steps per second: 62, episode reward: -64.730, mean reward: -0.487 [-100.000, 17.660], mean action: 1.812 [0.000, 3.000], mean observation: -0.111 [-0.768, 1.327], loss: 1.483684, mean_absolute_error: 35.085152, mean_q: 25.188934, mean_eps: 0.502394
  553043/2000000: episode: 4694, duration: 1.154s, episode steps: 79, steps per second: 68, episode reward: -129.219, mean reward: -1.636 [-100.000, 11.498], mean action: 1.519 [0.000, 3.000], mean observation: 0.160 [-2.843, 1.000], loss: 2.063412, mean_absolute_error: 34.422050, mean_q: 23.825041, mean_eps: 0.502298
  553146/2000000: episode: 4695, duration: 1.530s, episode steps: 103, steps per second: 67, episode reward: -113.596, mean reward: -1.103 [-100.000, 9.753], mean action: 1.709 [0.000, 3.000], mean observation: -0.147 [-2.022, 1.000], loss: 1.511098, mean_absolute_error: 34.515344, mean_q: 26.190341, mean_eps: 0.502215
  553250/2000000: episode: 4696, duration: 1.509s, episode steps: 104, steps per second: 69, episode reward: -86.867, mean reward: -0.835 [-100.000, 21.495], mean action: 1.663 [0.000, 3.000], mean observation: -0.038 [-0.908, 1.000], loss: 1.486137, mean_absolute_error: 34.203843, mean_q: 24.972229, mean_eps: 0.502122
  553407/2000000: episode: 4697, duration: 2.294s, episode steps: 157, steps per second: 68, episode reward: -84.271, mean reward: -0.537 [-100.000, 13.143], mean action: 1.618 [0.000, 3.000], mean observation: 0.042 [-0.939, 2.413], loss: 1.859632, mean_absolute_error: 34.545326, mean_q: 25.400305, mean_eps: 0.502005
  553546/2000000: episode: 4698, duration: 2.023s, episode steps: 139, steps per second: 69, episode reward: -158.961, mean reward: -1.144 [-100.000, 15.510], mean action: 1.705 [0.000, 3.000], mean observation: 0.156 [-3.563, 1.000], loss: 1.988576, mean_absolute_error: 35.092765, mean_q: 25.457288, mean_eps: 0.501872
  553720/2000000: episode: 4699, duration: 2.594s, episode steps: 174, steps per second: 67, episode reward: -72.689, mean reward: -0.418 [-100.000, 15.933], mean action: 1.626 [0.000, 3.000], mean observation: -0.044 [-0.942, 1.227], loss: 1.687258, mean_absolute_error: 34.220535, mean_q: 23.897982, mean_eps: 0.501731
  553875/2000000: episode: 4700, duration: 2.267s, episode steps: 155, steps per second: 68, episode reward: -119.183, mean reward: -0.769 [-100.000, 12.066], mean action: 1.600 [0.000, 3.000], mean observation: -0.119 [-0.974, 1.771], loss: 1.901301, mean_absolute_error: 35.552253, mean_q: 24.172053, mean_eps: 0.501584
  554078/2000000: episode: 4701, duration: 3.001s, episode steps: 203, steps per second: 68, episode reward: -85.831, mean reward: -0.423 [-100.000, 11.048], mean action: 1.754 [0.000, 3.000], mean observation: 0.096 [-0.647, 1.000], loss: 2.089748, mean_absolute_error: 35.048384, mean_q: 27.251669, mean_eps: 0.501422
  554260/2000000: episode: 4702, duration: 2.707s, episode steps: 182, steps per second: 67, episode reward: -66.681, mean reward: -0.366 [-100.000, 22.139], mean action: 1.709 [0.000, 3.000], mean observation: 0.028 [-0.837, 2.527], loss: 1.380812, mean_absolute_error: 34.770843, mean_q: 23.787052, mean_eps: 0.501249
  554355/2000000: episode: 4703, duration: 1.405s, episode steps: 95, steps per second: 68, episode reward: -96.591, mean reward: -1.017 [-100.000, 9.521], mean action: 1.800 [0.000, 3.000], mean observation: -0.097 [-1.073, 3.562], loss: 1.551878, mean_absolute_error: 34.857893, mean_q: 25.772496, mean_eps: 0.501125
  554468/2000000: episode: 4704, duration: 1.687s, episode steps: 113, steps per second: 67, episode reward: -63.778, mean reward: -0.564 [-100.000, 8.830], mean action: 1.717 [0.000, 3.000], mean observation: 0.074 [-0.739, 1.000], loss: 1.599869, mean_absolute_error: 34.609474, mean_q: 22.778304, mean_eps: 0.501031
  554578/2000000: episode: 4705, duration: 1.635s, episode steps: 110, steps per second: 67, episode reward: -71.380, mean reward: -0.649 [-100.000, 8.383], mean action: 1.709 [0.000, 3.000], mean observation: 0.046 [-0.679, 3.035], loss: 1.930751, mean_absolute_error: 36.496668, mean_q: 22.775968, mean_eps: 0.500930
  554830/2000000: episode: 4706, duration: 3.704s, episode steps: 252, steps per second: 68, episode reward: -112.993, mean reward: -0.448 [-100.000, 9.021], mean action: 1.754 [0.000, 3.000], mean observation: -0.015 [-0.555, 1.011], loss: 1.642037, mean_absolute_error: 34.937537, mean_q: 24.582076, mean_eps: 0.500766
  555012/2000000: episode: 4707, duration: 2.719s, episode steps: 182, steps per second: 67, episode reward: -220.379, mean reward: -1.211 [-100.000, 42.475], mean action: 1.725 [0.000, 3.000], mean observation: -0.139 [-1.743, 1.000], loss: 2.439016, mean_absolute_error: 35.853370, mean_q: 23.922858, mean_eps: 0.500572
  555202/2000000: episode: 4708, duration: 2.804s, episode steps: 190, steps per second: 68, episode reward: -70.479, mean reward: -0.371 [-100.000, 13.933], mean action: 1.747 [0.000, 3.000], mean observation: 0.037 [-1.396, 1.000], loss: 1.729059, mean_absolute_error: 35.050639, mean_q: 24.861307, mean_eps: 0.500405
  555415/2000000: episode: 4709, duration: 3.109s, episode steps: 213, steps per second: 69, episode reward: 29.157, mean reward: 0.137 [-100.000, 31.031], mean action: 1.822 [0.000, 3.000], mean observation: 0.061 [-0.771, 1.511], loss: 2.130062, mean_absolute_error: 34.763465, mean_q: 25.215330, mean_eps: 0.500223
  555591/2000000: episode: 4710, duration: 2.557s, episode steps: 176, steps per second: 69, episode reward: -128.129, mean reward: -0.728 [-100.000, 25.780], mean action: 1.688 [0.000, 3.000], mean observation: -0.083 [-1.367, 1.000], loss: 1.688028, mean_absolute_error: 34.728629, mean_q: 25.086106, mean_eps: 0.500048
  555714/2000000: episode: 4711, duration: 1.825s, episode steps: 123, steps per second: 67, episode reward: -18.391, mean reward: -0.150 [-100.000, 21.167], mean action: 1.854 [0.000, 3.000], mean observation: -0.084 [-0.677, 1.000], loss: 1.753145, mean_absolute_error: 35.821766, mean_q: 22.826230, mean_eps: 0.499913
  556714/2000000: episode: 4712, duration: 15.798s, episode steps: 1000, steps per second: 63, episode reward: 59.083, mean reward: 0.059 [-24.204, 22.848], mean action: 1.234 [0.000, 3.000], mean observation: 0.132 [-1.227, 1.000], loss: 1.748407, mean_absolute_error: 35.130782, mean_q: 24.243748, mean_eps: 0.499407
  557714/2000000: episode: 4713, duration: 15.254s, episode steps: 1000, steps per second: 66, episode reward: 32.150, mean reward: 0.032 [-22.667, 24.668], mean action: 1.015 [0.000, 3.000], mean observation: 0.088 [-1.076, 1.000], loss: 1.667073, mean_absolute_error: 34.517782, mean_q: 26.304193, mean_eps: 0.498507
  558714/2000000: episode: 4714, duration: 16.213s, episode steps: 1000, steps per second: 62, episode reward: 41.914, mean reward: 0.042 [-24.311, 24.767], mean action: 1.394 [0.000, 3.000], mean observation: 0.119 [-1.498, 1.000], loss: 1.687603, mean_absolute_error: 35.341281, mean_q: 24.875686, mean_eps: 0.497607
  559714/2000000: episode: 4715, duration: 15.531s, episode steps: 1000, steps per second: 64, episode reward: 43.643, mean reward: 0.044 [-21.793, 24.719], mean action: 1.554 [0.000, 3.000], mean observation: 0.247 [-1.258, 1.000], loss: 1.714828, mean_absolute_error: 35.341017, mean_q: 24.753637, mean_eps: 0.496707
  559858/2000000: episode: 4716, duration: 2.123s, episode steps: 144, steps per second: 68, episode reward: -62.462, mean reward: -0.434 [-100.000, 20.670], mean action: 1.812 [0.000, 3.000], mean observation: -0.052 [-0.796, 2.281], loss: 1.848524, mean_absolute_error: 34.655073, mean_q: 24.721045, mean_eps: 0.496193
  559982/2000000: episode: 4717, duration: 1.828s, episode steps: 124, steps per second: 68, episode reward: -84.017, mean reward: -0.678 [-100.000, 15.359], mean action: 1.734 [0.000, 3.000], mean observation: -0.144 [-0.824, 1.000], loss: 1.603047, mean_absolute_error: 34.824834, mean_q: 27.404457, mean_eps: 0.496072
  560217/2000000: episode: 4718, duration: 3.484s, episode steps: 235, steps per second: 67, episode reward: -285.772, mean reward: -1.216 [-100.000, 5.388], mean action: 1.796 [0.000, 3.000], mean observation: -0.057 [-1.379, 0.958], loss: 1.741063, mean_absolute_error: 34.640178, mean_q: 24.205949, mean_eps: 0.495910
  560295/2000000: episode: 4719, duration: 1.119s, episode steps: 78, steps per second: 70, episode reward: -65.933, mean reward: -0.845 [-100.000, 16.378], mean action: 1.821 [0.000, 3.000], mean observation: -0.055 [-1.018, 3.022], loss: 1.846313, mean_absolute_error: 36.016241, mean_q: 27.213504, mean_eps: 0.495770
  560421/2000000: episode: 4720, duration: 1.851s, episode steps: 126, steps per second: 68, episode reward: -62.164, mean reward: -0.493 [-100.000, 12.966], mean action: 1.667 [0.000, 3.000], mean observation: 0.097 [-0.651, 1.000], loss: 1.747440, mean_absolute_error: 33.501124, mean_q: 23.975003, mean_eps: 0.495678
  560595/2000000: episode: 4721, duration: 2.520s, episode steps: 174, steps per second: 69, episode reward: -18.456, mean reward: -0.106 [-100.000, 15.530], mean action: 1.603 [0.000, 3.000], mean observation: -0.027 [-1.192, 1.028], loss: 2.213855, mean_absolute_error: 35.725009, mean_q: 22.712526, mean_eps: 0.495543
  561595/2000000: episode: 4722, duration: 15.632s, episode steps: 1000, steps per second: 64, episode reward: -20.597, mean reward: -0.021 [-23.671, 36.533], mean action: 1.427 [0.000, 3.000], mean observation: 0.209 [-1.447, 1.086], loss: 1.697451, mean_absolute_error: 34.350232, mean_q: 24.369258, mean_eps: 0.495015
  561721/2000000: episode: 4723, duration: 1.869s, episode steps: 126, steps per second: 67, episode reward: -206.882, mean reward: -1.642 [-100.000, 46.779], mean action: 1.841 [0.000, 3.000], mean observation: 0.181 [-0.795, 1.925], loss: 1.371273, mean_absolute_error: 35.389683, mean_q: 25.801154, mean_eps: 0.494508
  562721/2000000: episode: 4724, duration: 15.739s, episode steps: 1000, steps per second: 64, episode reward: 75.853, mean reward: 0.076 [-21.648, 22.457], mean action: 1.296 [0.000, 3.000], mean observation: 0.172 [-0.633, 1.000], loss: 1.528400, mean_absolute_error: 34.783283, mean_q: 25.557677, mean_eps: 0.494000
  562827/2000000: episode: 4725, duration: 1.530s, episode steps: 106, steps per second: 69, episode reward: -8.612, mean reward: -0.081 [-100.000, 18.722], mean action: 1.774 [0.000, 3.000], mean observation: 0.035 [-0.885, 1.177], loss: 2.047166, mean_absolute_error: 34.622557, mean_q: 22.430109, mean_eps: 0.493503
  563827/2000000: episode: 4726, duration: 15.659s, episode steps: 1000, steps per second: 64, episode reward: 91.327, mean reward: 0.091 [-24.457, 23.235], mean action: 1.113 [0.000, 3.000], mean observation: 0.135 [-0.552, 1.000], loss: 1.673870, mean_absolute_error: 34.683842, mean_q: 25.394984, mean_eps: 0.493007
  564827/2000000: episode: 4727, duration: 15.153s, episode steps: 1000, steps per second: 66, episode reward: 11.452, mean reward: 0.011 [-24.829, 23.638], mean action: 1.201 [0.000, 3.000], mean observation: 0.071 [-1.211, 1.000], loss: 1.680617, mean_absolute_error: 34.437045, mean_q: 24.036887, mean_eps: 0.492107
  564996/2000000: episode: 4728, duration: 2.517s, episode steps: 169, steps per second: 67, episode reward: -87.030, mean reward: -0.515 [-100.000, 11.719], mean action: 1.716 [0.000, 3.000], mean observation: -0.082 [-0.716, 1.342], loss: 1.780393, mean_absolute_error: 34.697377, mean_q: 26.232128, mean_eps: 0.491581
  565101/2000000: episode: 4729, duration: 1.586s, episode steps: 105, steps per second: 66, episode reward: -85.716, mean reward: -0.816 [-100.000, 16.833], mean action: 1.790 [0.000, 3.000], mean observation: 0.048 [-0.866, 1.697], loss: 1.610890, mean_absolute_error: 34.979781, mean_q: 22.703708, mean_eps: 0.491457
  565354/2000000: episode: 4730, duration: 3.714s, episode steps: 253, steps per second: 68, episode reward: -43.183, mean reward: -0.171 [-100.000, 12.490], mean action: 1.660 [0.000, 3.000], mean observation: 0.057 [-0.447, 1.000], loss: 1.788105, mean_absolute_error: 35.563890, mean_q: 25.553993, mean_eps: 0.491295
  565445/2000000: episode: 4731, duration: 1.363s, episode steps: 91, steps per second: 67, episode reward: -120.339, mean reward: -1.322 [-100.000, 6.253], mean action: 1.495 [0.000, 3.000], mean observation: 0.152 [-2.832, 1.000], loss: 2.024852, mean_absolute_error: 33.852679, mean_q: 26.008361, mean_eps: 0.491140
  565612/2000000: episode: 4732, duration: 2.447s, episode steps: 167, steps per second: 68, episode reward: -71.446, mean reward: -0.428 [-100.000, 15.679], mean action: 1.731 [0.000, 3.000], mean observation: -0.006 [-0.996, 1.000], loss: 1.462432, mean_absolute_error: 34.435812, mean_q: 26.650163, mean_eps: 0.491025
  565713/2000000: episode: 4733, duration: 1.529s, episode steps: 101, steps per second: 66, episode reward: -135.526, mean reward: -1.342 [-100.000, 7.047], mean action: 1.653 [0.000, 3.000], mean observation: 0.061 [-1.134, 1.000], loss: 2.677830, mean_absolute_error: 34.609291, mean_q: 28.308640, mean_eps: 0.490904
  565840/2000000: episode: 4734, duration: 1.848s, episode steps: 127, steps per second: 69, episode reward: -97.376, mean reward: -0.767 [-100.000, 11.215], mean action: 1.512 [0.000, 3.000], mean observation: -0.063 [-0.793, 2.640], loss: 1.686350, mean_absolute_error: 34.894073, mean_q: 26.029519, mean_eps: 0.490802
  566840/2000000: episode: 4735, duration: 16.269s, episode steps: 1000, steps per second: 61, episode reward: 38.212, mean reward: 0.038 [-20.580, 24.279], mean action: 1.033 [0.000, 3.000], mean observation: 0.109 [-0.999, 1.023], loss: 1.959468, mean_absolute_error: 34.791007, mean_q: 24.755407, mean_eps: 0.490296
  567840/2000000: episode: 4736, duration: 16.100s, episode steps: 1000, steps per second: 62, episode reward: 45.002, mean reward: 0.045 [-20.018, 23.213], mean action: 1.124 [0.000, 3.000], mean observation: 0.155 [-0.438, 1.000], loss: 1.651146, mean_absolute_error: 34.376996, mean_q: 24.394608, mean_eps: 0.489396
  567944/2000000: episode: 4737, duration: 1.607s, episode steps: 104, steps per second: 65, episode reward: -20.446, mean reward: -0.197 [-100.000, 16.627], mean action: 1.750 [0.000, 3.000], mean observation: -0.091 [-0.769, 1.000], loss: 1.541877, mean_absolute_error: 36.149643, mean_q: 26.230488, mean_eps: 0.488899
  568052/2000000: episode: 4738, duration: 1.620s, episode steps: 108, steps per second: 67, episode reward: -38.136, mean reward: -0.353 [-100.000, 18.392], mean action: 1.769 [0.000, 3.000], mean observation: -0.011 [-0.756, 1.000], loss: 1.705198, mean_absolute_error: 34.865469, mean_q: 24.448663, mean_eps: 0.488804
  568148/2000000: episode: 4739, duration: 1.474s, episode steps: 96, steps per second: 65, episode reward: -127.281, mean reward: -1.326 [-100.000, 17.152], mean action: 1.688 [0.000, 3.000], mean observation: 0.072 [-3.800, 1.000], loss: 1.741036, mean_absolute_error: 32.282377, mean_q: 26.242626, mean_eps: 0.488712
  568296/2000000: episode: 4740, duration: 2.234s, episode steps: 148, steps per second: 66, episode reward: -57.597, mean reward: -0.389 [-100.000, 10.940], mean action: 1.676 [0.000, 3.000], mean observation: 0.057 [-0.793, 1.626], loss: 1.773500, mean_absolute_error: 34.919062, mean_q: 24.476860, mean_eps: 0.488602
  568562/2000000: episode: 4741, duration: 4.181s, episode steps: 266, steps per second: 64, episode reward: -183.107, mean reward: -0.688 [-100.000, 16.586], mean action: 1.564 [0.000, 3.000], mean observation: 0.176 [-1.233, 1.005], loss: 2.198640, mean_absolute_error: 35.632410, mean_q: 25.404848, mean_eps: 0.488415
  568681/2000000: episode: 4742, duration: 1.781s, episode steps: 119, steps per second: 67, episode reward: -60.496, mean reward: -0.508 [-100.000, 20.684], mean action: 1.874 [0.000, 3.000], mean observation: -0.069 [-0.759, 2.152], loss: 1.723388, mean_absolute_error: 34.295377, mean_q: 22.856006, mean_eps: 0.488240
  568747/2000000: episode: 4743, duration: 0.936s, episode steps: 66, steps per second: 71, episode reward: -122.587, mean reward: -1.857 [-100.000, 10.196], mean action: 1.818 [0.000, 3.000], mean observation: 0.075 [-1.254, 4.156], loss: 2.273576, mean_absolute_error: 34.347615, mean_q: 23.908363, mean_eps: 0.488157
  568851/2000000: episode: 4744, duration: 1.510s, episode steps: 104, steps per second: 69, episode reward: -100.214, mean reward: -0.964 [-100.000, 18.988], mean action: 1.663 [0.000, 3.000], mean observation: -0.053 [-0.904, 3.452], loss: 2.112559, mean_absolute_error: 34.364466, mean_q: 26.410022, mean_eps: 0.488082
  569018/2000000: episode: 4745, duration: 2.481s, episode steps: 167, steps per second: 67, episode reward: -58.442, mean reward: -0.350 [-100.000, 16.024], mean action: 1.802 [0.000, 3.000], mean observation: -0.051 [-0.691, 1.502], loss: 1.856193, mean_absolute_error: 33.841884, mean_q: 24.929855, mean_eps: 0.487959
  569141/2000000: episode: 4746, duration: 1.814s, episode steps: 123, steps per second: 68, episode reward: -162.133, mean reward: -1.318 [-100.000, 10.080], mean action: 1.699 [0.000, 3.000], mean observation: -0.128 [-1.040, 1.000], loss: 1.786280, mean_absolute_error: 33.749871, mean_q: 25.193934, mean_eps: 0.487828
  569277/2000000: episode: 4747, duration: 1.978s, episode steps: 136, steps per second: 69, episode reward: -92.845, mean reward: -0.683 [-100.000, 7.164], mean action: 1.588 [0.000, 3.000], mean observation: 0.097 [-1.288, 1.000], loss: 2.383018, mean_absolute_error: 34.925547, mean_q: 25.038717, mean_eps: 0.487711
  569415/2000000: episode: 4748, duration: 2.002s, episode steps: 138, steps per second: 69, episode reward: -96.740, mean reward: -0.701 [-100.000, 13.506], mean action: 1.717 [0.000, 3.000], mean observation: -0.103 [-0.844, 2.605], loss: 1.905488, mean_absolute_error: 35.074613, mean_q: 25.459618, mean_eps: 0.487589
  570415/2000000: episode: 4749, duration: 15.598s, episode steps: 1000, steps per second: 64, episode reward: 72.535, mean reward: 0.073 [-21.847, 33.457], mean action: 1.018 [0.000, 3.000], mean observation: 0.150 [-1.214, 1.000], loss: 1.748042, mean_absolute_error: 34.562144, mean_q: 25.539881, mean_eps: 0.487077
  570694/2000000: episode: 4750, duration: 4.146s, episode steps: 279, steps per second: 67, episode reward: -103.622, mean reward: -0.371 [-100.000, 12.473], mean action: 1.767 [0.000, 3.000], mean observation: 0.005 [-1.861, 1.000], loss: 1.925409, mean_absolute_error: 34.709050, mean_q: 25.214116, mean_eps: 0.486501
  570796/2000000: episode: 4751, duration: 1.509s, episode steps: 102, steps per second: 68, episode reward: -63.726, mean reward: -0.625 [-100.000, 16.742], mean action: 1.882 [0.000, 3.000], mean observation: -0.161 [-0.906, 1.297], loss: 1.566901, mean_absolute_error: 34.209234, mean_q: 24.658762, mean_eps: 0.486330
  570891/2000000: episode: 4752, duration: 1.389s, episode steps: 95, steps per second: 68, episode reward: -166.251, mean reward: -1.750 [-100.000, 12.080], mean action: 1.505 [0.000, 3.000], mean observation: -0.131 [-1.265, 4.247], loss: 1.852121, mean_absolute_error: 34.512498, mean_q: 24.579202, mean_eps: 0.486242
  571080/2000000: episode: 4753, duration: 2.792s, episode steps: 189, steps per second: 68, episode reward: -33.589, mean reward: -0.178 [-100.000, 13.255], mean action: 1.688 [0.000, 3.000], mean observation: 0.041 [-1.273, 1.011], loss: 2.122222, mean_absolute_error: 35.134844, mean_q: 25.378597, mean_eps: 0.486114
  572080/2000000: episode: 4754, duration: 15.599s, episode steps: 1000, steps per second: 64, episode reward: 42.333, mean reward: 0.042 [-22.895, 23.996], mean action: 1.121 [0.000, 3.000], mean observation: 0.045 [-0.852, 1.023], loss: 1.818264, mean_absolute_error: 34.768341, mean_q: 25.946333, mean_eps: 0.485580
  572194/2000000: episode: 4755, duration: 1.686s, episode steps: 114, steps per second: 68, episode reward: -56.145, mean reward: -0.493 [-100.000, 9.322], mean action: 1.719 [0.000, 3.000], mean observation: -0.159 [-0.805, 1.752], loss: 1.590349, mean_absolute_error: 33.846902, mean_q: 24.468044, mean_eps: 0.485078
  572322/2000000: episode: 4756, duration: 1.864s, episode steps: 128, steps per second: 69, episode reward: -96.259, mean reward: -0.752 [-100.000, 7.274], mean action: 1.812 [0.000, 3.000], mean observation: 0.006 [-1.905, 1.000], loss: 1.771706, mean_absolute_error: 34.425183, mean_q: 25.784752, mean_eps: 0.484968
  573322/2000000: episode: 4757, duration: 15.468s, episode steps: 1000, steps per second: 65, episode reward: 70.564, mean reward: 0.071 [-24.444, 23.136], mean action: 0.958 [0.000, 3.000], mean observation: 0.145 [-0.988, 1.000], loss: 1.615903, mean_absolute_error: 34.543716, mean_q: 26.285955, mean_eps: 0.484460
  573474/2000000: episode: 4758, duration: 2.233s, episode steps: 152, steps per second: 68, episode reward: -47.810, mean reward: -0.315 [-100.000, 12.157], mean action: 1.757 [0.000, 3.000], mean observation: 0.085 [-0.824, 1.513], loss: 1.608452, mean_absolute_error: 33.559951, mean_q: 25.845424, mean_eps: 0.483942
  573559/2000000: episode: 4759, duration: 1.252s, episode steps: 85, steps per second: 68, episode reward: -102.261, mean reward: -1.203 [-100.000, 8.684], mean action: 1.588 [0.000, 3.000], mean observation: 0.017 [-0.909, 3.089], loss: 2.080413, mean_absolute_error: 33.283075, mean_q: 21.603210, mean_eps: 0.483836
  573650/2000000: episode: 4760, duration: 1.342s, episode steps: 91, steps per second: 68, episode reward: -105.776, mean reward: -1.162 [-100.000, 7.892], mean action: 1.791 [0.000, 3.000], mean observation: -0.083 [-2.532, 1.000], loss: 1.515726, mean_absolute_error: 33.543578, mean_q: 26.252469, mean_eps: 0.483756
  574650/2000000: episode: 4761, duration: 15.562s, episode steps: 1000, steps per second: 64, episode reward: 67.539, mean reward: 0.068 [-23.399, 22.714], mean action: 1.285 [0.000, 3.000], mean observation: 0.158 [-0.453, 1.000], loss: 1.802121, mean_absolute_error: 34.236962, mean_q: 24.630644, mean_eps: 0.483265
  574774/2000000: episode: 4762, duration: 1.814s, episode steps: 124, steps per second: 68, episode reward: -104.828, mean reward: -0.845 [-100.000, 10.860], mean action: 1.766 [0.000, 3.000], mean observation: -0.139 [-0.806, 2.640], loss: 1.839655, mean_absolute_error: 34.953628, mean_q: 23.951314, mean_eps: 0.482759
  574898/2000000: episode: 4763, duration: 1.826s, episode steps: 124, steps per second: 68, episode reward: -15.136, mean reward: -0.122 [-100.000, 13.763], mean action: 1.750 [0.000, 3.000], mean observation: 0.120 [-0.759, 1.000], loss: 1.784351, mean_absolute_error: 34.385537, mean_q: 24.412040, mean_eps: 0.482648
  575898/2000000: episode: 4764, duration: 16.200s, episode steps: 1000, steps per second: 62, episode reward: 48.079, mean reward: 0.048 [-22.559, 23.319], mean action: 1.134 [0.000, 3.000], mean observation: 0.177 [-0.632, 1.000], loss: 1.785748, mean_absolute_error: 34.453556, mean_q: 25.811506, mean_eps: 0.482142
  576016/2000000: episode: 4765, duration: 1.739s, episode steps: 118, steps per second: 68, episode reward: -36.115, mean reward: -0.306 [-100.000, 11.016], mean action: 1.653 [0.000, 3.000], mean observation: -0.025 [-1.471, 1.000], loss: 1.319616, mean_absolute_error: 33.114565, mean_q: 25.430125, mean_eps: 0.481640
  576135/2000000: episode: 4766, duration: 1.755s, episode steps: 119, steps per second: 68, episode reward: -116.733, mean reward: -0.981 [-100.000, 9.476], mean action: 1.748 [0.000, 3.000], mean observation: -0.040 [-0.926, 3.495], loss: 1.295378, mean_absolute_error: 33.412039, mean_q: 24.484366, mean_eps: 0.481533
  576230/2000000: episode: 4767, duration: 1.401s, episode steps: 95, steps per second: 68, episode reward: -92.741, mean reward: -0.976 [-100.000, 11.174], mean action: 1.758 [0.000, 3.000], mean observation: -0.050 [-0.907, 2.822], loss: 1.727966, mean_absolute_error: 33.756969, mean_q: 27.222609, mean_eps: 0.481436
  577230/2000000: episode: 4768, duration: 15.581s, episode steps: 1000, steps per second: 64, episode reward: -26.053, mean reward: -0.026 [-21.240, 21.179], mean action: 1.302 [0.000, 3.000], mean observation: 0.226 [-0.643, 1.000], loss: 1.744163, mean_absolute_error: 34.586540, mean_q: 26.550325, mean_eps: 0.480943
  577300/2000000: episode: 4769, duration: 1.049s, episode steps: 70, steps per second: 67, episode reward: -49.307, mean reward: -0.704 [-100.000, 8.333], mean action: 1.500 [0.000, 3.000], mean observation: 0.068 [-0.953, 3.259], loss: 1.756935, mean_absolute_error: 34.377734, mean_q: 26.429271, mean_eps: 0.480462
  577546/2000000: episode: 4770, duration: 3.633s, episode steps: 246, steps per second: 68, episode reward: -39.571, mean reward: -0.161 [-100.000, 16.902], mean action: 1.837 [0.000, 3.000], mean observation: 0.002 [-1.208, 1.070], loss: 2.049685, mean_absolute_error: 33.986337, mean_q: 25.586067, mean_eps: 0.480320
  577649/2000000: episode: 4771, duration: 1.522s, episode steps: 103, steps per second: 68, episode reward: -100.971, mean reward: -0.980 [-100.000, 25.548], mean action: 1.816 [0.000, 3.000], mean observation: -0.025 [-0.933, 3.572], loss: 1.722481, mean_absolute_error: 33.593760, mean_q: 24.364183, mean_eps: 0.480162
  577813/2000000: episode: 4772, duration: 2.391s, episode steps: 164, steps per second: 69, episode reward: -54.978, mean reward: -0.335 [-100.000, 18.790], mean action: 1.671 [0.000, 3.000], mean observation: 0.006 [-2.206, 1.000], loss: 1.830878, mean_absolute_error: 34.395796, mean_q: 24.438632, mean_eps: 0.480041
  577940/2000000: episode: 4773, duration: 1.850s, episode steps: 127, steps per second: 69, episode reward: -109.972, mean reward: -0.866 [-100.000, 9.505], mean action: 1.669 [0.000, 3.000], mean observation: -0.098 [-0.959, 3.006], loss: 1.691934, mean_absolute_error: 33.966277, mean_q: 27.142142, mean_eps: 0.479912
  578110/2000000: episode: 4774, duration: 2.522s, episode steps: 170, steps per second: 67, episode reward: -47.059, mean reward: -0.277 [-100.000, 13.703], mean action: 1.682 [0.000, 3.000], mean observation: 0.018 [-1.144, 1.000], loss: 1.697442, mean_absolute_error: 33.667588, mean_q: 25.747126, mean_eps: 0.479778
  578243/2000000: episode: 4775, duration: 1.930s, episode steps: 133, steps per second: 69, episode reward: -83.206, mean reward: -0.626 [-100.000, 13.793], mean action: 1.654 [0.000, 3.000], mean observation: -0.034 [-0.893, 2.817], loss: 2.524207, mean_absolute_error: 35.218565, mean_q: 23.402242, mean_eps: 0.479642
  578342/2000000: episode: 4776, duration: 1.453s, episode steps: 99, steps per second: 68, episode reward: -134.561, mean reward: -1.359 [-100.000, 7.781], mean action: 1.818 [0.000, 3.000], mean observation: -0.149 [-0.916, 2.723], loss: 1.315197, mean_absolute_error: 33.565079, mean_q: 26.505525, mean_eps: 0.479537
  578470/2000000: episode: 4777, duration: 1.862s, episode steps: 128, steps per second: 69, episode reward: -50.275, mean reward: -0.393 [-100.000, 16.737], mean action: 1.625 [0.000, 3.000], mean observation: 0.095 [-2.173, 1.000], loss: 2.030349, mean_absolute_error: 33.786062, mean_q: 25.748009, mean_eps: 0.479435
  579470/2000000: episode: 4778, duration: 16.545s, episode steps: 1000, steps per second: 60, episode reward: 35.047, mean reward: 0.035 [-23.900, 23.616], mean action: 1.117 [0.000, 3.000], mean observation: 0.157 [-0.402, 1.000], loss: 1.651532, mean_absolute_error: 33.902653, mean_q: 26.325759, mean_eps: 0.478927
  579624/2000000: episode: 4779, duration: 2.318s, episode steps: 154, steps per second: 66, episode reward: -40.147, mean reward: -0.261 [-100.000, 33.175], mean action: 1.818 [0.000, 3.000], mean observation: -0.111 [-0.870, 1.215], loss: 1.671860, mean_absolute_error: 34.391248, mean_q: 24.869868, mean_eps: 0.478409
  579719/2000000: episode: 4780, duration: 1.387s, episode steps: 95, steps per second: 69, episode reward: -90.726, mean reward: -0.955 [-100.000, 15.171], mean action: 1.853 [0.000, 3.000], mean observation: -0.122 [-0.809, 1.444], loss: 1.603597, mean_absolute_error: 33.813569, mean_q: 27.594641, mean_eps: 0.478297
  579881/2000000: episode: 4781, duration: 2.409s, episode steps: 162, steps per second: 67, episode reward: -184.476, mean reward: -1.139 [-100.000, 68.689], mean action: 1.519 [0.000, 3.000], mean observation: -0.161 [-1.844, 1.000], loss: 1.640264, mean_absolute_error: 34.784758, mean_q: 26.065459, mean_eps: 0.478180
  580037/2000000: episode: 4782, duration: 2.274s, episode steps: 156, steps per second: 69, episode reward: -32.979, mean reward: -0.211 [-100.000, 8.266], mean action: 1.641 [0.000, 3.000], mean observation: 0.102 [-1.408, 1.017], loss: 1.758098, mean_absolute_error: 33.183707, mean_q: 25.087849, mean_eps: 0.478036
  580164/2000000: episode: 4783, duration: 1.855s, episode steps: 127, steps per second: 68, episode reward: -133.088, mean reward: -1.048 [-100.000, 8.994], mean action: 1.591 [0.000, 3.000], mean observation: 0.002 [-1.096, 3.845], loss: 1.804373, mean_absolute_error: 34.767921, mean_q: 26.733826, mean_eps: 0.477910
  581164/2000000: episode: 4784, duration: 15.479s, episode steps: 1000, steps per second: 65, episode reward: -8.499, mean reward: -0.008 [-22.677, 23.546], mean action: 1.100 [0.000, 3.000], mean observation: 0.062 [-0.681, 1.000], loss: 1.907664, mean_absolute_error: 34.675620, mean_q: 26.732347, mean_eps: 0.477404
  582164/2000000: episode: 4785, duration: 15.820s, episode steps: 1000, steps per second: 63, episode reward: 52.268, mean reward: 0.052 [-21.682, 22.711], mean action: 1.220 [0.000, 3.000], mean observation: 0.138 [-0.572, 1.000], loss: 1.847944, mean_absolute_error: 34.314510, mean_q: 27.111195, mean_eps: 0.476504
  582323/2000000: episode: 4786, duration: 2.363s, episode steps: 159, steps per second: 67, episode reward: -86.832, mean reward: -0.546 [-100.000, 17.246], mean action: 1.642 [0.000, 3.000], mean observation: -0.069 [-0.725, 2.269], loss: 2.021856, mean_absolute_error: 34.219930, mean_q: 25.250784, mean_eps: 0.475982
  582426/2000000: episode: 4787, duration: 1.504s, episode steps: 103, steps per second: 68, episode reward: -50.860, mean reward: -0.494 [-100.000, 9.386], mean action: 1.806 [0.000, 3.000], mean observation: 0.097 [-0.779, 1.884], loss: 1.620312, mean_absolute_error: 33.702473, mean_q: 24.320410, mean_eps: 0.475863
  582575/2000000: episode: 4788, duration: 2.157s, episode steps: 149, steps per second: 69, episode reward: -71.792, mean reward: -0.482 [-100.000, 10.632], mean action: 1.705 [0.000, 3.000], mean observation: -0.034 [-1.454, 1.000], loss: 1.609292, mean_absolute_error: 34.048007, mean_q: 26.995414, mean_eps: 0.475750
  582697/2000000: episode: 4789, duration: 1.838s, episode steps: 122, steps per second: 66, episode reward: -71.309, mean reward: -0.584 [-100.000, 20.785], mean action: 1.746 [0.000, 3.000], mean observation: 0.007 [-0.760, 1.654], loss: 1.825550, mean_absolute_error: 33.953776, mean_q: 24.725697, mean_eps: 0.475628
  582802/2000000: episode: 4790, duration: 1.530s, episode steps: 105, steps per second: 69, episode reward: -101.908, mean reward: -0.971 [-100.000, 18.443], mean action: 1.762 [0.000, 3.000], mean observation: -0.018 [-0.836, 2.848], loss: 1.925331, mean_absolute_error: 35.102941, mean_q: 27.708054, mean_eps: 0.475525
  582912/2000000: episode: 4791, duration: 1.635s, episode steps: 110, steps per second: 67, episode reward: -100.032, mean reward: -0.909 [-100.000, 15.783], mean action: 1.518 [0.000, 3.000], mean observation: 0.007 [-0.882, 2.818], loss: 1.413888, mean_absolute_error: 34.109801, mean_q: 25.446842, mean_eps: 0.475430
  583912/2000000: episode: 4792, duration: 15.786s, episode steps: 1000, steps per second: 63, episode reward: 77.131, mean reward: 0.077 [-23.973, 23.177], mean action: 1.219 [0.000, 3.000], mean observation: 0.193 [-0.540, 1.549], loss: 1.677784, mean_absolute_error: 34.680848, mean_q: 26.637277, mean_eps: 0.474931
  584020/2000000: episode: 4793, duration: 1.655s, episode steps: 108, steps per second: 65, episode reward: -112.498, mean reward: -1.042 [-100.000, 10.724], mean action: 1.648 [0.000, 3.000], mean observation: -0.087 [-0.850, 1.622], loss: 2.102830, mean_absolute_error: 35.548121, mean_q: 28.353990, mean_eps: 0.474432
  585020/2000000: episode: 4794, duration: 16.574s, episode steps: 1000, steps per second: 60, episode reward: 59.606, mean reward: 0.060 [-21.995, 23.190], mean action: 1.309 [0.000, 3.000], mean observation: 0.173 [-0.527, 1.000], loss: 1.678858, mean_absolute_error: 34.304593, mean_q: 26.177196, mean_eps: 0.473934
  585206/2000000: episode: 4795, duration: 2.769s, episode steps: 186, steps per second: 67, episode reward: -15.225, mean reward: -0.082 [-100.000, 15.126], mean action: 1.774 [0.000, 3.000], mean observation: 0.018 [-0.654, 1.018], loss: 2.093503, mean_absolute_error: 33.556878, mean_q: 24.662443, mean_eps: 0.473399
  585389/2000000: episode: 4796, duration: 2.717s, episode steps: 183, steps per second: 67, episode reward: -38.886, mean reward: -0.212 [-100.000, 12.362], mean action: 1.716 [0.000, 3.000], mean observation: -0.004 [-1.658, 1.000], loss: 1.744276, mean_absolute_error: 34.108331, mean_q: 26.585810, mean_eps: 0.473232
  585490/2000000: episode: 4797, duration: 1.456s, episode steps: 101, steps per second: 69, episode reward: -65.540, mean reward: -0.649 [-100.000, 9.679], mean action: 1.663 [0.000, 3.000], mean observation: -0.029 [-0.785, 2.367], loss: 1.661198, mean_absolute_error: 35.985175, mean_q: 31.457654, mean_eps: 0.473104
  586490/2000000: episode: 4798, duration: 15.435s, episode steps: 1000, steps per second: 65, episode reward: 54.118, mean reward: 0.054 [-23.399, 23.745], mean action: 1.140 [0.000, 3.000], mean observation: 0.054 [-0.879, 1.055], loss: 1.775200, mean_absolute_error: 34.020992, mean_q: 26.934354, mean_eps: 0.472609
  587490/2000000: episode: 4799, duration: 15.700s, episode steps: 1000, steps per second: 64, episode reward: 65.126, mean reward: 0.065 [-23.338, 23.016], mean action: 1.331 [0.000, 3.000], mean observation: 0.159 [-0.673, 1.000], loss: 1.688253, mean_absolute_error: 34.383532, mean_q: 26.182963, mean_eps: 0.471709
  587614/2000000: episode: 4800, duration: 1.829s, episode steps: 124, steps per second: 68, episode reward: -182.063, mean reward: -1.468 [-100.000, 61.233], mean action: 1.774 [0.000, 3.000], mean observation: 0.147 [-0.834, 1.608], loss: 1.725551, mean_absolute_error: 34.811684, mean_q: 25.399705, mean_eps: 0.471203
  587781/2000000: episode: 4801, duration: 2.466s, episode steps: 167, steps per second: 68, episode reward: -115.242, mean reward: -0.690 [-100.000, 14.388], mean action: 1.695 [0.000, 3.000], mean observation: 0.119 [-0.844, 1.877], loss: 1.704746, mean_absolute_error: 34.315908, mean_q: 27.440223, mean_eps: 0.471072
  587912/2000000: episode: 4802, duration: 1.909s, episode steps: 131, steps per second: 69, episode reward: -25.534, mean reward: -0.195 [-100.000, 16.920], mean action: 1.718 [0.000, 3.000], mean observation: 0.020 [-0.729, 1.000], loss: 1.948523, mean_absolute_error: 35.618702, mean_q: 27.660801, mean_eps: 0.470939
  588037/2000000: episode: 4803, duration: 1.834s, episode steps: 125, steps per second: 68, episode reward: -15.199, mean reward: -0.122 [-100.000, 17.730], mean action: 1.632 [0.000, 3.000], mean observation: -0.034 [-0.718, 1.560], loss: 1.476727, mean_absolute_error: 32.945878, mean_q: 25.145497, mean_eps: 0.470823
  588157/2000000: episode: 4804, duration: 1.754s, episode steps: 120, steps per second: 68, episode reward: -42.524, mean reward: -0.354 [-100.000, 18.750], mean action: 1.783 [0.000, 3.000], mean observation: -0.001 [-0.793, 1.000], loss: 1.529408, mean_absolute_error: 34.127144, mean_q: 27.457994, mean_eps: 0.470712
  589157/2000000: episode: 4805, duration: 15.488s, episode steps: 1000, steps per second: 65, episode reward: 58.347, mean reward: 0.058 [-22.579, 25.439], mean action: 1.071 [0.000, 3.000], mean observation: 0.172 [-1.206, 1.000], loss: 1.843026, mean_absolute_error: 33.954696, mean_q: 26.109285, mean_eps: 0.470208
  589268/2000000: episode: 4806, duration: 1.646s, episode steps: 111, steps per second: 67, episode reward: -58.849, mean reward: -0.530 [-100.000, 11.451], mean action: 1.766 [0.000, 3.000], mean observation: 0.072 [-2.567, 1.000], loss: 1.531087, mean_absolute_error: 34.411260, mean_q: 23.811885, mean_eps: 0.469709
  589407/2000000: episode: 4807, duration: 2.039s, episode steps: 139, steps per second: 68, episode reward: -82.746, mean reward: -0.595 [-100.000, 18.746], mean action: 1.755 [0.000, 3.000], mean observation: 0.132 [-2.888, 1.002], loss: 2.314878, mean_absolute_error: 34.133571, mean_q: 22.919824, mean_eps: 0.469598
  589600/2000000: episode: 4808, duration: 2.864s, episode steps: 193, steps per second: 67, episode reward: -63.621, mean reward: -0.330 [-100.000, 12.191], mean action: 1.746 [0.000, 3.000], mean observation: 0.012 [-0.934, 1.000], loss: 1.613626, mean_absolute_error: 33.925655, mean_q: 26.593996, mean_eps: 0.469448
  589755/2000000: episode: 4809, duration: 2.287s, episode steps: 155, steps per second: 68, episode reward: -43.929, mean reward: -0.283 [-100.000, 15.910], mean action: 1.852 [0.000, 3.000], mean observation: 0.062 [-1.511, 1.000], loss: 1.448126, mean_absolute_error: 33.740450, mean_q: 25.645497, mean_eps: 0.469292
  589892/2000000: episode: 4810, duration: 2.059s, episode steps: 137, steps per second: 67, episode reward: -81.623, mean reward: -0.596 [-100.000, 7.218], mean action: 1.752 [0.000, 3.000], mean observation: -0.106 [-0.749, 1.000], loss: 1.796657, mean_absolute_error: 33.608199, mean_q: 28.541648, mean_eps: 0.469160
  590059/2000000: episode: 4811, duration: 2.467s, episode steps: 167, steps per second: 68, episode reward: -0.265, mean reward: -0.002 [-100.000, 19.386], mean action: 1.707 [0.000, 3.000], mean observation: -0.042 [-1.568, 1.000], loss: 1.564959, mean_absolute_error: 34.481354, mean_q: 27.866233, mean_eps: 0.469023
  591059/2000000: episode: 4812, duration: 16.757s, episode steps: 1000, steps per second: 60, episode reward: 48.607, mean reward: 0.049 [-23.266, 23.434], mean action: 1.323 [0.000, 3.000], mean observation: 0.207 [-0.708, 1.264], loss: 1.681625, mean_absolute_error: 34.246364, mean_q: 25.792923, mean_eps: 0.468498
  592059/2000000: episode: 4813, duration: 15.945s, episode steps: 1000, steps per second: 63, episode reward: 39.868, mean reward: 0.040 [-24.318, 23.761], mean action: 1.052 [0.000, 3.000], mean observation: 0.141 [-0.857, 1.000], loss: 1.791157, mean_absolute_error: 34.192118, mean_q: 26.473438, mean_eps: 0.467598
  593059/2000000: episode: 4814, duration: 15.584s, episode steps: 1000, steps per second: 64, episode reward: 117.419, mean reward: 0.117 [-24.034, 22.746], mean action: 0.978 [0.000, 3.000], mean observation: 0.161 [-1.232, 1.000], loss: 1.913743, mean_absolute_error: 33.833290, mean_q: 26.951346, mean_eps: 0.466698
  594059/2000000: episode: 4815, duration: 15.689s, episode steps: 1000, steps per second: 64, episode reward: 43.207, mean reward: 0.043 [-25.122, 32.252], mean action: 2.043 [0.000, 3.000], mean observation: 0.251 [-0.619, 1.545], loss: 1.686689, mean_absolute_error: 34.347281, mean_q: 25.104621, mean_eps: 0.465798
  594224/2000000: episode: 4816, duration: 2.450s, episode steps: 165, steps per second: 67, episode reward: -86.242, mean reward: -0.523 [-100.000, 19.394], mean action: 1.727 [0.000, 3.000], mean observation: 0.155 [-0.632, 1.479], loss: 1.319593, mean_absolute_error: 34.327523, mean_q: 27.642710, mean_eps: 0.465274
  594341/2000000: episode: 4817, duration: 1.748s, episode steps: 117, steps per second: 67, episode reward: -14.156, mean reward: -0.121 [-100.000, 17.116], mean action: 1.632 [0.000, 3.000], mean observation: 0.031 [-1.525, 1.000], loss: 1.734096, mean_absolute_error: 33.891548, mean_q: 27.001870, mean_eps: 0.465146
  594441/2000000: episode: 4818, duration: 1.466s, episode steps: 100, steps per second: 68, episode reward: -96.642, mean reward: -0.966 [-100.000, 9.471], mean action: 1.490 [0.000, 3.000], mean observation: -0.012 [-0.801, 2.233], loss: 1.693217, mean_absolute_error: 34.886791, mean_q: 28.175935, mean_eps: 0.465047
  594576/2000000: episode: 4819, duration: 1.987s, episode steps: 135, steps per second: 68, episode reward: -39.514, mean reward: -0.293 [-100.000, 11.058], mean action: 1.733 [0.000, 3.000], mean observation: 0.074 [-1.399, 1.000], loss: 1.989210, mean_absolute_error: 34.839615, mean_q: 27.786113, mean_eps: 0.464943
  594675/2000000: episode: 4820, duration: 1.456s, episode steps: 99, steps per second: 68, episode reward: -99.169, mean reward: -1.002 [-100.000, 10.148], mean action: 1.818 [0.000, 3.000], mean observation: -0.078 [-1.361, 1.000], loss: 1.616768, mean_absolute_error: 32.733555, mean_q: 25.588997, mean_eps: 0.464838
  594808/2000000: episode: 4821, duration: 1.982s, episode steps: 133, steps per second: 67, episode reward: -86.986, mean reward: -0.654 [-100.000, 10.892], mean action: 1.586 [0.000, 3.000], mean observation: 0.137 [-0.603, 1.000], loss: 1.647585, mean_absolute_error: 34.647095, mean_q: 27.481174, mean_eps: 0.464734
  594950/2000000: episode: 4822, duration: 2.106s, episode steps: 142, steps per second: 67, episode reward: -38.672, mean reward: -0.272 [-100.000, 15.614], mean action: 1.704 [0.000, 3.000], mean observation: 0.012 [-1.107, 1.000], loss: 1.944128, mean_absolute_error: 34.560733, mean_q: 25.405603, mean_eps: 0.464610
  595136/2000000: episode: 4823, duration: 2.747s, episode steps: 186, steps per second: 68, episode reward: -182.163, mean reward: -0.979 [-100.000, 45.881], mean action: 1.801 [0.000, 3.000], mean observation: 0.140 [-0.574, 1.506], loss: 1.626465, mean_absolute_error: 33.329284, mean_q: 28.151059, mean_eps: 0.464462
  595316/2000000: episode: 4824, duration: 2.669s, episode steps: 180, steps per second: 67, episode reward: -173.876, mean reward: -0.966 [-100.000, 9.656], mean action: 1.672 [0.000, 3.000], mean observation: 0.147 [-1.532, 1.001], loss: 1.970275, mean_absolute_error: 34.636644, mean_q: 26.458234, mean_eps: 0.464298
  595418/2000000: episode: 4825, duration: 1.529s, episode steps: 102, steps per second: 67, episode reward: -42.216, mean reward: -0.414 [-100.000, 7.292], mean action: 1.765 [0.000, 3.000], mean observation: -0.064 [-0.756, 2.645], loss: 1.954399, mean_absolute_error: 34.579708, mean_q: 28.249627, mean_eps: 0.464171
  595502/2000000: episode: 4826, duration: 1.227s, episode steps: 84, steps per second: 68, episode reward: -66.249, mean reward: -0.789 [-100.000, 12.966], mean action: 1.726 [0.000, 3.000], mean observation: 0.122 [-0.775, 2.522], loss: 2.103458, mean_absolute_error: 34.312345, mean_q: 25.219913, mean_eps: 0.464086
  595619/2000000: episode: 4827, duration: 1.698s, episode steps: 117, steps per second: 69, episode reward: -74.486, mean reward: -0.637 [-100.000, 10.045], mean action: 1.709 [0.000, 3.000], mean observation: -0.078 [-0.652, 1.642], loss: 1.474749, mean_absolute_error: 34.858932, mean_q: 27.887017, mean_eps: 0.463996
  595758/2000000: episode: 4828, duration: 2.055s, episode steps: 139, steps per second: 68, episode reward: -35.915, mean reward: -0.258 [-100.000, 10.023], mean action: 1.748 [0.000, 3.000], mean observation: -0.067 [-0.724, 2.468], loss: 1.552707, mean_absolute_error: 33.736726, mean_q: 28.357269, mean_eps: 0.463881
  595932/2000000: episode: 4829, duration: 2.585s, episode steps: 174, steps per second: 67, episode reward: -67.060, mean reward: -0.385 [-100.000, 10.325], mean action: 1.546 [0.000, 3.000], mean observation: 0.143 [-0.793, 1.465], loss: 1.557177, mean_absolute_error: 33.724826, mean_q: 26.120618, mean_eps: 0.463740
  596053/2000000: episode: 4830, duration: 1.787s, episode steps: 121, steps per second: 68, episode reward: -92.227, mean reward: -0.762 [-100.000, 11.117], mean action: 1.669 [0.000, 3.000], mean observation: 0.086 [-0.754, 1.000], loss: 1.557233, mean_absolute_error: 33.759098, mean_q: 27.766587, mean_eps: 0.463607
  596239/2000000: episode: 4831, duration: 2.721s, episode steps: 186, steps per second: 68, episode reward: -143.190, mean reward: -0.770 [-100.000, 14.861], mean action: 1.688 [0.000, 3.000], mean observation: 0.040 [-1.097, 1.655], loss: 1.810519, mean_absolute_error: 34.412181, mean_q: 27.213408, mean_eps: 0.463469
  596460/2000000: episode: 4832, duration: 3.275s, episode steps: 221, steps per second: 67, episode reward: -165.475, mean reward: -0.749 [-100.000, 58.597], mean action: 1.624 [0.000, 3.000], mean observation: -0.107 [-1.100, 1.000], loss: 2.088130, mean_absolute_error: 33.499648, mean_q: 26.572937, mean_eps: 0.463287
  596541/2000000: episode: 4833, duration: 1.252s, episode steps: 81, steps per second: 65, episode reward: -121.425, mean reward: -1.499 [-100.000, 7.504], mean action: 1.667 [0.000, 3.000], mean observation: 0.084 [-3.121, 1.000], loss: 2.140230, mean_absolute_error: 35.005650, mean_q: 27.598590, mean_eps: 0.463150
  596705/2000000: episode: 4834, duration: 2.386s, episode steps: 164, steps per second: 69, episode reward: -55.456, mean reward: -0.338 [-100.000, 10.617], mean action: 1.732 [0.000, 3.000], mean observation: 0.064 [-1.535, 1.000], loss: 1.611407, mean_absolute_error: 32.805529, mean_q: 26.527395, mean_eps: 0.463038
  596853/2000000: episode: 4835, duration: 2.156s, episode steps: 148, steps per second: 69, episode reward: -27.813, mean reward: -0.188 [-100.000, 16.816], mean action: 1.743 [0.000, 3.000], mean observation: -0.067 [-0.744, 1.000], loss: 1.759565, mean_absolute_error: 33.991742, mean_q: 28.434760, mean_eps: 0.462898
  597033/2000000: episode: 4836, duration: 2.644s, episode steps: 180, steps per second: 68, episode reward: -60.830, mean reward: -0.338 [-100.000, 20.319], mean action: 1.794 [0.000, 3.000], mean observation: -0.020 [-1.776, 1.000], loss: 1.432240, mean_absolute_error: 34.899222, mean_q: 27.837174, mean_eps: 0.462750
  598033/2000000: episode: 4837, duration: 15.720s, episode steps: 1000, steps per second: 64, episode reward: 75.567, mean reward: 0.076 [-23.155, 24.336], mean action: 1.624 [0.000, 3.000], mean observation: 0.216 [-1.298, 1.000], loss: 1.977308, mean_absolute_error: 34.194375, mean_q: 26.834181, mean_eps: 0.462219
  599033/2000000: episode: 4838, duration: 15.647s, episode steps: 1000, steps per second: 64, episode reward: 47.736, mean reward: 0.048 [-24.303, 23.160], mean action: 1.014 [0.000, 3.000], mean observation: 0.188 [-0.789, 1.504], loss: 1.654689, mean_absolute_error: 33.730595, mean_q: 26.861212, mean_eps: 0.461319
  599179/2000000: episode: 4839, duration: 2.119s, episode steps: 146, steps per second: 69, episode reward: -237.201, mean reward: -1.625 [-100.000, 61.333], mean action: 1.651 [0.000, 3.000], mean observation: -0.045 [-1.867, 1.000], loss: 1.868095, mean_absolute_error: 34.425938, mean_q: 25.877500, mean_eps: 0.460805
  599334/2000000: episode: 4840, duration: 2.296s, episode steps: 155, steps per second: 68, episode reward: -113.161, mean reward: -0.730 [-100.000, 16.484], mean action: 1.813 [0.000, 3.000], mean observation: -0.066 [-1.028, 1.000], loss: 1.448429, mean_absolute_error: 33.233369, mean_q: 24.816482, mean_eps: 0.460670
  599442/2000000: episode: 4841, duration: 1.598s, episode steps: 108, steps per second: 68, episode reward: -77.601, mean reward: -0.719 [-100.000, 17.865], mean action: 1.731 [0.000, 3.000], mean observation: 0.027 [-0.828, 1.000], loss: 1.809660, mean_absolute_error: 34.203913, mean_q: 29.149957, mean_eps: 0.460551
  599598/2000000: episode: 4842, duration: 2.391s, episode steps: 156, steps per second: 65, episode reward: -79.827, mean reward: -0.512 [-100.000, 10.164], mean action: 1.635 [0.000, 3.000], mean observation: 0.034 [-0.646, 1.000], loss: 1.997913, mean_absolute_error: 33.345556, mean_q: 27.596605, mean_eps: 0.460432
  599815/2000000: episode: 4843, duration: 3.242s, episode steps: 217, steps per second: 67, episode reward: -113.842, mean reward: -0.525 [-100.000, 12.344], mean action: 1.668 [0.000, 3.000], mean observation: -0.095 [-1.013, 1.439], loss: 1.880256, mean_absolute_error: 34.023174, mean_q: 27.739553, mean_eps: 0.460265
  600077/2000000: episode: 4844, duration: 3.912s, episode steps: 262, steps per second: 67, episode reward: -54.294, mean reward: -0.207 [-100.000, 9.870], mean action: 1.817 [0.000, 3.000], mean observation: -0.003 [-0.513, 1.000], loss: 1.960255, mean_absolute_error: 33.499982, mean_q: 26.320281, mean_eps: 0.460049
  600179/2000000: episode: 4845, duration: 1.441s, episode steps: 102, steps per second: 71, episode reward: -78.395, mean reward: -0.769 [-100.000, 15.162], mean action: 1.578 [0.000, 3.000], mean observation: 0.100 [-0.778, 1.243], loss: 1.511729, mean_absolute_error: 34.318895, mean_q: 27.112362, mean_eps: 0.459885
  600311/2000000: episode: 4846, duration: 1.920s, episode steps: 132, steps per second: 69, episode reward: -31.402, mean reward: -0.238 [-100.000, 19.199], mean action: 1.659 [0.000, 3.000], mean observation: 0.115 [-1.349, 1.000], loss: 2.265533, mean_absolute_error: 33.304469, mean_q: 27.174044, mean_eps: 0.459780
  600431/2000000: episode: 4847, duration: 1.768s, episode steps: 120, steps per second: 68, episode reward: 7.698, mean reward: 0.064 [-100.000, 14.656], mean action: 1.925 [0.000, 3.000], mean observation: 0.013 [-1.225, 1.000], loss: 1.476016, mean_absolute_error: 34.946734, mean_q: 26.957723, mean_eps: 0.459667
  600551/2000000: episode: 4848, duration: 1.740s, episode steps: 120, steps per second: 69, episode reward: -13.190, mean reward: -0.110 [-100.000, 16.928], mean action: 1.825 [0.000, 3.000], mean observation: 0.026 [-0.705, 1.000], loss: 1.529818, mean_absolute_error: 34.658053, mean_q: 28.858379, mean_eps: 0.459559
  600644/2000000: episode: 4849, duration: 1.394s, episode steps: 93, steps per second: 67, episode reward: -141.149, mean reward: -1.518 [-100.000, 16.875], mean action: 1.624 [0.000, 3.000], mean observation: -0.140 [-1.175, 3.521], loss: 1.643345, mean_absolute_error: 33.906400, mean_q: 26.776373, mean_eps: 0.459464
  601644/2000000: episode: 4850, duration: 15.629s, episode steps: 1000, steps per second: 64, episode reward: 62.838, mean reward: 0.063 [-22.471, 56.555], mean action: 0.950 [0.000, 3.000], mean observation: 0.116 [-0.771, 1.000], loss: 2.023967, mean_absolute_error: 34.117803, mean_q: 26.006991, mean_eps: 0.458972
  601863/2000000: episode: 4851, duration: 3.211s, episode steps: 219, steps per second: 68, episode reward: -268.793, mean reward: -1.227 [-100.000, 84.100], mean action: 1.575 [0.000, 3.000], mean observation: 0.074 [-1.723, 1.011], loss: 1.633713, mean_absolute_error: 33.505162, mean_q: 26.617736, mean_eps: 0.458423
  602863/2000000: episode: 4852, duration: 16.386s, episode steps: 1000, steps per second: 61, episode reward: 48.444, mean reward: 0.048 [-24.051, 23.466], mean action: 1.140 [0.000, 3.000], mean observation: 0.112 [-0.677, 1.032], loss: 1.881696, mean_absolute_error: 34.018690, mean_q: 26.910206, mean_eps: 0.457874
  602963/2000000: episode: 4853, duration: 1.472s, episode steps: 100, steps per second: 68, episode reward: -65.217, mean reward: -0.652 [-100.000, 19.767], mean action: 1.800 [0.000, 3.000], mean observation: -0.117 [-0.970, 1.000], loss: 1.828197, mean_absolute_error: 34.296259, mean_q: 24.844265, mean_eps: 0.457379
  603087/2000000: episode: 4854, duration: 1.812s, episode steps: 124, steps per second: 68, episode reward: -73.585, mean reward: -0.593 [-100.000, 9.197], mean action: 1.790 [0.000, 3.000], mean observation: -0.053 [-0.779, 2.668], loss: 1.525247, mean_absolute_error: 33.218784, mean_q: 27.660195, mean_eps: 0.457278
  603215/2000000: episode: 4855, duration: 1.877s, episode steps: 128, steps per second: 68, episode reward: -34.656, mean reward: -0.271 [-100.000, 26.536], mean action: 1.688 [0.000, 3.000], mean observation: -0.058 [-0.997, 1.342], loss: 1.412147, mean_absolute_error: 32.245303, mean_q: 26.675218, mean_eps: 0.457165
  603386/2000000: episode: 4856, duration: 2.533s, episode steps: 171, steps per second: 68, episode reward: -29.878, mean reward: -0.175 [-100.000, 19.957], mean action: 1.731 [0.000, 3.000], mean observation: 0.051 [-1.179, 1.000], loss: 1.649687, mean_absolute_error: 33.852383, mean_q: 26.686260, mean_eps: 0.457030
  604386/2000000: episode: 4857, duration: 15.724s, episode steps: 1000, steps per second: 64, episode reward: 75.509, mean reward: 0.076 [-24.272, 22.852], mean action: 1.112 [0.000, 3.000], mean observation: 0.163 [-1.087, 1.001], loss: 1.848158, mean_absolute_error: 33.825523, mean_q: 25.351549, mean_eps: 0.456503
  604630/2000000: episode: 4858, duration: 3.588s, episode steps: 244, steps per second: 68, episode reward: -43.843, mean reward: -0.180 [-100.000, 13.504], mean action: 1.684 [0.000, 3.000], mean observation: 0.114 [-0.546, 1.112], loss: 1.847960, mean_absolute_error: 33.371472, mean_q: 25.833950, mean_eps: 0.455943
  604731/2000000: episode: 4859, duration: 1.499s, episode steps: 101, steps per second: 67, episode reward: -88.176, mean reward: -0.873 [-100.000, 10.211], mean action: 1.624 [0.000, 3.000], mean observation: -0.078 [-0.948, 3.518], loss: 1.867071, mean_absolute_error: 33.551677, mean_q: 26.234347, mean_eps: 0.455788
  605731/2000000: episode: 4860, duration: 16.421s, episode steps: 1000, steps per second: 61, episode reward: 77.325, mean reward: 0.077 [-24.577, 23.289], mean action: 1.193 [0.000, 3.000], mean observation: 0.169 [-0.609, 1.060], loss: 1.842054, mean_absolute_error: 33.513215, mean_q: 25.616734, mean_eps: 0.455293
  606085/2000000: episode: 4861, duration: 5.398s, episode steps: 354, steps per second: 66, episode reward: -78.323, mean reward: -0.221 [-100.000, 13.655], mean action: 1.828 [0.000, 3.000], mean observation: -0.014 [-0.520, 1.014], loss: 1.758490, mean_absolute_error: 33.021079, mean_q: 26.712543, mean_eps: 0.454683
  607085/2000000: episode: 4862, duration: 15.886s, episode steps: 1000, steps per second: 63, episode reward: 67.584, mean reward: 0.068 [-24.105, 22.913], mean action: 1.053 [0.000, 3.000], mean observation: 0.151 [-1.283, 1.000], loss: 1.972844, mean_absolute_error: 33.946466, mean_q: 26.970077, mean_eps: 0.454073
  607289/2000000: episode: 4863, duration: 3.089s, episode steps: 204, steps per second: 66, episode reward: -27.498, mean reward: -0.135 [-100.000, 16.511], mean action: 1.618 [0.000, 3.000], mean observation: 0.055 [-1.548, 1.001], loss: 1.572639, mean_absolute_error: 33.009952, mean_q: 27.731797, mean_eps: 0.453531
  608289/2000000: episode: 4864, duration: 15.680s, episode steps: 1000, steps per second: 64, episode reward: 49.097, mean reward: 0.049 [-23.219, 23.523], mean action: 1.031 [0.000, 3.000], mean observation: 0.137 [-1.309, 1.000], loss: 1.762954, mean_absolute_error: 33.634822, mean_q: 25.976058, mean_eps: 0.452989
  608471/2000000: episode: 4865, duration: 2.653s, episode steps: 182, steps per second: 69, episode reward: -69.096, mean reward: -0.380 [-100.000, 16.476], mean action: 1.676 [0.000, 3.000], mean observation: -0.023 [-1.544, 1.000], loss: 1.781584, mean_absolute_error: 34.354934, mean_q: 27.884475, mean_eps: 0.452458
  608564/2000000: episode: 4866, duration: 1.390s, episode steps: 93, steps per second: 67, episode reward: -108.792, mean reward: -1.170 [-100.000, 10.263], mean action: 1.806 [0.000, 3.000], mean observation: -0.078 [-1.036, 3.579], loss: 1.546375, mean_absolute_error: 32.458533, mean_q: 26.241722, mean_eps: 0.452336
  609564/2000000: episode: 4867, duration: 15.778s, episode steps: 1000, steps per second: 63, episode reward: 44.829, mean reward: 0.045 [-21.538, 23.496], mean action: 1.097 [0.000, 3.000], mean observation: 0.186 [-0.782, 1.000], loss: 1.709070, mean_absolute_error: 33.197654, mean_q: 26.632617, mean_eps: 0.451844
  609636/2000000: episode: 4868, duration: 1.114s, episode steps: 72, steps per second: 65, episode reward: -96.944, mean reward: -1.346 [-100.000, 9.733], mean action: 1.569 [0.000, 3.000], mean observation: 0.106 [-0.994, 3.909], loss: 1.616622, mean_absolute_error: 33.787949, mean_q: 27.668835, mean_eps: 0.451362
  609779/2000000: episode: 4869, duration: 2.117s, episode steps: 143, steps per second: 68, episode reward: -33.311, mean reward: -0.233 [-100.000, 39.176], mean action: 1.741 [0.000, 3.000], mean observation: 0.096 [-1.246, 1.000], loss: 1.944031, mean_absolute_error: 34.193611, mean_q: 28.893890, mean_eps: 0.451265
  610779/2000000: episode: 4870, duration: 15.261s, episode steps: 1000, steps per second: 66, episode reward: 51.650, mean reward: 0.052 [-23.493, 24.383], mean action: 0.957 [0.000, 3.000], mean observation: 0.154 [-0.988, 1.000], loss: 1.665545, mean_absolute_error: 33.562433, mean_q: 27.277557, mean_eps: 0.450750
  610988/2000000: episode: 4871, duration: 3.104s, episode steps: 209, steps per second: 67, episode reward: -46.311, mean reward: -0.222 [-100.000, 12.441], mean action: 1.694 [0.000, 3.000], mean observation: 0.092 [-0.678, 1.000], loss: 2.023085, mean_absolute_error: 33.532312, mean_q: 27.338243, mean_eps: 0.450206
  611988/2000000: episode: 4872, duration: 15.765s, episode steps: 1000, steps per second: 63, episode reward: -3.120, mean reward: -0.003 [-22.314, 23.478], mean action: 1.236 [0.000, 3.000], mean observation: 0.090 [-1.073, 1.000], loss: 1.913981, mean_absolute_error: 33.414903, mean_q: 27.250202, mean_eps: 0.449663
  612133/2000000: episode: 4873, duration: 2.178s, episode steps: 145, steps per second: 67, episode reward: -83.144, mean reward: -0.573 [-100.000, 9.565], mean action: 1.834 [0.000, 3.000], mean observation: 0.138 [-1.227, 1.054], loss: 2.044302, mean_absolute_error: 34.282088, mean_q: 26.453581, mean_eps: 0.449146
  612294/2000000: episode: 4874, duration: 2.366s, episode steps: 161, steps per second: 68, episode reward: 17.164, mean reward: 0.107 [-100.000, 15.129], mean action: 1.919 [0.000, 3.000], mean observation: 0.005 [-1.360, 1.000], loss: 1.751365, mean_absolute_error: 32.754957, mean_q: 26.516566, mean_eps: 0.449007
  612421/2000000: episode: 4875, duration: 1.879s, episode steps: 127, steps per second: 68, episode reward: -89.720, mean reward: -0.706 [-100.000, 12.096], mean action: 1.724 [0.000, 3.000], mean observation: 0.058 [-0.759, 1.000], loss: 1.781381, mean_absolute_error: 34.030903, mean_q: 26.306054, mean_eps: 0.448878
  613421/2000000: episode: 4876, duration: 16.085s, episode steps: 1000, steps per second: 62, episode reward: 57.995, mean reward: 0.058 [-23.700, 22.949], mean action: 1.109 [0.000, 3.000], mean observation: 0.143 [-1.070, 1.000], loss: 1.784116, mean_absolute_error: 33.341436, mean_q: 27.289221, mean_eps: 0.448370
  613572/2000000: episode: 4877, duration: 2.245s, episode steps: 151, steps per second: 67, episode reward: -98.245, mean reward: -0.651 [-100.000, 10.721], mean action: 1.636 [0.000, 3.000], mean observation: 0.133 [-1.302, 1.000], loss: 1.694313, mean_absolute_error: 33.437680, mean_q: 27.727359, mean_eps: 0.447854
  613684/2000000: episode: 4878, duration: 1.701s, episode steps: 112, steps per second: 66, episode reward: -78.385, mean reward: -0.700 [-100.000, 9.993], mean action: 1.795 [0.000, 3.000], mean observation: 0.093 [-1.597, 1.000], loss: 1.820700, mean_absolute_error: 32.812124, mean_q: 25.559853, mean_eps: 0.447737
  613769/2000000: episode: 4879, duration: 1.287s, episode steps: 85, steps per second: 66, episode reward: -55.541, mean reward: -0.653 [-100.000, 16.989], mean action: 1.706 [0.000, 3.000], mean observation: -0.063 [-2.713, 1.000], loss: 1.620146, mean_absolute_error: 33.988757, mean_q: 27.699439, mean_eps: 0.447647
  613892/2000000: episode: 4880, duration: 1.824s, episode steps: 123, steps per second: 67, episode reward: -44.316, mean reward: -0.360 [-100.000, 18.164], mean action: 1.724 [0.000, 3.000], mean observation: -0.023 [-0.791, 1.526], loss: 1.811596, mean_absolute_error: 33.588938, mean_q: 27.344979, mean_eps: 0.447553
  613998/2000000: episode: 4881, duration: 1.581s, episode steps: 106, steps per second: 67, episode reward: -91.624, mean reward: -0.864 [-100.000, 11.840], mean action: 1.698 [0.000, 3.000], mean observation: -0.094 [-1.128, 1.000], loss: 1.682866, mean_absolute_error: 33.937296, mean_q: 26.860617, mean_eps: 0.447450
  614998/2000000: episode: 4882, duration: 15.368s, episode steps: 1000, steps per second: 65, episode reward: 51.317, mean reward: 0.051 [-20.318, 54.887], mean action: 1.013 [0.000, 3.000], mean observation: 0.056 [-1.379, 1.000], loss: 1.788716, mean_absolute_error: 33.475490, mean_q: 26.607321, mean_eps: 0.446952
  615123/2000000: episode: 4883, duration: 2.052s, episode steps: 125, steps per second: 61, episode reward: -56.182, mean reward: -0.449 [-100.000, 19.045], mean action: 1.856 [0.000, 3.000], mean observation: -0.040 [-0.892, 2.619], loss: 1.824503, mean_absolute_error: 33.400829, mean_q: 27.022793, mean_eps: 0.446446
  615259/2000000: episode: 4884, duration: 2.033s, episode steps: 136, steps per second: 67, episode reward: -14.208, mean reward: -0.104 [-100.000, 36.816], mean action: 1.699 [0.000, 3.000], mean observation: 0.128 [-1.252, 1.214], loss: 1.589355, mean_absolute_error: 32.576947, mean_q: 27.652165, mean_eps: 0.446329
  615408/2000000: episode: 4885, duration: 2.223s, episode steps: 149, steps per second: 67, episode reward: -35.761, mean reward: -0.240 [-100.000, 18.160], mean action: 1.819 [0.000, 3.000], mean observation: 0.051 [-1.243, 1.000], loss: 2.075959, mean_absolute_error: 33.762473, mean_q: 25.643103, mean_eps: 0.446201
  615606/2000000: episode: 4886, duration: 2.971s, episode steps: 198, steps per second: 67, episode reward: -74.225, mean reward: -0.375 [-100.000, 13.932], mean action: 1.828 [0.000, 3.000], mean observation: -0.058 [-0.678, 1.000], loss: 2.173485, mean_absolute_error: 32.957177, mean_q: 25.967277, mean_eps: 0.446045
  615709/2000000: episode: 4887, duration: 1.537s, episode steps: 103, steps per second: 67, episode reward: -111.212, mean reward: -1.080 [-100.000, 13.101], mean action: 1.631 [0.000, 3.000], mean observation: 0.105 [-0.787, 1.000], loss: 2.202092, mean_absolute_error: 32.946547, mean_q: 27.194550, mean_eps: 0.445908
  616709/2000000: episode: 4888, duration: 15.594s, episode steps: 1000, steps per second: 64, episode reward: 125.808, mean reward: 0.126 [-20.624, 22.702], mean action: 1.032 [0.000, 3.000], mean observation: 0.207 [-1.151, 1.000], loss: 1.830661, mean_absolute_error: 33.521199, mean_q: 27.753399, mean_eps: 0.445411
  617709/2000000: episode: 4889, duration: 15.914s, episode steps: 1000, steps per second: 63, episode reward: 123.587, mean reward: 0.124 [-20.124, 23.300], mean action: 1.161 [0.000, 3.000], mean observation: 0.193 [-0.772, 1.000], loss: 1.761843, mean_absolute_error: 32.975191, mean_q: 26.205020, mean_eps: 0.444511
  617817/2000000: episode: 4890, duration: 1.595s, episode steps: 108, steps per second: 68, episode reward: -54.257, mean reward: -0.502 [-100.000, 9.417], mean action: 1.815 [0.000, 3.000], mean observation: -0.076 [-0.795, 1.702], loss: 2.031309, mean_absolute_error: 33.095673, mean_q: 25.919038, mean_eps: 0.444012
  618817/2000000: episode: 4891, duration: 15.367s, episode steps: 1000, steps per second: 65, episode reward: -25.493, mean reward: -0.025 [-22.248, 22.517], mean action: 1.136 [0.000, 3.000], mean observation: 0.063 [-0.837, 1.000], loss: 1.746361, mean_absolute_error: 33.455538, mean_q: 27.093723, mean_eps: 0.443514
  618942/2000000: episode: 4892, duration: 1.847s, episode steps: 125, steps per second: 68, episode reward: -71.418, mean reward: -0.571 [-100.000, 11.147], mean action: 1.584 [0.000, 3.000], mean observation: 0.094 [-0.643, 1.000], loss: 1.982343, mean_absolute_error: 33.806128, mean_q: 27.435168, mean_eps: 0.443008
  619091/2000000: episode: 4893, duration: 2.156s, episode steps: 149, steps per second: 69, episode reward: -41.914, mean reward: -0.281 [-100.000, 15.000], mean action: 1.718 [0.000, 3.000], mean observation: -0.087 [-0.746, 2.083], loss: 1.597989, mean_absolute_error: 33.125536, mean_q: 28.686232, mean_eps: 0.442886
  620091/2000000: episode: 4894, duration: 15.289s, episode steps: 1000, steps per second: 65, episode reward: 64.346, mean reward: 0.064 [-24.175, 25.667], mean action: 1.106 [0.000, 3.000], mean observation: 0.184 [-0.647, 1.724], loss: 1.832639, mean_absolute_error: 33.338060, mean_q: 26.565079, mean_eps: 0.442369
  620303/2000000: episode: 4895, duration: 3.113s, episode steps: 212, steps per second: 68, episode reward: -145.402, mean reward: -0.686 [-100.000, 9.361], mean action: 1.816 [0.000, 3.000], mean observation: 0.099 [-1.372, 1.001], loss: 2.079187, mean_absolute_error: 33.391605, mean_q: 25.886085, mean_eps: 0.441824
  620498/2000000: episode: 4896, duration: 2.872s, episode steps: 195, steps per second: 68, episode reward: -368.314, mean reward: -1.889 [-100.000, 23.040], mean action: 1.790 [0.000, 3.000], mean observation: -0.027 [-1.157, 2.188], loss: 2.676459, mean_absolute_error: 32.812326, mean_q: 25.414954, mean_eps: 0.441640
  621498/2000000: episode: 4897, duration: 15.372s, episode steps: 1000, steps per second: 65, episode reward: 79.821, mean reward: 0.080 [-21.588, 22.680], mean action: 1.147 [0.000, 3.000], mean observation: 0.196 [-0.736, 1.589], loss: 1.959640, mean_absolute_error: 33.194908, mean_q: 26.606487, mean_eps: 0.441102
  622498/2000000: episode: 4898, duration: 15.731s, episode steps: 1000, steps per second: 64, episode reward: 24.804, mean reward: 0.025 [-22.469, 23.375], mean action: 1.144 [0.000, 3.000], mean observation: 0.228 [-0.701, 1.000], loss: 1.714324, mean_absolute_error: 32.555973, mean_q: 27.024445, mean_eps: 0.440202
  622705/2000000: episode: 4899, duration: 3.081s, episode steps: 207, steps per second: 67, episode reward: -66.073, mean reward: -0.319 [-100.000, 14.612], mean action: 1.696 [0.000, 3.000], mean observation: 0.035 [-1.086, 1.000], loss: 2.162146, mean_absolute_error: 32.920014, mean_q: 27.416855, mean_eps: 0.439658
  622795/2000000: episode: 4900, duration: 1.358s, episode steps: 90, steps per second: 66, episode reward: -36.783, mean reward: -0.409 [-100.000, 17.011], mean action: 1.900 [0.000, 3.000], mean observation: -0.139 [-0.861, 1.000], loss: 1.817536, mean_absolute_error: 32.708195, mean_q: 27.867192, mean_eps: 0.439525
  623019/2000000: episode: 4901, duration: 3.430s, episode steps: 224, steps per second: 65, episode reward: -46.387, mean reward: -0.207 [-100.000, 9.097], mean action: 1.696 [0.000, 3.000], mean observation: 0.085 [-1.429, 1.000], loss: 1.857334, mean_absolute_error: 32.928230, mean_q: 27.620833, mean_eps: 0.439385
  623141/2000000: episode: 4902, duration: 1.834s, episode steps: 122, steps per second: 67, episode reward: -51.098, mean reward: -0.419 [-100.000, 18.715], mean action: 1.811 [0.000, 3.000], mean observation: -0.099 [-0.760, 1.000], loss: 1.849765, mean_absolute_error: 32.428245, mean_q: 24.941140, mean_eps: 0.439228
  623234/2000000: episode: 4903, duration: 1.333s, episode steps: 93, steps per second: 70, episode reward: -42.596, mean reward: -0.458 [-100.000, 20.171], mean action: 1.753 [0.000, 3.000], mean observation: 0.072 [-1.917, 1.000], loss: 1.737072, mean_absolute_error: 32.477665, mean_q: 29.586803, mean_eps: 0.439131
  623440/2000000: episode: 4904, duration: 3.054s, episode steps: 206, steps per second: 67, episode reward: -32.324, mean reward: -0.157 [-100.000, 11.874], mean action: 1.670 [0.000, 3.000], mean observation: 0.098 [-0.924, 1.000], loss: 1.652373, mean_absolute_error: 32.570579, mean_q: 26.589914, mean_eps: 0.438998
  623579/2000000: episode: 4905, duration: 2.065s, episode steps: 139, steps per second: 67, episode reward: -77.987, mean reward: -0.561 [-100.000, 17.040], mean action: 1.763 [0.000, 3.000], mean observation: -0.065 [-0.856, 2.699], loss: 1.868795, mean_absolute_error: 33.658206, mean_q: 25.047534, mean_eps: 0.438843
  623716/2000000: episode: 4906, duration: 2.058s, episode steps: 137, steps per second: 67, episode reward: -70.969, mean reward: -0.518 [-100.000, 29.783], mean action: 1.540 [0.000, 3.000], mean observation: 0.150 [-0.645, 1.795], loss: 1.603697, mean_absolute_error: 32.942954, mean_q: 27.671294, mean_eps: 0.438719
  624716/2000000: episode: 4907, duration: 16.579s, episode steps: 1000, steps per second: 60, episode reward: -8.162, mean reward: -0.008 [-25.069, 24.647], mean action: 1.931 [0.000, 3.000], mean observation: 0.216 [-0.692, 1.173], loss: 1.762294, mean_absolute_error: 32.835466, mean_q: 26.491093, mean_eps: 0.438207
  625716/2000000: episode: 4908, duration: 16.224s, episode steps: 1000, steps per second: 62, episode reward: -5.699, mean reward: -0.006 [-23.870, 22.292], mean action: 2.017 [0.000, 3.000], mean observation: 0.227 [-0.937, 1.000], loss: 1.778262, mean_absolute_error: 32.844421, mean_q: 26.655698, mean_eps: 0.437307
  625890/2000000: episode: 4909, duration: 2.588s, episode steps: 174, steps per second: 67, episode reward: -132.821, mean reward: -0.763 [-100.000, 25.494], mean action: 1.736 [0.000, 3.000], mean observation: -0.064 [-2.642, 1.000], loss: 2.089694, mean_absolute_error: 33.309893, mean_q: 27.967257, mean_eps: 0.436778
  626890/2000000: episode: 4910, duration: 15.485s, episode steps: 1000, steps per second: 65, episode reward: 107.651, mean reward: 0.108 [-23.025, 24.582], mean action: 1.064 [0.000, 3.000], mean observation: 0.144 [-1.161, 1.000], loss: 1.820192, mean_absolute_error: 32.347600, mean_q: 26.880699, mean_eps: 0.436249
  627890/2000000: episode: 4911, duration: 16.483s, episode steps: 1000, steps per second: 61, episode reward: 40.456, mean reward: 0.040 [-22.422, 22.770], mean action: 1.187 [0.000, 3.000], mean observation: 0.122 [-0.704, 1.000], loss: 1.751380, mean_absolute_error: 32.457460, mean_q: 27.166392, mean_eps: 0.435349
  628007/2000000: episode: 4912, duration: 1.712s, episode steps: 117, steps per second: 68, episode reward: -27.772, mean reward: -0.237 [-100.000, 12.025], mean action: 1.718 [0.000, 3.000], mean observation: 0.102 [-1.451, 1.000], loss: 1.757409, mean_absolute_error: 31.554622, mean_q: 27.148730, mean_eps: 0.434847
  628378/2000000: episode: 4913, duration: 5.543s, episode steps: 371, steps per second: 67, episode reward: -217.785, mean reward: -0.587 [-100.000, 18.734], mean action: 1.585 [0.000, 3.000], mean observation: 0.129 [-1.086, 1.003], loss: 1.765234, mean_absolute_error: 31.799837, mean_q: 26.340039, mean_eps: 0.434627
  628567/2000000: episode: 4914, duration: 2.788s, episode steps: 189, steps per second: 68, episode reward: -98.866, mean reward: -0.523 [-100.000, 15.701], mean action: 1.841 [0.000, 3.000], mean observation: -0.015 [-1.464, 1.000], loss: 1.892514, mean_absolute_error: 32.379885, mean_q: 26.723069, mean_eps: 0.434375
  629567/2000000: episode: 4915, duration: 15.522s, episode steps: 1000, steps per second: 64, episode reward: 104.641, mean reward: 0.105 [-20.680, 22.978], mean action: 1.115 [0.000, 3.000], mean observation: 0.178 [-0.631, 1.538], loss: 1.856481, mean_absolute_error: 32.543421, mean_q: 25.629146, mean_eps: 0.433841
  630567/2000000: episode: 4916, duration: 16.017s, episode steps: 1000, steps per second: 62, episode reward: 3.520, mean reward: 0.004 [-24.790, 23.944], mean action: 1.074 [0.000, 3.000], mean observation: 0.105 [-1.357, 1.000], loss: 1.892523, mean_absolute_error: 32.410094, mean_q: 27.003394, mean_eps: 0.432941
  630772/2000000: episode: 4917, duration: 3.085s, episode steps: 205, steps per second: 66, episode reward: -142.683, mean reward: -0.696 [-100.000, 8.643], mean action: 1.776 [0.000, 3.000], mean observation: -0.118 [-1.127, 1.000], loss: 2.135792, mean_absolute_error: 31.888599, mean_q: 25.607093, mean_eps: 0.432399
  631772/2000000: episode: 4918, duration: 15.662s, episode steps: 1000, steps per second: 64, episode reward: 59.970, mean reward: 0.060 [-24.088, 23.209], mean action: 1.315 [0.000, 3.000], mean observation: 0.182 [-0.657, 1.000], loss: 1.761646, mean_absolute_error: 32.150273, mean_q: 26.471971, mean_eps: 0.431857
  631904/2000000: episode: 4919, duration: 2.024s, episode steps: 132, steps per second: 65, episode reward: -37.992, mean reward: -0.288 [-100.000, 16.945], mean action: 1.742 [0.000, 3.000], mean observation: 0.029 [-0.804, 1.000], loss: 2.032200, mean_absolute_error: 32.223223, mean_q: 27.517282, mean_eps: 0.431348
  632549/2000000: episode: 4920, duration: 10.105s, episode steps: 645, steps per second: 64, episode reward: -291.829, mean reward: -0.452 [-100.000, 22.558], mean action: 1.436 [0.000, 3.000], mean observation: 0.144 [-1.452, 1.056], loss: 1.706900, mean_absolute_error: 32.218887, mean_q: 26.270271, mean_eps: 0.430997
  632738/2000000: episode: 4921, duration: 2.764s, episode steps: 189, steps per second: 68, episode reward: -37.676, mean reward: -0.199 [-100.000, 17.603], mean action: 1.720 [0.000, 3.000], mean observation: 0.021 [-1.176, 1.000], loss: 1.657975, mean_absolute_error: 32.837204, mean_q: 28.556215, mean_eps: 0.430620
  633738/2000000: episode: 4922, duration: 15.231s, episode steps: 1000, steps per second: 66, episode reward: 62.713, mean reward: 0.063 [-20.516, 22.696], mean action: 1.020 [0.000, 3.000], mean observation: 0.171 [-0.984, 1.000], loss: 1.985452, mean_absolute_error: 32.411771, mean_q: 26.680867, mean_eps: 0.430086
  633861/2000000: episode: 4923, duration: 1.848s, episode steps: 123, steps per second: 67, episode reward: -78.752, mean reward: -0.640 [-100.000, 19.200], mean action: 1.780 [0.000, 3.000], mean observation: -0.017 [-0.948, 1.504], loss: 1.590096, mean_absolute_error: 32.143062, mean_q: 25.425668, mean_eps: 0.429580
  633952/2000000: episode: 4924, duration: 1.332s, episode steps: 91, steps per second: 68, episode reward: -34.936, mean reward: -0.384 [-100.000, 20.221], mean action: 1.714 [0.000, 3.000], mean observation: -0.063 [-0.910, 1.026], loss: 2.024555, mean_absolute_error: 32.118267, mean_q: 25.670216, mean_eps: 0.429485
  634952/2000000: episode: 4925, duration: 15.936s, episode steps: 1000, steps per second: 63, episode reward: 28.465, mean reward: 0.028 [-24.404, 37.553], mean action: 1.241 [0.000, 3.000], mean observation: 0.265 [-0.747, 1.000], loss: 1.823186, mean_absolute_error: 31.814386, mean_q: 26.937660, mean_eps: 0.428995
  635114/2000000: episode: 4926, duration: 2.468s, episode steps: 162, steps per second: 66, episode reward: -43.033, mean reward: -0.266 [-100.000, 13.013], mean action: 1.765 [0.000, 3.000], mean observation: 0.077 [-0.671, 1.000], loss: 2.473462, mean_absolute_error: 32.747688, mean_q: 26.678441, mean_eps: 0.428471
  636114/2000000: episode: 4927, duration: 15.276s, episode steps: 1000, steps per second: 65, episode reward: 91.078, mean reward: 0.091 [-20.725, 23.025], mean action: 1.076 [0.000, 3.000], mean observation: 0.142 [-1.169, 1.000], loss: 1.900233, mean_absolute_error: 32.080706, mean_q: 27.045270, mean_eps: 0.427947
  636331/2000000: episode: 4928, duration: 3.218s, episode steps: 217, steps per second: 67, episode reward: -33.308, mean reward: -0.153 [-100.000, 24.388], mean action: 1.788 [0.000, 3.000], mean observation: 0.039 [-0.530, 1.000], loss: 2.029263, mean_absolute_error: 32.173939, mean_q: 25.974807, mean_eps: 0.427400
  636481/2000000: episode: 4929, duration: 2.253s, episode steps: 150, steps per second: 67, episode reward: -56.446, mean reward: -0.376 [-100.000, 12.567], mean action: 1.800 [0.000, 3.000], mean observation: -0.041 [-0.824, 1.031], loss: 2.091092, mean_absolute_error: 31.700070, mean_q: 24.903368, mean_eps: 0.427235
  637481/2000000: episode: 4930, duration: 16.667s, episode steps: 1000, steps per second: 60, episode reward: 47.154, mean reward: 0.047 [-22.270, 23.809], mean action: 1.253 [0.000, 3.000], mean observation: 0.201 [-0.520, 1.000], loss: 1.766974, mean_absolute_error: 31.923167, mean_q: 26.360991, mean_eps: 0.426716
  637632/2000000: episode: 4931, duration: 2.252s, episode steps: 151, steps per second: 67, episode reward: -55.101, mean reward: -0.365 [-100.000, 17.549], mean action: 1.589 [0.000, 3.000], mean observation: 0.010 [-1.619, 1.000], loss: 1.643661, mean_absolute_error: 31.697558, mean_q: 26.119252, mean_eps: 0.426200
  637791/2000000: episode: 4932, duration: 2.358s, episode steps: 159, steps per second: 67, episode reward: -70.384, mean reward: -0.443 [-100.000, 19.785], mean action: 1.686 [0.000, 3.000], mean observation: 0.020 [-0.599, 1.159], loss: 1.959310, mean_absolute_error: 32.054661, mean_q: 27.127870, mean_eps: 0.426061
  637894/2000000: episode: 4933, duration: 1.540s, episode steps: 103, steps per second: 67, episode reward: -139.595, mean reward: -1.355 [-100.000, 13.533], mean action: 1.583 [0.000, 3.000], mean observation: 0.023 [-0.920, 1.000], loss: 1.940653, mean_absolute_error: 31.673424, mean_q: 23.436194, mean_eps: 0.425942
  638209/2000000: episode: 4934, duration: 4.989s, episode steps: 315, steps per second: 63, episode reward: -232.374, mean reward: -0.738 [-100.000, 4.898], mean action: 1.778 [0.000, 3.000], mean observation: -0.064 [-3.116, 0.946], loss: 1.896115, mean_absolute_error: 32.637395, mean_q: 26.865357, mean_eps: 0.425753
  638438/2000000: episode: 4935, duration: 3.385s, episode steps: 229, steps per second: 68, episode reward: -197.184, mean reward: -0.861 [-100.000, 8.725], mean action: 1.624 [0.000, 3.000], mean observation: 0.141 [-0.667, 1.004], loss: 1.428094, mean_absolute_error: 31.873866, mean_q: 28.606282, mean_eps: 0.425508
  638583/2000000: episode: 4936, duration: 2.125s, episode steps: 145, steps per second: 68, episode reward: -16.116, mean reward: -0.111 [-100.000, 16.855], mean action: 1.759 [0.000, 3.000], mean observation: 0.015 [-0.740, 1.378], loss: 1.822041, mean_absolute_error: 32.032493, mean_q: 26.718841, mean_eps: 0.425341
  638803/2000000: episode: 4937, duration: 3.441s, episode steps: 220, steps per second: 64, episode reward: -119.233, mean reward: -0.542 [-100.000, 14.117], mean action: 1.905 [0.000, 3.000], mean observation: -0.040 [-1.123, 1.000], loss: 2.238937, mean_absolute_error: 32.590131, mean_q: 26.639958, mean_eps: 0.425177
  638940/2000000: episode: 4938, duration: 2.054s, episode steps: 137, steps per second: 67, episode reward: -79.570, mean reward: -0.581 [-100.000, 14.756], mean action: 1.818 [0.000, 3.000], mean observation: 0.010 [-0.697, 1.027], loss: 1.865670, mean_absolute_error: 31.196114, mean_q: 27.331897, mean_eps: 0.425017
  639056/2000000: episode: 4939, duration: 1.766s, episode steps: 116, steps per second: 66, episode reward: -106.893, mean reward: -0.921 [-100.000, 12.545], mean action: 1.707 [0.000, 3.000], mean observation: -0.075 [-1.098, 3.260], loss: 1.715025, mean_absolute_error: 31.072556, mean_q: 25.555719, mean_eps: 0.424904
  639214/2000000: episode: 4940, duration: 2.347s, episode steps: 158, steps per second: 67, episode reward: -86.977, mean reward: -0.550 [-100.000, 9.113], mean action: 1.576 [0.000, 3.000], mean observation: 0.023 [-1.347, 1.000], loss: 1.735249, mean_absolute_error: 31.438463, mean_q: 26.429482, mean_eps: 0.424779
  639319/2000000: episode: 4941, duration: 1.569s, episode steps: 105, steps per second: 67, episode reward: -57.106, mean reward: -0.544 [-100.000, 14.920], mean action: 1.676 [0.000, 3.000], mean observation: 0.143 [-0.768, 1.169], loss: 1.909013, mean_absolute_error: 31.889545, mean_q: 23.865400, mean_eps: 0.424661
  640319/2000000: episode: 4942, duration: 15.514s, episode steps: 1000, steps per second: 64, episode reward: 83.366, mean reward: 0.083 [-22.538, 22.796], mean action: 1.075 [0.000, 3.000], mean observation: 0.135 [-1.076, 1.385], loss: 1.770107, mean_absolute_error: 32.131283, mean_q: 26.345014, mean_eps: 0.424164
  640446/2000000: episode: 4943, duration: 1.871s, episode steps: 127, steps per second: 68, episode reward: -237.603, mean reward: -1.871 [-100.000, 57.000], mean action: 1.835 [0.000, 3.000], mean observation: 0.140 [-0.838, 1.945], loss: 1.434230, mean_absolute_error: 31.019046, mean_q: 25.869306, mean_eps: 0.423656
  640563/2000000: episode: 4944, duration: 1.720s, episode steps: 117, steps per second: 68, episode reward: -15.435, mean reward: -0.132 [-100.000, 19.012], mean action: 1.684 [0.000, 3.000], mean observation: -0.062 [-0.765, 1.188], loss: 1.800126, mean_absolute_error: 31.390932, mean_q: 26.551632, mean_eps: 0.423546
  640778/2000000: episode: 4945, duration: 3.179s, episode steps: 215, steps per second: 68, episode reward: -240.364, mean reward: -1.118 [-100.000, 23.813], mean action: 1.521 [0.000, 3.000], mean observation: 0.059 [-1.347, 1.131], loss: 1.600608, mean_absolute_error: 31.954602, mean_q: 27.671159, mean_eps: 0.423397
  641778/2000000: episode: 4946, duration: 16.045s, episode steps: 1000, steps per second: 62, episode reward: -66.740, mean reward: -0.067 [-24.414, 23.641], mean action: 1.072 [0.000, 3.000], mean observation: 0.056 [-1.130, 1.000], loss: 1.832856, mean_absolute_error: 31.840491, mean_q: 26.265289, mean_eps: 0.422850
  641952/2000000: episode: 4947, duration: 2.621s, episode steps: 174, steps per second: 66, episode reward: -165.039, mean reward: -0.948 [-100.000, 7.852], mean action: 1.701 [0.000, 3.000], mean observation: -0.101 [-1.006, 1.023], loss: 1.640756, mean_absolute_error: 31.510106, mean_q: 28.422492, mean_eps: 0.422322
  642952/2000000: episode: 4948, duration: 16.837s, episode steps: 1000, steps per second: 59, episode reward: 59.296, mean reward: 0.059 [-23.421, 26.747], mean action: 1.206 [0.000, 3.000], mean observation: 0.141 [-0.549, 1.000], loss: 1.786980, mean_absolute_error: 32.139466, mean_q: 25.474803, mean_eps: 0.421795
  643171/2000000: episode: 4949, duration: 3.240s, episode steps: 219, steps per second: 68, episode reward: -183.004, mean reward: -0.836 [-100.000, 9.678], mean action: 1.781 [0.000, 3.000], mean observation: -0.079 [-1.002, 1.250], loss: 1.898331, mean_absolute_error: 31.634525, mean_q: 26.005620, mean_eps: 0.421246
  644171/2000000: episode: 4950, duration: 15.135s, episode steps: 1000, steps per second: 66, episode reward: 81.476, mean reward: 0.081 [-23.267, 23.405], mean action: 0.865 [0.000, 3.000], mean observation: 0.184 [-1.272, 1.000], loss: 1.762753, mean_absolute_error: 32.302376, mean_q: 26.678455, mean_eps: 0.420697
  645171/2000000: episode: 4951, duration: 15.978s, episode steps: 1000, steps per second: 63, episode reward: 28.804, mean reward: 0.029 [-24.184, 23.142], mean action: 1.053 [0.000, 3.000], mean observation: 0.236 [-0.862, 1.000], loss: 1.930171, mean_absolute_error: 32.106309, mean_q: 26.231989, mean_eps: 0.419797
  646171/2000000: episode: 4952, duration: 15.456s, episode steps: 1000, steps per second: 65, episode reward: 38.771, mean reward: 0.039 [-23.929, 24.804], mean action: 1.343 [0.000, 3.000], mean observation: 0.245 [-0.799, 1.196], loss: 1.901597, mean_absolute_error: 31.660766, mean_q: 26.511006, mean_eps: 0.418897
  647171/2000000: episode: 4953, duration: 15.557s, episode steps: 1000, steps per second: 64, episode reward: -47.671, mean reward: -0.048 [-24.755, 23.943], mean action: 1.340 [0.000, 3.000], mean observation: 0.055 [-1.177, 1.000], loss: 1.786036, mean_absolute_error: 31.676057, mean_q: 26.765158, mean_eps: 0.417997
  648171/2000000: episode: 4954, duration: 15.723s, episode steps: 1000, steps per second: 64, episode reward: 82.697, mean reward: 0.083 [-20.531, 23.387], mean action: 0.862 [0.000, 3.000], mean observation: 0.197 [-0.674, 1.000], loss: 1.746268, mean_absolute_error: 31.561293, mean_q: 25.697188, mean_eps: 0.417097
  648272/2000000: episode: 4955, duration: 1.520s, episode steps: 101, steps per second: 66, episode reward: -73.515, mean reward: -0.728 [-100.000, 13.498], mean action: 1.842 [0.000, 3.000], mean observation: -0.132 [-0.828, 1.889], loss: 1.453126, mean_absolute_error: 31.341629, mean_q: 27.347101, mean_eps: 0.416602
  648404/2000000: episode: 4956, duration: 1.992s, episode steps: 132, steps per second: 66, episode reward: -54.291, mean reward: -0.411 [-100.000, 11.668], mean action: 1.659 [0.000, 3.000], mean observation: -0.038 [-1.384, 1.000], loss: 1.768881, mean_absolute_error: 31.580047, mean_q: 27.861812, mean_eps: 0.416498
  649404/2000000: episode: 4957, duration: 15.398s, episode steps: 1000, steps per second: 65, episode reward: 22.346, mean reward: 0.022 [-22.723, 23.427], mean action: 0.993 [0.000, 3.000], mean observation: 0.057 [-0.851, 1.000], loss: 1.695550, mean_absolute_error: 31.444825, mean_q: 26.301040, mean_eps: 0.415988
  650404/2000000: episode: 4958, duration: 15.990s, episode steps: 1000, steps per second: 63, episode reward: -18.017, mean reward: -0.018 [-22.972, 24.093], mean action: 1.752 [0.000, 3.000], mean observation: 0.248 [-0.913, 1.126], loss: 1.635724, mean_absolute_error: 31.189865, mean_q: 26.712391, mean_eps: 0.415088
  651404/2000000: episode: 4959, duration: 16.597s, episode steps: 1000, steps per second: 60, episode reward: 4.454, mean reward: 0.004 [-19.863, 24.210], mean action: 2.025 [0.000, 3.000], mean observation: 0.226 [-1.244, 1.040], loss: 1.780083, mean_absolute_error: 31.774451, mean_q: 26.450528, mean_eps: 0.414188
  652404/2000000: episode: 4960, duration: 16.018s, episode steps: 1000, steps per second: 62, episode reward: 4.031, mean reward: 0.004 [-21.358, 22.576], mean action: 1.037 [0.000, 3.000], mean observation: 0.177 [-0.645, 1.008], loss: 1.833523, mean_absolute_error: 31.797163, mean_q: 25.725214, mean_eps: 0.413288
  652525/2000000: episode: 4961, duration: 1.849s, episode steps: 121, steps per second: 65, episode reward: 4.577, mean reward: 0.038 [-100.000, 20.091], mean action: 1.802 [0.000, 3.000], mean observation: -0.022 [-1.133, 1.000], loss: 1.610341, mean_absolute_error: 31.340829, mean_q: 25.738791, mean_eps: 0.412782
  653525/2000000: episode: 4962, duration: 15.299s, episode steps: 1000, steps per second: 65, episode reward: 59.385, mean reward: 0.059 [-23.105, 23.175], mean action: 0.969 [0.000, 3.000], mean observation: 0.182 [-0.751, 1.396], loss: 1.802668, mean_absolute_error: 31.361272, mean_q: 26.886133, mean_eps: 0.412277
  654525/2000000: episode: 4963, duration: 16.257s, episode steps: 1000, steps per second: 62, episode reward: 45.251, mean reward: 0.045 [-23.550, 22.948], mean action: 1.220 [0.000, 3.000], mean observation: 0.130 [-0.694, 1.000], loss: 1.815568, mean_absolute_error: 31.568624, mean_q: 26.479662, mean_eps: 0.411377
  655525/2000000: episode: 4964, duration: 16.590s, episode steps: 1000, steps per second: 60, episode reward: 61.078, mean reward: 0.061 [-23.778, 22.566], mean action: 1.473 [0.000, 3.000], mean observation: 0.260 [-0.493, 1.017], loss: 1.935546, mean_absolute_error: 31.323056, mean_q: 27.196648, mean_eps: 0.410477
  655686/2000000: episode: 4965, duration: 2.643s, episode steps: 161, steps per second: 61, episode reward: -84.259, mean reward: -0.523 [-100.000, 40.405], mean action: 1.814 [0.000, 3.000], mean observation: -0.095 [-1.034, 1.130], loss: 2.138483, mean_absolute_error: 31.202992, mean_q: 25.457035, mean_eps: 0.409955
  656686/2000000: episode: 4966, duration: 15.337s, episode steps: 1000, steps per second: 65, episode reward: 63.190, mean reward: 0.063 [-19.902, 23.068], mean action: 1.081 [0.000, 3.000], mean observation: 0.198 [-0.633, 1.000], loss: 1.726237, mean_absolute_error: 31.402719, mean_q: 27.069175, mean_eps: 0.409433
  656825/2000000: episode: 4967, duration: 2.071s, episode steps: 139, steps per second: 67, episode reward: -9.446, mean reward: -0.068 [-100.000, 15.033], mean action: 1.655 [0.000, 3.000], mean observation: 0.035 [-0.738, 1.000], loss: 1.565906, mean_absolute_error: 31.037373, mean_q: 26.821424, mean_eps: 0.408920
  657825/2000000: episode: 4968, duration: 15.788s, episode steps: 1000, steps per second: 63, episode reward: 64.024, mean reward: 0.064 [-22.104, 22.629], mean action: 0.993 [0.000, 3.000], mean observation: 0.172 [-0.526, 1.201], loss: 1.860625, mean_absolute_error: 31.431894, mean_q: 25.695876, mean_eps: 0.408407
  658036/2000000: episode: 4969, duration: 3.167s, episode steps: 211, steps per second: 67, episode reward: -33.730, mean reward: -0.160 [-100.000, 20.278], mean action: 1.872 [0.000, 3.000], mean observation: 0.041 [-1.490, 1.000], loss: 1.587875, mean_absolute_error: 31.857951, mean_q: 26.640014, mean_eps: 0.407863
  658138/2000000: episode: 4970, duration: 1.538s, episode steps: 102, steps per second: 66, episode reward: -85.319, mean reward: -0.836 [-100.000, 7.910], mean action: 1.696 [0.000, 3.000], mean observation: 0.055 [-3.215, 1.000], loss: 1.510562, mean_absolute_error: 31.059518, mean_q: 25.305908, mean_eps: 0.407723
  658271/2000000: episode: 4971, duration: 1.960s, episode steps: 133, steps per second: 68, episode reward: -162.873, mean reward: -1.225 [-100.000, 6.106], mean action: 1.782 [0.000, 3.000], mean observation: 0.051 [-1.269, 1.000], loss: 1.534466, mean_absolute_error: 31.228279, mean_q: 25.664361, mean_eps: 0.407616
  659271/2000000: episode: 4972, duration: 15.710s, episode steps: 1000, steps per second: 64, episode reward: 3.126, mean reward: 0.003 [-22.344, 24.209], mean action: 1.154 [0.000, 3.000], mean observation: 0.064 [-1.251, 1.000], loss: 1.750252, mean_absolute_error: 30.828408, mean_q: 27.238672, mean_eps: 0.407107
  659396/2000000: episode: 4973, duration: 1.919s, episode steps: 125, steps per second: 65, episode reward: -76.946, mean reward: -0.616 [-100.000, 7.538], mean action: 1.784 [0.000, 3.000], mean observation: 0.136 [-0.670, 1.483], loss: 1.333056, mean_absolute_error: 31.294721, mean_q: 25.585436, mean_eps: 0.406601
  659775/2000000: episode: 4974, duration: 5.789s, episode steps: 379, steps per second: 65, episode reward: -246.781, mean reward: -0.651 [-100.000, 13.184], mean action: 1.483 [0.000, 3.000], mean observation: 0.008 [-1.002, 1.000], loss: 1.654439, mean_absolute_error: 30.716409, mean_q: 27.040666, mean_eps: 0.406374
  660018/2000000: episode: 4975, duration: 3.673s, episode steps: 243, steps per second: 66, episode reward: -276.280, mean reward: -1.137 [-100.000, 9.339], mean action: 1.782 [0.000, 3.000], mean observation: 0.106 [-0.612, 2.013], loss: 1.550078, mean_absolute_error: 31.311716, mean_q: 28.386115, mean_eps: 0.406094
  660132/2000000: episode: 4976, duration: 1.721s, episode steps: 114, steps per second: 66, episode reward: -93.052, mean reward: -0.816 [-100.000, 9.094], mean action: 1.737 [0.000, 3.000], mean observation: -0.061 [-0.764, 2.640], loss: 1.651560, mean_absolute_error: 30.280861, mean_q: 26.033486, mean_eps: 0.405933
  660265/2000000: episode: 4977, duration: 2.006s, episode steps: 133, steps per second: 66, episode reward: -43.067, mean reward: -0.324 [-100.000, 28.049], mean action: 1.662 [0.000, 3.000], mean observation: 0.101 [-1.615, 1.000], loss: 1.261972, mean_absolute_error: 31.487724, mean_q: 27.842675, mean_eps: 0.405822
  660421/2000000: episode: 4978, duration: 2.322s, episode steps: 156, steps per second: 67, episode reward: -51.383, mean reward: -0.329 [-100.000, 15.761], mean action: 1.731 [0.000, 3.000], mean observation: 0.015 [-0.678, 1.428], loss: 1.763533, mean_absolute_error: 30.148466, mean_q: 26.531990, mean_eps: 0.405690
  660573/2000000: episode: 4979, duration: 2.260s, episode steps: 152, steps per second: 67, episode reward: -253.424, mean reward: -1.667 [-100.000, 50.561], mean action: 1.730 [0.000, 3.000], mean observation: -0.111 [-2.052, 1.013], loss: 2.010135, mean_absolute_error: 32.215652, mean_q: 28.067457, mean_eps: 0.405552
  661573/2000000: episode: 4980, duration: 16.748s, episode steps: 1000, steps per second: 60, episode reward: 55.073, mean reward: 0.055 [-18.888, 12.476], mean action: 1.281 [0.000, 3.000], mean observation: 0.160 [-0.369, 1.097], loss: 1.865386, mean_absolute_error: 31.168546, mean_q: 26.074020, mean_eps: 0.405033
  662573/2000000: episode: 4981, duration: 16.468s, episode steps: 1000, steps per second: 61, episode reward: 75.223, mean reward: 0.075 [-23.641, 22.801], mean action: 1.181 [0.000, 3.000], mean observation: 0.236 [-0.791, 1.000], loss: 1.843164, mean_absolute_error: 30.977368, mean_q: 26.590225, mean_eps: 0.404133
  662682/2000000: episode: 4982, duration: 1.628s, episode steps: 109, steps per second: 67, episode reward: -72.530, mean reward: -0.665 [-100.000, 20.207], mean action: 1.771 [0.000, 3.000], mean observation: -0.005 [-0.864, 1.947], loss: 1.615095, mean_absolute_error: 31.171584, mean_q: 26.964543, mean_eps: 0.403635
  663682/2000000: episode: 4983, duration: 16.320s, episode steps: 1000, steps per second: 61, episode reward: 33.622, mean reward: 0.034 [-20.396, 46.115], mean action: 0.977 [0.000, 3.000], mean observation: 0.196 [-0.703, 1.063], loss: 1.681135, mean_absolute_error: 30.726219, mean_q: 26.841073, mean_eps: 0.403136
  664105/2000000: episode: 4984, duration: 6.700s, episode steps: 423, steps per second: 63, episode reward: -148.521, mean reward: -0.351 [-100.000, 31.623], mean action: 1.716 [0.000, 3.000], mean observation: -0.026 [-0.970, 1.000], loss: 2.028874, mean_absolute_error: 30.507300, mean_q: 26.770139, mean_eps: 0.402495
  665105/2000000: episode: 4985, duration: 16.244s, episode steps: 1000, steps per second: 62, episode reward: 67.163, mean reward: 0.067 [-23.758, 23.115], mean action: 0.841 [0.000, 3.000], mean observation: 0.173 [-0.781, 1.109], loss: 1.839333, mean_absolute_error: 30.946791, mean_q: 26.737198, mean_eps: 0.401855
  665308/2000000: episode: 4986, duration: 3.547s, episode steps: 203, steps per second: 57, episode reward: -84.673, mean reward: -0.417 [-100.000, 17.948], mean action: 1.532 [0.000, 3.000], mean observation: 0.009 [-0.838, 2.524], loss: 1.763006, mean_absolute_error: 30.835494, mean_q: 28.175753, mean_eps: 0.401315
  665621/2000000: episode: 4987, duration: 5.043s, episode steps: 313, steps per second: 62, episode reward: -33.690, mean reward: -0.108 [-100.000, 18.332], mean action: 1.770 [0.000, 3.000], mean observation: 0.028 [-1.799, 1.004], loss: 1.627980, mean_absolute_error: 30.658432, mean_q: 27.853651, mean_eps: 0.401082
  665772/2000000: episode: 4988, duration: 2.302s, episode steps: 151, steps per second: 66, episode reward: -140.254, mean reward: -0.929 [-100.000, 4.312], mean action: 1.556 [0.000, 3.000], mean observation: 0.124 [-0.670, 1.000], loss: 1.732634, mean_absolute_error: 30.308300, mean_q: 26.710945, mean_eps: 0.400874
  665973/2000000: episode: 4989, duration: 3.303s, episode steps: 201, steps per second: 61, episode reward: -230.400, mean reward: -1.146 [-100.000, 5.232], mean action: 1.582 [0.000, 3.000], mean observation: 0.159 [-1.156, 1.031], loss: 1.940490, mean_absolute_error: 31.361977, mean_q: 25.959506, mean_eps: 0.400715
  666973/2000000: episode: 4990, duration: 16.262s, episode steps: 1000, steps per second: 61, episode reward: -6.745, mean reward: -0.007 [-22.462, 23.142], mean action: 1.337 [0.000, 3.000], mean observation: 0.037 [-0.878, 1.008], loss: 1.639931, mean_absolute_error: 30.701950, mean_q: 26.512610, mean_eps: 0.400173
  667973/2000000: episode: 4991, duration: 16.170s, episode steps: 1000, steps per second: 62, episode reward: 36.967, mean reward: 0.037 [-24.442, 24.553], mean action: 1.449 [0.000, 3.000], mean observation: 0.264 [-1.091, 1.000], loss: 1.709496, mean_absolute_error: 30.413834, mean_q: 26.178709, mean_eps: 0.399273
  668187/2000000: episode: 4992, duration: 3.274s, episode steps: 214, steps per second: 65, episode reward: -35.845, mean reward: -0.167 [-100.000, 25.250], mean action: 1.785 [0.000, 3.000], mean observation: 0.032 [-0.708, 1.709], loss: 1.688456, mean_absolute_error: 31.139408, mean_q: 26.796893, mean_eps: 0.398728
  668412/2000000: episode: 4993, duration: 3.707s, episode steps: 225, steps per second: 61, episode reward: -7.439, mean reward: -0.033 [-100.000, 15.391], mean action: 1.716 [0.000, 3.000], mean observation: -0.005 [-0.650, 1.060], loss: 1.772014, mean_absolute_error: 30.457341, mean_q: 25.961798, mean_eps: 0.398532
  669412/2000000: episode: 4994, duration: 16.259s, episode steps: 1000, steps per second: 62, episode reward: 80.176, mean reward: 0.080 [-22.122, 23.353], mean action: 1.018 [0.000, 3.000], mean observation: 0.194 [-0.723, 1.000], loss: 1.764768, mean_absolute_error: 30.213355, mean_q: 27.236100, mean_eps: 0.397981
  670412/2000000: episode: 4995, duration: 16.062s, episode steps: 1000, steps per second: 62, episode reward: 23.892, mean reward: 0.024 [-22.688, 23.345], mean action: 0.956 [0.000, 3.000], mean observation: 0.145 [-1.494, 1.000], loss: 1.789221, mean_absolute_error: 30.541479, mean_q: 27.336774, mean_eps: 0.397081
  671412/2000000: episode: 4996, duration: 16.219s, episode steps: 1000, steps per second: 62, episode reward: 88.001, mean reward: 0.088 [-21.136, 33.448], mean action: 0.918 [0.000, 3.000], mean observation: 0.151 [-1.127, 1.000], loss: 1.760714, mean_absolute_error: 30.390653, mean_q: 27.535149, mean_eps: 0.396181
  672412/2000000: episode: 4997, duration: 16.053s, episode steps: 1000, steps per second: 62, episode reward: 97.260, mean reward: 0.097 [-23.807, 23.019], mean action: 0.814 [0.000, 3.000], mean observation: 0.165 [-0.815, 1.000], loss: 1.716778, mean_absolute_error: 30.865101, mean_q: 28.703915, mean_eps: 0.395281
  672550/2000000: episode: 4998, duration: 2.138s, episode steps: 138, steps per second: 65, episode reward: -105.839, mean reward: -0.767 [-100.000, 15.965], mean action: 1.638 [0.000, 3.000], mean observation: 0.035 [-0.745, 1.937], loss: 2.350045, mean_absolute_error: 30.796289, mean_q: 26.931401, mean_eps: 0.394768
  672655/2000000: episode: 4999, duration: 1.624s, episode steps: 105, steps per second: 65, episode reward: 16.928, mean reward: 0.161 [-100.000, 14.170], mean action: 1.781 [0.000, 3.000], mean observation: -0.086 [-0.931, 1.630], loss: 1.628763, mean_absolute_error: 30.618353, mean_q: 27.533923, mean_eps: 0.394658
  672870/2000000: episode: 5000, duration: 3.337s, episode steps: 215, steps per second: 64, episode reward: -58.818, mean reward: -0.274 [-100.000, 24.011], mean action: 1.800 [0.000, 3.000], mean observation: 0.040 [-1.968, 1.000], loss: 1.705603, mean_absolute_error: 30.966395, mean_q: 28.421659, mean_eps: 0.394514
  672970/2000000: episode: 5001, duration: 1.541s, episode steps: 100, steps per second: 65, episode reward: -43.485, mean reward: -0.435 [-100.000, 11.381], mean action: 1.610 [0.000, 3.000], mean observation: -0.009 [-0.799, 1.024], loss: 1.816373, mean_absolute_error: 30.922335, mean_q: 29.318241, mean_eps: 0.394372
  673206/2000000: episode: 5002, duration: 3.637s, episode steps: 236, steps per second: 65, episode reward: -78.626, mean reward: -0.333 [-100.000, 12.311], mean action: 1.792 [0.000, 3.000], mean observation: -0.015 [-0.502, 1.000], loss: 1.887224, mean_absolute_error: 30.377411, mean_q: 28.135226, mean_eps: 0.394221
  674206/2000000: episode: 5003, duration: 16.656s, episode steps: 1000, steps per second: 60, episode reward: 27.670, mean reward: 0.028 [-23.226, 22.432], mean action: 1.084 [0.000, 3.000], mean observation: 0.080 [-0.987, 1.000], loss: 1.710972, mean_absolute_error: 30.675490, mean_q: 27.573482, mean_eps: 0.393665
  674310/2000000: episode: 5004, duration: 1.609s, episode steps: 104, steps per second: 65, episode reward: 1.613, mean reward: 0.016 [-100.000, 20.557], mean action: 1.721 [0.000, 3.000], mean observation: 0.077 [-1.222, 1.000], loss: 1.541948, mean_absolute_error: 30.395847, mean_q: 27.852930, mean_eps: 0.393168
  675310/2000000: episode: 5005, duration: 16.354s, episode steps: 1000, steps per second: 61, episode reward: -7.570, mean reward: -0.008 [-23.129, 33.644], mean action: 1.926 [0.000, 3.000], mean observation: 0.260 [-1.185, 1.000], loss: 1.797067, mean_absolute_error: 30.708012, mean_q: 28.437615, mean_eps: 0.392671
  675442/2000000: episode: 5006, duration: 2.027s, episode steps: 132, steps per second: 65, episode reward: -98.163, mean reward: -0.744 [-100.000, 24.016], mean action: 1.795 [0.000, 3.000], mean observation: 0.143 [-0.651, 1.000], loss: 1.579438, mean_absolute_error: 30.969949, mean_q: 28.239171, mean_eps: 0.392162
  676442/2000000: episode: 5007, duration: 16.598s, episode steps: 1000, steps per second: 60, episode reward: -14.144, mean reward: -0.014 [-23.770, 22.968], mean action: 1.017 [0.000, 3.000], mean observation: 0.096 [-0.695, 1.000], loss: 1.697032, mean_absolute_error: 30.218861, mean_q: 27.742221, mean_eps: 0.391652
  676525/2000000: episode: 5008, duration: 1.305s, episode steps: 83, steps per second: 64, episode reward: -60.025, mean reward: -0.723 [-100.000, 8.472], mean action: 1.783 [0.000, 3.000], mean observation: -0.010 [-2.652, 1.000], loss: 1.791347, mean_absolute_error: 31.000166, mean_q: 31.035609, mean_eps: 0.391164
  676616/2000000: episode: 5009, duration: 1.395s, episode steps: 91, steps per second: 65, episode reward: -104.760, mean reward: -1.151 [-100.000, 24.587], mean action: 1.813 [0.000, 3.000], mean observation: -0.082 [-0.948, 2.734], loss: 1.644343, mean_absolute_error: 30.981599, mean_q: 30.886517, mean_eps: 0.391087
  676824/2000000: episode: 5010, duration: 3.218s, episode steps: 208, steps per second: 65, episode reward: -236.468, mean reward: -1.137 [-100.000, 9.370], mean action: 1.764 [0.000, 3.000], mean observation: 0.004 [-0.687, 1.904], loss: 1.755636, mean_absolute_error: 30.248008, mean_q: 27.526309, mean_eps: 0.390954
  677067/2000000: episode: 5011, duration: 3.759s, episode steps: 243, steps per second: 65, episode reward: -255.551, mean reward: -1.052 [-100.000, 19.531], mean action: 2.078 [0.000, 3.000], mean observation: -0.012 [-0.924, 1.704], loss: 1.732479, mean_absolute_error: 30.621558, mean_q: 28.058695, mean_eps: 0.390750
  677167/2000000: episode: 5012, duration: 1.538s, episode steps: 100, steps per second: 65, episode reward: -50.801, mean reward: -0.508 [-100.000, 17.493], mean action: 1.760 [0.000, 3.000], mean observation: -0.048 [-0.806, 2.558], loss: 1.481050, mean_absolute_error: 29.585436, mean_q: 27.125069, mean_eps: 0.390596
  678167/2000000: episode: 5013, duration: 16.157s, episode steps: 1000, steps per second: 62, episode reward: 9.715, mean reward: 0.010 [-21.877, 24.189], mean action: 1.719 [0.000, 3.000], mean observation: 0.262 [-0.731, 1.327], loss: 1.890114, mean_absolute_error: 30.234281, mean_q: 26.717606, mean_eps: 0.390101
  678370/2000000: episode: 5014, duration: 3.093s, episode steps: 203, steps per second: 66, episode reward: -198.366, mean reward: -0.977 [-100.000, 6.728], mean action: 1.557 [0.000, 3.000], mean observation: -0.112 [-0.982, 1.885], loss: 1.711622, mean_absolute_error: 30.105844, mean_q: 27.582950, mean_eps: 0.389559
  679249/2000000: episode: 5015, duration: 13.854s, episode steps: 879, steps per second: 63, episode reward: 171.136, mean reward: 0.195 [-23.393, 100.000], mean action: 1.158 [0.000, 3.000], mean observation: 0.107 [-0.690, 1.000], loss: 1.711839, mean_absolute_error: 30.021304, mean_q: 27.524933, mean_eps: 0.389071
  680249/2000000: episode: 5016, duration: 16.304s, episode steps: 1000, steps per second: 61, episode reward: 60.226, mean reward: 0.060 [-22.876, 23.335], mean action: 1.065 [0.000, 3.000], mean observation: 0.123 [-0.503, 1.012], loss: 1.709743, mean_absolute_error: 30.525501, mean_q: 27.629568, mean_eps: 0.388225
  681249/2000000: episode: 5017, duration: 16.855s, episode steps: 1000, steps per second: 59, episode reward: -28.193, mean reward: -0.028 [-20.901, 23.073], mean action: 1.326 [0.000, 3.000], mean observation: 0.192 [-0.633, 1.000], loss: 1.898245, mean_absolute_error: 30.549952, mean_q: 28.576118, mean_eps: 0.387325
  681353/2000000: episode: 5018, duration: 1.566s, episode steps: 104, steps per second: 66, episode reward: -86.961, mean reward: -0.836 [-100.000, 10.857], mean action: 1.788 [0.000, 3.000], mean observation: -0.091 [-1.042, 3.019], loss: 1.833119, mean_absolute_error: 29.711930, mean_q: 28.784700, mean_eps: 0.386828
  681631/2000000: episode: 5019, duration: 4.142s, episode steps: 278, steps per second: 67, episode reward: -116.861, mean reward: -0.420 [-100.000, 10.220], mean action: 1.781 [0.000, 3.000], mean observation: 0.083 [-0.590, 1.303], loss: 1.452495, mean_absolute_error: 30.282590, mean_q: 29.232727, mean_eps: 0.386657
  681852/2000000: episode: 5020, duration: 3.321s, episode steps: 221, steps per second: 67, episode reward: -54.300, mean reward: -0.246 [-100.000, 18.763], mean action: 1.855 [0.000, 3.000], mean observation: 0.065 [-1.808, 1.072], loss: 1.564854, mean_absolute_error: 30.324231, mean_q: 27.562758, mean_eps: 0.386434
  681969/2000000: episode: 5021, duration: 1.779s, episode steps: 117, steps per second: 66, episode reward: -143.663, mean reward: -1.228 [-100.000, 12.321], mean action: 1.675 [0.000, 3.000], mean observation: -0.020 [-0.851, 2.318], loss: 1.964177, mean_absolute_error: 30.828031, mean_q: 27.892062, mean_eps: 0.386281
  682969/2000000: episode: 5022, duration: 15.813s, episode steps: 1000, steps per second: 63, episode reward: 89.735, mean reward: 0.090 [-22.621, 22.724], mean action: 0.878 [0.000, 3.000], mean observation: 0.191 [-1.086, 1.000], loss: 1.780039, mean_absolute_error: 30.299912, mean_q: 27.961350, mean_eps: 0.385777
  683969/2000000: episode: 5023, duration: 16.063s, episode steps: 1000, steps per second: 62, episode reward: 71.674, mean reward: 0.072 [-22.569, 23.392], mean action: 1.111 [0.000, 3.000], mean observation: 0.208 [-0.804, 1.007], loss: 1.784886, mean_absolute_error: 30.030525, mean_q: 28.775586, mean_eps: 0.384877
  684183/2000000: episode: 5024, duration: 3.121s, episode steps: 214, steps per second: 69, episode reward: -147.233, mean reward: -0.688 [-100.000, 7.874], mean action: 1.706 [0.000, 3.000], mean observation: -0.071 [-0.890, 1.000], loss: 1.608862, mean_absolute_error: 30.038793, mean_q: 29.755377, mean_eps: 0.384332
  685183/2000000: episode: 5025, duration: 15.402s, episode steps: 1000, steps per second: 65, episode reward: 22.680, mean reward: 0.023 [-23.889, 25.067], mean action: 1.618 [0.000, 3.000], mean observation: 0.157 [-0.849, 1.004], loss: 1.729463, mean_absolute_error: 30.511051, mean_q: 28.734358, mean_eps: 0.383786
  685452/2000000: episode: 5026, duration: 4.049s, episode steps: 269, steps per second: 66, episode reward: -131.025, mean reward: -0.487 [-100.000, 13.906], mean action: 1.907 [0.000, 3.000], mean observation: -0.028 [-0.713, 1.000], loss: 1.578526, mean_absolute_error: 29.594666, mean_q: 28.112180, mean_eps: 0.383216
  686452/2000000: episode: 5027, duration: 15.515s, episode steps: 1000, steps per second: 64, episode reward: 120.027, mean reward: 0.120 [-24.654, 23.297], mean action: 0.943 [0.000, 3.000], mean observation: 0.211 [-0.857, 1.000], loss: 1.810079, mean_absolute_error: 30.523087, mean_q: 28.925906, mean_eps: 0.382645
  686573/2000000: episode: 5028, duration: 1.833s, episode steps: 121, steps per second: 66, episode reward: -106.954, mean reward: -0.884 [-100.000, 11.368], mean action: 1.702 [0.000, 3.000], mean observation: 0.074 [-0.832, 1.785], loss: 1.870631, mean_absolute_error: 30.123945, mean_q: 26.586061, mean_eps: 0.382139
  687573/2000000: episode: 5029, duration: 16.059s, episode steps: 1000, steps per second: 62, episode reward: -43.551, mean reward: -0.044 [-21.751, 23.837], mean action: 1.357 [0.000, 3.000], mean observation: 0.167 [-0.804, 1.000], loss: 1.874389, mean_absolute_error: 30.266776, mean_q: 28.365085, mean_eps: 0.381633
  687989/2000000: episode: 5030, duration: 6.245s, episode steps: 416, steps per second: 67, episode reward: -208.815, mean reward: -0.502 [-100.000, 4.718], mean action: 1.690 [0.000, 3.000], mean observation: 0.194 [-0.622, 1.001], loss: 1.855597, mean_absolute_error: 30.257276, mean_q: 28.198280, mean_eps: 0.380996
  688989/2000000: episode: 5031, duration: 15.822s, episode steps: 1000, steps per second: 63, episode reward: 47.675, mean reward: 0.048 [-21.346, 23.685], mean action: 1.087 [0.000, 3.000], mean observation: 0.223 [-0.715, 1.370], loss: 1.776439, mean_absolute_error: 30.363791, mean_q: 28.644075, mean_eps: 0.380359
  689102/2000000: episode: 5032, duration: 1.672s, episode steps: 113, steps per second: 68, episode reward: -138.269, mean reward: -1.224 [-100.000, 10.625], mean action: 1.779 [0.000, 3.000], mean observation: -0.087 [-0.979, 1.000], loss: 2.285968, mean_absolute_error: 29.233497, mean_q: 25.633742, mean_eps: 0.379859
  689190/2000000: episode: 5033, duration: 1.282s, episode steps: 88, steps per second: 69, episode reward: -53.949, mean reward: -0.613 [-100.000, 13.802], mean action: 1.761 [0.000, 3.000], mean observation: -0.125 [-0.858, 1.937], loss: 2.235288, mean_absolute_error: 31.114858, mean_q: 30.110525, mean_eps: 0.379769
  690190/2000000: episode: 5034, duration: 15.742s, episode steps: 1000, steps per second: 64, episode reward: 54.730, mean reward: 0.055 [-21.317, 23.057], mean action: 1.113 [0.000, 3.000], mean observation: 0.193 [-0.669, 1.262], loss: 1.666533, mean_absolute_error: 29.754149, mean_q: 28.337517, mean_eps: 0.379279
  690616/2000000: episode: 5035, duration: 6.532s, episode steps: 426, steps per second: 65, episode reward: -122.485, mean reward: -0.288 [-100.000, 16.238], mean action: 1.761 [0.000, 3.000], mean observation: -0.068 [-0.860, 1.622], loss: 1.858804, mean_absolute_error: 31.045997, mean_q: 29.751444, mean_eps: 0.378638
  691616/2000000: episode: 5036, duration: 15.935s, episode steps: 1000, steps per second: 63, episode reward: 51.711, mean reward: 0.052 [-22.855, 22.724], mean action: 0.925 [0.000, 3.000], mean observation: 0.188 [-1.407, 1.000], loss: 1.832468, mean_absolute_error: 30.243870, mean_q: 29.273910, mean_eps: 0.377997
  691887/2000000: episode: 5037, duration: 4.145s, episode steps: 271, steps per second: 65, episode reward: -149.753, mean reward: -0.553 [-100.000, 7.867], mean action: 1.661 [0.000, 3.000], mean observation: 0.087 [-1.034, 1.000], loss: 1.651142, mean_absolute_error: 30.665329, mean_q: 31.146437, mean_eps: 0.377425
  692887/2000000: episode: 5038, duration: 15.881s, episode steps: 1000, steps per second: 63, episode reward: 107.728, mean reward: 0.108 [-23.647, 23.615], mean action: 0.892 [0.000, 3.000], mean observation: 0.162 [-0.744, 1.000], loss: 1.959031, mean_absolute_error: 30.831026, mean_q: 30.500514, mean_eps: 0.376853
  693887/2000000: episode: 5039, duration: 15.867s, episode steps: 1000, steps per second: 63, episode reward: 67.552, mean reward: 0.068 [-22.612, 23.396], mean action: 0.904 [0.000, 3.000], mean observation: 0.150 [-0.673, 1.000], loss: 1.909021, mean_absolute_error: 30.421831, mean_q: 29.889231, mean_eps: 0.375953
  694887/2000000: episode: 5040, duration: 15.511s, episode steps: 1000, steps per second: 64, episode reward: 65.050, mean reward: 0.065 [-21.830, 23.215], mean action: 1.044 [0.000, 3.000], mean observation: 0.144 [-0.624, 1.000], loss: 1.829632, mean_absolute_error: 30.371613, mean_q: 29.067616, mean_eps: 0.375053
  695887/2000000: episode: 5041, duration: 15.417s, episode steps: 1000, steps per second: 65, episode reward: 73.469, mean reward: 0.073 [-24.712, 22.679], mean action: 0.871 [0.000, 3.000], mean observation: 0.185 [-0.583, 1.000], loss: 1.830296, mean_absolute_error: 30.619423, mean_q: 29.802773, mean_eps: 0.374153
  696887/2000000: episode: 5042, duration: 16.046s, episode steps: 1000, steps per second: 62, episode reward: 61.439, mean reward: 0.061 [-21.349, 23.292], mean action: 0.992 [0.000, 3.000], mean observation: 0.152 [-1.207, 1.000], loss: 2.064829, mean_absolute_error: 30.193855, mean_q: 29.065450, mean_eps: 0.373253
  697085/2000000: episode: 5043, duration: 2.986s, episode steps: 198, steps per second: 66, episode reward: -63.419, mean reward: -0.320 [-100.000, 18.018], mean action: 1.702 [0.000, 3.000], mean observation: -0.021 [-2.673, 1.014], loss: 1.959077, mean_absolute_error: 30.208727, mean_q: 30.056900, mean_eps: 0.372713
  698085/2000000: episode: 5044, duration: 15.694s, episode steps: 1000, steps per second: 64, episode reward: 71.466, mean reward: 0.071 [-22.942, 23.742], mean action: 0.945 [0.000, 3.000], mean observation: 0.222 [-0.744, 1.143], loss: 1.747975, mean_absolute_error: 30.282541, mean_q: 30.138585, mean_eps: 0.372173
  698570/2000000: episode: 5045, duration: 7.636s, episode steps: 485, steps per second: 64, episode reward: -85.807, mean reward: -0.177 [-100.000, 11.148], mean action: 1.810 [0.000, 3.000], mean observation: 0.022 [-1.465, 1.042], loss: 1.754639, mean_absolute_error: 29.629876, mean_q: 29.315794, mean_eps: 0.371505
  699570/2000000: episode: 5046, duration: 15.813s, episode steps: 1000, steps per second: 63, episode reward: 99.643, mean reward: 0.100 [-22.460, 23.904], mean action: 0.822 [0.000, 3.000], mean observation: 0.160 [-0.689, 1.000], loss: 1.769791, mean_absolute_error: 30.085761, mean_q: 29.969502, mean_eps: 0.370837
  700570/2000000: episode: 5047, duration: 15.410s, episode steps: 1000, steps per second: 65, episode reward: 72.385, mean reward: 0.072 [-20.271, 32.595], mean action: 0.921 [0.000, 3.000], mean observation: 0.149 [-1.192, 1.248], loss: 1.828730, mean_absolute_error: 29.948345, mean_q: 29.554736, mean_eps: 0.369937
  701570/2000000: episode: 5048, duration: 15.893s, episode steps: 1000, steps per second: 63, episode reward: 48.104, mean reward: 0.048 [-24.281, 23.550], mean action: 0.851 [0.000, 3.000], mean observation: 0.159 [-0.961, 1.000], loss: 1.764838, mean_absolute_error: 30.234289, mean_q: 30.536275, mean_eps: 0.369037
  701692/2000000: episode: 5049, duration: 1.863s, episode steps: 122, steps per second: 65, episode reward: -61.303, mean reward: -0.502 [-100.000, 4.527], mean action: 1.885 [0.000, 3.000], mean observation: -0.108 [-0.787, 0.925], loss: 2.567954, mean_absolute_error: 31.097891, mean_q: 29.587393, mean_eps: 0.368533
  702482/2000000: episode: 5050, duration: 12.396s, episode steps: 790, steps per second: 64, episode reward: 194.589, mean reward: 0.246 [-23.787, 100.000], mean action: 1.191 [0.000, 3.000], mean observation: 0.185 [-0.689, 1.000], loss: 1.838883, mean_absolute_error: 29.642766, mean_q: 29.406617, mean_eps: 0.368123
  702722/2000000: episode: 5051, duration: 3.614s, episode steps: 240, steps per second: 66, episode reward: -122.963, mean reward: -0.512 [-100.000, 25.889], mean action: 1.796 [0.000, 3.000], mean observation: 0.091 [-1.086, 1.000], loss: 1.656785, mean_absolute_error: 29.816784, mean_q: 30.564954, mean_eps: 0.367658
  703722/2000000: episode: 5052, duration: 15.552s, episode steps: 1000, steps per second: 64, episode reward: 96.536, mean reward: 0.097 [-21.636, 25.281], mean action: 0.803 [0.000, 3.000], mean observation: 0.172 [-1.053, 1.000], loss: 1.744113, mean_absolute_error: 29.932694, mean_q: 29.430559, mean_eps: 0.367100
  703823/2000000: episode: 5053, duration: 1.551s, episode steps: 101, steps per second: 65, episode reward: -35.482, mean reward: -0.351 [-100.000, 11.015], mean action: 1.842 [0.000, 3.000], mean observation: 0.036 [-1.299, 1.098], loss: 2.011241, mean_absolute_error: 30.177028, mean_q: 26.871626, mean_eps: 0.366605
  703919/2000000: episode: 5054, duration: 1.438s, episode steps: 96, steps per second: 67, episode reward: -37.247, mean reward: -0.388 [-100.000, 11.556], mean action: 1.729 [0.000, 3.000], mean observation: 0.055 [-0.894, 2.260], loss: 1.842266, mean_absolute_error: 28.820277, mean_q: 25.945329, mean_eps: 0.366517
  704101/2000000: episode: 5055, duration: 2.775s, episode steps: 182, steps per second: 66, episode reward: -51.970, mean reward: -0.286 [-100.000, 17.409], mean action: 1.747 [0.000, 3.000], mean observation: -0.046 [-0.628, 1.000], loss: 1.841821, mean_absolute_error: 29.845647, mean_q: 28.676717, mean_eps: 0.366391
  704419/2000000: episode: 5056, duration: 4.820s, episode steps: 318, steps per second: 66, episode reward: -210.720, mean reward: -0.663 [-100.000, 3.798], mean action: 1.767 [0.000, 3.000], mean observation: -0.071 [-1.000, 0.932], loss: 1.848918, mean_absolute_error: 30.069846, mean_q: 29.594525, mean_eps: 0.366166
  704711/2000000: episode: 5057, duration: 4.449s, episode steps: 292, steps per second: 66, episode reward: -101.438, mean reward: -0.347 [-100.000, 11.200], mean action: 1.801 [0.000, 3.000], mean observation: 0.104 [-3.123, 1.000], loss: 2.051624, mean_absolute_error: 30.267687, mean_q: 29.350841, mean_eps: 0.365892
  704794/2000000: episode: 5058, duration: 1.251s, episode steps: 83, steps per second: 66, episode reward: -77.745, mean reward: -0.937 [-100.000, 14.917], mean action: 1.578 [0.000, 3.000], mean observation: -0.116 [-0.890, 1.000], loss: 1.672275, mean_absolute_error: 29.142573, mean_q: 28.878043, mean_eps: 0.365723
  705794/2000000: episode: 5059, duration: 16.016s, episode steps: 1000, steps per second: 62, episode reward: 60.857, mean reward: 0.061 [-22.041, 22.970], mean action: 1.066 [0.000, 3.000], mean observation: 0.146 [-0.684, 1.000], loss: 2.035107, mean_absolute_error: 30.102757, mean_q: 30.704775, mean_eps: 0.365235
  706794/2000000: episode: 5060, duration: 16.118s, episode steps: 1000, steps per second: 62, episode reward: 107.572, mean reward: 0.108 [-22.975, 23.032], mean action: 1.038 [0.000, 3.000], mean observation: 0.204 [-0.679, 1.000], loss: 1.711257, mean_absolute_error: 29.864935, mean_q: 29.824673, mean_eps: 0.364335
  707794/2000000: episode: 5061, duration: 15.898s, episode steps: 1000, steps per second: 63, episode reward: 136.671, mean reward: 0.137 [-24.337, 22.413], mean action: 0.905 [0.000, 3.000], mean observation: 0.174 [-0.801, 1.000], loss: 1.800393, mean_absolute_error: 29.740635, mean_q: 28.689609, mean_eps: 0.363435
  707892/2000000: episode: 5062, duration: 1.506s, episode steps: 98, steps per second: 65, episode reward: -7.029, mean reward: -0.072 [-100.000, 20.104], mean action: 1.806 [0.000, 3.000], mean observation: 0.027 [-0.781, 1.000], loss: 2.212013, mean_absolute_error: 29.623204, mean_q: 28.880232, mean_eps: 0.362942
  708892/2000000: episode: 5063, duration: 15.583s, episode steps: 1000, steps per second: 64, episode reward: 54.617, mean reward: 0.055 [-23.674, 24.508], mean action: 0.918 [0.000, 3.000], mean observation: 0.084 [-1.281, 1.000], loss: 1.671923, mean_absolute_error: 29.426555, mean_q: 29.561827, mean_eps: 0.362449
  709029/2000000: episode: 5064, duration: 2.104s, episode steps: 137, steps per second: 65, episode reward: -73.545, mean reward: -0.537 [-100.000, 15.323], mean action: 1.788 [0.000, 3.000], mean observation: -0.153 [-1.512, 1.000], loss: 1.980843, mean_absolute_error: 28.279345, mean_q: 26.853245, mean_eps: 0.361936
  709181/2000000: episode: 5065, duration: 2.275s, episode steps: 152, steps per second: 67, episode reward: -38.058, mean reward: -0.250 [-100.000, 11.068], mean action: 1.829 [0.000, 3.000], mean observation: 0.069 [-0.970, 1.000], loss: 2.083442, mean_absolute_error: 28.798819, mean_q: 28.846466, mean_eps: 0.361805
  710181/2000000: episode: 5066, duration: 15.811s, episode steps: 1000, steps per second: 63, episode reward: 94.820, mean reward: 0.095 [-24.934, 23.322], mean action: 0.968 [0.000, 3.000], mean observation: 0.140 [-0.702, 1.250], loss: 1.796453, mean_absolute_error: 29.628825, mean_q: 29.558285, mean_eps: 0.361286
  711181/2000000: episode: 5067, duration: 15.600s, episode steps: 1000, steps per second: 64, episode reward: 70.996, mean reward: 0.071 [-21.696, 23.797], mean action: 0.871 [0.000, 3.000], mean observation: 0.213 [-1.076, 1.000], loss: 1.925097, mean_absolute_error: 29.775612, mean_q: 29.784709, mean_eps: 0.360386
  712181/2000000: episode: 5068, duration: 15.913s, episode steps: 1000, steps per second: 63, episode reward: 69.799, mean reward: 0.070 [-22.944, 24.370], mean action: 0.899 [0.000, 3.000], mean observation: 0.117 [-0.725, 1.000], loss: 1.749566, mean_absolute_error: 29.471177, mean_q: 30.531203, mean_eps: 0.359486
  712416/2000000: episode: 5069, duration: 3.513s, episode steps: 235, steps per second: 67, episode reward: -173.089, mean reward: -0.737 [-100.000, 16.863], mean action: 1.587 [0.000, 3.000], mean observation: 0.093 [-0.734, 1.535], loss: 1.587108, mean_absolute_error: 29.899460, mean_q: 30.439700, mean_eps: 0.358932
  713416/2000000: episode: 5070, duration: 16.244s, episode steps: 1000, steps per second: 62, episode reward: 16.383, mean reward: 0.016 [-24.122, 23.872], mean action: 1.018 [0.000, 3.000], mean observation: 0.066 [-0.821, 1.000], loss: 1.683413, mean_absolute_error: 29.893815, mean_q: 30.491639, mean_eps: 0.358377
  714416/2000000: episode: 5071, duration: 15.898s, episode steps: 1000, steps per second: 63, episode reward: 69.197, mean reward: 0.069 [-20.695, 23.620], mean action: 0.918 [0.000, 3.000], mean observation: 0.160 [-0.772, 1.000], loss: 1.791725, mean_absolute_error: 29.412208, mean_q: 29.923747, mean_eps: 0.357477
  715416/2000000: episode: 5072, duration: 16.140s, episode steps: 1000, steps per second: 62, episode reward: 40.138, mean reward: 0.040 [-22.428, 22.902], mean action: 0.960 [0.000, 3.000], mean observation: 0.145 [-0.708, 1.000], loss: 1.768979, mean_absolute_error: 29.600294, mean_q: 30.669316, mean_eps: 0.356577
  716416/2000000: episode: 5073, duration: 16.629s, episode steps: 1000, steps per second: 60, episode reward: 36.549, mean reward: 0.037 [-20.299, 23.621], mean action: 1.041 [0.000, 3.000], mean observation: 0.104 [-0.676, 1.000], loss: 1.778615, mean_absolute_error: 29.839757, mean_q: 31.019530, mean_eps: 0.355677
  717416/2000000: episode: 5074, duration: 15.753s, episode steps: 1000, steps per second: 63, episode reward: 71.218, mean reward: 0.071 [-21.753, 23.845], mean action: 0.802 [0.000, 3.000], mean observation: 0.115 [-0.674, 1.000], loss: 1.731310, mean_absolute_error: 29.765367, mean_q: 30.419645, mean_eps: 0.354777
  718416/2000000: episode: 5075, duration: 15.951s, episode steps: 1000, steps per second: 63, episode reward: 15.541, mean reward: 0.016 [-24.686, 21.927], mean action: 1.285 [0.000, 3.000], mean observation: 0.215 [-1.624, 1.000], loss: 1.571535, mean_absolute_error: 29.338179, mean_q: 29.692197, mean_eps: 0.353877
  719416/2000000: episode: 5076, duration: 16.016s, episode steps: 1000, steps per second: 62, episode reward: 26.187, mean reward: 0.026 [-22.230, 26.809], mean action: 1.480 [0.000, 3.000], mean observation: 0.121 [-0.801, 1.673], loss: 1.781275, mean_absolute_error: 29.040308, mean_q: 29.954228, mean_eps: 0.352977
  720416/2000000: episode: 5077, duration: 15.998s, episode steps: 1000, steps per second: 63, episode reward: -13.588, mean reward: -0.014 [-21.475, 21.960], mean action: 1.104 [0.000, 3.000], mean observation: 0.115 [-0.626, 1.006], loss: 1.641349, mean_absolute_error: 28.961161, mean_q: 30.612230, mean_eps: 0.352077
  721416/2000000: episode: 5078, duration: 16.396s, episode steps: 1000, steps per second: 61, episode reward: 49.326, mean reward: 0.049 [-24.580, 24.740], mean action: 1.162 [0.000, 3.000], mean observation: 0.187 [-1.352, 1.000], loss: 1.721923, mean_absolute_error: 29.522208, mean_q: 31.001851, mean_eps: 0.351177
  722416/2000000: episode: 5079, duration: 16.419s, episode steps: 1000, steps per second: 61, episode reward: 42.045, mean reward: 0.042 [-24.210, 23.666], mean action: 1.086 [0.000, 3.000], mean observation: 0.104 [-0.717, 1.000], loss: 1.656445, mean_absolute_error: 29.368860, mean_q: 30.994174, mean_eps: 0.350277
  722540/2000000: episode: 5080, duration: 1.881s, episode steps: 124, steps per second: 66, episode reward: -54.944, mean reward: -0.443 [-100.000, 27.269], mean action: 1.863 [0.000, 3.000], mean observation: -0.103 [-0.794, 1.031], loss: 2.035842, mean_absolute_error: 28.554317, mean_q: 28.995615, mean_eps: 0.349772
  723540/2000000: episode: 5081, duration: 16.306s, episode steps: 1000, steps per second: 61, episode reward: 9.921, mean reward: 0.010 [-20.113, 23.555], mean action: 1.051 [0.000, 3.000], mean observation: 0.157 [-0.811, 1.000], loss: 1.797468, mean_absolute_error: 29.228464, mean_q: 30.558542, mean_eps: 0.349266
  724540/2000000: episode: 5082, duration: 15.835s, episode steps: 1000, steps per second: 63, episode reward: 100.834, mean reward: 0.101 [-24.379, 22.556], mean action: 0.876 [0.000, 3.000], mean observation: 0.180 [-1.066, 1.000], loss: 1.700815, mean_absolute_error: 29.188296, mean_q: 30.411801, mean_eps: 0.348366
  725540/2000000: episode: 5083, duration: 15.504s, episode steps: 1000, steps per second: 64, episode reward: 111.538, mean reward: 0.112 [-23.878, 23.218], mean action: 1.060 [0.000, 3.000], mean observation: 0.210 [-0.709, 1.000], loss: 1.672107, mean_absolute_error: 28.990910, mean_q: 30.658513, mean_eps: 0.347466
  725662/2000000: episode: 5084, duration: 1.872s, episode steps: 122, steps per second: 65, episode reward: -12.434, mean reward: -0.102 [-100.000, 34.180], mean action: 1.672 [0.000, 3.000], mean observation: 0.067 [-0.793, 1.000], loss: 1.736359, mean_absolute_error: 29.798657, mean_q: 32.749031, mean_eps: 0.346960
  726662/2000000: episode: 5085, duration: 15.257s, episode steps: 1000, steps per second: 66, episode reward: 47.074, mean reward: 0.047 [-23.888, 23.301], mean action: 0.845 [0.000, 3.000], mean observation: 0.168 [-0.803, 1.000], loss: 1.572791, mean_absolute_error: 28.868379, mean_q: 30.485422, mean_eps: 0.346454
  727662/2000000: episode: 5086, duration: 16.494s, episode steps: 1000, steps per second: 61, episode reward: 57.375, mean reward: 0.057 [-23.636, 22.370], mean action: 1.300 [0.000, 3.000], mean observation: 0.097 [-0.652, 1.000], loss: 1.718677, mean_absolute_error: 28.625417, mean_q: 30.160959, mean_eps: 0.345554
  727943/2000000: episode: 5087, duration: 4.244s, episode steps: 281, steps per second: 66, episode reward: -212.198, mean reward: -0.755 [-100.000, 11.323], mean action: 1.569 [0.000, 3.000], mean observation: 0.157 [-0.746, 1.003], loss: 1.967830, mean_absolute_error: 29.200018, mean_q: 30.805898, mean_eps: 0.344978
  728943/2000000: episode: 5088, duration: 16.188s, episode steps: 1000, steps per second: 62, episode reward: 83.233, mean reward: 0.083 [-21.620, 26.401], mean action: 0.803 [0.000, 3.000], mean observation: 0.189 [-1.525, 1.000], loss: 1.790089, mean_absolute_error: 29.176546, mean_q: 30.015354, mean_eps: 0.344402
  729201/2000000: episode: 5089, duration: 3.896s, episode steps: 258, steps per second: 66, episode reward: -168.010, mean reward: -0.651 [-100.000, 15.056], mean action: 1.496 [0.000, 3.000], mean observation: 0.182 [-0.791, 1.007], loss: 1.564001, mean_absolute_error: 29.236013, mean_q: 31.662636, mean_eps: 0.343835
  729323/2000000: episode: 5090, duration: 1.809s, episode steps: 122, steps per second: 67, episode reward: -23.080, mean reward: -0.189 [-100.000, 7.862], mean action: 1.779 [0.000, 3.000], mean observation: 0.050 [-1.525, 1.000], loss: 1.589186, mean_absolute_error: 29.089393, mean_q: 32.164748, mean_eps: 0.343664
  730323/2000000: episode: 5091, duration: 16.434s, episode steps: 1000, steps per second: 61, episode reward: 36.957, mean reward: 0.037 [-21.910, 56.337], mean action: 1.517 [0.000, 3.000], mean observation: 0.273 [-1.235, 1.000], loss: 1.619997, mean_absolute_error: 29.043336, mean_q: 30.573427, mean_eps: 0.343160
  731323/2000000: episode: 5092, duration: 16.262s, episode steps: 1000, steps per second: 61, episode reward: 103.328, mean reward: 0.103 [-24.429, 22.837], mean action: 1.287 [0.000, 3.000], mean observation: 0.219 [-0.676, 1.000], loss: 1.944141, mean_absolute_error: 29.163535, mean_q: 30.042803, mean_eps: 0.342260
  732323/2000000: episode: 5093, duration: 16.293s, episode steps: 1000, steps per second: 61, episode reward: 77.675, mean reward: 0.078 [-23.654, 22.519], mean action: 1.051 [0.000, 3.000], mean observation: 0.128 [-0.786, 1.000], loss: 1.917027, mean_absolute_error: 29.007707, mean_q: 30.782590, mean_eps: 0.341360
  732659/2000000: episode: 5094, duration: 5.156s, episode steps: 336, steps per second: 65, episode reward: 197.665, mean reward: 0.588 [-18.112, 100.000], mean action: 1.330 [0.000, 3.000], mean observation: 0.053 [-0.835, 1.004], loss: 1.946797, mean_absolute_error: 29.082146, mean_q: 30.258247, mean_eps: 0.340759
  733659/2000000: episode: 5095, duration: 15.933s, episode steps: 1000, steps per second: 63, episode reward: 31.320, mean reward: 0.031 [-19.312, 23.519], mean action: 0.965 [0.000, 3.000], mean observation: 0.140 [-0.761, 1.000], loss: 1.791601, mean_absolute_error: 29.092721, mean_q: 30.091625, mean_eps: 0.340158
  733775/2000000: episode: 5096, duration: 1.754s, episode steps: 116, steps per second: 66, episode reward: -56.856, mean reward: -0.490 [-100.000, 11.209], mean action: 1.681 [0.000, 3.000], mean observation: 0.043 [-0.696, 1.415], loss: 1.717489, mean_absolute_error: 29.105299, mean_q: 29.680022, mean_eps: 0.339656
  733915/2000000: episode: 5097, duration: 2.084s, episode steps: 140, steps per second: 67, episode reward: -34.702, mean reward: -0.248 [-100.000, 16.955], mean action: 1.693 [0.000, 3.000], mean observation: -0.021 [-0.822, 1.000], loss: 1.799830, mean_absolute_error: 28.133152, mean_q: 29.234219, mean_eps: 0.339540
  734915/2000000: episode: 5098, duration: 16.356s, episode steps: 1000, steps per second: 61, episode reward: 74.390, mean reward: 0.074 [-23.642, 23.503], mean action: 1.090 [0.000, 3.000], mean observation: 0.096 [-0.767, 1.000], loss: 1.798545, mean_absolute_error: 29.008946, mean_q: 30.471564, mean_eps: 0.339027
  735741/2000000: episode: 5099, duration: 13.485s, episode steps: 826, steps per second: 61, episode reward: 154.010, mean reward: 0.186 [-20.468, 100.000], mean action: 1.096 [0.000, 3.000], mean observation: 0.128 [-0.520, 1.000], loss: 1.782646, mean_absolute_error: 28.908470, mean_q: 30.423808, mean_eps: 0.338205
  736741/2000000: episode: 5100, duration: 15.852s, episode steps: 1000, steps per second: 63, episode reward: 31.406, mean reward: 0.031 [-23.814, 47.318], mean action: 1.053 [0.000, 3.000], mean observation: 0.091 [-0.725, 1.000], loss: 1.731678, mean_absolute_error: 29.063629, mean_q: 31.290978, mean_eps: 0.337382
  737741/2000000: episode: 5101, duration: 16.478s, episode steps: 1000, steps per second: 61, episode reward: 35.300, mean reward: 0.035 [-23.489, 25.278], mean action: 1.096 [0.000, 3.000], mean observation: 0.165 [-0.675, 1.000], loss: 1.799034, mean_absolute_error: 29.085286, mean_q: 30.684125, mean_eps: 0.336482
  738741/2000000: episode: 5102, duration: 15.834s, episode steps: 1000, steps per second: 63, episode reward: 62.905, mean reward: 0.063 [-19.756, 22.685], mean action: 0.948 [0.000, 3.000], mean observation: 0.152 [-0.602, 1.000], loss: 1.753600, mean_absolute_error: 28.644572, mean_q: 30.458123, mean_eps: 0.335582
  739741/2000000: episode: 5103, duration: 15.855s, episode steps: 1000, steps per second: 63, episode reward: 90.196, mean reward: 0.090 [-21.964, 23.244], mean action: 0.797 [0.000, 3.000], mean observation: 0.190 [-0.790, 1.000], loss: 1.679176, mean_absolute_error: 28.900968, mean_q: 31.088966, mean_eps: 0.334682
  740144/2000000: episode: 5104, duration: 6.203s, episode steps: 403, steps per second: 65, episode reward: -327.082, mean reward: -0.812 [-100.000, 22.565], mean action: 1.762 [0.000, 3.000], mean observation: 0.179 [-1.182, 1.982], loss: 1.608529, mean_absolute_error: 28.211424, mean_q: 30.977458, mean_eps: 0.334052
  740527/2000000: episode: 5105, duration: 5.937s, episode steps: 383, steps per second: 65, episode reward: -208.025, mean reward: -0.543 [-100.000, 17.919], mean action: 1.909 [0.000, 3.000], mean observation: 0.030 [-1.006, 1.050], loss: 1.707557, mean_absolute_error: 28.890843, mean_q: 32.074683, mean_eps: 0.333699
  741527/2000000: episode: 5106, duration: 16.292s, episode steps: 1000, steps per second: 61, episode reward: 74.750, mean reward: 0.075 [-21.114, 23.713], mean action: 0.986 [0.000, 3.000], mean observation: 0.132 [-1.294, 1.000], loss: 1.599393, mean_absolute_error: 28.037817, mean_q: 29.857116, mean_eps: 0.333077
  741652/2000000: episode: 5107, duration: 1.926s, episode steps: 125, steps per second: 65, episode reward: -94.206, mean reward: -0.754 [-100.000, 13.339], mean action: 1.816 [0.000, 3.000], mean observation: -0.023 [-0.735, 1.000], loss: 1.503750, mean_absolute_error: 28.796539, mean_q: 32.272910, mean_eps: 0.332571
  742652/2000000: episode: 5108, duration: 16.392s, episode steps: 1000, steps per second: 61, episode reward: 45.194, mean reward: 0.045 [-23.714, 24.914], mean action: 1.226 [0.000, 3.000], mean observation: 0.142 [-0.699, 1.000], loss: 1.568009, mean_absolute_error: 28.357465, mean_q: 30.720772, mean_eps: 0.332065
  743652/2000000: episode: 5109, duration: 16.134s, episode steps: 1000, steps per second: 62, episode reward: 73.512, mean reward: 0.074 [-19.554, 22.670], mean action: 1.099 [0.000, 3.000], mean observation: 0.165 [-0.712, 1.623], loss: 1.724826, mean_absolute_error: 28.872397, mean_q: 30.875602, mean_eps: 0.331165
  744652/2000000: episode: 5110, duration: 15.889s, episode steps: 1000, steps per second: 63, episode reward: 63.153, mean reward: 0.063 [-24.197, 22.731], mean action: 0.814 [0.000, 3.000], mean observation: 0.176 [-1.202, 1.000], loss: 1.734643, mean_absolute_error: 28.505393, mean_q: 31.031655, mean_eps: 0.330265
  745652/2000000: episode: 5111, duration: 16.726s, episode steps: 1000, steps per second: 60, episode reward: 73.007, mean reward: 0.073 [-22.737, 23.114], mean action: 0.886 [0.000, 3.000], mean observation: 0.167 [-0.643, 1.104], loss: 1.611378, mean_absolute_error: 28.394799, mean_q: 30.709534, mean_eps: 0.329365
  745847/2000000: episode: 5112, duration: 2.963s, episode steps: 195, steps per second: 66, episode reward: -42.276, mean reward: -0.217 [-100.000, 10.392], mean action: 1.605 [0.000, 3.000], mean observation: 0.065 [-0.766, 1.000], loss: 1.618793, mean_absolute_error: 28.573345, mean_q: 31.767729, mean_eps: 0.328827
  746847/2000000: episode: 5113, duration: 15.612s, episode steps: 1000, steps per second: 64, episode reward: 66.514, mean reward: 0.067 [-24.125, 22.685], mean action: 0.852 [0.000, 3.000], mean observation: 0.196 [-0.698, 1.000], loss: 1.695542, mean_absolute_error: 28.450310, mean_q: 30.941679, mean_eps: 0.328289
  747847/2000000: episode: 5114, duration: 16.385s, episode steps: 1000, steps per second: 61, episode reward: 69.071, mean reward: 0.069 [-24.481, 22.795], mean action: 0.923 [0.000, 3.000], mean observation: 0.150 [-0.721, 1.000], loss: 1.507463, mean_absolute_error: 28.323486, mean_q: 30.999942, mean_eps: 0.327389
  748847/2000000: episode: 5115, duration: 15.971s, episode steps: 1000, steps per second: 63, episode reward: 83.507, mean reward: 0.084 [-22.209, 22.658], mean action: 0.819 [0.000, 3.000], mean observation: 0.185 [-1.040, 1.000], loss: 1.599135, mean_absolute_error: 28.325942, mean_q: 30.304670, mean_eps: 0.326489
  748979/2000000: episode: 5116, duration: 2.009s, episode steps: 132, steps per second: 66, episode reward: -76.874, mean reward: -0.582 [-100.000, 14.064], mean action: 1.720 [0.000, 3.000], mean observation: 0.103 [-0.739, 1.289], loss: 1.896840, mean_absolute_error: 27.883738, mean_q: 30.477685, mean_eps: 0.325979
  749979/2000000: episode: 5117, duration: 16.349s, episode steps: 1000, steps per second: 61, episode reward: 35.601, mean reward: 0.036 [-20.927, 23.114], mean action: 0.734 [0.000, 3.000], mean observation: 0.134 [-1.085, 1.000], loss: 1.582391, mean_absolute_error: 28.495818, mean_q: 31.138863, mean_eps: 0.325470
  750979/2000000: episode: 5118, duration: 15.598s, episode steps: 1000, steps per second: 64, episode reward: 18.850, mean reward: 0.019 [-19.602, 29.522], mean action: 0.957 [0.000, 3.000], mean observation: 0.252 [-0.837, 1.000], loss: 1.598431, mean_absolute_error: 28.116686, mean_q: 30.927683, mean_eps: 0.324570
  751979/2000000: episode: 5119, duration: 16.882s, episode steps: 1000, steps per second: 59, episode reward: 17.477, mean reward: 0.017 [-22.952, 23.270], mean action: 1.566 [0.000, 3.000], mean observation: 0.008 [-0.913, 1.000], loss: 1.672094, mean_absolute_error: 28.482151, mean_q: 30.987590, mean_eps: 0.323670
  752080/2000000: episode: 5120, duration: 1.551s, episode steps: 101, steps per second: 65, episode reward: -111.881, mean reward: -1.108 [-100.000, 9.396], mean action: 1.485 [0.000, 3.000], mean observation: -0.015 [-0.968, 1.955], loss: 2.062434, mean_absolute_error: 29.343911, mean_q: 31.395049, mean_eps: 0.323175
  753080/2000000: episode: 5121, duration: 15.884s, episode steps: 1000, steps per second: 63, episode reward: 20.483, mean reward: 0.020 [-19.027, 23.419], mean action: 1.097 [0.000, 3.000], mean observation: 0.210 [-0.707, 1.000], loss: 1.599421, mean_absolute_error: 28.131371, mean_q: 30.842858, mean_eps: 0.322680
  753316/2000000: episode: 5122, duration: 3.660s, episode steps: 236, steps per second: 64, episode reward: -19.065, mean reward: -0.081 [-100.000, 10.946], mean action: 1.814 [0.000, 3.000], mean observation: 0.060 [-0.685, 1.000], loss: 2.005585, mean_absolute_error: 28.795012, mean_q: 31.546032, mean_eps: 0.322124
  754316/2000000: episode: 5123, duration: 15.846s, episode steps: 1000, steps per second: 63, episode reward: 92.088, mean reward: 0.092 [-21.647, 22.817], mean action: 1.014 [0.000, 3.000], mean observation: 0.188 [-0.702, 1.000], loss: 1.600666, mean_absolute_error: 28.452172, mean_q: 31.413169, mean_eps: 0.321567
  755316/2000000: episode: 5124, duration: 16.654s, episode steps: 1000, steps per second: 60, episode reward: 62.471, mean reward: 0.062 [-20.354, 23.059], mean action: 1.019 [0.000, 3.000], mean observation: 0.158 [-0.454, 1.000], loss: 1.742605, mean_absolute_error: 28.294614, mean_q: 31.412563, mean_eps: 0.320667
  756316/2000000: episode: 5125, duration: 16.243s, episode steps: 1000, steps per second: 62, episode reward: 81.715, mean reward: 0.082 [-21.748, 23.208], mean action: 0.901 [0.000, 3.000], mean observation: 0.141 [-1.121, 1.000], loss: 1.723680, mean_absolute_error: 28.038554, mean_q: 30.774282, mean_eps: 0.319767
  756453/2000000: episode: 5126, duration: 2.110s, episode steps: 137, steps per second: 65, episode reward: -2.952, mean reward: -0.022 [-100.000, 14.639], mean action: 1.745 [0.000, 3.000], mean observation: -0.046 [-0.824, 1.163], loss: 1.635758, mean_absolute_error: 28.053516, mean_q: 32.743312, mean_eps: 0.319254
  757453/2000000: episode: 5127, duration: 17.037s, episode steps: 1000, steps per second: 59, episode reward: 51.280, mean reward: 0.051 [-18.800, 22.757], mean action: 1.341 [0.000, 3.000], mean observation: 0.077 [-0.720, 1.038], loss: 1.549675, mean_absolute_error: 27.619936, mean_q: 30.711688, mean_eps: 0.318741
  758193/2000000: episode: 5128, duration: 11.515s, episode steps: 740, steps per second: 64, episode reward: 160.768, mean reward: 0.217 [-19.945, 100.000], mean action: 1.377 [0.000, 3.000], mean observation: 0.153 [-0.894, 1.202], loss: 1.636553, mean_absolute_error: 27.788502, mean_q: 30.330102, mean_eps: 0.317958
  759193/2000000: episode: 5129, duration: 16.447s, episode steps: 1000, steps per second: 61, episode reward: 84.949, mean reward: 0.085 [-21.769, 23.085], mean action: 0.921 [0.000, 3.000], mean observation: 0.203 [-1.319, 1.000], loss: 1.690797, mean_absolute_error: 27.971037, mean_q: 30.857468, mean_eps: 0.317175
  759311/2000000: episode: 5130, duration: 1.786s, episode steps: 118, steps per second: 66, episode reward: -78.257, mean reward: -0.663 [-100.000, 9.906], mean action: 1.754 [0.000, 3.000], mean observation: 0.103 [-0.696, 1.624], loss: 2.159320, mean_absolute_error: 27.532741, mean_q: 30.266069, mean_eps: 0.316673
  760311/2000000: episode: 5131, duration: 16.700s, episode steps: 1000, steps per second: 60, episode reward: 87.714, mean reward: 0.088 [-19.631, 25.879], mean action: 0.928 [0.000, 3.000], mean observation: 0.154 [-0.635, 1.029], loss: 1.698198, mean_absolute_error: 27.845248, mean_q: 30.879053, mean_eps: 0.316171
  760744/2000000: episode: 5132, duration: 6.731s, episode steps: 433, steps per second: 64, episode reward: -167.904, mean reward: -0.388 [-100.000, 19.596], mean action: 1.282 [0.000, 3.000], mean observation: 0.044 [-0.973, 1.000], loss: 1.606072, mean_absolute_error: 28.298705, mean_q: 31.014512, mean_eps: 0.315527
  761744/2000000: episode: 5133, duration: 17.098s, episode steps: 1000, steps per second: 58, episode reward: 4.289, mean reward: 0.004 [-24.022, 22.133], mean action: 1.212 [0.000, 3.000], mean observation: 0.084 [-0.663, 1.000], loss: 1.641485, mean_absolute_error: 28.349246, mean_q: 31.852984, mean_eps: 0.314882
  762744/2000000: episode: 5134, duration: 19.297s, episode steps: 1000, steps per second: 52, episode reward: 54.918, mean reward: 0.055 [-19.460, 22.909], mean action: 1.062 [0.000, 3.000], mean observation: 0.124 [-0.792, 1.000], loss: 1.698310, mean_absolute_error: 28.267293, mean_q: 31.209338, mean_eps: 0.313982
  763744/2000000: episode: 5135, duration: 16.517s, episode steps: 1000, steps per second: 61, episode reward: 46.072, mean reward: 0.046 [-20.851, 21.729], mean action: 1.128 [0.000, 3.000], mean observation: 0.156 [-0.616, 1.000], loss: 1.627204, mean_absolute_error: 28.034288, mean_q: 30.725624, mean_eps: 0.313082
  763856/2000000: episode: 5136, duration: 1.762s, episode steps: 112, steps per second: 64, episode reward: -26.535, mean reward: -0.237 [-100.000, 15.806], mean action: 1.830 [0.000, 3.000], mean observation: -0.012 [-0.967, 1.153], loss: 1.579979, mean_absolute_error: 28.837608, mean_q: 31.829447, mean_eps: 0.312582
  764856/2000000: episode: 5137, duration: 15.891s, episode steps: 1000, steps per second: 63, episode reward: 52.402, mean reward: 0.052 [-20.520, 22.917], mean action: 0.969 [0.000, 3.000], mean observation: 0.160 [-0.599, 1.000], loss: 1.658481, mean_absolute_error: 28.060235, mean_q: 31.530146, mean_eps: 0.312081
  765856/2000000: episode: 5138, duration: 16.199s, episode steps: 1000, steps per second: 62, episode reward: -21.402, mean reward: -0.021 [-20.817, 23.302], mean action: 1.222 [0.000, 3.000], mean observation: 0.090 [-0.814, 1.000], loss: 1.670586, mean_absolute_error: 27.526248, mean_q: 30.550019, mean_eps: 0.311181
  766856/2000000: episode: 5139, duration: 15.572s, episode steps: 1000, steps per second: 64, episode reward: 89.611, mean reward: 0.090 [-22.376, 23.206], mean action: 0.849 [0.000, 3.000], mean observation: 0.209 [-0.988, 1.000], loss: 1.734295, mean_absolute_error: 27.579718, mean_q: 31.509958, mean_eps: 0.310281
  767856/2000000: episode: 5140, duration: 16.691s, episode steps: 1000, steps per second: 60, episode reward: 51.898, mean reward: 0.052 [-23.124, 23.271], mean action: 1.164 [0.000, 3.000], mean observation: 0.126 [-0.623, 1.010], loss: 1.680951, mean_absolute_error: 28.128470, mean_q: 31.470555, mean_eps: 0.309381
  768004/2000000: episode: 5141, duration: 2.301s, episode steps: 148, steps per second: 64, episode reward: -45.538, mean reward: -0.308 [-100.000, 13.381], mean action: 1.797 [0.000, 3.000], mean observation: -0.105 [-1.300, 1.000], loss: 1.593638, mean_absolute_error: 27.651198, mean_q: 30.093245, mean_eps: 0.308865
  768474/2000000: episode: 5142, duration: 7.674s, episode steps: 470, steps per second: 61, episode reward: 192.463, mean reward: 0.409 [-23.750, 100.000], mean action: 1.028 [0.000, 3.000], mean observation: 0.134 [-0.714, 1.000], loss: 1.915249, mean_absolute_error: 27.533261, mean_q: 29.945274, mean_eps: 0.308586
  768745/2000000: episode: 5143, duration: 4.175s, episode steps: 271, steps per second: 65, episode reward: -156.696, mean reward: -0.578 [-100.000, 14.493], mean action: 1.819 [0.000, 3.000], mean observation: 0.132 [-0.911, 1.193], loss: 1.675235, mean_absolute_error: 27.915593, mean_q: 31.353169, mean_eps: 0.308251
  769745/2000000: episode: 5144, duration: 16.280s, episode steps: 1000, steps per second: 61, episode reward: 53.146, mean reward: 0.053 [-21.084, 23.101], mean action: 0.870 [0.000, 3.000], mean observation: 0.138 [-0.849, 1.000], loss: 1.841285, mean_absolute_error: 28.033478, mean_q: 31.141498, mean_eps: 0.307679
  769856/2000000: episode: 5145, duration: 1.755s, episode steps: 111, steps per second: 63, episode reward: -37.944, mean reward: -0.342 [-100.000, 17.823], mean action: 1.829 [0.000, 3.000], mean observation: -0.065 [-0.732, 1.584], loss: 1.647795, mean_absolute_error: 27.187483, mean_q: 31.130210, mean_eps: 0.307180
  770742/2000000: episode: 5146, duration: 14.767s, episode steps: 886, steps per second: 60, episode reward: 99.827, mean reward: 0.113 [-20.298, 100.000], mean action: 1.205 [0.000, 3.000], mean observation: 0.086 [-0.744, 1.000], loss: 1.631443, mean_absolute_error: 28.331681, mean_q: 31.992165, mean_eps: 0.306732
  771742/2000000: episode: 5147, duration: 16.262s, episode steps: 1000, steps per second: 61, episode reward: 70.977, mean reward: 0.071 [-22.158, 23.062], mean action: 0.738 [0.000, 3.000], mean observation: 0.211 [-0.652, 1.000], loss: 1.694972, mean_absolute_error: 28.393625, mean_q: 32.230283, mean_eps: 0.305882
  771997/2000000: episode: 5148, duration: 4.101s, episode steps: 255, steps per second: 62, episode reward: -39.268, mean reward: -0.154 [-100.000, 14.877], mean action: 1.875 [0.000, 3.000], mean observation: 0.055 [-1.443, 1.000], loss: 1.579581, mean_absolute_error: 27.780075, mean_q: 31.505329, mean_eps: 0.305317
  772997/2000000: episode: 5149, duration: 16.861s, episode steps: 1000, steps per second: 59, episode reward: 26.354, mean reward: 0.026 [-23.636, 22.996], mean action: 1.287 [0.000, 3.000], mean observation: 0.066 [-0.684, 1.000], loss: 1.691479, mean_absolute_error: 28.221605, mean_q: 32.700160, mean_eps: 0.304752
  773997/2000000: episode: 5150, duration: 17.578s, episode steps: 1000, steps per second: 57, episode reward: 6.914, mean reward: 0.007 [-20.782, 22.911], mean action: 1.059 [0.000, 3.000], mean observation: 0.164 [-0.726, 1.000], loss: 1.505503, mean_absolute_error: 27.749091, mean_q: 31.608527, mean_eps: 0.303852
  774404/2000000: episode: 5151, duration: 6.255s, episode steps: 407, steps per second: 65, episode reward: -213.732, mean reward: -0.525 [-100.000, 16.228], mean action: 1.462 [0.000, 3.000], mean observation: 0.184 [-1.095, 1.013], loss: 1.670933, mean_absolute_error: 28.194219, mean_q: 32.192326, mean_eps: 0.303220
  775404/2000000: episode: 5152, duration: 15.869s, episode steps: 1000, steps per second: 63, episode reward: 111.114, mean reward: 0.111 [-19.382, 22.936], mean action: 0.982 [0.000, 3.000], mean observation: 0.201 [-0.605, 1.000], loss: 1.662795, mean_absolute_error: 27.791787, mean_q: 31.616914, mean_eps: 0.302588
  776404/2000000: episode: 5153, duration: 16.208s, episode steps: 1000, steps per second: 62, episode reward: 57.862, mean reward: 0.058 [-24.469, 22.273], mean action: 0.964 [0.000, 3.000], mean observation: 0.174 [-0.679, 1.000], loss: 1.572152, mean_absolute_error: 28.161062, mean_q: 32.396964, mean_eps: 0.301688
  776543/2000000: episode: 5154, duration: 2.148s, episode steps: 139, steps per second: 65, episode reward: -13.352, mean reward: -0.096 [-100.000, 10.766], mean action: 1.712 [0.000, 3.000], mean observation: 0.033 [-0.682, 1.000], loss: 1.500331, mean_absolute_error: 27.664656, mean_q: 31.494406, mean_eps: 0.301175
  777543/2000000: episode: 5155, duration: 15.957s, episode steps: 1000, steps per second: 63, episode reward: 28.328, mean reward: 0.028 [-20.076, 22.652], mean action: 0.871 [0.000, 3.000], mean observation: 0.131 [-0.769, 1.000], loss: 1.608308, mean_absolute_error: 27.570422, mean_q: 31.772116, mean_eps: 0.300662
  777647/2000000: episode: 5156, duration: 1.562s, episode steps: 104, steps per second: 67, episode reward: -63.979, mean reward: -0.615 [-100.000, 12.533], mean action: 1.798 [0.000, 3.000], mean observation: 0.162 [-0.764, 1.477], loss: 1.766825, mean_absolute_error: 28.584428, mean_q: 32.820623, mean_eps: 0.300165
  778647/2000000: episode: 5157, duration: 15.918s, episode steps: 1000, steps per second: 63, episode reward: 71.248, mean reward: 0.071 [-22.855, 24.483], mean action: 1.362 [0.000, 3.000], mean observation: 0.156 [-0.722, 1.000], loss: 1.539236, mean_absolute_error: 27.891209, mean_q: 32.469911, mean_eps: 0.299669
  779549/2000000: episode: 5158, duration: 15.595s, episode steps: 902, steps per second: 58, episode reward: 181.431, mean reward: 0.201 [-20.610, 100.000], mean action: 1.119 [0.000, 3.000], mean observation: 0.123 [-0.717, 1.014], loss: 1.608131, mean_absolute_error: 27.867519, mean_q: 32.138457, mean_eps: 0.298812
  780508/2000000: episode: 5159, duration: 17.522s, episode steps: 959, steps per second: 55, episode reward: 119.236, mean reward: 0.124 [-24.485, 100.000], mean action: 0.970 [0.000, 3.000], mean observation: 0.136 [-0.907, 1.000], loss: 1.598568, mean_absolute_error: 27.791012, mean_q: 32.143999, mean_eps: 0.297975
  781033/2000000: episode: 5160, duration: 9.202s, episode steps: 525, steps per second: 57, episode reward: 134.895, mean reward: 0.257 [-21.297, 100.000], mean action: 1.339 [0.000, 3.000], mean observation: 0.143 [-1.025, 1.000], loss: 1.645122, mean_absolute_error: 28.270415, mean_q: 32.513835, mean_eps: 0.297307
  782033/2000000: episode: 5161, duration: 17.342s, episode steps: 1000, steps per second: 58, episode reward: 73.689, mean reward: 0.074 [-22.761, 23.247], mean action: 1.029 [0.000, 3.000], mean observation: 0.128 [-0.862, 1.000], loss: 1.695609, mean_absolute_error: 28.174205, mean_q: 32.117785, mean_eps: 0.296619
  782310/2000000: episode: 5162, duration: 4.583s, episode steps: 277, steps per second: 60, episode reward: -78.043, mean reward: -0.282 [-100.000, 11.591], mean action: 1.823 [0.000, 3.000], mean observation: -0.052 [-0.798, 1.000], loss: 1.681736, mean_absolute_error: 27.424066, mean_q: 32.125955, mean_eps: 0.296045
  783310/2000000: episode: 5163, duration: 16.824s, episode steps: 1000, steps per second: 59, episode reward: 96.977, mean reward: 0.097 [-21.819, 23.209], mean action: 0.821 [0.000, 3.000], mean observation: 0.162 [-0.819, 1.000], loss: 1.563949, mean_absolute_error: 27.742728, mean_q: 32.500503, mean_eps: 0.295471
  784310/2000000: episode: 5164, duration: 17.772s, episode steps: 1000, steps per second: 56, episode reward: 79.047, mean reward: 0.079 [-20.325, 22.666], mean action: 1.024 [0.000, 3.000], mean observation: 0.117 [-0.738, 1.000], loss: 1.584317, mean_absolute_error: 27.618974, mean_q: 32.041374, mean_eps: 0.294571
  785310/2000000: episode: 5165, duration: 18.300s, episode steps: 1000, steps per second: 55, episode reward: 93.586, mean reward: 0.094 [-21.613, 22.835], mean action: 0.779 [0.000, 3.000], mean observation: 0.183 [-1.057, 1.000], loss: 1.533391, mean_absolute_error: 27.449629, mean_q: 31.930973, mean_eps: 0.293671
  786310/2000000: episode: 5166, duration: 18.490s, episode steps: 1000, steps per second: 54, episode reward: 25.307, mean reward: 0.025 [-22.496, 24.118], mean action: 1.270 [0.000, 3.000], mean observation: 0.166 [-1.073, 1.000], loss: 1.651389, mean_absolute_error: 27.762410, mean_q: 32.098107, mean_eps: 0.292771
  787310/2000000: episode: 5167, duration: 18.874s, episode steps: 1000, steps per second: 53, episode reward: 132.617, mean reward: 0.133 [-21.610, 23.232], mean action: 0.894 [0.000, 3.000], mean observation: 0.207 [-0.731, 1.000], loss: 1.560591, mean_absolute_error: 27.442802, mean_q: 31.455699, mean_eps: 0.291871
  788310/2000000: episode: 5168, duration: 20.012s, episode steps: 1000, steps per second: 50, episode reward: 11.406, mean reward: 0.011 [-18.282, 21.425], mean action: 1.152 [0.000, 3.000], mean observation: 0.107 [-0.700, 1.000], loss: 1.553491, mean_absolute_error: 27.556300, mean_q: 32.379662, mean_eps: 0.290971
  789310/2000000: episode: 5169, duration: 18.631s, episode steps: 1000, steps per second: 54, episode reward: 78.842, mean reward: 0.079 [-24.469, 22.042], mean action: 1.111 [0.000, 3.000], mean observation: 0.135 [-0.697, 1.000], loss: 1.600632, mean_absolute_error: 27.545576, mean_q: 32.106889, mean_eps: 0.290071
  790310/2000000: episode: 5170, duration: 17.210s, episode steps: 1000, steps per second: 58, episode reward: 9.082, mean reward: 0.009 [-19.580, 23.135], mean action: 1.426 [0.000, 3.000], mean observation: 0.058 [-0.695, 1.041], loss: 1.499084, mean_absolute_error: 27.325513, mean_q: 31.828851, mean_eps: 0.289171
  791310/2000000: episode: 5171, duration: 16.384s, episode steps: 1000, steps per second: 61, episode reward: 77.030, mean reward: 0.077 [-24.445, 23.833], mean action: 0.708 [0.000, 3.000], mean observation: 0.221 [-1.454, 1.000], loss: 1.472923, mean_absolute_error: 27.646579, mean_q: 32.952505, mean_eps: 0.288271
  791728/2000000: episode: 5172, duration: 6.594s, episode steps: 418, steps per second: 63, episode reward: -135.512, mean reward: -0.324 [-100.000, 11.314], mean action: 1.780 [0.000, 3.000], mean observation: 0.083 [-0.778, 1.001], loss: 1.524424, mean_absolute_error: 27.879372, mean_q: 32.451488, mean_eps: 0.287634
  792477/2000000: episode: 5173, duration: 12.544s, episode steps: 749, steps per second: 60, episode reward: 205.200, mean reward: 0.274 [-23.595, 100.000], mean action: 0.889 [0.000, 3.000], mean observation: 0.184 [-0.688, 1.000], loss: 1.675448, mean_absolute_error: 27.156831, mean_q: 31.693550, mean_eps: 0.287108
  793477/2000000: episode: 5174, duration: 17.493s, episode steps: 1000, steps per second: 57, episode reward: 20.360, mean reward: 0.020 [-22.551, 22.636], mean action: 1.017 [0.000, 3.000], mean observation: 0.128 [-0.551, 1.000], loss: 1.558386, mean_absolute_error: 26.996952, mean_q: 31.810917, mean_eps: 0.286320
  794477/2000000: episode: 5175, duration: 16.727s, episode steps: 1000, steps per second: 60, episode reward: 56.758, mean reward: 0.057 [-21.517, 23.116], mean action: 1.028 [0.000, 3.000], mean observation: 0.140 [-0.739, 1.000], loss: 1.591463, mean_absolute_error: 26.654792, mean_q: 31.153587, mean_eps: 0.285420
  794672/2000000: episode: 5176, duration: 3.001s, episode steps: 195, steps per second: 65, episode reward: -90.183, mean reward: -0.462 [-100.000, 10.934], mean action: 1.697 [0.000, 3.000], mean observation: 0.119 [-0.531, 1.309], loss: 1.611706, mean_absolute_error: 27.685998, mean_q: 32.736161, mean_eps: 0.284883
  795672/2000000: episode: 5177, duration: 16.049s, episode steps: 1000, steps per second: 62, episode reward: 67.439, mean reward: 0.067 [-24.474, 25.991], mean action: 0.835 [0.000, 3.000], mean observation: 0.200 [-0.765, 1.000], loss: 1.502271, mean_absolute_error: 27.488757, mean_q: 32.065554, mean_eps: 0.284347
  796672/2000000: episode: 5178, duration: 16.410s, episode steps: 1000, steps per second: 61, episode reward: 89.390, mean reward: 0.089 [-23.066, 23.052], mean action: 0.695 [0.000, 3.000], mean observation: 0.202 [-0.630, 1.000], loss: 1.526793, mean_absolute_error: 26.540763, mean_q: 31.295272, mean_eps: 0.283447
  796776/2000000: episode: 5179, duration: 1.634s, episode steps: 104, steps per second: 64, episode reward: -40.176, mean reward: -0.386 [-100.000, 18.244], mean action: 1.856 [0.000, 3.000], mean observation: -0.117 [-0.775, 1.529], loss: 1.117757, mean_absolute_error: 25.285096, mean_q: 30.662249, mean_eps: 0.282950
  797776/2000000: episode: 5180, duration: 16.525s, episode steps: 1000, steps per second: 61, episode reward: 11.779, mean reward: 0.012 [-20.809, 23.667], mean action: 1.161 [0.000, 3.000], mean observation: 0.108 [-0.562, 1.000], loss: 1.573965, mean_absolute_error: 27.132446, mean_q: 32.386471, mean_eps: 0.282453
  798186/2000000: episode: 5181, duration: 6.343s, episode steps: 410, steps per second: 65, episode reward: -70.066, mean reward: -0.171 [-100.000, 14.533], mean action: 1.500 [0.000, 3.000], mean observation: 0.145 [-0.825, 1.000], loss: 1.634918, mean_absolute_error: 27.273114, mean_q: 32.461489, mean_eps: 0.281818
  798698/2000000: episode: 5182, duration: 8.169s, episode steps: 512, steps per second: 63, episode reward: 189.803, mean reward: 0.371 [-19.036, 100.000], mean action: 1.215 [0.000, 3.000], mean observation: 0.127 [-0.762, 1.000], loss: 1.627370, mean_absolute_error: 27.197207, mean_q: 32.207046, mean_eps: 0.281402
  799141/2000000: episode: 5183, duration: 6.959s, episode steps: 443, steps per second: 64, episode reward: -70.585, mean reward: -0.159 [-100.000, 16.781], mean action: 1.641 [0.000, 3.000], mean observation: -0.021 [-0.711, 1.000], loss: 1.639271, mean_absolute_error: 26.864400, mean_q: 31.990415, mean_eps: 0.280972
  800141/2000000: episode: 5184, duration: 15.826s, episode steps: 1000, steps per second: 63, episode reward: 123.978, mean reward: 0.124 [-24.387, 23.299], mean action: 0.877 [0.000, 3.000], mean observation: 0.142 [-0.832, 1.000], loss: 1.535022, mean_absolute_error: 27.172957, mean_q: 32.345647, mean_eps: 0.280322
  801141/2000000: episode: 5185, duration: 16.299s, episode steps: 1000, steps per second: 61, episode reward: 24.990, mean reward: 0.025 [-23.568, 22.602], mean action: 0.876 [0.000, 3.000], mean observation: 0.140 [-0.863, 1.000], loss: 1.489128, mean_absolute_error: 27.130842, mean_q: 32.314675, mean_eps: 0.279422
  802141/2000000: episode: 5186, duration: 16.631s, episode steps: 1000, steps per second: 60, episode reward: 44.013, mean reward: 0.044 [-19.654, 23.051], mean action: 1.139 [0.000, 3.000], mean observation: 0.107 [-0.668, 1.000], loss: 1.553638, mean_absolute_error: 26.911796, mean_q: 32.244380, mean_eps: 0.278522
  802312/2000000: episode: 5187, duration: 2.630s, episode steps: 171, steps per second: 65, episode reward: -40.214, mean reward: -0.235 [-100.000, 18.455], mean action: 1.772 [0.000, 3.000], mean observation: 0.055 [-0.730, 1.232], loss: 1.457096, mean_absolute_error: 27.304779, mean_q: 32.313961, mean_eps: 0.277997
  802480/2000000: episode: 5188, duration: 2.632s, episode steps: 168, steps per second: 64, episode reward: -32.064, mean reward: -0.191 [-100.000, 10.228], mean action: 1.762 [0.000, 3.000], mean observation: 0.068 [-0.715, 1.000], loss: 1.494457, mean_absolute_error: 27.580970, mean_q: 32.469226, mean_eps: 0.277845
  803384/2000000: episode: 5189, duration: 14.768s, episode steps: 904, steps per second: 61, episode reward: 172.035, mean reward: 0.190 [-22.513, 100.000], mean action: 1.131 [0.000, 3.000], mean observation: 0.265 [-0.637, 1.000], loss: 1.483940, mean_absolute_error: 27.207103, mean_q: 32.412740, mean_eps: 0.277363
  804384/2000000: episode: 5190, duration: 16.056s, episode steps: 1000, steps per second: 62, episode reward: 136.196, mean reward: 0.136 [-21.360, 23.101], mean action: 0.846 [0.000, 3.000], mean observation: 0.218 [-0.762, 1.095], loss: 1.569225, mean_absolute_error: 26.954201, mean_q: 31.958018, mean_eps: 0.276506
  805384/2000000: episode: 5191, duration: 16.072s, episode steps: 1000, steps per second: 62, episode reward: 97.570, mean reward: 0.098 [-23.812, 23.431], mean action: 1.287 [0.000, 3.000], mean observation: 0.236 [-0.749, 1.514], loss: 1.479574, mean_absolute_error: 27.208107, mean_q: 32.444717, mean_eps: 0.275606
  805899/2000000: episode: 5192, duration: 8.419s, episode steps: 515, steps per second: 61, episode reward: 138.781, mean reward: 0.269 [-19.148, 100.000], mean action: 1.458 [0.000, 3.000], mean observation: 0.076 [-0.678, 1.000], loss: 1.494184, mean_absolute_error: 26.645367, mean_q: 30.631861, mean_eps: 0.274924
  806108/2000000: episode: 5193, duration: 3.250s, episode steps: 209, steps per second: 64, episode reward: -57.476, mean reward: -0.275 [-100.000, 11.436], mean action: 1.651 [0.000, 3.000], mean observation: -0.059 [-0.784, 1.000], loss: 1.406243, mean_absolute_error: 26.877419, mean_q: 31.672130, mean_eps: 0.274598
  807108/2000000: episode: 5194, duration: 17.504s, episode steps: 1000, steps per second: 57, episode reward: 41.758, mean reward: 0.042 [-21.634, 23.064], mean action: 1.032 [0.000, 3.000], mean observation: 0.157 [-0.509, 1.000], loss: 1.558592, mean_absolute_error: 26.837676, mean_q: 31.580344, mean_eps: 0.274055
  808108/2000000: episode: 5195, duration: 16.419s, episode steps: 1000, steps per second: 61, episode reward: 31.075, mean reward: 0.031 [-24.416, 23.035], mean action: 0.929 [0.000, 3.000], mean observation: 0.150 [-0.886, 1.000], loss: 1.501571, mean_absolute_error: 26.483047, mean_q: 31.387076, mean_eps: 0.273155
  809108/2000000: episode: 5196, duration: 17.030s, episode steps: 1000, steps per second: 59, episode reward: 53.895, mean reward: 0.054 [-20.648, 23.037], mean action: 1.279 [0.000, 3.000], mean observation: 0.179 [-0.580, 1.000], loss: 1.539098, mean_absolute_error: 26.859609, mean_q: 31.956837, mean_eps: 0.272255
  809213/2000000: episode: 5197, duration: 1.659s, episode steps: 105, steps per second: 63, episode reward: -51.618, mean reward: -0.492 [-100.000, 10.333], mean action: 1.743 [0.000, 3.000], mean observation: 0.094 [-0.701, 1.000], loss: 1.445693, mean_absolute_error: 27.049370, mean_q: 32.360944, mean_eps: 0.271756
  810213/2000000: episode: 5198, duration: 15.761s, episode steps: 1000, steps per second: 63, episode reward: 124.924, mean reward: 0.125 [-20.406, 23.146], mean action: 0.732 [0.000, 3.000], mean observation: 0.205 [-0.660, 1.506], loss: 1.505798, mean_absolute_error: 26.882974, mean_q: 32.151587, mean_eps: 0.271257
  810327/2000000: episode: 5199, duration: 1.714s, episode steps: 114, steps per second: 67, episode reward: -2.117, mean reward: -0.019 [-100.000, 19.587], mean action: 1.781 [0.000, 3.000], mean observation: -0.113 [-0.888, 1.674], loss: 1.749095, mean_absolute_error: 27.401212, mean_q: 33.700066, mean_eps: 0.270757
  811327/2000000: episode: 5200, duration: 15.949s, episode steps: 1000, steps per second: 63, episode reward: -4.849, mean reward: -0.005 [-20.689, 23.597], mean action: 1.656 [0.000, 3.000], mean observation: 0.060 [-0.784, 1.000], loss: 1.566594, mean_absolute_error: 26.911705, mean_q: 32.426089, mean_eps: 0.270257
  812327/2000000: episode: 5201, duration: 15.874s, episode steps: 1000, steps per second: 63, episode reward: 121.335, mean reward: 0.121 [-23.934, 23.203], mean action: 1.001 [0.000, 3.000], mean observation: 0.209 [-0.666, 1.000], loss: 1.580404, mean_absolute_error: 26.793643, mean_q: 31.828593, mean_eps: 0.269357
  812512/2000000: episode: 5202, duration: 2.892s, episode steps: 185, steps per second: 64, episode reward: -91.903, mean reward: -0.497 [-100.000, 10.152], mean action: 1.708 [0.000, 3.000], mean observation: -0.043 [-1.099, 1.000], loss: 1.560511, mean_absolute_error: 27.355009, mean_q: 32.322636, mean_eps: 0.268824
  813371/2000000: episode: 5203, duration: 13.402s, episode steps: 859, steps per second: 64, episode reward: 159.016, mean reward: 0.185 [-18.476, 100.000], mean action: 0.993 [0.000, 3.000], mean observation: 0.148 [-0.716, 1.001], loss: 1.542557, mean_absolute_error: 27.132253, mean_q: 32.425357, mean_eps: 0.268354
  814371/2000000: episode: 5204, duration: 15.792s, episode steps: 1000, steps per second: 63, episode reward: 92.420, mean reward: 0.092 [-22.130, 23.369], mean action: 0.942 [0.000, 3.000], mean observation: 0.188 [-0.725, 1.000], loss: 1.402730, mean_absolute_error: 26.821945, mean_q: 32.515615, mean_eps: 0.267517
  815318/2000000: episode: 5205, duration: 15.243s, episode steps: 947, steps per second: 62, episode reward: 170.974, mean reward: 0.181 [-21.271, 100.000], mean action: 0.902 [0.000, 3.000], mean observation: 0.184 [-0.813, 1.000], loss: 1.614460, mean_absolute_error: 27.103196, mean_q: 31.877271, mean_eps: 0.266640
  816213/2000000: episode: 5206, duration: 14.613s, episode steps: 895, steps per second: 61, episode reward: 182.496, mean reward: 0.204 [-20.642, 100.000], mean action: 0.820 [0.000, 3.000], mean observation: 0.155 [-1.037, 1.000], loss: 1.491906, mean_absolute_error: 27.248187, mean_q: 32.737026, mean_eps: 0.265811
  817213/2000000: episode: 5207, duration: 16.599s, episode steps: 1000, steps per second: 60, episode reward: 76.618, mean reward: 0.077 [-20.326, 23.150], mean action: 1.020 [0.000, 3.000], mean observation: 0.210 [-0.820, 1.000], loss: 1.475681, mean_absolute_error: 26.902568, mean_q: 32.377838, mean_eps: 0.264957
  818213/2000000: episode: 5208, duration: 16.493s, episode steps: 1000, steps per second: 61, episode reward: 59.281, mean reward: 0.059 [-23.072, 23.707], mean action: 0.934 [0.000, 3.000], mean observation: 0.234 [-0.609, 1.000], loss: 1.346333, mean_absolute_error: 26.556866, mean_q: 32.131979, mean_eps: 0.264057
  819213/2000000: episode: 5209, duration: 16.833s, episode steps: 1000, steps per second: 59, episode reward: 51.258, mean reward: 0.051 [-22.365, 22.917], mean action: 0.984 [0.000, 3.000], mean observation: 0.148 [-0.547, 1.000], loss: 1.468789, mean_absolute_error: 26.457209, mean_q: 32.236144, mean_eps: 0.263157
  820213/2000000: episode: 5210, duration: 17.156s, episode steps: 1000, steps per second: 58, episode reward: 53.574, mean reward: 0.054 [-20.781, 23.287], mean action: 1.105 [0.000, 3.000], mean observation: 0.116 [-0.926, 1.023], loss: 1.472628, mean_absolute_error: 26.686011, mean_q: 32.161659, mean_eps: 0.262257
  821213/2000000: episode: 5211, duration: 16.559s, episode steps: 1000, steps per second: 60, episode reward: 85.669, mean reward: 0.086 [-21.194, 22.858], mean action: 0.818 [0.000, 3.000], mean observation: 0.190 [-0.700, 1.000], loss: 1.625583, mean_absolute_error: 27.219002, mean_q: 32.841267, mean_eps: 0.261357
  822213/2000000: episode: 5212, duration: 16.023s, episode steps: 1000, steps per second: 62, episode reward: 90.202, mean reward: 0.090 [-19.393, 23.069], mean action: 0.826 [0.000, 3.000], mean observation: 0.179 [-0.650, 1.000], loss: 1.562342, mean_absolute_error: 26.705439, mean_q: 32.426835, mean_eps: 0.260457
  822610/2000000: episode: 5213, duration: 6.267s, episode steps: 397, steps per second: 63, episode reward: -89.845, mean reward: -0.226 [-100.000, 19.134], mean action: 1.720 [0.000, 3.000], mean observation: 0.050 [-0.875, 1.000], loss: 1.527870, mean_absolute_error: 26.908825, mean_q: 32.894897, mean_eps: 0.259829
  823072/2000000: episode: 5214, duration: 7.242s, episode steps: 462, steps per second: 64, episode reward: 184.575, mean reward: 0.400 [-22.610, 100.000], mean action: 1.701 [0.000, 3.000], mean observation: 0.222 [-0.763, 1.301], loss: 1.604965, mean_absolute_error: 27.201847, mean_q: 32.993592, mean_eps: 0.259444
  823986/2000000: episode: 5215, duration: 15.187s, episode steps: 914, steps per second: 60, episode reward: 197.045, mean reward: 0.216 [-23.151, 100.000], mean action: 0.899 [0.000, 3.000], mean observation: 0.151 [-0.774, 1.000], loss: 1.554634, mean_absolute_error: 26.769864, mean_q: 32.092213, mean_eps: 0.258825
  824381/2000000: episode: 5216, duration: 6.155s, episode steps: 395, steps per second: 64, episode reward: -76.644, mean reward: -0.194 [-100.000, 12.507], mean action: 1.719 [0.000, 3.000], mean observation: 0.108 [-0.961, 1.000], loss: 1.340793, mean_absolute_error: 26.180905, mean_q: 31.858438, mean_eps: 0.258234
  824662/2000000: episode: 5217, duration: 4.320s, episode steps: 281, steps per second: 65, episode reward: -63.726, mean reward: -0.227 [-100.000, 20.759], mean action: 1.754 [0.000, 3.000], mean observation: 0.087 [-0.735, 1.000], loss: 1.475030, mean_absolute_error: 27.533572, mean_q: 33.264043, mean_eps: 0.257930
  825375/2000000: episode: 5218, duration: 11.840s, episode steps: 713, steps per second: 60, episode reward: 179.330, mean reward: 0.252 [-20.684, 100.000], mean action: 1.199 [0.000, 3.000], mean observation: 0.136 [-1.064, 1.000], loss: 1.424132, mean_absolute_error: 26.867911, mean_q: 32.349795, mean_eps: 0.257484
  826375/2000000: episode: 5219, duration: 16.237s, episode steps: 1000, steps per second: 62, episode reward: 58.264, mean reward: 0.058 [-20.105, 26.911], mean action: 0.887 [0.000, 3.000], mean observation: 0.155 [-0.599, 1.000], loss: 1.417798, mean_absolute_error: 27.000449, mean_q: 32.982139, mean_eps: 0.256713
  827375/2000000: episode: 5220, duration: 16.681s, episode steps: 1000, steps per second: 60, episode reward: 66.607, mean reward: 0.067 [-21.728, 23.009], mean action: 0.778 [0.000, 3.000], mean observation: 0.201 [-1.307, 1.000], loss: 1.584699, mean_absolute_error: 26.748440, mean_q: 32.647894, mean_eps: 0.255813
  828072/2000000: episode: 5221, duration: 11.113s, episode steps: 697, steps per second: 63, episode reward: 180.812, mean reward: 0.259 [-19.753, 100.000], mean action: 0.941 [0.000, 3.000], mean observation: 0.157 [-0.591, 1.000], loss: 1.436688, mean_absolute_error: 26.366307, mean_q: 32.188539, mean_eps: 0.255050
  829072/2000000: episode: 5222, duration: 15.963s, episode steps: 1000, steps per second: 63, episode reward: 104.172, mean reward: 0.104 [-23.662, 22.940], mean action: 0.875 [0.000, 3.000], mean observation: 0.172 [-0.599, 1.000], loss: 1.458564, mean_absolute_error: 26.766903, mean_q: 32.599525, mean_eps: 0.254287
  830072/2000000: episode: 5223, duration: 16.315s, episode steps: 1000, steps per second: 61, episode reward: 52.355, mean reward: 0.052 [-20.674, 23.143], mean action: 0.915 [0.000, 3.000], mean observation: 0.172 [-0.832, 1.000], loss: 1.425727, mean_absolute_error: 26.653070, mean_q: 32.849434, mean_eps: 0.253387
  830811/2000000: episode: 5224, duration: 12.184s, episode steps: 739, steps per second: 61, episode reward: 233.157, mean reward: 0.316 [-20.658, 100.000], mean action: 1.145 [0.000, 3.000], mean observation: 0.203 [-0.771, 1.000], loss: 1.353312, mean_absolute_error: 26.078743, mean_q: 32.157096, mean_eps: 0.252604
  831811/2000000: episode: 5225, duration: 17.326s, episode steps: 1000, steps per second: 58, episode reward: 72.329, mean reward: 0.072 [-21.187, 23.300], mean action: 1.145 [0.000, 3.000], mean observation: 0.100 [-0.782, 1.000], loss: 1.523225, mean_absolute_error: 26.137540, mean_q: 31.749722, mean_eps: 0.251821
  832119/2000000: episode: 5226, duration: 4.899s, episode steps: 308, steps per second: 63, episode reward: -83.943, mean reward: -0.273 [-100.000, 14.686], mean action: 1.672 [0.000, 3.000], mean observation: 0.075 [-0.870, 1.000], loss: 1.598349, mean_absolute_error: 27.161070, mean_q: 33.538297, mean_eps: 0.251232
  832902/2000000: episode: 5227, duration: 13.309s, episode steps: 783, steps per second: 59, episode reward: 162.076, mean reward: 0.207 [-24.181, 100.000], mean action: 1.023 [0.000, 3.000], mean observation: 0.143 [-0.624, 1.000], loss: 1.553007, mean_absolute_error: 26.422300, mean_q: 32.474876, mean_eps: 0.250741
  833423/2000000: episode: 5228, duration: 8.330s, episode steps: 521, steps per second: 63, episode reward: 198.737, mean reward: 0.381 [-18.139, 100.000], mean action: 1.004 [0.000, 3.000], mean observation: 0.146 [-0.604, 1.000], loss: 1.673676, mean_absolute_error: 26.737017, mean_q: 32.711235, mean_eps: 0.250154
  834423/2000000: episode: 5229, duration: 16.801s, episode steps: 1000, steps per second: 60, episode reward: 101.015, mean reward: 0.101 [-19.030, 23.042], mean action: 0.949 [0.000, 3.000], mean observation: 0.119 [-0.804, 1.000], loss: 1.492940, mean_absolute_error: 26.586142, mean_q: 32.366716, mean_eps: 0.249470
  835423/2000000: episode: 5230, duration: 16.518s, episode steps: 1000, steps per second: 61, episode reward: 52.696, mean reward: 0.053 [-24.879, 23.372], mean action: 1.159 [0.000, 3.000], mean observation: 0.120 [-0.757, 1.062], loss: 1.515218, mean_absolute_error: 26.866609, mean_q: 32.735999, mean_eps: 0.248570
  836083/2000000: episode: 5231, duration: 10.897s, episode steps: 660, steps per second: 61, episode reward: 198.789, mean reward: 0.301 [-19.124, 100.000], mean action: 1.127 [0.000, 3.000], mean observation: 0.122 [-0.733, 1.000], loss: 1.404137, mean_absolute_error: 26.732938, mean_q: 32.863760, mean_eps: 0.247823
  837083/2000000: episode: 5232, duration: 16.197s, episode steps: 1000, steps per second: 62, episode reward: 51.852, mean reward: 0.052 [-20.781, 22.753], mean action: 1.016 [0.000, 3.000], mean observation: 0.177 [-0.636, 1.000], loss: 1.495915, mean_absolute_error: 26.348939, mean_q: 32.242412, mean_eps: 0.247076
  837441/2000000: episode: 5233, duration: 5.760s, episode steps: 358, steps per second: 62, episode reward: -86.572, mean reward: -0.242 [-100.000, 10.785], mean action: 1.718 [0.000, 3.000], mean observation: 0.080 [-0.843, 1.000], loss: 1.394629, mean_absolute_error: 26.732856, mean_q: 32.731487, mean_eps: 0.246464
  838441/2000000: episode: 5234, duration: 16.312s, episode steps: 1000, steps per second: 61, episode reward: 114.146, mean reward: 0.114 [-24.552, 22.829], mean action: 0.769 [0.000, 3.000], mean observation: 0.175 [-0.565, 1.000], loss: 1.496548, mean_absolute_error: 26.699798, mean_q: 32.719739, mean_eps: 0.245852
  839441/2000000: episode: 5235, duration: 16.816s, episode steps: 1000, steps per second: 59, episode reward: 25.634, mean reward: 0.026 [-21.125, 22.818], mean action: 1.037 [0.000, 3.000], mean observation: 0.151 [-0.574, 1.000], loss: 1.430505, mean_absolute_error: 26.406705, mean_q: 31.879881, mean_eps: 0.244952
  840374/2000000: episode: 5236, duration: 15.572s, episode steps: 933, steps per second: 60, episode reward: 153.315, mean reward: 0.164 [-19.124, 100.000], mean action: 0.850 [0.000, 3.000], mean observation: 0.182 [-0.580, 1.000], loss: 1.484847, mean_absolute_error: 26.216367, mean_q: 32.420352, mean_eps: 0.244083
  841234/2000000: episode: 5237, duration: 15.014s, episode steps: 860, steps per second: 57, episode reward: 184.168, mean reward: 0.214 [-19.757, 100.000], mean action: 0.986 [0.000, 3.000], mean observation: 0.145 [-0.687, 1.018], loss: 1.459832, mean_absolute_error: 26.561177, mean_q: 32.834543, mean_eps: 0.243276
  842234/2000000: episode: 5238, duration: 16.593s, episode steps: 1000, steps per second: 60, episode reward: 73.603, mean reward: 0.074 [-22.823, 22.313], mean action: 0.793 [0.000, 3.000], mean observation: 0.181 [-1.060, 1.000], loss: 1.540812, mean_absolute_error: 26.541714, mean_q: 33.037153, mean_eps: 0.242439
  843225/2000000: episode: 5239, duration: 16.250s, episode steps: 991, steps per second: 61, episode reward: 164.976, mean reward: 0.166 [-23.849, 100.000], mean action: 0.829 [0.000, 3.000], mean observation: 0.143 [-0.595, 1.000], loss: 1.583424, mean_absolute_error: 26.271085, mean_q: 32.684186, mean_eps: 0.241543
  844225/2000000: episode: 5240, duration: 16.663s, episode steps: 1000, steps per second: 60, episode reward: 92.993, mean reward: 0.093 [-22.839, 22.272], mean action: 0.815 [0.000, 3.000], mean observation: 0.164 [-0.698, 1.016], loss: 1.443801, mean_absolute_error: 25.922428, mean_q: 32.013046, mean_eps: 0.240647
  845225/2000000: episode: 5241, duration: 16.428s, episode steps: 1000, steps per second: 61, episode reward: 102.139, mean reward: 0.102 [-21.364, 23.900], mean action: 0.891 [0.000, 3.000], mean observation: 0.162 [-0.576, 1.009], loss: 1.459295, mean_absolute_error: 26.116700, mean_q: 32.370616, mean_eps: 0.239747
  845539/2000000: episode: 5242, duration: 4.961s, episode steps: 314, steps per second: 63, episode reward: -85.329, mean reward: -0.272 [-100.000, 17.218], mean action: 1.758 [0.000, 3.000], mean observation: 0.083 [-1.020, 1.000], loss: 1.650177, mean_absolute_error: 26.732102, mean_q: 33.166123, mean_eps: 0.239156
  846539/2000000: episode: 5243, duration: 16.022s, episode steps: 1000, steps per second: 62, episode reward: 65.012, mean reward: 0.065 [-19.598, 23.251], mean action: 0.734 [0.000, 3.000], mean observation: 0.238 [-0.760, 1.000], loss: 1.432007, mean_absolute_error: 26.690535, mean_q: 33.382235, mean_eps: 0.238566
  846799/2000000: episode: 5244, duration: 4.362s, episode steps: 260, steps per second: 60, episode reward: -90.649, mean reward: -0.349 [-100.000, 9.827], mean action: 1.646 [0.000, 3.000], mean observation: 0.101 [-1.367, 1.000], loss: 1.661689, mean_absolute_error: 26.219210, mean_q: 31.641335, mean_eps: 0.237999
  847799/2000000: episode: 5245, duration: 16.442s, episode steps: 1000, steps per second: 61, episode reward: -19.943, mean reward: -0.020 [-25.677, 22.931], mean action: 1.671 [0.000, 3.000], mean observation: 0.110 [-0.582, 1.000], loss: 1.544655, mean_absolute_error: 26.276894, mean_q: 32.727778, mean_eps: 0.237432
  847915/2000000: episode: 5246, duration: 1.810s, episode steps: 116, steps per second: 64, episode reward: -44.860, mean reward: -0.387 [-100.000, 19.998], mean action: 1.853 [0.000, 3.000], mean observation: -0.018 [-0.704, 1.419], loss: 1.345863, mean_absolute_error: 24.621543, mean_q: 31.442961, mean_eps: 0.236930
  848789/2000000: episode: 5247, duration: 14.461s, episode steps: 874, steps per second: 60, episode reward: 177.723, mean reward: 0.203 [-19.260, 100.000], mean action: 0.928 [0.000, 3.000], mean observation: 0.129 [-0.720, 1.000], loss: 1.537296, mean_absolute_error: 26.195135, mean_q: 32.184419, mean_eps: 0.236483
  849295/2000000: episode: 5248, duration: 8.231s, episode steps: 506, steps per second: 61, episode reward: 161.061, mean reward: 0.318 [-22.785, 100.000], mean action: 1.377 [0.000, 3.000], mean observation: 0.069 [-0.651, 1.000], loss: 1.522654, mean_absolute_error: 25.834044, mean_q: 32.159618, mean_eps: 0.235862
  849919/2000000: episode: 5249, duration: 10.263s, episode steps: 624, steps per second: 61, episode reward: 180.309, mean reward: 0.289 [-20.365, 100.000], mean action: 1.378 [0.000, 3.000], mean observation: 0.176 [-0.884, 1.000], loss: 1.433561, mean_absolute_error: 25.911501, mean_q: 32.550752, mean_eps: 0.235355
  850919/2000000: episode: 5250, duration: 15.989s, episode steps: 1000, steps per second: 63, episode reward: 113.206, mean reward: 0.113 [-18.692, 22.911], mean action: 1.042 [0.000, 3.000], mean observation: 0.196 [-0.676, 1.000], loss: 1.405404, mean_absolute_error: 25.830840, mean_q: 32.215771, mean_eps: 0.234624
  851919/2000000: episode: 5251, duration: 17.313s, episode steps: 1000, steps per second: 58, episode reward: 87.678, mean reward: 0.088 [-18.535, 21.688], mean action: 0.922 [0.000, 3.000], mean observation: 0.136 [-0.642, 1.000], loss: 1.364407, mean_absolute_error: 26.407766, mean_q: 33.270058, mean_eps: 0.233724
  852919/2000000: episode: 5252, duration: 17.304s, episode steps: 1000, steps per second: 58, episode reward: -40.988, mean reward: -0.041 [-24.248, 23.993], mean action: 1.413 [0.000, 3.000], mean observation: 0.125 [-0.915, 1.000], loss: 1.425120, mean_absolute_error: 25.925682, mean_q: 32.536116, mean_eps: 0.232824
  853584/2000000: episode: 5253, duration: 10.720s, episode steps: 665, steps per second: 62, episode reward: 206.546, mean reward: 0.311 [-19.408, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.201 [-0.715, 1.000], loss: 1.437394, mean_absolute_error: 26.019685, mean_q: 32.879111, mean_eps: 0.232075
  853770/2000000: episode: 5254, duration: 2.956s, episode steps: 186, steps per second: 63, episode reward: 7.340, mean reward: 0.039 [-100.000, 20.831], mean action: 1.930 [0.000, 3.000], mean observation: 0.018 [-1.930, 1.000], loss: 1.426908, mean_absolute_error: 26.233814, mean_q: 32.359804, mean_eps: 0.231692
  854770/2000000: episode: 5255, duration: 16.571s, episode steps: 1000, steps per second: 60, episode reward: 95.678, mean reward: 0.096 [-18.938, 22.470], mean action: 0.975 [0.000, 3.000], mean observation: 0.179 [-0.691, 1.000], loss: 1.506306, mean_absolute_error: 26.259516, mean_q: 32.584644, mean_eps: 0.231157
  855597/2000000: episode: 5256, duration: 13.601s, episode steps: 827, steps per second: 61, episode reward: 112.020, mean reward: 0.135 [-18.764, 100.000], mean action: 1.478 [0.000, 3.000], mean observation: 0.104 [-1.037, 1.000], loss: 1.493438, mean_absolute_error: 26.224638, mean_q: 32.639534, mean_eps: 0.230334
  856269/2000000: episode: 5257, duration: 10.739s, episode steps: 672, steps per second: 63, episode reward: 208.815, mean reward: 0.311 [-20.661, 100.000], mean action: 0.823 [0.000, 3.000], mean observation: 0.164 [-0.786, 1.011], loss: 1.499971, mean_absolute_error: 26.128470, mean_q: 32.653516, mean_eps: 0.229659
  857096/2000000: episode: 5258, duration: 13.574s, episode steps: 827, steps per second: 61, episode reward: 77.380, mean reward: 0.094 [-19.538, 100.000], mean action: 1.394 [0.000, 3.000], mean observation: 0.137 [-0.991, 1.000], loss: 1.471938, mean_absolute_error: 25.896394, mean_q: 32.577463, mean_eps: 0.228986
  857851/2000000: episode: 5259, duration: 12.106s, episode steps: 755, steps per second: 62, episode reward: 186.244, mean reward: 0.247 [-18.459, 100.000], mean action: 1.507 [0.000, 3.000], mean observation: 0.140 [-1.049, 1.000], loss: 1.402541, mean_absolute_error: 26.103149, mean_q: 32.841937, mean_eps: 0.228275
  858851/2000000: episode: 5260, duration: 17.662s, episode steps: 1000, steps per second: 57, episode reward: 7.210, mean reward: 0.007 [-24.390, 22.639], mean action: 1.209 [0.000, 3.000], mean observation: 0.149 [-0.714, 1.000], loss: 1.555182, mean_absolute_error: 25.925755, mean_q: 31.948412, mean_eps: 0.227485
  859598/2000000: episode: 5261, duration: 13.021s, episode steps: 747, steps per second: 57, episode reward: 173.281, mean reward: 0.232 [-21.860, 100.000], mean action: 1.312 [0.000, 3.000], mean observation: 0.056 [-0.704, 1.000], loss: 1.409504, mean_absolute_error: 26.161719, mean_q: 32.747102, mean_eps: 0.226698
  860407/2000000: episode: 5262, duration: 13.767s, episode steps: 809, steps per second: 59, episode reward: 213.672, mean reward: 0.264 [-23.503, 100.000], mean action: 1.124 [0.000, 3.000], mean observation: 0.105 [-0.720, 1.000], loss: 1.471554, mean_absolute_error: 25.499342, mean_q: 32.109890, mean_eps: 0.225998
  860628/2000000: episode: 5263, duration: 3.475s, episode steps: 221, steps per second: 64, episode reward: -35.654, mean reward: -0.161 [-100.000, 14.068], mean action: 1.783 [0.000, 3.000], mean observation: 0.100 [-0.803, 1.000], loss: 1.384559, mean_absolute_error: 25.933816, mean_q: 32.371705, mean_eps: 0.225536
  860783/2000000: episode: 5264, duration: 2.421s, episode steps: 155, steps per second: 64, episode reward: -13.070, mean reward: -0.084 [-100.000, 12.562], mean action: 1.723 [0.000, 3.000], mean observation: 0.098 [-0.740, 1.000], loss: 1.718990, mean_absolute_error: 26.108390, mean_q: 32.017736, mean_eps: 0.225366
  861636/2000000: episode: 5265, duration: 14.025s, episode steps: 853, steps per second: 61, episode reward: 233.736, mean reward: 0.274 [-20.596, 100.000], mean action: 0.921 [0.000, 3.000], mean observation: 0.145 [-0.714, 1.013], loss: 1.515729, mean_absolute_error: 25.815571, mean_q: 32.037876, mean_eps: 0.224913
  862031/2000000: episode: 5266, duration: 6.259s, episode steps: 395, steps per second: 63, episode reward: 220.467, mean reward: 0.558 [-17.468, 100.000], mean action: 1.486 [0.000, 3.000], mean observation: 0.127 [-0.574, 1.000], loss: 1.422885, mean_absolute_error: 26.052825, mean_q: 33.026219, mean_eps: 0.224351
  862711/2000000: episode: 5267, duration: 11.346s, episode steps: 680, steps per second: 60, episode reward: 139.196, mean reward: 0.205 [-19.048, 100.000], mean action: 1.488 [0.000, 3.000], mean observation: 0.116 [-1.339, 1.000], loss: 1.405800, mean_absolute_error: 26.002266, mean_q: 32.777097, mean_eps: 0.223867
  863233/2000000: episode: 5268, duration: 8.604s, episode steps: 522, steps per second: 61, episode reward: 126.189, mean reward: 0.242 [-19.062, 100.000], mean action: 1.169 [0.000, 3.000], mean observation: 0.185 [-0.630, 1.000], loss: 1.437128, mean_absolute_error: 26.540920, mean_q: 33.933730, mean_eps: 0.223325
  864014/2000000: episode: 5269, duration: 13.231s, episode steps: 781, steps per second: 59, episode reward: 140.040, mean reward: 0.179 [-20.258, 100.000], mean action: 1.392 [0.000, 3.000], mean observation: 0.116 [-0.710, 1.000], loss: 1.322601, mean_absolute_error: 25.915756, mean_q: 32.829994, mean_eps: 0.222738
  865014/2000000: episode: 5270, duration: 17.219s, episode steps: 1000, steps per second: 58, episode reward: 74.993, mean reward: 0.075 [-21.437, 22.883], mean action: 0.847 [0.000, 3.000], mean observation: 0.156 [-0.735, 1.000], loss: 1.459007, mean_absolute_error: 25.994739, mean_q: 32.755985, mean_eps: 0.221937
  865127/2000000: episode: 5271, duration: 1.789s, episode steps: 113, steps per second: 63, episode reward: -50.389, mean reward: -0.446 [-100.000, 15.851], mean action: 1.664 [0.000, 3.000], mean observation: -0.139 [-0.909, 1.000], loss: 1.280965, mean_absolute_error: 25.790454, mean_q: 32.203961, mean_eps: 0.221437
  866127/2000000: episode: 5272, duration: 16.432s, episode steps: 1000, steps per second: 61, episode reward: 32.391, mean reward: 0.032 [-23.756, 20.998], mean action: 1.249 [0.000, 3.000], mean observation: 0.151 [-0.754, 1.000], loss: 1.542192, mean_absolute_error: 26.106456, mean_q: 33.065652, mean_eps: 0.220937
  867079/2000000: episode: 5273, duration: 16.276s, episode steps: 952, steps per second: 58, episode reward: 173.254, mean reward: 0.182 [-19.100, 100.000], mean action: 1.090 [0.000, 3.000], mean observation: 0.099 [-0.513, 1.000], loss: 1.428580, mean_absolute_error: 25.726905, mean_q: 32.404343, mean_eps: 0.220058
  867872/2000000: episode: 5274, duration: 12.890s, episode steps: 793, steps per second: 62, episode reward: 200.676, mean reward: 0.253 [-23.233, 100.000], mean action: 0.827 [0.000, 3.000], mean observation: 0.188 [-0.690, 1.013], loss: 1.487365, mean_absolute_error: 26.043037, mean_q: 33.280321, mean_eps: 0.219273
  868292/2000000: episode: 5275, duration: 6.930s, episode steps: 420, steps per second: 61, episode reward: -61.285, mean reward: -0.146 [-100.000, 10.613], mean action: 1.605 [0.000, 3.000], mean observation: 0.013 [-0.704, 1.000], loss: 1.575034, mean_absolute_error: 26.023989, mean_q: 33.095386, mean_eps: 0.218728
  869292/2000000: episode: 5276, duration: 17.057s, episode steps: 1000, steps per second: 59, episode reward: 48.109, mean reward: 0.048 [-21.031, 22.785], mean action: 0.859 [0.000, 3.000], mean observation: 0.164 [-0.609, 1.000], loss: 1.390181, mean_absolute_error: 25.840862, mean_q: 32.799031, mean_eps: 0.218089
  870292/2000000: episode: 5277, duration: 17.166s, episode steps: 1000, steps per second: 58, episode reward: 64.894, mean reward: 0.065 [-19.773, 22.340], mean action: 0.899 [0.000, 3.000], mean observation: 0.147 [-0.686, 1.000], loss: 1.300614, mean_absolute_error: 25.916350, mean_q: 32.481177, mean_eps: 0.217189
  871292/2000000: episode: 5278, duration: 16.425s, episode steps: 1000, steps per second: 61, episode reward: 90.530, mean reward: 0.091 [-23.716, 22.740], mean action: 0.943 [0.000, 3.000], mean observation: 0.149 [-0.718, 1.000], loss: 1.313501, mean_absolute_error: 25.730106, mean_q: 32.388863, mean_eps: 0.216289
  872292/2000000: episode: 5279, duration: 16.829s, episode steps: 1000, steps per second: 59, episode reward: 103.876, mean reward: 0.104 [-19.881, 22.381], mean action: 0.947 [0.000, 3.000], mean observation: 0.152 [-0.793, 1.000], loss: 1.410532, mean_absolute_error: 25.399385, mean_q: 32.010239, mean_eps: 0.215389
  873026/2000000: episode: 5280, duration: 11.935s, episode steps: 734, steps per second: 62, episode reward: 193.429, mean reward: 0.264 [-23.286, 100.000], mean action: 0.759 [0.000, 3.000], mean observation: 0.185 [-0.941, 1.000], loss: 1.394367, mean_absolute_error: 25.503038, mean_q: 32.078161, mean_eps: 0.214608
  873338/2000000: episode: 5281, duration: 4.973s, episode steps: 312, steps per second: 63, episode reward: -43.224, mean reward: -0.139 [-100.000, 11.839], mean action: 1.804 [0.000, 3.000], mean observation: 0.114 [-0.759, 1.000], loss: 1.307095, mean_absolute_error: 25.812435, mean_q: 32.807879, mean_eps: 0.214136
  874338/2000000: episode: 5282, duration: 16.845s, episode steps: 1000, steps per second: 59, episode reward: 32.655, mean reward: 0.033 [-20.486, 23.516], mean action: 1.023 [0.000, 3.000], mean observation: 0.149 [-0.647, 1.000], loss: 1.474761, mean_absolute_error: 25.353769, mean_q: 31.962355, mean_eps: 0.213546
  875338/2000000: episode: 5283, duration: 17.024s, episode steps: 1000, steps per second: 59, episode reward: 4.362, mean reward: 0.004 [-23.409, 22.705], mean action: 1.068 [0.000, 3.000], mean observation: 0.133 [-0.679, 1.000], loss: 1.378746, mean_absolute_error: 25.821802, mean_q: 32.709218, mean_eps: 0.212646
  876294/2000000: episode: 5284, duration: 15.643s, episode steps: 956, steps per second: 61, episode reward: 120.482, mean reward: 0.126 [-22.597, 100.000], mean action: 1.309 [0.000, 3.000], mean observation: 0.255 [-1.188, 1.000], loss: 1.497505, mean_absolute_error: 25.859587, mean_q: 32.261408, mean_eps: 0.211766
  877105/2000000: episode: 5285, duration: 14.747s, episode steps: 811, steps per second: 55, episode reward: 113.924, mean reward: 0.140 [-19.300, 100.000], mean action: 1.345 [0.000, 3.000], mean observation: 0.147 [-0.660, 1.000], loss: 1.370284, mean_absolute_error: 24.919330, mean_q: 31.434590, mean_eps: 0.210970
  878105/2000000: episode: 5286, duration: 17.019s, episode steps: 1000, steps per second: 59, episode reward: -14.069, mean reward: -0.014 [-25.371, 21.263], mean action: 1.274 [0.000, 3.000], mean observation: 0.154 [-0.671, 1.000], loss: 1.267375, mean_absolute_error: 25.661395, mean_q: 32.760926, mean_eps: 0.210155
  879105/2000000: episode: 5287, duration: 16.532s, episode steps: 1000, steps per second: 60, episode reward: 84.870, mean reward: 0.085 [-22.685, 22.821], mean action: 1.119 [0.000, 3.000], mean observation: 0.111 [-0.623, 1.000], loss: 1.367989, mean_absolute_error: 25.549468, mean_q: 32.347777, mean_eps: 0.209255
  879274/2000000: episode: 5288, duration: 2.635s, episode steps: 169, steps per second: 64, episode reward: 8.654, mean reward: 0.051 [-100.000, 8.761], mean action: 1.905 [0.000, 3.000], mean observation: 0.070 [-0.642, 1.201], loss: 1.242950, mean_absolute_error: 24.569628, mean_q: 31.980691, mean_eps: 0.208729
  880274/2000000: episode: 5289, duration: 17.274s, episode steps: 1000, steps per second: 58, episode reward: 68.499, mean reward: 0.068 [-22.228, 22.944], mean action: 1.086 [0.000, 3.000], mean observation: 0.109 [-0.643, 1.009], loss: 1.352389, mean_absolute_error: 25.390745, mean_q: 32.574012, mean_eps: 0.208203
  880808/2000000: episode: 5290, duration: 8.930s, episode steps: 534, steps per second: 60, episode reward: 145.199, mean reward: 0.272 [-21.243, 100.000], mean action: 1.573 [0.000, 3.000], mean observation: 0.048 [-0.525, 1.000], loss: 1.322392, mean_absolute_error: 25.378471, mean_q: 32.165925, mean_eps: 0.207514
  881606/2000000: episode: 5291, duration: 13.296s, episode steps: 798, steps per second: 60, episode reward: 204.083, mean reward: 0.256 [-21.158, 100.000], mean action: 0.905 [0.000, 3.000], mean observation: 0.134 [-0.887, 1.000], loss: 1.386623, mean_absolute_error: 24.945738, mean_q: 31.934895, mean_eps: 0.206915
  882089/2000000: episode: 5292, duration: 7.768s, episode steps: 483, steps per second: 62, episode reward: -112.326, mean reward: -0.233 [-100.000, 8.897], mean action: 1.580 [0.000, 3.000], mean observation: 0.107 [-0.598, 1.000], loss: 1.231294, mean_absolute_error: 25.539203, mean_q: 32.459705, mean_eps: 0.206337
  882754/2000000: episode: 5293, duration: 11.459s, episode steps: 665, steps per second: 58, episode reward: 138.756, mean reward: 0.209 [-20.335, 100.000], mean action: 1.229 [0.000, 3.000], mean observation: 0.100 [-0.637, 1.000], loss: 1.351340, mean_absolute_error: 25.038361, mean_q: 32.392057, mean_eps: 0.205820
  883456/2000000: episode: 5294, duration: 11.576s, episode steps: 702, steps per second: 61, episode reward: 145.825, mean reward: 0.208 [-25.338, 100.000], mean action: 1.189 [0.000, 3.000], mean observation: 0.164 [-1.032, 1.000], loss: 1.457227, mean_absolute_error: 25.169996, mean_q: 32.323512, mean_eps: 0.205206
  883862/2000000: episode: 5295, duration: 6.494s, episode steps: 406, steps per second: 63, episode reward: 221.316, mean reward: 0.545 [-14.959, 100.000], mean action: 1.744 [0.000, 3.000], mean observation: 0.119 [-0.648, 1.000], loss: 1.219930, mean_absolute_error: 25.176373, mean_q: 32.395511, mean_eps: 0.204708
  884862/2000000: episode: 5296, duration: 17.317s, episode steps: 1000, steps per second: 58, episode reward: 8.096, mean reward: 0.008 [-19.210, 22.215], mean action: 1.063 [0.000, 3.000], mean observation: 0.127 [-0.685, 1.000], loss: 1.471596, mean_absolute_error: 25.033733, mean_q: 31.986100, mean_eps: 0.204074
  885498/2000000: episode: 5297, duration: 10.424s, episode steps: 636, steps per second: 61, episode reward: -156.897, mean reward: -0.247 [-100.000, 13.518], mean action: 1.629 [0.000, 3.000], mean observation: 0.119 [-0.693, 1.019], loss: 1.427475, mean_absolute_error: 24.472036, mean_q: 31.467663, mean_eps: 0.203338
  886498/2000000: episode: 5298, duration: 16.698s, episode steps: 1000, steps per second: 60, episode reward: 43.880, mean reward: 0.044 [-24.628, 22.117], mean action: 0.872 [0.000, 3.000], mean observation: 0.176 [-0.603, 1.000], loss: 1.357946, mean_absolute_error: 24.998891, mean_q: 31.835955, mean_eps: 0.202602
  886745/2000000: episode: 5299, duration: 3.905s, episode steps: 247, steps per second: 63, episode reward: -55.229, mean reward: -0.224 [-100.000, 14.337], mean action: 1.680 [0.000, 3.000], mean observation: 0.116 [-0.733, 1.000], loss: 1.624274, mean_absolute_error: 24.766107, mean_q: 32.091575, mean_eps: 0.202040
  887107/2000000: episode: 5300, duration: 5.658s, episode steps: 362, steps per second: 64, episode reward: -48.839, mean reward: -0.135 [-100.000, 10.698], mean action: 1.594 [0.000, 3.000], mean observation: -0.006 [-0.805, 1.000], loss: 1.328443, mean_absolute_error: 24.456259, mean_q: 30.474878, mean_eps: 0.201767
  888107/2000000: episode: 5301, duration: 16.905s, episode steps: 1000, steps per second: 59, episode reward: 71.948, mean reward: 0.072 [-24.268, 22.969], mean action: 0.836 [0.000, 3.000], mean observation: 0.178 [-0.629, 1.000], loss: 1.378160, mean_absolute_error: 24.611390, mean_q: 31.542586, mean_eps: 0.201155
  888845/2000000: episode: 5302, duration: 12.727s, episode steps: 738, steps per second: 58, episode reward: 164.034, mean reward: 0.222 [-19.203, 100.000], mean action: 1.061 [0.000, 3.000], mean observation: 0.135 [-0.671, 1.000], loss: 1.316774, mean_absolute_error: 24.584418, mean_q: 31.709982, mean_eps: 0.200372
  889594/2000000: episode: 5303, duration: 13.038s, episode steps: 749, steps per second: 57, episode reward: 189.893, mean reward: 0.254 [-19.858, 100.000], mean action: 1.230 [0.000, 3.000], mean observation: 0.190 [-0.701, 1.000], loss: 1.349383, mean_absolute_error: 24.725217, mean_q: 31.469763, mean_eps: 0.199702
  890169/2000000: episode: 5304, duration: 9.574s, episode steps: 575, steps per second: 60, episode reward: 158.687, mean reward: 0.276 [-18.218, 100.000], mean action: 1.063 [0.000, 3.000], mean observation: 0.118 [-0.732, 1.000], loss: 1.290007, mean_absolute_error: 24.693160, mean_q: 31.896393, mean_eps: 0.199106
  891169/2000000: episode: 5305, duration: 16.723s, episode steps: 1000, steps per second: 60, episode reward: 19.855, mean reward: 0.020 [-18.966, 22.334], mean action: 1.200 [0.000, 3.000], mean observation: 0.070 [-0.676, 1.000], loss: 1.400163, mean_absolute_error: 25.127187, mean_q: 32.335191, mean_eps: 0.198397
  892169/2000000: episode: 5306, duration: 17.070s, episode steps: 1000, steps per second: 59, episode reward: -15.376, mean reward: -0.015 [-24.130, 21.637], mean action: 1.278 [0.000, 3.000], mean observation: 0.168 [-0.848, 1.000], loss: 1.300734, mean_absolute_error: 25.511510, mean_q: 33.035624, mean_eps: 0.197497
  892998/2000000: episode: 5307, duration: 14.096s, episode steps: 829, steps per second: 59, episode reward: 156.603, mean reward: 0.189 [-19.331, 100.000], mean action: 1.636 [0.000, 3.000], mean observation: 0.170 [-0.610, 1.000], loss: 1.285517, mean_absolute_error: 25.234394, mean_q: 32.329240, mean_eps: 0.196674
  893424/2000000: episode: 5308, duration: 6.804s, episode steps: 426, steps per second: 63, episode reward: 230.932, mean reward: 0.542 [-17.477, 100.000], mean action: 1.380 [0.000, 3.000], mean observation: 0.019 [-0.803, 1.000], loss: 1.720605, mean_absolute_error: 25.157142, mean_q: 31.360364, mean_eps: 0.196111
  894311/2000000: episode: 5309, duration: 15.313s, episode steps: 887, steps per second: 58, episode reward: 153.363, mean reward: 0.173 [-17.921, 100.000], mean action: 1.041 [0.000, 3.000], mean observation: 0.117 [-0.451, 1.000], loss: 1.297100, mean_absolute_error: 24.158925, mean_q: 31.291525, mean_eps: 0.195521
  895203/2000000: episode: 5310, duration: 15.625s, episode steps: 892, steps per second: 57, episode reward: 196.231, mean reward: 0.220 [-19.819, 100.000], mean action: 0.937 [0.000, 3.000], mean observation: 0.145 [-0.693, 1.041], loss: 1.414115, mean_absolute_error: 25.046409, mean_q: 32.281403, mean_eps: 0.194720
  896152/2000000: episode: 5311, duration: 16.417s, episode steps: 949, steps per second: 58, episode reward: 171.118, mean reward: 0.180 [-18.619, 100.000], mean action: 1.030 [0.000, 3.000], mean observation: 0.173 [-0.876, 1.000], loss: 1.404532, mean_absolute_error: 24.382953, mean_q: 31.194936, mean_eps: 0.193892
  896642/2000000: episode: 5312, duration: 8.236s, episode steps: 490, steps per second: 59, episode reward: -118.469, mean reward: -0.242 [-100.000, 14.100], mean action: 1.718 [0.000, 3.000], mean observation: 0.038 [-0.798, 1.000], loss: 1.420235, mean_absolute_error: 24.666618, mean_q: 31.640493, mean_eps: 0.193244
  897357/2000000: episode: 5313, duration: 12.406s, episode steps: 715, steps per second: 58, episode reward: 194.033, mean reward: 0.271 [-20.341, 100.000], mean action: 1.024 [0.000, 3.000], mean observation: 0.138 [-0.646, 1.019], loss: 1.355246, mean_absolute_error: 24.925553, mean_q: 31.725670, mean_eps: 0.192700
  897740/2000000: episode: 5314, duration: 6.217s, episode steps: 383, steps per second: 62, episode reward: -96.487, mean reward: -0.252 [-100.000, 15.544], mean action: 1.700 [0.000, 3.000], mean observation: 0.118 [-0.665, 1.000], loss: 1.325732, mean_absolute_error: 24.139650, mean_q: 31.084728, mean_eps: 0.192207
  898399/2000000: episode: 5315, duration: 10.761s, episode steps: 659, steps per second: 61, episode reward: 149.521, mean reward: 0.227 [-5.316, 100.000], mean action: 1.449 [0.000, 3.000], mean observation: 0.116 [-0.552, 1.000], loss: 1.340659, mean_absolute_error: 24.748805, mean_q: 31.601153, mean_eps: 0.191739
  898829/2000000: episode: 5316, duration: 7.187s, episode steps: 430, steps per second: 60, episode reward: 184.089, mean reward: 0.428 [-17.692, 100.000], mean action: 1.377 [0.000, 3.000], mean observation: 0.074 [-0.721, 1.000], loss: 1.386474, mean_absolute_error: 24.503026, mean_q: 31.709300, mean_eps: 0.191247
  899180/2000000: episode: 5317, duration: 5.592s, episode steps: 351, steps per second: 63, episode reward: -96.276, mean reward: -0.274 [-100.000, 10.486], mean action: 1.456 [0.000, 3.000], mean observation: 0.094 [-0.767, 1.000], loss: 1.304499, mean_absolute_error: 24.466873, mean_q: 31.601553, mean_eps: 0.190896
  899664/2000000: episode: 5318, duration: 7.944s, episode steps: 484, steps per second: 61, episode reward: 208.345, mean reward: 0.430 [-17.953, 100.000], mean action: 1.198 [0.000, 3.000], mean observation: 0.067 [-0.739, 1.000], loss: 1.338130, mean_absolute_error: 24.202776, mean_q: 31.211438, mean_eps: 0.190522
  900664/2000000: episode: 5319, duration: 17.510s, episode steps: 1000, steps per second: 57, episode reward: 65.518, mean reward: 0.066 [-19.627, 23.050], mean action: 0.814 [0.000, 3.000], mean observation: 0.213 [-0.633, 1.000], loss: 1.426322, mean_absolute_error: 24.776815, mean_q: 32.054821, mean_eps: 0.189854
  901235/2000000: episode: 5320, duration: 9.480s, episode steps: 571, steps per second: 60, episode reward: 183.555, mean reward: 0.321 [-17.741, 100.000], mean action: 1.378 [0.000, 3.000], mean observation: 0.047 [-0.736, 1.000], loss: 1.400399, mean_absolute_error: 24.491407, mean_q: 31.797970, mean_eps: 0.189147
  901780/2000000: episode: 5321, duration: 9.288s, episode steps: 545, steps per second: 59, episode reward: 192.580, mean reward: 0.353 [-18.042, 100.000], mean action: 1.268 [0.000, 3.000], mean observation: 0.079 [-0.653, 1.001], loss: 1.289640, mean_absolute_error: 24.798402, mean_q: 32.297819, mean_eps: 0.188645
  902780/2000000: episode: 5322, duration: 17.750s, episode steps: 1000, steps per second: 56, episode reward: 22.894, mean reward: 0.023 [-18.167, 22.667], mean action: 1.983 [0.000, 3.000], mean observation: 0.146 [-0.679, 1.000], loss: 1.314064, mean_absolute_error: 24.733892, mean_q: 32.012475, mean_eps: 0.187950
  903190/2000000: episode: 5323, duration: 6.753s, episode steps: 410, steps per second: 61, episode reward: 154.227, mean reward: 0.376 [-19.424, 100.000], mean action: 1.417 [0.000, 3.000], mean observation: 0.044 [-0.578, 1.000], loss: 1.546428, mean_absolute_error: 25.620172, mean_q: 33.130061, mean_eps: 0.187314
  903316/2000000: episode: 5324, duration: 2.014s, episode steps: 126, steps per second: 63, episode reward: -61.469, mean reward: -0.488 [-100.000, 12.585], mean action: 1.810 [0.000, 3.000], mean observation: 0.098 [-0.726, 1.033], loss: 1.309942, mean_absolute_error: 24.436835, mean_q: 32.540999, mean_eps: 0.187073
  903530/2000000: episode: 5325, duration: 3.405s, episode steps: 214, steps per second: 63, episode reward: 224.849, mean reward: 1.051 [-8.669, 100.000], mean action: 1.252 [0.000, 3.000], mean observation: 0.029 [-0.823, 1.000], loss: 1.209166, mean_absolute_error: 24.527389, mean_q: 32.249419, mean_eps: 0.186920
  904038/2000000: episode: 5326, duration: 8.265s, episode steps: 508, steps per second: 61, episode reward: 175.224, mean reward: 0.345 [-20.221, 100.000], mean action: 2.028 [0.000, 3.000], mean observation: 0.112 [-0.716, 1.042], loss: 1.253530, mean_absolute_error: 24.452599, mean_q: 31.570244, mean_eps: 0.186594
  904372/2000000: episode: 5327, duration: 5.360s, episode steps: 334, steps per second: 62, episode reward: -113.918, mean reward: -0.341 [-100.000, 9.435], mean action: 1.632 [0.000, 3.000], mean observation: 0.144 [-0.697, 1.000], loss: 1.507083, mean_absolute_error: 24.998275, mean_q: 32.220499, mean_eps: 0.186216
  905372/2000000: episode: 5328, duration: 16.125s, episode steps: 1000, steps per second: 62, episode reward: 20.197, mean reward: 0.020 [-21.364, 12.540], mean action: 1.811 [0.000, 3.000], mean observation: 0.091 [-0.786, 1.000], loss: 1.350856, mean_absolute_error: 25.047815, mean_q: 32.431766, mean_eps: 0.185617
  906276/2000000: episode: 5329, duration: 15.081s, episode steps: 904, steps per second: 60, episode reward: 206.180, mean reward: 0.228 [-21.152, 100.000], mean action: 0.887 [0.000, 3.000], mean observation: 0.140 [-0.596, 1.000], loss: 1.333798, mean_absolute_error: 25.316850, mean_q: 32.681500, mean_eps: 0.184760
  907276/2000000: episode: 5330, duration: 16.161s, episode steps: 1000, steps per second: 62, episode reward: 50.995, mean reward: 0.051 [-21.756, 25.935], mean action: 1.121 [0.000, 3.000], mean observation: 0.170 [-1.534, 1.000], loss: 1.504981, mean_absolute_error: 24.748552, mean_q: 32.006784, mean_eps: 0.183903
  908140/2000000: episode: 5331, duration: 14.521s, episode steps: 864, steps per second: 59, episode reward: 127.560, mean reward: 0.148 [-20.990, 100.000], mean action: 2.160 [0.000, 3.000], mean observation: 0.207 [-0.672, 1.000], loss: 1.304967, mean_absolute_error: 24.927649, mean_q: 32.602440, mean_eps: 0.183065
  909140/2000000: episode: 5332, duration: 17.042s, episode steps: 1000, steps per second: 59, episode reward: -5.029, mean reward: -0.005 [-25.126, 19.755], mean action: 1.396 [0.000, 3.000], mean observation: 0.163 [-1.008, 1.000], loss: 1.305000, mean_absolute_error: 24.899180, mean_q: 32.587309, mean_eps: 0.182226
  909835/2000000: episode: 5333, duration: 11.860s, episode steps: 695, steps per second: 59, episode reward: 179.539, mean reward: 0.258 [-17.907, 100.000], mean action: 1.304 [0.000, 3.000], mean observation: 0.068 [-0.599, 1.000], loss: 1.338795, mean_absolute_error: 24.984909, mean_q: 32.645941, mean_eps: 0.181463
  910730/2000000: episode: 5334, duration: 15.437s, episode steps: 895, steps per second: 58, episode reward: 171.393, mean reward: 0.192 [-18.702, 100.000], mean action: 1.091 [0.000, 3.000], mean observation: 0.174 [-0.702, 1.000], loss: 1.348702, mean_absolute_error: 24.497818, mean_q: 31.705148, mean_eps: 0.180746
  911578/2000000: episode: 5335, duration: 14.954s, episode steps: 848, steps per second: 57, episode reward: 102.626, mean reward: 0.121 [-19.757, 100.000], mean action: 2.086 [0.000, 3.000], mean observation: 0.161 [-0.644, 1.000], loss: 1.281648, mean_absolute_error: 24.055673, mean_q: 31.066292, mean_eps: 0.179961
  912484/2000000: episode: 5336, duration: 15.355s, episode steps: 906, steps per second: 59, episode reward: 197.391, mean reward: 0.218 [-19.875, 100.000], mean action: 0.897 [0.000, 3.000], mean observation: 0.142 [-0.669, 1.000], loss: 1.292416, mean_absolute_error: 24.335984, mean_q: 31.466632, mean_eps: 0.179173
  912925/2000000: episode: 5337, duration: 7.278s, episode steps: 441, steps per second: 61, episode reward: 175.286, mean reward: 0.397 [-15.191, 100.000], mean action: 1.397 [0.000, 3.000], mean observation: 0.026 [-0.675, 1.000], loss: 1.279394, mean_absolute_error: 24.288563, mean_q: 31.297485, mean_eps: 0.178566
  913892/2000000: episode: 5338, duration: 16.008s, episode steps: 967, steps per second: 60, episode reward: 161.352, mean reward: 0.167 [-24.115, 100.000], mean action: 0.886 [0.000, 3.000], mean observation: 0.189 [-0.680, 1.000], loss: 1.303361, mean_absolute_error: 24.043402, mean_q: 31.379230, mean_eps: 0.177933
  914722/2000000: episode: 5339, duration: 13.886s, episode steps: 830, steps per second: 60, episode reward: 181.349, mean reward: 0.218 [-19.732, 100.000], mean action: 1.210 [0.000, 3.000], mean observation: 0.104 [-0.735, 1.000], loss: 1.289765, mean_absolute_error: 24.498132, mean_q: 32.009082, mean_eps: 0.177125
  915120/2000000: episode: 5340, duration: 6.493s, episode steps: 398, steps per second: 61, episode reward: 226.070, mean reward: 0.568 [-9.077, 100.000], mean action: 1.357 [0.000, 3.000], mean observation: 0.020 [-0.727, 1.000], loss: 1.406131, mean_absolute_error: 24.103867, mean_q: 31.050856, mean_eps: 0.176572
  915701/2000000: episode: 5341, duration: 9.784s, episode steps: 581, steps per second: 59, episode reward: 215.623, mean reward: 0.371 [-21.019, 100.000], mean action: 1.355 [0.000, 3.000], mean observation: 0.149 [-0.969, 1.066], loss: 1.259339, mean_absolute_error: 23.685249, mean_q: 30.601494, mean_eps: 0.176131
  916653/2000000: episode: 5342, duration: 15.503s, episode steps: 952, steps per second: 61, episode reward: 160.935, mean reward: 0.169 [-20.008, 100.000], mean action: 0.966 [0.000, 3.000], mean observation: 0.126 [-0.766, 1.000], loss: 1.313572, mean_absolute_error: 24.403263, mean_q: 31.718362, mean_eps: 0.175440
  917653/2000000: episode: 5343, duration: 17.246s, episode steps: 1000, steps per second: 58, episode reward: 49.247, mean reward: 0.049 [-20.348, 22.038], mean action: 1.492 [0.000, 3.000], mean observation: 0.119 [-0.790, 1.000], loss: 1.294628, mean_absolute_error: 24.031033, mean_q: 31.350994, mean_eps: 0.174561
  918444/2000000: episode: 5344, duration: 12.956s, episode steps: 791, steps per second: 61, episode reward: 205.846, mean reward: 0.260 [-23.985, 100.000], mean action: 0.922 [0.000, 3.000], mean observation: 0.132 [-0.673, 1.000], loss: 1.223955, mean_absolute_error: 23.884041, mean_q: 31.160155, mean_eps: 0.173757
  918867/2000000: episode: 5345, duration: 6.959s, episode steps: 423, steps per second: 61, episode reward: 196.798, mean reward: 0.465 [-17.665, 100.000], mean action: 1.142 [0.000, 3.000], mean observation: 0.087 [-0.727, 1.000], loss: 1.320113, mean_absolute_error: 23.923718, mean_q: 30.844314, mean_eps: 0.173211
  919488/2000000: episode: 5346, duration: 10.368s, episode steps: 621, steps per second: 60, episode reward: 123.111, mean reward: 0.198 [-5.396, 100.000], mean action: 1.467 [0.000, 3.000], mean observation: 0.107 [-0.676, 1.000], loss: 1.225167, mean_absolute_error: 23.791051, mean_q: 31.367008, mean_eps: 0.172742
  920398/2000000: episode: 5347, duration: 15.785s, episode steps: 910, steps per second: 58, episode reward: 179.435, mean reward: 0.197 [-18.239, 100.000], mean action: 0.884 [0.000, 3.000], mean observation: 0.120 [-0.721, 1.000], loss: 1.226364, mean_absolute_error: 24.569543, mean_q: 31.785346, mean_eps: 0.172052
  920757/2000000: episode: 5348, duration: 5.754s, episode steps: 359, steps per second: 62, episode reward: 183.312, mean reward: 0.511 [-9.827, 100.000], mean action: 1.476 [0.000, 3.000], mean observation: 0.078 [-0.662, 1.000], loss: 1.320262, mean_absolute_error: 24.173078, mean_q: 31.529775, mean_eps: 0.171480
  921757/2000000: episode: 5349, duration: 17.197s, episode steps: 1000, steps per second: 58, episode reward: 89.707, mean reward: 0.090 [-19.279, 22.757], mean action: 0.957 [0.000, 3.000], mean observation: 0.116 [-0.784, 1.000], loss: 1.353485, mean_absolute_error: 24.047781, mean_q: 31.348656, mean_eps: 0.170868
  922472/2000000: episode: 5350, duration: 11.716s, episode steps: 715, steps per second: 61, episode reward: 170.094, mean reward: 0.238 [-18.359, 100.000], mean action: 0.940 [0.000, 3.000], mean observation: 0.131 [-0.631, 1.000], loss: 1.118074, mean_absolute_error: 24.570736, mean_q: 32.370699, mean_eps: 0.170097
  923472/2000000: episode: 5351, duration: 17.710s, episode steps: 1000, steps per second: 56, episode reward: 7.969, mean reward: 0.008 [-20.430, 22.826], mean action: 1.104 [0.000, 3.000], mean observation: 0.104 [-0.726, 1.000], loss: 1.431592, mean_absolute_error: 24.410053, mean_q: 31.896083, mean_eps: 0.169327
  923845/2000000: episode: 5352, duration: 6.253s, episode steps: 373, steps per second: 60, episode reward: -82.255, mean reward: -0.221 [-100.000, 13.477], mean action: 1.684 [0.000, 3.000], mean observation: 0.048 [-1.130, 1.000], loss: 1.289360, mean_absolute_error: 24.614402, mean_q: 32.330982, mean_eps: 0.168708
  924594/2000000: episode: 5353, duration: 12.199s, episode steps: 749, steps per second: 61, episode reward: 143.506, mean reward: 0.192 [-24.454, 100.000], mean action: 1.327 [0.000, 3.000], mean observation: 0.191 [-0.674, 1.000], loss: 1.322639, mean_absolute_error: 24.092650, mean_q: 31.875040, mean_eps: 0.168202
  925594/2000000: episode: 5354, duration: 17.912s, episode steps: 1000, steps per second: 56, episode reward: 55.484, mean reward: 0.055 [-19.560, 23.531], mean action: 1.534 [0.000, 3.000], mean observation: 0.161 [-1.277, 1.000], loss: 1.372169, mean_absolute_error: 23.873440, mean_q: 31.312900, mean_eps: 0.167415
  925930/2000000: episode: 5355, duration: 5.395s, episode steps: 336, steps per second: 62, episode reward: 206.593, mean reward: 0.615 [-9.309, 100.000], mean action: 1.568 [0.000, 3.000], mean observation: 0.095 [-0.794, 1.000], loss: 1.195238, mean_absolute_error: 24.441915, mean_q: 32.446738, mean_eps: 0.166814
  926930/2000000: episode: 5356, duration: 16.567s, episode steps: 1000, steps per second: 60, episode reward: 29.536, mean reward: 0.030 [-25.325, 23.890], mean action: 1.285 [0.000, 3.000], mean observation: 0.155 [-0.703, 1.000], loss: 1.277446, mean_absolute_error: 23.906473, mean_q: 31.458676, mean_eps: 0.166213
  927458/2000000: episode: 5357, duration: 8.886s, episode steps: 528, steps per second: 59, episode reward: 188.920, mean reward: 0.358 [-17.529, 100.000], mean action: 1.360 [0.000, 3.000], mean observation: 0.058 [-0.731, 1.000], loss: 1.382060, mean_absolute_error: 24.513554, mean_q: 32.076130, mean_eps: 0.165525
  928049/2000000: episode: 5358, duration: 10.017s, episode steps: 591, steps per second: 59, episode reward: 178.907, mean reward: 0.303 [-17.999, 100.000], mean action: 1.088 [0.000, 3.000], mean observation: 0.086 [-0.774, 1.000], loss: 1.382110, mean_absolute_error: 23.927235, mean_q: 31.020606, mean_eps: 0.165021
  928481/2000000: episode: 5359, duration: 6.947s, episode steps: 432, steps per second: 62, episode reward: 188.173, mean reward: 0.436 [-18.330, 100.000], mean action: 1.620 [0.000, 3.000], mean observation: 0.121 [-0.738, 1.000], loss: 1.124943, mean_absolute_error: 23.742345, mean_q: 31.132729, mean_eps: 0.164561
  928914/2000000: episode: 5360, duration: 6.944s, episode steps: 433, steps per second: 62, episode reward: 188.830, mean reward: 0.436 [-22.673, 100.000], mean action: 1.390 [0.000, 3.000], mean observation: 0.060 [-0.762, 1.000], loss: 1.186649, mean_absolute_error: 24.003801, mean_q: 31.559695, mean_eps: 0.164172
  929242/2000000: episode: 5361, duration: 5.302s, episode steps: 328, steps per second: 62, episode reward: -83.532, mean reward: -0.255 [-100.000, 14.158], mean action: 1.860 [0.000, 3.000], mean observation: 0.056 [-1.220, 1.000], loss: 1.263570, mean_absolute_error: 23.546183, mean_q: 31.042206, mean_eps: 0.163830
  929802/2000000: episode: 5362, duration: 9.135s, episode steps: 560, steps per second: 61, episode reward: -101.257, mean reward: -0.181 [-100.000, 8.966], mean action: 1.884 [0.000, 3.000], mean observation: 0.119 [-0.605, 1.000], loss: 1.259805, mean_absolute_error: 23.830571, mean_q: 31.108646, mean_eps: 0.163430
  930196/2000000: episode: 5363, duration: 6.714s, episode steps: 394, steps per second: 59, episode reward: 185.972, mean reward: 0.472 [-17.576, 100.000], mean action: 1.173 [0.000, 3.000], mean observation: 0.112 [-0.925, 1.000], loss: 1.180607, mean_absolute_error: 24.054361, mean_q: 31.788140, mean_eps: 0.163002
  931136/2000000: episode: 5364, duration: 16.436s, episode steps: 940, steps per second: 57, episode reward: 161.436, mean reward: 0.172 [-19.572, 100.000], mean action: 0.793 [0.000, 3.000], mean observation: 0.146 [-0.660, 1.000], loss: 1.228875, mean_absolute_error: 24.065389, mean_q: 31.739883, mean_eps: 0.162402
  931684/2000000: episode: 5365, duration: 9.344s, episode steps: 548, steps per second: 59, episode reward: 152.688, mean reward: 0.279 [-18.022, 100.000], mean action: 1.255 [0.000, 3.000], mean observation: 0.156 [-0.680, 1.000], loss: 1.287302, mean_absolute_error: 23.549396, mean_q: 31.065077, mean_eps: 0.161733
  932307/2000000: episode: 5366, duration: 10.125s, episode steps: 623, steps per second: 62, episode reward: 209.416, mean reward: 0.336 [-20.562, 100.000], mean action: 1.258 [0.000, 3.000], mean observation: 0.093 [-0.597, 1.014], loss: 1.377812, mean_absolute_error: 23.595623, mean_q: 30.877676, mean_eps: 0.161205
  933019/2000000: episode: 5367, duration: 11.830s, episode steps: 712, steps per second: 60, episode reward: 208.210, mean reward: 0.292 [-18.804, 100.000], mean action: 1.212 [0.000, 3.000], mean observation: 0.070 [-0.736, 1.000], loss: 1.139136, mean_absolute_error: 24.012895, mean_q: 31.634727, mean_eps: 0.160604
  933585/2000000: episode: 5368, duration: 9.068s, episode steps: 566, steps per second: 62, episode reward: 196.534, mean reward: 0.347 [-8.820, 100.000], mean action: 1.528 [0.000, 3.000], mean observation: 0.015 [-0.737, 1.000], loss: 1.459233, mean_absolute_error: 24.454290, mean_q: 32.153809, mean_eps: 0.160028
  934018/2000000: episode: 5369, duration: 6.906s, episode steps: 433, steps per second: 63, episode reward: -83.708, mean reward: -0.193 [-100.000, 16.351], mean action: 1.718 [0.000, 3.000], mean observation: 0.053 [-1.008, 1.000], loss: 1.337588, mean_absolute_error: 24.041337, mean_q: 31.340399, mean_eps: 0.159578
  934951/2000000: episode: 5370, duration: 15.209s, episode steps: 933, steps per second: 61, episode reward: 172.271, mean reward: 0.185 [-18.325, 100.000], mean action: 1.093 [0.000, 3.000], mean observation: 0.146 [-0.590, 1.000], loss: 1.277755, mean_absolute_error: 23.912378, mean_q: 31.681241, mean_eps: 0.158964
  935951/2000000: episode: 5371, duration: 17.142s, episode steps: 1000, steps per second: 58, episode reward: 58.805, mean reward: 0.059 [-24.669, 23.382], mean action: 0.995 [0.000, 3.000], mean observation: 0.215 [-0.721, 1.000], loss: 1.317480, mean_absolute_error: 23.416598, mean_q: 30.510036, mean_eps: 0.158095
  936844/2000000: episode: 5372, duration: 15.060s, episode steps: 893, steps per second: 59, episode reward: 183.596, mean reward: 0.206 [-21.290, 100.000], mean action: 0.742 [0.000, 3.000], mean observation: 0.167 [-0.733, 1.000], loss: 1.271623, mean_absolute_error: 23.829563, mean_q: 31.366092, mean_eps: 0.157244
  937576/2000000: episode: 5373, duration: 12.068s, episode steps: 732, steps per second: 61, episode reward: 176.734, mean reward: 0.241 [-18.101, 100.000], mean action: 1.284 [0.000, 3.000], mean observation: 0.212 [-0.706, 1.000], loss: 1.231218, mean_absolute_error: 23.768643, mean_q: 31.019806, mean_eps: 0.156513
  938156/2000000: episode: 5374, duration: 9.932s, episode steps: 580, steps per second: 58, episode reward: 216.583, mean reward: 0.373 [-17.810, 100.000], mean action: 1.400 [0.000, 3.000], mean observation: 0.068 [-0.698, 1.000], loss: 1.228427, mean_absolute_error: 23.704535, mean_q: 31.264249, mean_eps: 0.155922
  938980/2000000: episode: 5375, duration: 14.641s, episode steps: 824, steps per second: 56, episode reward: 128.138, mean reward: 0.156 [-21.217, 100.000], mean action: 2.097 [0.000, 3.000], mean observation: 0.180 [-0.656, 1.000], loss: 1.339953, mean_absolute_error: 23.572318, mean_q: 31.090346, mean_eps: 0.155291
  939319/2000000: episode: 5376, duration: 5.453s, episode steps: 339, steps per second: 62, episode reward: -76.781, mean reward: -0.226 [-100.000, 10.084], mean action: 1.758 [0.000, 3.000], mean observation: 0.079 [-0.718, 1.000], loss: 1.075987, mean_absolute_error: 23.096062, mean_q: 30.351151, mean_eps: 0.154767
  940192/2000000: episode: 5377, duration: 14.587s, episode steps: 873, steps per second: 60, episode reward: 163.910, mean reward: 0.188 [-23.216, 100.000], mean action: 1.286 [0.000, 3.000], mean observation: 0.163 [-0.700, 1.000], loss: 1.436174, mean_absolute_error: 23.655693, mean_q: 30.764234, mean_eps: 0.154221
  940503/2000000: episode: 5378, duration: 5.045s, episode steps: 311, steps per second: 62, episode reward: -41.212, mean reward: -0.133 [-100.000, 13.054], mean action: 1.794 [0.000, 3.000], mean observation: 0.051 [-0.801, 1.195], loss: 1.137715, mean_absolute_error: 23.937320, mean_q: 31.701561, mean_eps: 0.153689
  941123/2000000: episode: 5379, duration: 10.771s, episode steps: 620, steps per second: 58, episode reward: 170.672, mean reward: 0.275 [-18.225, 100.000], mean action: 1.339 [0.000, 3.000], mean observation: 0.138 [-0.744, 1.000], loss: 1.314397, mean_absolute_error: 23.540764, mean_q: 30.705046, mean_eps: 0.153269
  941479/2000000: episode: 5380, duration: 5.711s, episode steps: 356, steps per second: 62, episode reward: -66.191, mean reward: -0.186 [-100.000, 16.341], mean action: 1.784 [0.000, 3.000], mean observation: 0.093 [-0.683, 1.256], loss: 1.262165, mean_absolute_error: 23.514949, mean_q: 30.804225, mean_eps: 0.152830
  942013/2000000: episode: 5381, duration: 8.855s, episode steps: 534, steps per second: 60, episode reward: 204.494, mean reward: 0.383 [-18.757, 100.000], mean action: 1.277 [0.000, 3.000], mean observation: 0.113 [-0.648, 1.010], loss: 1.231615, mean_absolute_error: 23.230252, mean_q: 30.237023, mean_eps: 0.152429
  943013/2000000: episode: 5382, duration: 16.732s, episode steps: 1000, steps per second: 60, episode reward: 54.402, mean reward: 0.054 [-19.241, 16.249], mean action: 1.029 [0.000, 3.000], mean observation: 0.193 [-0.719, 1.000], loss: 1.249870, mean_absolute_error: 23.378894, mean_q: 30.871603, mean_eps: 0.151737
  943730/2000000: episode: 5383, duration: 12.275s, episode steps: 717, steps per second: 58, episode reward: 128.271, mean reward: 0.179 [-11.192, 100.000], mean action: 1.725 [0.000, 3.000], mean observation: 0.129 [-0.714, 1.000], loss: 1.357863, mean_absolute_error: 23.912101, mean_q: 31.293606, mean_eps: 0.150965
  944586/2000000: episode: 5384, duration: 14.536s, episode steps: 856, steps per second: 59, episode reward: 178.620, mean reward: 0.209 [-19.759, 100.000], mean action: 1.019 [0.000, 3.000], mean observation: 0.120 [-0.565, 1.000], loss: 1.194138, mean_absolute_error: 23.649849, mean_q: 31.200422, mean_eps: 0.150258
  944835/2000000: episode: 5385, duration: 3.978s, episode steps: 249, steps per second: 63, episode reward: -85.096, mean reward: -0.342 [-100.000, 12.515], mean action: 1.731 [0.000, 3.000], mean observation: 0.083 [-0.704, 1.000], loss: 1.317652, mean_absolute_error: 22.871468, mean_q: 29.994213, mean_eps: 0.149761
  945286/2000000: episode: 5386, duration: 7.311s, episode steps: 451, steps per second: 62, episode reward: 194.061, mean reward: 0.430 [-17.536, 100.000], mean action: 1.333 [0.000, 3.000], mean observation: 0.090 [-1.230, 1.000], loss: 1.304818, mean_absolute_error: 23.310946, mean_q: 30.450490, mean_eps: 0.149446
  945874/2000000: episode: 5387, duration: 9.959s, episode steps: 588, steps per second: 59, episode reward: 183.186, mean reward: 0.312 [-17.338, 100.000], mean action: 1.286 [0.000, 3.000], mean observation: 0.072 [-0.709, 1.000], loss: 1.091885, mean_absolute_error: 23.805865, mean_q: 31.460434, mean_eps: 0.148978
  946561/2000000: episode: 5388, duration: 11.201s, episode steps: 687, steps per second: 61, episode reward: 94.721, mean reward: 0.138 [-20.347, 100.000], mean action: 1.707 [0.000, 3.000], mean observation: 0.099 [-0.756, 1.000], loss: 1.298937, mean_absolute_error: 23.268128, mean_q: 30.816695, mean_eps: 0.148404
  947076/2000000: episode: 5389, duration: 8.442s, episode steps: 515, steps per second: 61, episode reward: 169.492, mean reward: 0.329 [-18.033, 100.000], mean action: 1.472 [0.000, 3.000], mean observation: 0.122 [-0.695, 1.000], loss: 1.218799, mean_absolute_error: 23.402529, mean_q: 30.772940, mean_eps: 0.147864
  947712/2000000: episode: 5390, duration: 10.401s, episode steps: 636, steps per second: 61, episode reward: 187.381, mean reward: 0.295 [-18.453, 100.000], mean action: 0.953 [0.000, 3.000], mean observation: 0.123 [-0.706, 1.000], loss: 1.296853, mean_absolute_error: 23.589203, mean_q: 31.291431, mean_eps: 0.147347
  948712/2000000: episode: 5391, duration: 16.657s, episode steps: 1000, steps per second: 60, episode reward: 33.357, mean reward: 0.033 [-18.906, 22.310], mean action: 1.177 [0.000, 3.000], mean observation: 0.177 [-0.731, 1.000], loss: 1.265650, mean_absolute_error: 23.718170, mean_q: 30.938650, mean_eps: 0.146611
  949138/2000000: episode: 5392, duration: 6.820s, episode steps: 426, steps per second: 62, episode reward: 227.739, mean reward: 0.535 [-13.017, 100.000], mean action: 1.819 [0.000, 3.000], mean observation: 0.149 [-0.725, 1.000], loss: 1.225988, mean_absolute_error: 23.567166, mean_q: 31.120376, mean_eps: 0.145968
  950013/2000000: episode: 5393, duration: 15.038s, episode steps: 875, steps per second: 58, episode reward: 89.958, mean reward: 0.103 [-21.048, 100.000], mean action: 1.219 [0.000, 3.000], mean observation: 0.159 [-1.301, 1.000], loss: 1.215898, mean_absolute_error: 23.423905, mean_q: 30.731785, mean_eps: 0.145382
  950526/2000000: episode: 5394, duration: 8.516s, episode steps: 513, steps per second: 60, episode reward: 142.167, mean reward: 0.277 [-17.876, 100.000], mean action: 1.199 [0.000, 3.000], mean observation: 0.091 [-0.723, 1.000], loss: 1.195181, mean_absolute_error: 23.459805, mean_q: 31.231443, mean_eps: 0.144757
  951526/2000000: episode: 5395, duration: 16.656s, episode steps: 1000, steps per second: 60, episode reward: 18.877, mean reward: 0.019 [-22.387, 15.613], mean action: 2.138 [0.000, 3.000], mean observation: 0.147 [-0.633, 1.000], loss: 1.249761, mean_absolute_error: 23.886678, mean_q: 31.548130, mean_eps: 0.144077
  952140/2000000: episode: 5396, duration: 9.840s, episode steps: 614, steps per second: 62, episode reward: -293.661, mean reward: -0.478 [-100.000, 15.074], mean action: 1.792 [0.000, 3.000], mean observation: 0.117 [-1.019, 1.006], loss: 1.279336, mean_absolute_error: 23.473898, mean_q: 31.325182, mean_eps: 0.143351
  952596/2000000: episode: 5397, duration: 7.726s, episode steps: 456, steps per second: 59, episode reward: 170.099, mean reward: 0.373 [-11.830, 100.000], mean action: 1.535 [0.000, 3.000], mean observation: 0.039 [-0.716, 1.000], loss: 1.161479, mean_absolute_error: 23.243763, mean_q: 30.948696, mean_eps: 0.142871
  953002/2000000: episode: 5398, duration: 6.847s, episode steps: 406, steps per second: 59, episode reward: 195.925, mean reward: 0.483 [-11.481, 100.000], mean action: 1.384 [0.000, 3.000], mean observation: 0.053 [-0.744, 1.000], loss: 1.233348, mean_absolute_error: 23.199596, mean_q: 30.094375, mean_eps: 0.142482
  953476/2000000: episode: 5399, duration: 8.033s, episode steps: 474, steps per second: 59, episode reward: 181.955, mean reward: 0.384 [-20.754, 100.000], mean action: 1.331 [0.000, 3.000], mean observation: 0.069 [-0.717, 1.000], loss: 1.268824, mean_absolute_error: 23.055459, mean_q: 30.278346, mean_eps: 0.142086
  953763/2000000: episode: 5400, duration: 4.563s, episode steps: 287, steps per second: 63, episode reward: -55.750, mean reward: -0.194 [-100.000, 15.361], mean action: 1.861 [0.000, 3.000], mean observation: 0.123 [-0.807, 1.008], loss: 1.204561, mean_absolute_error: 23.741127, mean_q: 31.672421, mean_eps: 0.141744
  954325/2000000: episode: 5401, duration: 9.891s, episode steps: 562, steps per second: 57, episode reward: 206.294, mean reward: 0.367 [-17.949, 100.000], mean action: 1.146 [0.000, 3.000], mean observation: 0.064 [-0.700, 1.000], loss: 1.216385, mean_absolute_error: 23.146252, mean_q: 30.624861, mean_eps: 0.141360
  954888/2000000: episode: 5402, duration: 9.061s, episode steps: 563, steps per second: 62, episode reward: -330.165, mean reward: -0.586 [-100.000, 10.114], mean action: 1.636 [0.000, 3.000], mean observation: 0.098 [-0.781, 1.539], loss: 1.311574, mean_absolute_error: 23.307726, mean_q: 30.684167, mean_eps: 0.140855
  955033/2000000: episode: 5403, duration: 2.284s, episode steps: 145, steps per second: 63, episode reward: -24.882, mean reward: -0.172 [-100.000, 17.096], mean action: 1.697 [0.000, 3.000], mean observation: 0.057 [-0.610, 1.000], loss: 1.888068, mean_absolute_error: 23.937253, mean_q: 30.913226, mean_eps: 0.140536
  955451/2000000: episode: 5404, duration: 6.686s, episode steps: 418, steps per second: 63, episode reward: -140.198, mean reward: -0.335 [-100.000, 12.431], mean action: 1.663 [0.000, 3.000], mean observation: 0.093 [-0.776, 1.000], loss: 1.354348, mean_absolute_error: 23.755580, mean_q: 31.446997, mean_eps: 0.140282
  955787/2000000: episode: 5405, duration: 5.311s, episode steps: 336, steps per second: 63, episode reward: 200.889, mean reward: 0.598 [-7.186, 100.000], mean action: 1.488 [0.000, 3.000], mean observation: 0.020 [-0.776, 1.000], loss: 1.120622, mean_absolute_error: 23.346969, mean_q: 31.297524, mean_eps: 0.139944
  956585/2000000: episode: 5406, duration: 13.134s, episode steps: 798, steps per second: 61, episode reward: 127.695, mean reward: 0.160 [-21.978, 100.000], mean action: 1.350 [0.000, 3.000], mean observation: 0.162 [-0.685, 1.000], loss: 1.224199, mean_absolute_error: 23.177969, mean_q: 30.932892, mean_eps: 0.139433
  957525/2000000: episode: 5407, duration: 15.229s, episode steps: 940, steps per second: 62, episode reward: 188.479, mean reward: 0.201 [-19.221, 100.000], mean action: 1.491 [0.000, 3.000], mean observation: 0.158 [-0.743, 1.000], loss: 1.234765, mean_absolute_error: 23.958700, mean_q: 31.498592, mean_eps: 0.138650
  957898/2000000: episode: 5408, duration: 5.870s, episode steps: 373, steps per second: 64, episode reward: -69.811, mean reward: -0.187 [-100.000, 11.040], mean action: 1.732 [0.000, 3.000], mean observation: 0.088 [-0.912, 1.000], loss: 1.405138, mean_absolute_error: 23.861555, mean_q: 31.560990, mean_eps: 0.138059
  958689/2000000: episode: 5409, duration: 13.424s, episode steps: 791, steps per second: 59, episode reward: 149.184, mean reward: 0.189 [-19.327, 100.000], mean action: 1.099 [0.000, 3.000], mean observation: 0.131 [-0.610, 1.000], loss: 1.181146, mean_absolute_error: 23.528368, mean_q: 31.347717, mean_eps: 0.137535
  959416/2000000: episode: 5410, duration: 12.039s, episode steps: 727, steps per second: 60, episode reward: 191.152, mean reward: 0.263 [-20.036, 100.000], mean action: 1.018 [0.000, 3.000], mean observation: 0.139 [-0.593, 1.008], loss: 1.247821, mean_absolute_error: 23.913962, mean_q: 31.645103, mean_eps: 0.136853
  960416/2000000: episode: 5411, duration: 17.461s, episode steps: 1000, steps per second: 57, episode reward: 62.603, mean reward: 0.063 [-23.683, 22.936], mean action: 0.761 [0.000, 3.000], mean observation: 0.184 [-0.729, 1.000], loss: 1.121697, mean_absolute_error: 22.849744, mean_q: 30.375823, mean_eps: 0.136077
  960783/2000000: episode: 5412, duration: 5.758s, episode steps: 367, steps per second: 64, episode reward: 151.806, mean reward: 0.414 [-6.570, 100.000], mean action: 1.477 [0.000, 3.000], mean observation: 0.128 [-1.037, 1.000], loss: 1.047760, mean_absolute_error: 22.939420, mean_q: 30.421372, mean_eps: 0.135462
  961482/2000000: episode: 5413, duration: 11.426s, episode steps: 699, steps per second: 61, episode reward: 110.762, mean reward: 0.158 [-19.183, 100.000], mean action: 1.524 [0.000, 3.000], mean observation: 0.130 [-0.690, 1.000], loss: 1.185413, mean_absolute_error: 23.213919, mean_q: 30.854963, mean_eps: 0.134981
  961769/2000000: episode: 5414, duration: 4.544s, episode steps: 287, steps per second: 63, episode reward: 11.269, mean reward: 0.039 [-100.000, 12.636], mean action: 1.801 [0.000, 3.000], mean observation: -0.025 [-0.798, 1.352], loss: 1.102184, mean_absolute_error: 22.578167, mean_q: 30.361946, mean_eps: 0.134537
  962659/2000000: episode: 5415, duration: 15.498s, episode steps: 890, steps per second: 57, episode reward: 92.641, mean reward: 0.104 [-17.701, 100.000], mean action: 1.254 [0.000, 3.000], mean observation: 0.104 [-0.674, 1.000], loss: 1.172327, mean_absolute_error: 23.386085, mean_q: 31.103624, mean_eps: 0.134007
  963322/2000000: episode: 5416, duration: 11.543s, episode steps: 663, steps per second: 57, episode reward: 144.474, mean reward: 0.218 [-17.615, 100.000], mean action: 1.558 [0.000, 3.000], mean observation: 0.111 [-0.709, 1.000], loss: 1.233605, mean_absolute_error: 22.490649, mean_q: 30.088188, mean_eps: 0.133309
  963769/2000000: episode: 5417, duration: 7.288s, episode steps: 447, steps per second: 61, episode reward: 182.150, mean reward: 0.407 [-17.400, 100.000], mean action: 1.304 [0.000, 3.000], mean observation: 0.083 [-0.696, 1.000], loss: 1.365311, mean_absolute_error: 23.455645, mean_q: 31.293689, mean_eps: 0.132809
  964217/2000000: episode: 5418, duration: 7.343s, episode steps: 448, steps per second: 61, episode reward: 186.730, mean reward: 0.417 [-18.830, 100.000], mean action: 1.429 [0.000, 3.000], mean observation: 0.056 [-0.651, 1.000], loss: 1.437172, mean_absolute_error: 23.337662, mean_q: 30.779598, mean_eps: 0.132405
  965217/2000000: episode: 5419, duration: 16.421s, episode steps: 1000, steps per second: 61, episode reward: -25.109, mean reward: -0.025 [-17.553, 22.657], mean action: 1.269 [0.000, 3.000], mean observation: 0.147 [-1.030, 1.000], loss: 1.301141, mean_absolute_error: 23.544222, mean_q: 31.288383, mean_eps: 0.131754
  965859/2000000: episode: 5420, duration: 10.597s, episode steps: 642, steps per second: 61, episode reward: 142.455, mean reward: 0.222 [-20.126, 100.000], mean action: 1.132 [0.000, 3.000], mean observation: 0.152 [-0.696, 1.000], loss: 1.138789, mean_absolute_error: 22.791558, mean_q: 30.044317, mean_eps: 0.131016
  966307/2000000: episode: 5421, duration: 7.109s, episode steps: 448, steps per second: 63, episode reward: 131.986, mean reward: 0.295 [-18.798, 100.000], mean action: 1.518 [0.000, 3.000], mean observation: 0.167 [-0.635, 1.000], loss: 1.344918, mean_absolute_error: 23.158619, mean_q: 30.643291, mean_eps: 0.130526
  966848/2000000: episode: 5422, duration: 8.759s, episode steps: 541, steps per second: 62, episode reward: 207.108, mean reward: 0.383 [-19.095, 100.000], mean action: 1.214 [0.000, 3.000], mean observation: 0.085 [-0.639, 1.019], loss: 1.127225, mean_absolute_error: 23.082370, mean_q: 30.740589, mean_eps: 0.130082
  967829/2000000: episode: 5423, duration: 16.406s, episode steps: 981, steps per second: 60, episode reward: 105.632, mean reward: 0.108 [-18.390, 100.000], mean action: 1.444 [0.000, 3.000], mean observation: 0.152 [-0.968, 1.000], loss: 1.209946, mean_absolute_error: 23.436297, mean_q: 31.122410, mean_eps: 0.129396
  968199/2000000: episode: 5424, duration: 5.740s, episode steps: 370, steps per second: 64, episode reward: 189.447, mean reward: 0.512 [-17.829, 100.000], mean action: 1.554 [0.000, 3.000], mean observation: 0.162 [-0.749, 1.000], loss: 1.113334, mean_absolute_error: 23.360520, mean_q: 30.663996, mean_eps: 0.128787
  968676/2000000: episode: 5425, duration: 7.573s, episode steps: 477, steps per second: 63, episode reward: 191.555, mean reward: 0.402 [-19.899, 100.000], mean action: 1.574 [0.000, 3.000], mean observation: 0.032 [-0.715, 1.000], loss: 1.229241, mean_absolute_error: 23.198111, mean_q: 30.563528, mean_eps: 0.128408
  968921/2000000: episode: 5426, duration: 3.841s, episode steps: 245, steps per second: 64, episode reward: -18.288, mean reward: -0.075 [-100.000, 13.015], mean action: 1.820 [0.000, 3.000], mean observation: -0.035 [-0.764, 1.000], loss: 1.235515, mean_absolute_error: 23.447634, mean_q: 30.507702, mean_eps: 0.128082
  969463/2000000: episode: 5427, duration: 8.837s, episode steps: 542, steps per second: 61, episode reward: 212.433, mean reward: 0.392 [-18.002, 100.000], mean action: 1.528 [0.000, 3.000], mean observation: 0.043 [-0.735, 1.000], loss: 1.079635, mean_absolute_error: 23.364433, mean_q: 31.113124, mean_eps: 0.127727
  969911/2000000: episode: 5428, duration: 7.197s, episode steps: 448, steps per second: 62, episode reward: 190.643, mean reward: 0.426 [-17.349, 100.000], mean action: 1.098 [0.000, 3.000], mean observation: 0.094 [-0.674, 1.000], loss: 1.213294, mean_absolute_error: 22.907925, mean_q: 30.294054, mean_eps: 0.127283
  970267/2000000: episode: 5429, duration: 5.757s, episode steps: 356, steps per second: 62, episode reward: 209.205, mean reward: 0.588 [-8.485, 100.000], mean action: 1.643 [0.000, 3.000], mean observation: 0.124 [-0.698, 1.000], loss: 1.144289, mean_absolute_error: 22.777501, mean_q: 30.346103, mean_eps: 0.126921
  970882/2000000: episode: 5430, duration: 10.388s, episode steps: 615, steps per second: 59, episode reward: 187.026, mean reward: 0.304 [-18.132, 100.000], mean action: 1.169 [0.000, 3.000], mean observation: 0.087 [-0.621, 1.002], loss: 1.191948, mean_absolute_error: 23.432182, mean_q: 31.381409, mean_eps: 0.126483
  971482/2000000: episode: 5431, duration: 9.781s, episode steps: 600, steps per second: 61, episode reward: 127.384, mean reward: 0.212 [-20.208, 100.000], mean action: 1.665 [0.000, 3.000], mean observation: 0.113 [-0.696, 1.000], loss: 1.217213, mean_absolute_error: 23.066611, mean_q: 30.543100, mean_eps: 0.125936
  971884/2000000: episode: 5432, duration: 6.407s, episode steps: 402, steps per second: 63, episode reward: 204.102, mean reward: 0.508 [-17.996, 100.000], mean action: 0.943 [0.000, 3.000], mean observation: 0.134 [-0.573, 1.000], loss: 1.202614, mean_absolute_error: 23.057373, mean_q: 30.800563, mean_eps: 0.125486
  972264/2000000: episode: 5433, duration: 6.133s, episode steps: 380, steps per second: 62, episode reward: 205.673, mean reward: 0.541 [-2.713, 100.000], mean action: 1.411 [0.000, 3.000], mean observation: 0.016 [-0.758, 1.000], loss: 1.057613, mean_absolute_error: 22.473251, mean_q: 29.994854, mean_eps: 0.125135
  972741/2000000: episode: 5434, duration: 7.594s, episode steps: 477, steps per second: 63, episode reward: 180.839, mean reward: 0.379 [-19.854, 100.000], mean action: 1.019 [0.000, 3.000], mean observation: 0.116 [-0.708, 1.000], loss: 1.185374, mean_absolute_error: 22.752518, mean_q: 30.271498, mean_eps: 0.124748
  973149/2000000: episode: 5435, duration: 6.415s, episode steps: 408, steps per second: 64, episode reward: 221.512, mean reward: 0.543 [-17.868, 100.000], mean action: 0.939 [0.000, 3.000], mean observation: 0.157 [-0.723, 1.000], loss: 1.159788, mean_absolute_error: 22.926952, mean_q: 30.434263, mean_eps: 0.124349
  973459/2000000: episode: 5436, duration: 4.870s, episode steps: 310, steps per second: 64, episode reward: -30.460, mean reward: -0.098 [-100.000, 19.482], mean action: 1.787 [0.000, 3.000], mean observation: 0.118 [-0.756, 1.535], loss: 1.070548, mean_absolute_error: 23.633242, mean_q: 31.764882, mean_eps: 0.124026
  973725/2000000: episode: 5437, duration: 4.151s, episode steps: 266, steps per second: 64, episode reward: -93.808, mean reward: -0.353 [-100.000, 15.809], mean action: 1.966 [0.000, 3.000], mean observation: 0.100 [-0.750, 1.000], loss: 1.139441, mean_absolute_error: 23.153362, mean_q: 30.715254, mean_eps: 0.123767
  974018/2000000: episode: 5438, duration: 4.573s, episode steps: 293, steps per second: 64, episode reward: -66.051, mean reward: -0.225 [-100.000, 11.381], mean action: 1.710 [0.000, 3.000], mean observation: 0.045 [-0.740, 1.000], loss: 1.183442, mean_absolute_error: 22.986955, mean_q: 30.496169, mean_eps: 0.123515
  974853/2000000: episode: 5439, duration: 13.850s, episode steps: 835, steps per second: 60, episode reward: 112.642, mean reward: 0.135 [-6.074, 100.000], mean action: 1.529 [0.000, 3.000], mean observation: 0.089 [-0.607, 1.020], loss: 1.220782, mean_absolute_error: 23.159888, mean_q: 30.865427, mean_eps: 0.123008
  975514/2000000: episode: 5440, duration: 11.316s, episode steps: 661, steps per second: 58, episode reward: 137.229, mean reward: 0.208 [-13.197, 100.000], mean action: 1.495 [0.000, 3.000], mean observation: 0.095 [-0.676, 1.000], loss: 1.269500, mean_absolute_error: 23.145979, mean_q: 30.801957, mean_eps: 0.122334
  975807/2000000: episode: 5441, duration: 4.624s, episode steps: 293, steps per second: 63, episode reward: 201.317, mean reward: 0.687 [-8.515, 100.000], mean action: 1.567 [0.000, 3.000], mean observation: 0.044 [-0.753, 1.000], loss: 1.087659, mean_absolute_error: 23.064082, mean_q: 30.606934, mean_eps: 0.121906
  976777/2000000: episode: 5442, duration: 16.185s, episode steps: 970, steps per second: 60, episode reward: 99.723, mean reward: 0.103 [-18.793, 100.000], mean action: 1.345 [0.000, 3.000], mean observation: 0.131 [-1.450, 1.000], loss: 1.157035, mean_absolute_error: 23.317191, mean_q: 30.803720, mean_eps: 0.121337
  977266/2000000: episode: 5443, duration: 7.808s, episode steps: 489, steps per second: 63, episode reward: 135.842, mean reward: 0.278 [-12.918, 100.000], mean action: 1.352 [0.000, 3.000], mean observation: 0.074 [-0.663, 1.000], loss: 1.179970, mean_absolute_error: 22.977906, mean_q: 30.605490, mean_eps: 0.120680
  977717/2000000: episode: 5444, duration: 7.329s, episode steps: 451, steps per second: 62, episode reward: 159.090, mean reward: 0.353 [-11.566, 100.000], mean action: 1.619 [0.000, 3.000], mean observation: 0.060 [-0.716, 1.000], loss: 1.107889, mean_absolute_error: 22.674919, mean_q: 30.569841, mean_eps: 0.120257
  978248/2000000: episode: 5445, duration: 8.574s, episode steps: 531, steps per second: 62, episode reward: 229.284, mean reward: 0.432 [-13.306, 100.000], mean action: 1.081 [0.000, 3.000], mean observation: 0.104 [-0.738, 1.067], loss: 1.242315, mean_absolute_error: 23.386555, mean_q: 30.887094, mean_eps: 0.119816
  979063/2000000: episode: 5446, duration: 13.681s, episode steps: 815, steps per second: 60, episode reward: 177.710, mean reward: 0.218 [-19.097, 100.000], mean action: 0.872 [0.000, 3.000], mean observation: 0.155 [-0.720, 1.000], loss: 1.157521, mean_absolute_error: 22.744299, mean_q: 30.329502, mean_eps: 0.119211
  979730/2000000: episode: 5447, duration: 11.282s, episode steps: 667, steps per second: 59, episode reward: 183.925, mean reward: 0.276 [-17.566, 100.000], mean action: 1.210 [0.000, 3.000], mean observation: 0.077 [-0.561, 1.000], loss: 1.213166, mean_absolute_error: 22.550219, mean_q: 29.813204, mean_eps: 0.118544
  980117/2000000: episode: 5448, duration: 6.241s, episode steps: 387, steps per second: 62, episode reward: 161.435, mean reward: 0.417 [-4.655, 100.000], mean action: 1.382 [0.000, 3.000], mean observation: 0.014 [-0.676, 1.000], loss: 1.175486, mean_absolute_error: 22.441378, mean_q: 30.213396, mean_eps: 0.118068
  980499/2000000: episode: 5449, duration: 6.029s, episode steps: 382, steps per second: 63, episode reward: 234.856, mean reward: 0.615 [-7.068, 100.000], mean action: 1.469 [0.000, 3.000], mean observation: 0.056 [-0.611, 1.018], loss: 1.264469, mean_absolute_error: 23.600605, mean_q: 31.430399, mean_eps: 0.117723
  980886/2000000: episode: 5450, duration: 6.173s, episode steps: 387, steps per second: 63, episode reward: 177.409, mean reward: 0.458 [-18.772, 100.000], mean action: 1.127 [0.000, 3.000], mean observation: 0.071 [-0.679, 1.000], loss: 1.139396, mean_absolute_error: 23.353583, mean_q: 30.914159, mean_eps: 0.117377
  981873/2000000: episode: 5451, duration: 16.647s, episode steps: 987, steps per second: 59, episode reward: 164.345, mean reward: 0.167 [-17.827, 100.000], mean action: 1.197 [0.000, 3.000], mean observation: 0.189 [-0.536, 1.000], loss: 1.247482, mean_absolute_error: 23.212568, mean_q: 30.768666, mean_eps: 0.116758
  982859/2000000: episode: 5452, duration: 15.945s, episode steps: 986, steps per second: 62, episode reward: 72.547, mean reward: 0.074 [-12.915, 100.000], mean action: 1.423 [0.000, 3.000], mean observation: 0.111 [-0.657, 1.000], loss: 1.104798, mean_absolute_error: 23.056539, mean_q: 30.811979, mean_eps: 0.115871
  983672/2000000: episode: 5453, duration: 13.391s, episode steps: 813, steps per second: 61, episode reward: 180.104, mean reward: 0.222 [-22.028, 100.000], mean action: 1.230 [0.000, 3.000], mean observation: 0.165 [-0.705, 1.000], loss: 1.167203, mean_absolute_error: 22.975702, mean_q: 30.371980, mean_eps: 0.115062
  983976/2000000: episode: 5454, duration: 4.798s, episode steps: 304, steps per second: 63, episode reward: 194.150, mean reward: 0.639 [-19.739, 100.000], mean action: 1.296 [0.000, 3.000], mean observation: 0.084 [-0.726, 1.000], loss: 1.318823, mean_absolute_error: 22.934002, mean_q: 30.166435, mean_eps: 0.114560
  984501/2000000: episode: 5455, duration: 8.514s, episode steps: 525, steps per second: 62, episode reward: 176.493, mean reward: 0.336 [-18.456, 100.000], mean action: 1.364 [0.000, 3.000], mean observation: 0.048 [-0.622, 1.000], loss: 1.233501, mean_absolute_error: 23.098518, mean_q: 30.619686, mean_eps: 0.114186
  985481/2000000: episode: 5456, duration: 16.212s, episode steps: 980, steps per second: 60, episode reward: 103.649, mean reward: 0.106 [-18.867, 100.000], mean action: 1.161 [0.000, 3.000], mean observation: 0.157 [-0.721, 1.000], loss: 1.130861, mean_absolute_error: 23.062111, mean_q: 30.706019, mean_eps: 0.113507
  986130/2000000: episode: 5457, duration: 11.036s, episode steps: 649, steps per second: 59, episode reward: 191.202, mean reward: 0.295 [-18.092, 100.000], mean action: 1.331 [0.000, 3.000], mean observation: 0.047 [-0.686, 1.000], loss: 1.111893, mean_absolute_error: 22.532610, mean_q: 30.171607, mean_eps: 0.112775
  986602/2000000: episode: 5458, duration: 7.596s, episode steps: 472, steps per second: 62, episode reward: 205.234, mean reward: 0.435 [-17.777, 100.000], mean action: 1.320 [0.000, 3.000], mean observation: 0.057 [-0.748, 1.000], loss: 1.162050, mean_absolute_error: 22.803015, mean_q: 30.574691, mean_eps: 0.112271
  987019/2000000: episode: 5459, duration: 6.836s, episode steps: 417, steps per second: 61, episode reward: 196.889, mean reward: 0.472 [-10.186, 100.000], mean action: 1.463 [0.000, 3.000], mean observation: 0.021 [-0.774, 1.000], loss: 1.203722, mean_absolute_error: 22.831564, mean_q: 30.412107, mean_eps: 0.111871
  987654/2000000: episode: 5460, duration: 10.254s, episode steps: 635, steps per second: 62, episode reward: 183.536, mean reward: 0.289 [-19.318, 100.000], mean action: 1.606 [0.000, 3.000], mean observation: 0.004 [-0.755, 1.000], loss: 1.081767, mean_absolute_error: 23.116823, mean_q: 30.742834, mean_eps: 0.111398
  988460/2000000: episode: 5461, duration: 13.939s, episode steps: 806, steps per second: 58, episode reward: 146.125, mean reward: 0.181 [-20.937, 100.000], mean action: 2.104 [0.000, 3.000], mean observation: 0.182 [-0.629, 1.000], loss: 1.108626, mean_absolute_error: 23.042117, mean_q: 30.741131, mean_eps: 0.110750
  988826/2000000: episode: 5462, duration: 5.750s, episode steps: 366, steps per second: 64, episode reward: 232.726, mean reward: 0.636 [-10.681, 100.000], mean action: 1.107 [0.000, 3.000], mean observation: 0.143 [-0.612, 1.010], loss: 1.061929, mean_absolute_error: 23.292087, mean_q: 30.918937, mean_eps: 0.110222
  989435/2000000: episode: 5463, duration: 10.089s, episode steps: 609, steps per second: 60, episode reward: 162.525, mean reward: 0.267 [-18.852, 100.000], mean action: 0.882 [0.000, 3.000], mean observation: 0.121 [-0.670, 1.000], loss: 1.191880, mean_absolute_error: 23.193057, mean_q: 30.835097, mean_eps: 0.109783
  989969/2000000: episode: 5464, duration: 8.851s, episode steps: 534, steps per second: 60, episode reward: 169.433, mean reward: 0.317 [-17.939, 100.000], mean action: 1.097 [0.000, 3.000], mean observation: 0.076 [-0.664, 1.000], loss: 1.219812, mean_absolute_error: 23.617356, mean_q: 31.256027, mean_eps: 0.109268
  990783/2000000: episode: 5465, duration: 14.814s, episode steps: 814, steps per second: 55, episode reward: 124.223, mean reward: 0.153 [-19.440, 100.000], mean action: 1.378 [0.000, 3.000], mean observation: 0.103 [-0.672, 1.000], loss: 1.166988, mean_absolute_error: 23.033451, mean_q: 30.908586, mean_eps: 0.108662
  991477/2000000: episode: 5466, duration: 11.372s, episode steps: 694, steps per second: 61, episode reward: 190.300, mean reward: 0.274 [-18.633, 100.000], mean action: 0.990 [0.000, 3.000], mean observation: 0.127 [-0.635, 1.000], loss: 1.211763, mean_absolute_error: 22.743917, mean_q: 30.221160, mean_eps: 0.107983
  991876/2000000: episode: 5467, duration: 6.414s, episode steps: 399, steps per second: 62, episode reward: -50.497, mean reward: -0.127 [-100.000, 12.980], mean action: 1.825 [0.000, 3.000], mean observation: 0.030 [-0.696, 1.000], loss: 1.048104, mean_absolute_error: 23.138436, mean_q: 31.065457, mean_eps: 0.107492
  992340/2000000: episode: 5468, duration: 7.487s, episode steps: 464, steps per second: 62, episode reward: 174.493, mean reward: 0.376 [-10.771, 100.000], mean action: 1.442 [0.000, 3.000], mean observation: 0.046 [-0.534, 1.000], loss: 1.251243, mean_absolute_error: 22.497395, mean_q: 30.190914, mean_eps: 0.107105
  992570/2000000: episode: 5469, duration: 3.606s, episode steps: 230, steps per second: 64, episode reward: 213.936, mean reward: 0.930 [-9.574, 100.000], mean action: 1.357 [0.000, 3.000], mean observation: 0.095 [-0.556, 1.000], loss: 1.180586, mean_absolute_error: 22.757908, mean_q: 30.297855, mean_eps: 0.106791
  993031/2000000: episode: 5470, duration: 7.404s, episode steps: 461, steps per second: 62, episode reward: 199.207, mean reward: 0.432 [-19.758, 100.000], mean action: 1.239 [0.000, 3.000], mean observation: 0.066 [-0.703, 1.000], loss: 1.070350, mean_absolute_error: 23.146541, mean_q: 30.995245, mean_eps: 0.106480
  993458/2000000: episode: 5471, duration: 6.863s, episode steps: 427, steps per second: 62, episode reward: 160.612, mean reward: 0.376 [-18.506, 100.000], mean action: 1.475 [0.000, 3.000], mean observation: 0.040 [-0.957, 1.000], loss: 1.154232, mean_absolute_error: 23.129378, mean_q: 30.862812, mean_eps: 0.106080
  994284/2000000: episode: 5472, duration: 13.176s, episode steps: 826, steps per second: 63, episode reward: -157.383, mean reward: -0.191 [-100.000, 10.780], mean action: 1.711 [0.000, 3.000], mean observation: 0.071 [-0.844, 1.000], loss: 1.186329, mean_absolute_error: 23.132617, mean_q: 30.935885, mean_eps: 0.105517
  995037/2000000: episode: 5473, duration: 12.650s, episode steps: 753, steps per second: 60, episode reward: 164.527, mean reward: 0.218 [-17.538, 100.000], mean action: 1.092 [0.000, 3.000], mean observation: 0.076 [-0.765, 1.000], loss: 1.059579, mean_absolute_error: 22.684975, mean_q: 30.198688, mean_eps: 0.104806
  995461/2000000: episode: 5474, duration: 6.790s, episode steps: 424, steps per second: 62, episode reward: 165.822, mean reward: 0.391 [-8.551, 100.000], mean action: 1.219 [0.000, 3.000], mean observation: 0.064 [-0.705, 1.000], loss: 1.166339, mean_absolute_error: 23.026205, mean_q: 30.794548, mean_eps: 0.104275
  995851/2000000: episode: 5475, duration: 6.113s, episode steps: 390, steps per second: 64, episode reward: 209.119, mean reward: 0.536 [-9.338, 100.000], mean action: 1.408 [0.000, 3.000], mean observation: 0.036 [-0.597, 1.000], loss: 1.072059, mean_absolute_error: 23.163269, mean_q: 31.221014, mean_eps: 0.103910
  996114/2000000: episode: 5476, duration: 4.096s, episode steps: 263, steps per second: 64, episode reward: 203.270, mean reward: 0.773 [-13.381, 100.000], mean action: 1.837 [0.000, 3.000], mean observation: 0.095 [-0.688, 1.000], loss: 1.088364, mean_absolute_error: 22.782693, mean_q: 30.185003, mean_eps: 0.103616
  996593/2000000: episode: 5477, duration: 8.293s, episode steps: 479, steps per second: 58, episode reward: 172.899, mean reward: 0.361 [-12.006, 100.000], mean action: 1.413 [0.000, 3.000], mean observation: 0.047 [-0.617, 1.000], loss: 1.298447, mean_absolute_error: 22.583322, mean_q: 30.082561, mean_eps: 0.103281
  996887/2000000: episode: 5478, duration: 4.517s, episode steps: 294, steps per second: 65, episode reward: -79.471, mean reward: -0.270 [-100.000, 9.895], mean action: 1.616 [0.000, 3.000], mean observation: 0.077 [-0.704, 1.000], loss: 1.242367, mean_absolute_error: 23.154076, mean_q: 30.566234, mean_eps: 0.102934
  997291/2000000: episode: 5479, duration: 6.460s, episode steps: 404, steps per second: 63, episode reward: 176.935, mean reward: 0.438 [-18.285, 100.000], mean action: 1.344 [0.000, 3.000], mean observation: 0.076 [-0.637, 1.000], loss: 1.282128, mean_absolute_error: 22.911271, mean_q: 30.694719, mean_eps: 0.102621
  997838/2000000: episode: 5480, duration: 8.827s, episode steps: 547, steps per second: 62, episode reward: 203.932, mean reward: 0.373 [-17.673, 100.000], mean action: 1.316 [0.000, 3.000], mean observation: 0.045 [-0.696, 1.000], loss: 1.108875, mean_absolute_error: 22.443228, mean_q: 29.597904, mean_eps: 0.102192
  998838/2000000: episode: 5481, duration: 17.084s, episode steps: 1000, steps per second: 59, episode reward: 42.489, mean reward: 0.042 [-18.848, 22.211], mean action: 1.052 [0.000, 3.000], mean observation: 0.197 [-0.706, 1.000], loss: 1.155572, mean_absolute_error: 23.035110, mean_q: 30.751842, mean_eps: 0.101496
  999407/2000000: episode: 5482, duration: 9.078s, episode steps: 569, steps per second: 63, episode reward: 163.594, mean reward: 0.288 [-18.636, 100.000], mean action: 1.097 [0.000, 3.000], mean observation: 0.095 [-0.630, 1.000], loss: 1.199792, mean_absolute_error: 22.932464, mean_q: 30.886816, mean_eps: 0.100790
  999970/2000000: episode: 5483, duration: 9.103s, episode steps: 563, steps per second: 62, episode reward: 233.857, mean reward: 0.415 [-19.484, 100.000], mean action: 1.835 [0.000, 3.000], mean observation: 0.220 [-0.709, 1.000], loss: 1.122963, mean_absolute_error: 22.890343, mean_q: 30.919725, mean_eps: 0.100281
 1000970/2000000: episode: 5484, duration: 17.427s, episode steps: 1000, steps per second: 57, episode reward: 40.020, mean reward: 0.040 [-19.634, 12.170], mean action: 1.089 [0.000, 3.000], mean observation: 0.151 [-0.752, 1.013], loss: 1.041321, mean_absolute_error: 23.057741, mean_q: 30.809052, mean_eps: 0.100000
 1001464/2000000: episode: 5485, duration: 7.877s, episode steps: 494, steps per second: 63, episode reward: 204.906, mean reward: 0.415 [-18.043, 100.000], mean action: 1.399 [0.000, 3.000], mean observation: 0.043 [-0.750, 1.000], loss: 1.084751, mean_absolute_error: 23.266918, mean_q: 30.721083, mean_eps: 0.100000
 1001935/2000000: episode: 5486, duration: 7.713s, episode steps: 471, steps per second: 61, episode reward: 197.190, mean reward: 0.419 [-17.736, 100.000], mean action: 1.310 [0.000, 3.000], mean observation: 0.038 [-0.741, 1.000], loss: 1.314913, mean_absolute_error: 23.536180, mean_q: 31.420831, mean_eps: 0.100000
 1002387/2000000: episode: 5487, duration: 7.296s, episode steps: 452, steps per second: 62, episode reward: 178.949, mean reward: 0.396 [-8.497, 100.000], mean action: 1.354 [0.000, 3.000], mean observation: 0.035 [-0.606, 1.000], loss: 1.224552, mean_absolute_error: 23.177980, mean_q: 30.757164, mean_eps: 0.100000
 1002893/2000000: episode: 5488, duration: 8.125s, episode steps: 506, steps per second: 62, episode reward: 161.995, mean reward: 0.320 [-18.872, 100.000], mean action: 1.328 [0.000, 3.000], mean observation: 0.038 [-0.611, 1.000], loss: 1.106006, mean_absolute_error: 23.112511, mean_q: 30.818670, mean_eps: 0.100000
 1003393/2000000: episode: 5489, duration: 8.028s, episode steps: 500, steps per second: 62, episode reward: 206.622, mean reward: 0.413 [-14.350, 100.000], mean action: 1.656 [0.000, 3.000], mean observation: 0.085 [-0.644, 1.000], loss: 1.124036, mean_absolute_error: 23.154962, mean_q: 31.000962, mean_eps: 0.100000
 1003802/2000000: episode: 5490, duration: 6.763s, episode steps: 409, steps per second: 60, episode reward: 189.098, mean reward: 0.462 [-11.757, 100.000], mean action: 1.484 [0.000, 3.000], mean observation: 0.048 [-0.729, 1.000], loss: 1.201114, mean_absolute_error: 23.550835, mean_q: 31.201022, mean_eps: 0.100000
 1004359/2000000: episode: 5491, duration: 9.339s, episode steps: 557, steps per second: 60, episode reward: 158.247, mean reward: 0.284 [-17.723, 100.000], mean action: 1.129 [0.000, 3.000], mean observation: 0.101 [-0.644, 1.000], loss: 1.186465, mean_absolute_error: 23.592188, mean_q: 31.466848, mean_eps: 0.100000
 1005143/2000000: episode: 5492, duration: 13.204s, episode steps: 784, steps per second: 59, episode reward: 164.890, mean reward: 0.210 [-22.101, 100.000], mean action: 1.152 [0.000, 3.000], mean observation: 0.117 [-0.654, 1.000], loss: 1.048735, mean_absolute_error: 22.894661, mean_q: 30.942320, mean_eps: 0.100000
 1005785/2000000: episode: 5493, duration: 10.568s, episode steps: 642, steps per second: 61, episode reward: 183.395, mean reward: 0.286 [-17.365, 100.000], mean action: 1.332 [0.000, 3.000], mean observation: 0.066 [-0.649, 1.000], loss: 1.088829, mean_absolute_error: 22.466076, mean_q: 29.899442, mean_eps: 0.100000
 1006363/2000000: episode: 5494, duration: 9.601s, episode steps: 578, steps per second: 60, episode reward: 179.848, mean reward: 0.311 [-18.427, 100.000], mean action: 1.308 [0.000, 3.000], mean observation: 0.042 [-0.718, 1.000], loss: 1.138107, mean_absolute_error: 23.075217, mean_q: 31.032626, mean_eps: 0.100000
 1006992/2000000: episode: 5495, duration: 10.384s, episode steps: 629, steps per second: 61, episode reward: 190.429, mean reward: 0.303 [-19.848, 100.000], mean action: 1.118 [0.000, 3.000], mean observation: 0.094 [-0.592, 1.000], loss: 1.110027, mean_absolute_error: 22.914622, mean_q: 30.627065, mean_eps: 0.100000
 1007992/2000000: episode: 5496, duration: 17.787s, episode steps: 1000, steps per second: 56, episode reward: -7.464, mean reward: -0.007 [-10.412, 23.358], mean action: 1.566 [0.000, 3.000], mean observation: 0.067 [-0.738, 1.000], loss: 1.148843, mean_absolute_error: 22.464793, mean_q: 30.123648, mean_eps: 0.100000
 1008464/2000000: episode: 5497, duration: 7.636s, episode steps: 472, steps per second: 62, episode reward: 200.662, mean reward: 0.425 [-11.346, 100.000], mean action: 1.496 [0.000, 3.000], mean observation: 0.057 [-0.733, 1.000], loss: 1.114542, mean_absolute_error: 22.525343, mean_q: 30.071837, mean_eps: 0.100000
 1009009/2000000: episode: 5498, duration: 8.993s, episode steps: 545, steps per second: 61, episode reward: 208.375, mean reward: 0.382 [-12.247, 100.000], mean action: 1.215 [0.000, 3.000], mean observation: 0.040 [-0.699, 1.000], loss: 1.082861, mean_absolute_error: 22.426807, mean_q: 30.146137, mean_eps: 0.100000
 1009764/2000000: episode: 5499, duration: 12.101s, episode steps: 755, steps per second: 62, episode reward: 151.732, mean reward: 0.201 [-17.429, 100.000], mean action: 1.323 [0.000, 3.000], mean observation: 0.067 [-0.575, 1.000], loss: 1.120678, mean_absolute_error: 22.797885, mean_q: 30.805850, mean_eps: 0.100000
 1010248/2000000: episode: 5500, duration: 7.922s, episode steps: 484, steps per second: 61, episode reward: 169.577, mean reward: 0.350 [-18.124, 100.000], mean action: 1.233 [0.000, 3.000], mean observation: 0.134 [-0.751, 1.000], loss: 1.027279, mean_absolute_error: 23.275976, mean_q: 31.216775, mean_eps: 0.100000
 1010599/2000000: episode: 5501, duration: 5.568s, episode steps: 351, steps per second: 63, episode reward: 198.957, mean reward: 0.567 [-11.624, 100.000], mean action: 1.533 [0.000, 3.000], mean observation: 0.057 [-0.552, 1.000], loss: 0.987974, mean_absolute_error: 22.494515, mean_q: 30.213856, mean_eps: 0.100000
 1011318/2000000: episode: 5502, duration: 11.884s, episode steps: 719, steps per second: 60, episode reward: 160.918, mean reward: 0.224 [-19.864, 100.000], mean action: 1.089 [0.000, 3.000], mean observation: 0.170 [-0.860, 1.000], loss: 1.133852, mean_absolute_error: 22.635938, mean_q: 30.507257, mean_eps: 0.100000
 1011809/2000000: episode: 5503, duration: 7.859s, episode steps: 491, steps per second: 62, episode reward: 200.037, mean reward: 0.407 [-17.863, 100.000], mean action: 1.295 [0.000, 3.000], mean observation: 0.051 [-0.735, 1.000], loss: 0.961973, mean_absolute_error: 22.566775, mean_q: 30.523734, mean_eps: 0.100000
 1012177/2000000: episode: 5504, duration: 5.855s, episode steps: 368, steps per second: 63, episode reward: 229.567, mean reward: 0.624 [-17.750, 100.000], mean action: 1.861 [0.000, 3.000], mean observation: 0.153 [-0.473, 1.000], loss: 0.911597, mean_absolute_error: 22.736397, mean_q: 30.483248, mean_eps: 0.100000
 1012490/2000000: episode: 5505, duration: 4.872s, episode steps: 313, steps per second: 64, episode reward: 210.489, mean reward: 0.672 [-18.249, 100.000], mean action: 0.904 [0.000, 3.000], mean observation: 0.122 [-0.687, 1.000], loss: 1.040742, mean_absolute_error: 22.277501, mean_q: 29.721395, mean_eps: 0.100000
 1013490/2000000: episode: 5506, duration: 16.662s, episode steps: 1000, steps per second: 60, episode reward: -9.501, mean reward: -0.010 [-23.064, 17.554], mean action: 1.444 [0.000, 3.000], mean observation: 0.060 [-0.600, 1.000], loss: 1.100021, mean_absolute_error: 23.091017, mean_q: 30.770981, mean_eps: 0.100000
 1014490/2000000: episode: 5507, duration: 17.098s, episode steps: 1000, steps per second: 58, episode reward: 62.725, mean reward: 0.063 [-21.781, 22.802], mean action: 0.983 [0.000, 3.000], mean observation: 0.150 [-0.710, 1.000], loss: 1.177772, mean_absolute_error: 22.485356, mean_q: 30.038683, mean_eps: 0.100000
 1014940/2000000: episode: 5508, duration: 7.260s, episode steps: 450, steps per second: 62, episode reward: 174.741, mean reward: 0.388 [-11.177, 100.000], mean action: 1.338 [0.000, 3.000], mean observation: 0.045 [-0.692, 1.000], loss: 1.172409, mean_absolute_error: 22.228361, mean_q: 29.719912, mean_eps: 0.100000
 1015446/2000000: episode: 5509, duration: 8.230s, episode steps: 506, steps per second: 61, episode reward: 181.984, mean reward: 0.360 [-17.902, 100.000], mean action: 1.518 [0.000, 3.000], mean observation: 0.022 [-0.603, 1.000], loss: 1.141282, mean_absolute_error: 22.323515, mean_q: 29.558772, mean_eps: 0.100000
 1016046/2000000: episode: 5510, duration: 9.861s, episode steps: 600, steps per second: 61, episode reward: 171.466, mean reward: 0.286 [-4.112, 100.000], mean action: 1.525 [0.000, 3.000], mean observation: 0.047 [-0.508, 1.000], loss: 1.172031, mean_absolute_error: 22.763769, mean_q: 30.223406, mean_eps: 0.100000
 1016477/2000000: episode: 5511, duration: 6.864s, episode steps: 431, steps per second: 63, episode reward: 190.604, mean reward: 0.442 [-10.479, 100.000], mean action: 1.260 [0.000, 3.000], mean observation: 0.044 [-0.662, 1.000], loss: 1.059748, mean_absolute_error: 22.382764, mean_q: 30.077017, mean_eps: 0.100000
 1017099/2000000: episode: 5512, duration: 10.358s, episode steps: 622, steps per second: 60, episode reward: 180.665, mean reward: 0.290 [-17.475, 100.000], mean action: 1.297 [0.000, 3.000], mean observation: 0.045 [-0.698, 1.000], loss: 1.118815, mean_absolute_error: 22.512933, mean_q: 30.007447, mean_eps: 0.100000
 1017511/2000000: episode: 5513, duration: 6.589s, episode steps: 412, steps per second: 63, episode reward: 202.444, mean reward: 0.491 [-17.434, 100.000], mean action: 1.575 [0.000, 3.000], mean observation: 0.020 [-0.615, 1.000], loss: 1.290372, mean_absolute_error: 22.817536, mean_q: 30.439605, mean_eps: 0.100000
 1018261/2000000: episode: 5514, duration: 12.289s, episode steps: 750, steps per second: 61, episode reward: 125.086, mean reward: 0.167 [-17.220, 100.000], mean action: 1.375 [0.000, 3.000], mean observation: 0.109 [-0.631, 1.000], loss: 1.132840, mean_absolute_error: 22.372106, mean_q: 30.054332, mean_eps: 0.100000
 1018668/2000000: episode: 5515, duration: 6.546s, episode steps: 407, steps per second: 62, episode reward: 217.518, mean reward: 0.534 [-17.757, 100.000], mean action: 1.872 [0.000, 3.000], mean observation: 0.114 [-0.579, 1.000], loss: 0.971059, mean_absolute_error: 22.576665, mean_q: 30.498767, mean_eps: 0.100000
 1018915/2000000: episode: 5516, duration: 3.882s, episode steps: 247, steps per second: 64, episode reward: 220.527, mean reward: 0.893 [-3.418, 100.000], mean action: 1.474 [0.000, 3.000], mean observation: 0.094 [-0.621, 1.000], loss: 1.029949, mean_absolute_error: 22.686465, mean_q: 30.605263, mean_eps: 0.100000
 1019386/2000000: episode: 5517, duration: 7.683s, episode steps: 471, steps per second: 61, episode reward: 184.913, mean reward: 0.393 [-11.856, 100.000], mean action: 1.476 [0.000, 3.000], mean observation: 0.041 [-0.547, 1.000], loss: 0.969999, mean_absolute_error: 22.684531, mean_q: 30.533879, mean_eps: 0.100000
 1019827/2000000: episode: 5518, duration: 7.067s, episode steps: 441, steps per second: 62, episode reward: 218.356, mean reward: 0.495 [-11.156, 100.000], mean action: 1.660 [0.000, 3.000], mean observation: 0.047 [-1.210, 1.000], loss: 1.093670, mean_absolute_error: 22.729709, mean_q: 30.485911, mean_eps: 0.100000
 1020420/2000000: episode: 5519, duration: 9.752s, episode steps: 593, steps per second: 61, episode reward: 212.599, mean reward: 0.359 [-17.377, 100.000], mean action: 1.334 [0.000, 3.000], mean observation: 0.067 [-0.759, 1.000], loss: 1.074038, mean_absolute_error: 22.759292, mean_q: 30.417251, mean_eps: 0.100000
 1021420/2000000: episode: 5520, duration: 16.661s, episode steps: 1000, steps per second: 60, episode reward: -68.642, mean reward: -0.069 [-4.668, 6.394], mean action: 1.566 [0.000, 3.000], mean observation: 0.096 [-0.411, 0.981], loss: 1.039265, mean_absolute_error: 22.324835, mean_q: 29.985229, mean_eps: 0.100000
 1022002/2000000: episode: 5521, duration: 9.661s, episode steps: 582, steps per second: 60, episode reward: 182.484, mean reward: 0.314 [-18.178, 100.000], mean action: 0.887 [0.000, 3.000], mean observation: 0.116 [-0.598, 1.000], loss: 1.038518, mean_absolute_error: 22.262146, mean_q: 29.730709, mean_eps: 0.100000
 1022675/2000000: episode: 5522, duration: 11.557s, episode steps: 673, steps per second: 58, episode reward: 133.696, mean reward: 0.199 [-17.488, 100.000], mean action: 1.407 [0.000, 3.000], mean observation: 0.083 [-0.744, 1.000], loss: 1.042080, mean_absolute_error: 22.065936, mean_q: 29.549207, mean_eps: 0.100000
 1023133/2000000: episode: 5523, duration: 7.475s, episode steps: 458, steps per second: 61, episode reward: 147.865, mean reward: 0.323 [-7.957, 100.000], mean action: 1.522 [0.000, 3.000], mean observation: 0.048 [-0.597, 1.000], loss: 1.032576, mean_absolute_error: 22.209270, mean_q: 29.767016, mean_eps: 0.100000
 1024133/2000000: episode: 5524, duration: 17.109s, episode steps: 1000, steps per second: 58, episode reward: -140.751, mean reward: -0.141 [-5.994, 4.265], mean action: 1.905 [0.000, 3.000], mean observation: -0.066 [-0.971, 0.926], loss: 1.015774, mean_absolute_error: 22.247461, mean_q: 30.089936, mean_eps: 0.100000
 1025133/2000000: episode: 5525, duration: 18.791s, episode steps: 1000, steps per second: 53, episode reward: -33.796, mean reward: -0.034 [-3.898, 5.076], mean action: 1.460 [0.000, 3.000], mean observation: 0.052 [-0.702, 0.927], loss: 0.998902, mean_absolute_error: 22.439642, mean_q: 29.991350, mean_eps: 0.100000
 1026133/2000000: episode: 5526, duration: 16.788s, episode steps: 1000, steps per second: 60, episode reward: -32.559, mean reward: -0.033 [-4.261, 5.628], mean action: 1.552 [0.000, 3.000], mean observation: 0.083 [-0.454, 0.982], loss: 1.054229, mean_absolute_error: 22.083835, mean_q: 29.603089, mean_eps: 0.100000
 1026717/2000000: episode: 5527, duration: 9.471s, episode steps: 584, steps per second: 62, episode reward: 164.681, mean reward: 0.282 [-17.894, 100.000], mean action: 2.045 [0.000, 3.000], mean observation: 0.135 [-0.619, 1.000], loss: 1.119743, mean_absolute_error: 22.055700, mean_q: 29.693803, mean_eps: 0.100000
 1027208/2000000: episode: 5528, duration: 7.889s, episode steps: 491, steps per second: 62, episode reward: 233.525, mean reward: 0.476 [-17.621, 100.000], mean action: 1.220 [0.000, 3.000], mean observation: 0.062 [-0.791, 1.000], loss: 1.108198, mean_absolute_error: 22.425844, mean_q: 30.118379, mean_eps: 0.100000
 1027728/2000000: episode: 5529, duration: 8.469s, episode steps: 520, steps per second: 61, episode reward: 183.713, mean reward: 0.353 [-19.957, 100.000], mean action: 1.319 [0.000, 3.000], mean observation: 0.098 [-0.612, 1.013], loss: 1.076564, mean_absolute_error: 22.254483, mean_q: 29.965105, mean_eps: 0.100000
 1028374/2000000: episode: 5530, duration: 10.566s, episode steps: 646, steps per second: 61, episode reward: 216.603, mean reward: 0.335 [-17.930, 100.000], mean action: 0.985 [0.000, 3.000], mean observation: 0.092 [-0.684, 1.000], loss: 1.046312, mean_absolute_error: 22.208732, mean_q: 29.897457, mean_eps: 0.100000
 1029283/2000000: episode: 5531, duration: 14.893s, episode steps: 909, steps per second: 61, episode reward: 100.487, mean reward: 0.111 [-19.309, 100.000], mean action: 1.465 [0.000, 3.000], mean observation: 0.051 [-0.738, 1.000], loss: 1.121359, mean_absolute_error: 22.402145, mean_q: 30.150245, mean_eps: 0.100000
 1029910/2000000: episode: 5532, duration: 9.956s, episode steps: 627, steps per second: 63, episode reward: 195.912, mean reward: 0.312 [-17.545, 100.000], mean action: 1.255 [0.000, 3.000], mean observation: 0.045 [-0.698, 1.000], loss: 1.047516, mean_absolute_error: 21.835006, mean_q: 29.211495, mean_eps: 0.100000
 1030464/2000000: episode: 5533, duration: 9.190s, episode steps: 554, steps per second: 60, episode reward: 198.725, mean reward: 0.359 [-18.880, 100.000], mean action: 1.119 [0.000, 3.000], mean observation: 0.083 [-0.722, 1.000], loss: 1.035157, mean_absolute_error: 22.453514, mean_q: 30.031224, mean_eps: 0.100000
 1031150/2000000: episode: 5534, duration: 11.009s, episode steps: 686, steps per second: 62, episode reward: 128.263, mean reward: 0.187 [-10.266, 100.000], mean action: 1.402 [0.000, 3.000], mean observation: 0.065 [-0.723, 1.000], loss: 0.993668, mean_absolute_error: 22.619550, mean_q: 30.641598, mean_eps: 0.100000
 1031693/2000000: episode: 5535, duration: 8.739s, episode steps: 543, steps per second: 62, episode reward: 158.234, mean reward: 0.291 [-18.538, 100.000], mean action: 1.208 [0.000, 3.000], mean observation: 0.069 [-0.731, 1.000], loss: 1.100210, mean_absolute_error: 22.165130, mean_q: 29.491280, mean_eps: 0.100000
 1032665/2000000: episode: 5536, duration: 16.207s, episode steps: 972, steps per second: 60, episode reward: 99.022, mean reward: 0.102 [-8.544, 100.000], mean action: 1.378 [0.000, 3.000], mean observation: 0.086 [-0.747, 1.000], loss: 0.995270, mean_absolute_error: 22.127390, mean_q: 29.621109, mean_eps: 0.100000
 1033100/2000000: episode: 5537, duration: 6.947s, episode steps: 435, steps per second: 63, episode reward: 222.824, mean reward: 0.512 [-3.754, 100.000], mean action: 1.478 [0.000, 3.000], mean observation: 0.021 [-0.768, 1.000], loss: 0.945143, mean_absolute_error: 22.477006, mean_q: 30.238860, mean_eps: 0.100000
 1034100/2000000: episode: 5538, duration: 16.205s, episode steps: 1000, steps per second: 62, episode reward: 36.118, mean reward: 0.036 [-19.088, 22.362], mean action: 1.070 [0.000, 3.000], mean observation: 0.146 [-0.788, 1.000], loss: 1.053253, mean_absolute_error: 21.944154, mean_q: 29.474514, mean_eps: 0.100000
 1034800/2000000: episode: 5539, duration: 11.740s, episode steps: 700, steps per second: 60, episode reward: 152.339, mean reward: 0.218 [-11.761, 100.000], mean action: 1.249 [0.000, 3.000], mean observation: 0.062 [-0.669, 1.000], loss: 1.133283, mean_absolute_error: 22.371944, mean_q: 29.578942, mean_eps: 0.100000
 1035800/2000000: episode: 5540, duration: 18.030s, episode steps: 1000, steps per second: 55, episode reward: -33.783, mean reward: -0.034 [-12.835, 14.475], mean action: 1.639 [0.000, 3.000], mean observation: 0.063 [-0.592, 1.000], loss: 1.105479, mean_absolute_error: 21.814622, mean_q: 29.381428, mean_eps: 0.100000
 1036167/2000000: episode: 5541, duration: 5.852s, episode steps: 367, steps per second: 63, episode reward: 179.551, mean reward: 0.489 [-7.860, 100.000], mean action: 1.450 [0.000, 3.000], mean observation: 0.043 [-0.751, 1.000], loss: 1.004632, mean_absolute_error: 22.887215, mean_q: 30.684934, mean_eps: 0.100000
 1037167/2000000: episode: 5542, duration: 16.623s, episode steps: 1000, steps per second: 60, episode reward: -2.621, mean reward: -0.003 [-13.390, 12.616], mean action: 1.305 [0.000, 3.000], mean observation: 0.110 [-0.734, 1.000], loss: 1.004114, mean_absolute_error: 22.367943, mean_q: 29.818016, mean_eps: 0.100000
 1038167/2000000: episode: 5543, duration: 16.432s, episode steps: 1000, steps per second: 61, episode reward: -27.155, mean reward: -0.027 [-20.792, 21.119], mean action: 1.514 [0.000, 3.000], mean observation: 0.104 [-0.691, 1.000], loss: 1.003293, mean_absolute_error: 22.197801, mean_q: 29.935699, mean_eps: 0.100000
 1038783/2000000: episode: 5544, duration: 10.225s, episode steps: 616, steps per second: 60, episode reward: 177.363, mean reward: 0.288 [-17.756, 100.000], mean action: 1.328 [0.000, 3.000], mean observation: 0.074 [-0.725, 1.000], loss: 0.906529, mean_absolute_error: 22.267477, mean_q: 29.713429, mean_eps: 0.100000
 1039332/2000000: episode: 5545, duration: 9.072s, episode steps: 549, steps per second: 61, episode reward: 159.316, mean reward: 0.290 [-18.386, 100.000], mean action: 1.306 [0.000, 3.000], mean observation: 0.073 [-0.665, 1.000], loss: 1.164171, mean_absolute_error: 21.802136, mean_q: 29.363642, mean_eps: 0.100000
 1039691/2000000: episode: 5546, duration: 5.850s, episode steps: 359, steps per second: 61, episode reward: 171.041, mean reward: 0.476 [-19.527, 100.000], mean action: 1.390 [0.000, 3.000], mean observation: 0.051 [-0.682, 1.000], loss: 1.044389, mean_absolute_error: 22.298784, mean_q: 29.963894, mean_eps: 0.100000
 1040194/2000000: episode: 5547, duration: 8.346s, episode steps: 503, steps per second: 60, episode reward: 214.349, mean reward: 0.426 [-3.872, 100.000], mean action: 1.487 [0.000, 3.000], mean observation: 0.060 [-0.670, 1.024], loss: 0.885716, mean_absolute_error: 21.909660, mean_q: 29.359334, mean_eps: 0.100000
 1040865/2000000: episode: 5548, duration: 11.902s, episode steps: 671, steps per second: 56, episode reward: 165.025, mean reward: 0.246 [-18.414, 100.000], mean action: 1.392 [0.000, 3.000], mean observation: 0.042 [-0.743, 1.000], loss: 0.955163, mean_absolute_error: 21.911554, mean_q: 29.612388, mean_eps: 0.100000
 1041445/2000000: episode: 5549, duration: 9.967s, episode steps: 580, steps per second: 58, episode reward: 187.228, mean reward: 0.323 [-10.849, 100.000], mean action: 1.426 [0.000, 3.000], mean observation: 0.075 [-0.661, 1.015], loss: 0.987756, mean_absolute_error: 21.628637, mean_q: 29.014182, mean_eps: 0.100000
 1042445/2000000: episode: 5550, duration: 16.686s, episode steps: 1000, steps per second: 60, episode reward: -56.042, mean reward: -0.056 [-4.503, 4.754], mean action: 1.515 [0.000, 3.000], mean observation: 0.066 [-0.717, 0.925], loss: 1.043639, mean_absolute_error: 21.806358, mean_q: 29.298541, mean_eps: 0.100000
 1043445/2000000: episode: 5551, duration: 16.970s, episode steps: 1000, steps per second: 59, episode reward: -79.575, mean reward: -0.080 [-4.343, 5.132], mean action: 1.628 [0.000, 3.000], mean observation: 0.080 [-0.441, 0.950], loss: 1.075549, mean_absolute_error: 21.724096, mean_q: 29.309918, mean_eps: 0.100000
 1044445/2000000: episode: 5552, duration: 17.361s, episode steps: 1000, steps per second: 58, episode reward: -121.884, mean reward: -0.122 [-4.743, 5.196], mean action: 1.585 [0.000, 3.000], mean observation: 0.086 [-0.614, 0.941], loss: 0.979979, mean_absolute_error: 21.512554, mean_q: 28.943309, mean_eps: 0.100000
 1044902/2000000: episode: 5553, duration: 7.330s, episode steps: 457, steps per second: 62, episode reward: 176.029, mean reward: 0.385 [-12.153, 100.000], mean action: 1.317 [0.000, 3.000], mean observation: 0.046 [-0.646, 1.000], loss: 0.988393, mean_absolute_error: 21.703897, mean_q: 28.843578, mean_eps: 0.100000
 1045364/2000000: episode: 5554, duration: 7.468s, episode steps: 462, steps per second: 62, episode reward: 187.854, mean reward: 0.407 [-9.272, 100.000], mean action: 1.253 [0.000, 3.000], mean observation: 0.024 [-0.671, 1.000], loss: 0.950442, mean_absolute_error: 21.677706, mean_q: 29.333337, mean_eps: 0.100000
 1046364/2000000: episode: 5555, duration: 16.320s, episode steps: 1000, steps per second: 61, episode reward: 28.798, mean reward: 0.029 [-13.798, 18.668], mean action: 1.524 [0.000, 3.000], mean observation: 0.087 [-0.462, 1.000], loss: 1.011858, mean_absolute_error: 21.933283, mean_q: 29.512899, mean_eps: 0.100000
 1047050/2000000: episode: 5556, duration: 11.976s, episode steps: 686, steps per second: 57, episode reward: 155.176, mean reward: 0.226 [-19.237, 100.000], mean action: 1.313 [0.000, 3.000], mean observation: 0.073 [-0.715, 1.000], loss: 0.972260, mean_absolute_error: 21.306618, mean_q: 28.649774, mean_eps: 0.100000
 1048050/2000000: episode: 5557, duration: 16.842s, episode steps: 1000, steps per second: 59, episode reward: -58.410, mean reward: -0.058 [-4.052, 5.863], mean action: 1.787 [0.000, 3.000], mean observation: 0.100 [-0.685, 1.001], loss: 1.112685, mean_absolute_error: 21.560056, mean_q: 28.757696, mean_eps: 0.100000
 1048896/2000000: episode: 5558, duration: 14.344s, episode steps: 846, steps per second: 59, episode reward: 65.054, mean reward: 0.077 [-14.693, 100.000], mean action: 1.772 [0.000, 3.000], mean observation: 0.024 [-0.729, 1.000], loss: 0.926222, mean_absolute_error: 21.597841, mean_q: 29.006140, mean_eps: 0.100000
 1049684/2000000: episode: 5559, duration: 13.351s, episode steps: 788, steps per second: 59, episode reward: 138.017, mean reward: 0.175 [-17.400, 100.000], mean action: 1.152 [0.000, 3.000], mean observation: 0.090 [-0.691, 1.000], loss: 1.069076, mean_absolute_error: 21.544535, mean_q: 29.057503, mean_eps: 0.100000
 1050684/2000000: episode: 5560, duration: 18.271s, episode steps: 1000, steps per second: 55, episode reward: -58.879, mean reward: -0.059 [-4.244, 5.316], mean action: 1.560 [0.000, 3.000], mean observation: 0.076 [-0.470, 0.935], loss: 0.953057, mean_absolute_error: 21.860329, mean_q: 29.376456, mean_eps: 0.100000
 1051158/2000000: episode: 5561, duration: 7.956s, episode steps: 474, steps per second: 60, episode reward: 191.322, mean reward: 0.404 [-20.261, 100.000], mean action: 1.013 [0.000, 3.000], mean observation: 0.089 [-0.793, 1.000], loss: 1.100392, mean_absolute_error: 21.799080, mean_q: 29.307441, mean_eps: 0.100000
 1051532/2000000: episode: 5562, duration: 6.079s, episode steps: 374, steps per second: 62, episode reward: 196.026, mean reward: 0.524 [-3.606, 100.000], mean action: 1.388 [0.000, 3.000], mean observation: 0.072 [-0.583, 1.000], loss: 0.967163, mean_absolute_error: 22.067374, mean_q: 29.608670, mean_eps: 0.100000
 1052060/2000000: episode: 5563, duration: 8.868s, episode steps: 528, steps per second: 60, episode reward: 159.824, mean reward: 0.303 [-9.878, 100.000], mean action: 1.462 [0.000, 3.000], mean observation: 0.038 [-0.653, 1.000], loss: 0.928605, mean_absolute_error: 21.048239, mean_q: 28.157564, mean_eps: 0.100000
 1053060/2000000: episode: 5564, duration: 16.980s, episode steps: 1000, steps per second: 59, episode reward: 57.206, mean reward: 0.057 [-24.640, 21.515], mean action: 1.347 [0.000, 3.000], mean observation: 0.110 [-0.594, 1.030], loss: 0.874025, mean_absolute_error: 21.588891, mean_q: 28.988618, mean_eps: 0.100000
 1053476/2000000: episode: 5565, duration: 6.756s, episode steps: 416, steps per second: 62, episode reward: 197.863, mean reward: 0.476 [-17.438, 100.000], mean action: 1.389 [0.000, 3.000], mean observation: 0.059 [-0.702, 1.000], loss: 0.997145, mean_absolute_error: 21.359957, mean_q: 28.800116, mean_eps: 0.100000
 1054325/2000000: episode: 5566, duration: 14.494s, episode steps: 849, steps per second: 59, episode reward: 157.363, mean reward: 0.185 [-17.638, 100.000], mean action: 1.252 [0.000, 3.000], mean observation: 0.097 [-0.666, 1.000], loss: 0.985091, mean_absolute_error: 21.972815, mean_q: 29.679744, mean_eps: 0.100000
 1054891/2000000: episode: 5567, duration: 9.412s, episode steps: 566, steps per second: 60, episode reward: 195.043, mean reward: 0.345 [-8.924, 100.000], mean action: 1.652 [0.000, 3.000], mean observation: 0.012 [-0.661, 1.005], loss: 0.987004, mean_absolute_error: 21.108920, mean_q: 28.508884, mean_eps: 0.100000
 1055546/2000000: episode: 5568, duration: 10.949s, episode steps: 655, steps per second: 60, episode reward: 178.461, mean reward: 0.272 [-3.916, 100.000], mean action: 1.779 [0.000, 3.000], mean observation: -0.008 [-0.742, 1.000], loss: 0.925916, mean_absolute_error: 21.262991, mean_q: 28.684297, mean_eps: 0.100000
 1056414/2000000: episode: 5569, duration: 14.668s, episode steps: 868, steps per second: 59, episode reward: 151.441, mean reward: 0.174 [-17.985, 100.000], mean action: 1.373 [0.000, 3.000], mean observation: 0.064 [-0.590, 1.000], loss: 1.015946, mean_absolute_error: 21.334956, mean_q: 28.733519, mean_eps: 0.100000
 1057136/2000000: episode: 5570, duration: 12.092s, episode steps: 722, steps per second: 60, episode reward: 153.992, mean reward: 0.213 [-10.613, 100.000], mean action: 1.622 [0.000, 3.000], mean observation: 0.003 [-0.727, 1.000], loss: 1.032532, mean_absolute_error: 21.642844, mean_q: 28.899250, mean_eps: 0.100000
 1058136/2000000: episode: 5571, duration: 16.593s, episode steps: 1000, steps per second: 60, episode reward: -59.433, mean reward: -0.059 [-4.558, 6.507], mean action: 1.676 [0.000, 3.000], mean observation: 0.078 [-0.494, 0.944], loss: 0.909495, mean_absolute_error: 21.365173, mean_q: 28.622436, mean_eps: 0.100000
 1059136/2000000: episode: 5572, duration: 17.560s, episode steps: 1000, steps per second: 57, episode reward: -52.988, mean reward: -0.053 [-4.452, 4.356], mean action: 1.534 [0.000, 3.000], mean observation: 0.080 [-0.534, 0.933], loss: 0.913453, mean_absolute_error: 21.212594, mean_q: 28.513436, mean_eps: 0.100000
 1059706/2000000: episode: 5573, duration: 9.171s, episode steps: 570, steps per second: 62, episode reward: 182.471, mean reward: 0.320 [-17.352, 100.000], mean action: 1.330 [0.000, 3.000], mean observation: 0.064 [-0.726, 1.012], loss: 0.967307, mean_absolute_error: 21.254461, mean_q: 28.527297, mean_eps: 0.100000
 1060302/2000000: episode: 5574, duration: 9.949s, episode steps: 596, steps per second: 60, episode reward: 165.461, mean reward: 0.278 [-12.010, 100.000], mean action: 1.497 [0.000, 3.000], mean observation: 0.015 [-0.648, 1.000], loss: 0.897738, mean_absolute_error: 21.294871, mean_q: 28.823013, mean_eps: 0.100000
 1061059/2000000: episode: 5575, duration: 13.168s, episode steps: 757, steps per second: 57, episode reward: 125.230, mean reward: 0.165 [-12.243, 100.000], mean action: 1.357 [0.000, 3.000], mean observation: 0.054 [-0.508, 1.000], loss: 0.980709, mean_absolute_error: 21.530958, mean_q: 28.989341, mean_eps: 0.100000
 1061607/2000000: episode: 5576, duration: 12.381s, episode steps: 548, steps per second: 44, episode reward: 119.635, mean reward: 0.218 [-9.453, 100.000], mean action: 1.451 [0.000, 3.000], mean observation: 0.052 [-0.742, 1.000], loss: 0.931181, mean_absolute_error: 21.512357, mean_q: 29.193934, mean_eps: 0.100000
 1061920/2000000: episode: 5577, duration: 5.138s, episode steps: 313, steps per second: 61, episode reward: 223.419, mean reward: 0.714 [-3.534, 100.000], mean action: 1.438 [0.000, 3.000], mean observation: 0.064 [-0.690, 1.000], loss: 1.068443, mean_absolute_error: 21.559027, mean_q: 28.536128, mean_eps: 0.100000
 1062500/2000000: episode: 5578, duration: 10.460s, episode steps: 580, steps per second: 55, episode reward: 156.576, mean reward: 0.270 [-17.957, 100.000], mean action: 1.488 [0.000, 3.000], mean observation: 0.025 [-0.643, 1.000], loss: 0.966474, mean_absolute_error: 21.239700, mean_q: 28.611084, mean_eps: 0.100000
 1063500/2000000: episode: 5579, duration: 16.890s, episode steps: 1000, steps per second: 59, episode reward: -26.899, mean reward: -0.027 [-3.700, 6.820], mean action: 1.561 [0.000, 3.000], mean observation: 0.055 [-0.577, 0.958], loss: 0.983417, mean_absolute_error: 21.274938, mean_q: 28.831650, mean_eps: 0.100000
 1064428/2000000: episode: 5580, duration: 16.420s, episode steps: 928, steps per second: 57, episode reward: 110.156, mean reward: 0.119 [-9.565, 100.000], mean action: 1.515 [0.000, 3.000], mean observation: 0.055 [-0.460, 1.000], loss: 0.857608, mean_absolute_error: 21.264685, mean_q: 28.768200, mean_eps: 0.100000
 1065047/2000000: episode: 5581, duration: 10.628s, episode steps: 619, steps per second: 58, episode reward: 186.629, mean reward: 0.302 [-3.646, 100.000], mean action: 1.567 [0.000, 3.000], mean observation: 0.027 [-0.671, 1.000], loss: 1.035222, mean_absolute_error: 21.793612, mean_q: 29.164401, mean_eps: 0.100000
 1066047/2000000: episode: 5582, duration: 16.504s, episode steps: 1000, steps per second: 61, episode reward: -61.969, mean reward: -0.062 [-3.702, 4.964], mean action: 1.658 [0.000, 3.000], mean observation: 0.052 [-0.686, 0.928], loss: 1.024486, mean_absolute_error: 21.329275, mean_q: 28.583319, mean_eps: 0.100000
 1066581/2000000: episode: 5583, duration: 8.918s, episode steps: 534, steps per second: 60, episode reward: 152.606, mean reward: 0.286 [-18.137, 100.000], mean action: 1.348 [0.000, 3.000], mean observation: 0.080 [-0.676, 1.000], loss: 1.126314, mean_absolute_error: 21.530694, mean_q: 28.990717, mean_eps: 0.100000
 1067366/2000000: episode: 5584, duration: 13.286s, episode steps: 785, steps per second: 59, episode reward: 144.789, mean reward: 0.184 [-11.232, 100.000], mean action: 1.572 [0.000, 3.000], mean observation: 0.097 [-0.584, 1.000], loss: 0.959898, mean_absolute_error: 21.201351, mean_q: 28.747203, mean_eps: 0.100000
 1068366/2000000: episode: 5585, duration: 16.720s, episode steps: 1000, steps per second: 60, episode reward: -34.671, mean reward: -0.035 [-4.667, 6.684], mean action: 1.730 [0.000, 3.000], mean observation: 0.094 [-0.650, 1.029], loss: 1.018727, mean_absolute_error: 21.676954, mean_q: 29.113320, mean_eps: 0.100000
 1069231/2000000: episode: 5586, duration: 14.819s, episode steps: 865, steps per second: 58, episode reward: 105.720, mean reward: 0.122 [-20.208, 100.000], mean action: 1.576 [0.000, 3.000], mean observation: 0.052 [-0.717, 1.000], loss: 1.021918, mean_absolute_error: 21.563053, mean_q: 29.033027, mean_eps: 0.100000
 1069984/2000000: episode: 5587, duration: 12.932s, episode steps: 753, steps per second: 58, episode reward: 142.250, mean reward: 0.189 [-13.615, 100.000], mean action: 1.305 [0.000, 3.000], mean observation: 0.090 [-0.693, 1.000], loss: 1.085204, mean_absolute_error: 21.396982, mean_q: 28.879149, mean_eps: 0.100000
 1070880/2000000: episode: 5588, duration: 15.232s, episode steps: 896, steps per second: 59, episode reward: 112.382, mean reward: 0.125 [-11.235, 100.000], mean action: 1.470 [0.000, 3.000], mean observation: 0.058 [-0.448, 1.000], loss: 0.899428, mean_absolute_error: 21.232054, mean_q: 28.501686, mean_eps: 0.100000
 1071260/2000000: episode: 5589, duration: 6.122s, episode steps: 380, steps per second: 62, episode reward: 190.817, mean reward: 0.502 [-11.340, 100.000], mean action: 1.403 [0.000, 3.000], mean observation: 0.082 [-0.532, 1.000], loss: 0.924602, mean_absolute_error: 21.587644, mean_q: 28.663977, mean_eps: 0.100000
 1072260/2000000: episode: 5590, duration: 16.446s, episode steps: 1000, steps per second: 61, episode reward: 36.913, mean reward: 0.037 [-22.957, 13.136], mean action: 1.350 [0.000, 3.000], mean observation: 0.176 [-0.682, 1.000], loss: 0.908431, mean_absolute_error: 21.031260, mean_q: 28.419491, mean_eps: 0.100000
 1072725/2000000: episode: 5591, duration: 7.775s, episode steps: 465, steps per second: 60, episode reward: 167.322, mean reward: 0.360 [-8.277, 100.000], mean action: 1.297 [0.000, 3.000], mean observation: 0.041 [-0.698, 1.000], loss: 0.974984, mean_absolute_error: 21.066723, mean_q: 28.318375, mean_eps: 0.100000
 1073725/2000000: episode: 5592, duration: 16.169s, episode steps: 1000, steps per second: 62, episode reward: -56.996, mean reward: -0.057 [-3.743, 4.769], mean action: 1.519 [0.000, 3.000], mean observation: 0.074 [-0.654, 0.936], loss: 0.946848, mean_absolute_error: 21.789288, mean_q: 29.368069, mean_eps: 0.100000
 1074187/2000000: episode: 5593, duration: 7.720s, episode steps: 462, steps per second: 60, episode reward: 168.275, mean reward: 0.364 [-5.573, 100.000], mean action: 1.571 [0.000, 3.000], mean observation: 0.046 [-1.046, 1.000], loss: 1.118001, mean_absolute_error: 21.785746, mean_q: 28.905669, mean_eps: 0.100000
 1075187/2000000: episode: 5594, duration: 17.238s, episode steps: 1000, steps per second: 58, episode reward: -26.262, mean reward: -0.026 [-18.887, 13.253], mean action: 1.727 [0.000, 3.000], mean observation: 0.036 [-0.685, 1.000], loss: 0.946018, mean_absolute_error: 21.348206, mean_q: 28.752132, mean_eps: 0.100000
 1076187/2000000: episode: 5595, duration: 17.654s, episode steps: 1000, steps per second: 57, episode reward: -89.020, mean reward: -0.089 [-4.903, 4.764], mean action: 1.553 [0.000, 3.000], mean observation: 0.039 [-0.643, 0.947], loss: 0.960410, mean_absolute_error: 21.129190, mean_q: 28.504858, mean_eps: 0.100000
 1077187/2000000: episode: 5596, duration: 17.594s, episode steps: 1000, steps per second: 57, episode reward: -50.908, mean reward: -0.051 [-5.496, 5.130], mean action: 1.531 [0.000, 3.000], mean observation: 0.025 [-0.706, 0.926], loss: 0.949702, mean_absolute_error: 21.237721, mean_q: 28.544938, mean_eps: 0.100000
 1078060/2000000: episode: 5597, duration: 14.380s, episode steps: 873, steps per second: 61, episode reward: 165.193, mean reward: 0.189 [-24.502, 100.000], mean action: 0.930 [0.000, 3.000], mean observation: 0.135 [-0.641, 1.000], loss: 0.874299, mean_absolute_error: 20.896227, mean_q: 28.392666, mean_eps: 0.100000
 1078508/2000000: episode: 5598, duration: 7.428s, episode steps: 448, steps per second: 60, episode reward: 194.842, mean reward: 0.435 [-8.611, 100.000], mean action: 1.478 [0.000, 3.000], mean observation: 0.024 [-0.741, 1.000], loss: 0.952575, mean_absolute_error: 21.014766, mean_q: 28.481989, mean_eps: 0.100000
 1079008/2000000: episode: 5599, duration: 8.601s, episode steps: 500, steps per second: 58, episode reward: 186.567, mean reward: 0.373 [-8.405, 100.000], mean action: 1.518 [0.000, 3.000], mean observation: 0.043 [-0.742, 1.000], loss: 0.878615, mean_absolute_error: 21.462287, mean_q: 29.156893, mean_eps: 0.100000
 1079973/2000000: episode: 5600, duration: 16.152s, episode steps: 965, steps per second: 60, episode reward: 99.688, mean reward: 0.103 [-9.545, 100.000], mean action: 1.432 [0.000, 3.000], mean observation: 0.054 [-0.658, 1.000], loss: 1.019752, mean_absolute_error: 20.759913, mean_q: 27.817080, mean_eps: 0.100000
 1080404/2000000: episode: 5601, duration: 7.046s, episode steps: 431, steps per second: 61, episode reward: 135.611, mean reward: 0.315 [-12.689, 100.000], mean action: 1.956 [0.000, 3.000], mean observation: 0.124 [-1.035, 1.000], loss: 0.827887, mean_absolute_error: 20.888952, mean_q: 27.878928, mean_eps: 0.100000
 1080922/2000000: episode: 5602, duration: 8.583s, episode steps: 518, steps per second: 60, episode reward: 130.986, mean reward: 0.253 [-9.736, 100.000], mean action: 1.575 [0.000, 3.000], mean observation: 0.022 [-0.631, 1.000], loss: 0.847866, mean_absolute_error: 21.132565, mean_q: 28.664295, mean_eps: 0.100000
 1081527/2000000: episode: 5603, duration: 10.073s, episode steps: 605, steps per second: 60, episode reward: 156.160, mean reward: 0.258 [-17.638, 100.000], mean action: 1.268 [0.000, 3.000], mean observation: 0.077 [-1.299, 1.000], loss: 0.934330, mean_absolute_error: 21.298125, mean_q: 28.390491, mean_eps: 0.100000
 1082000/2000000: episode: 5604, duration: 7.837s, episode steps: 473, steps per second: 60, episode reward: 173.019, mean reward: 0.366 [-9.630, 100.000], mean action: 1.338 [0.000, 3.000], mean observation: 0.049 [-0.678, 1.000], loss: 0.794333, mean_absolute_error: 21.219328, mean_q: 28.383993, mean_eps: 0.100000
 1083000/2000000: episode: 5605, duration: 16.884s, episode steps: 1000, steps per second: 59, episode reward: -49.993, mean reward: -0.050 [-4.707, 8.098], mean action: 1.501 [0.000, 3.000], mean observation: 0.093 [-0.559, 1.048], loss: 0.878012, mean_absolute_error: 20.694842, mean_q: 27.727597, mean_eps: 0.100000
 1083438/2000000: episode: 5606, duration: 7.154s, episode steps: 438, steps per second: 61, episode reward: 210.951, mean reward: 0.482 [-18.071, 100.000], mean action: 1.292 [0.000, 3.000], mean observation: 0.053 [-0.776, 1.000], loss: 0.897729, mean_absolute_error: 20.429573, mean_q: 27.348772, mean_eps: 0.100000
 1084072/2000000: episode: 5607, duration: 10.672s, episode steps: 634, steps per second: 59, episode reward: 184.303, mean reward: 0.291 [-17.842, 100.000], mean action: 1.380 [0.000, 3.000], mean observation: 0.046 [-0.601, 1.000], loss: 0.868500, mean_absolute_error: 20.985700, mean_q: 28.029332, mean_eps: 0.100000
 1085072/2000000: episode: 5608, duration: 17.284s, episode steps: 1000, steps per second: 58, episode reward: -18.873, mean reward: -0.019 [-17.912, 13.148], mean action: 1.429 [0.000, 3.000], mean observation: 0.035 [-0.875, 1.000], loss: 0.971947, mean_absolute_error: 20.535223, mean_q: 27.553484, mean_eps: 0.100000
 1085566/2000000: episode: 5609, duration: 8.100s, episode steps: 494, steps per second: 61, episode reward: 135.795, mean reward: 0.275 [-12.281, 100.000], mean action: 1.563 [0.000, 3.000], mean observation: 0.041 [-0.554, 1.000], loss: 1.095103, mean_absolute_error: 20.846788, mean_q: 27.802604, mean_eps: 0.100000
 1086566/2000000: episode: 5610, duration: 16.528s, episode steps: 1000, steps per second: 61, episode reward: -33.933, mean reward: -0.034 [-13.859, 11.763], mean action: 1.827 [0.000, 3.000], mean observation: -0.017 [-0.534, 1.000], loss: 0.939440, mean_absolute_error: 20.866637, mean_q: 27.893196, mean_eps: 0.100000
 1087337/2000000: episode: 5611, duration: 12.752s, episode steps: 771, steps per second: 60, episode reward: 130.730, mean reward: 0.170 [-19.344, 100.000], mean action: 1.254 [0.000, 3.000], mean observation: 0.066 [-0.692, 1.000], loss: 0.909602, mean_absolute_error: 20.880533, mean_q: 27.936625, mean_eps: 0.100000
 1087644/2000000: episode: 5612, duration: 4.837s, episode steps: 307, steps per second: 63, episode reward: 209.188, mean reward: 0.681 [-18.480, 100.000], mean action: 1.176 [0.000, 3.000], mean observation: 0.136 [-0.979, 1.000], loss: 0.992090, mean_absolute_error: 20.843731, mean_q: 27.981838, mean_eps: 0.100000
 1088644/2000000: episode: 5613, duration: 17.681s, episode steps: 1000, steps per second: 57, episode reward: -30.188, mean reward: -0.030 [-4.566, 27.073], mean action: 1.455 [0.000, 3.000], mean observation: 0.037 [-0.744, 1.000], loss: 0.913261, mean_absolute_error: 20.568726, mean_q: 27.811177, mean_eps: 0.100000
 1089145/2000000: episode: 5614, duration: 8.492s, episode steps: 501, steps per second: 59, episode reward: 182.148, mean reward: 0.364 [-10.247, 100.000], mean action: 1.337 [0.000, 3.000], mean observation: 0.051 [-0.732, 1.000], loss: 0.783321, mean_absolute_error: 20.808093, mean_q: 27.941799, mean_eps: 0.100000
 1090145/2000000: episode: 5615, duration: 16.529s, episode steps: 1000, steps per second: 60, episode reward: -31.952, mean reward: -0.032 [-3.977, 4.925], mean action: 1.597 [0.000, 3.000], mean observation: 0.068 [-0.650, 0.930], loss: 0.897345, mean_absolute_error: 20.658657, mean_q: 27.671230, mean_eps: 0.100000
 1091145/2000000: episode: 5616, duration: 18.330s, episode steps: 1000, steps per second: 55, episode reward: -74.964, mean reward: -0.075 [-4.885, 4.406], mean action: 1.693 [0.000, 3.000], mean observation: -0.008 [-0.602, 0.936], loss: 0.879220, mean_absolute_error: 20.728527, mean_q: 27.927488, mean_eps: 0.100000
 1092018/2000000: episode: 5617, duration: 15.266s, episode steps: 873, steps per second: 57, episode reward: 151.324, mean reward: 0.173 [-10.073, 100.000], mean action: 1.395 [0.000, 3.000], mean observation: 0.026 [-0.698, 1.000], loss: 0.988657, mean_absolute_error: 20.891316, mean_q: 28.090396, mean_eps: 0.100000
 1092875/2000000: episode: 5618, duration: 15.359s, episode steps: 857, steps per second: 56, episode reward: 134.757, mean reward: 0.157 [-17.899, 100.000], mean action: 1.477 [0.000, 3.000], mean observation: 0.019 [-0.689, 1.000], loss: 0.923621, mean_absolute_error: 20.508799, mean_q: 27.626866, mean_eps: 0.100000
 1093738/2000000: episode: 5619, duration: 15.132s, episode steps: 863, steps per second: 57, episode reward: 195.406, mean reward: 0.226 [-18.743, 100.000], mean action: 0.871 [0.000, 3.000], mean observation: 0.120 [-0.713, 1.000], loss: 0.838252, mean_absolute_error: 20.189824, mean_q: 27.320885, mean_eps: 0.100000
 1094045/2000000: episode: 5620, duration: 4.945s, episode steps: 307, steps per second: 62, episode reward: 168.184, mean reward: 0.548 [-3.989, 100.000], mean action: 1.661 [0.000, 3.000], mean observation: 0.037 [-0.698, 1.000], loss: 0.950929, mean_absolute_error: 21.108470, mean_q: 28.540811, mean_eps: 0.100000
 1094695/2000000: episode: 5621, duration: 10.701s, episode steps: 650, steps per second: 61, episode reward: 126.216, mean reward: 0.194 [-19.655, 100.000], mean action: 1.428 [0.000, 3.000], mean observation: 0.044 [-0.670, 1.000], loss: 0.869943, mean_absolute_error: 20.572641, mean_q: 27.695339, mean_eps: 0.100000
 1095529/2000000: episode: 5622, duration: 14.594s, episode steps: 834, steps per second: 57, episode reward: 108.988, mean reward: 0.131 [-3.656, 100.000], mean action: 1.772 [0.000, 3.000], mean observation: 0.000 [-0.728, 1.000], loss: 0.854674, mean_absolute_error: 20.584025, mean_q: 27.944631, mean_eps: 0.100000
 1096273/2000000: episode: 5623, duration: 12.635s, episode steps: 744, steps per second: 59, episode reward: 160.757, mean reward: 0.216 [-20.595, 100.000], mean action: 1.148 [0.000, 3.000], mean observation: 0.098 [-0.690, 1.000], loss: 0.912086, mean_absolute_error: 21.042429, mean_q: 28.069519, mean_eps: 0.100000
 1097102/2000000: episode: 5624, duration: 14.515s, episode steps: 829, steps per second: 57, episode reward: 208.065, mean reward: 0.251 [-18.702, 100.000], mean action: 0.846 [0.000, 3.000], mean observation: 0.138 [-0.680, 1.000], loss: 0.824153, mean_absolute_error: 20.668686, mean_q: 27.760677, mean_eps: 0.100000
 1098102/2000000: episode: 5625, duration: 18.019s, episode steps: 1000, steps per second: 55, episode reward: -54.461, mean reward: -0.054 [-4.115, 4.942], mean action: 1.680 [0.000, 3.000], mean observation: 0.049 [-0.504, 0.947], loss: 0.816570, mean_absolute_error: 20.875900, mean_q: 27.956351, mean_eps: 0.100000
 1099102/2000000: episode: 5626, duration: 18.084s, episode steps: 1000, steps per second: 55, episode reward: -56.008, mean reward: -0.056 [-4.841, 4.688], mean action: 1.798 [0.000, 3.000], mean observation: -0.006 [-0.734, 0.991], loss: 0.770540, mean_absolute_error: 20.478856, mean_q: 27.755703, mean_eps: 0.100000
 1100102/2000000: episode: 5627, duration: 18.416s, episode steps: 1000, steps per second: 54, episode reward: -51.900, mean reward: -0.052 [-4.251, 6.472], mean action: 1.736 [0.000, 3.000], mean observation: 0.073 [-0.615, 0.940], loss: 0.887995, mean_absolute_error: 20.282969, mean_q: 27.257493, mean_eps: 0.100000
 1101061/2000000: episode: 5628, duration: 17.586s, episode steps: 959, steps per second: 55, episode reward: 98.964, mean reward: 0.103 [-8.551, 100.000], mean action: 1.753 [0.000, 3.000], mean observation: 0.014 [-0.621, 1.000], loss: 0.855980, mean_absolute_error: 20.462976, mean_q: 27.699882, mean_eps: 0.100000
 1101808/2000000: episode: 5629, duration: 13.112s, episode steps: 747, steps per second: 57, episode reward: 158.883, mean reward: 0.213 [-17.340, 100.000], mean action: 1.301 [0.000, 3.000], mean observation: 0.069 [-0.702, 1.000], loss: 0.807228, mean_absolute_error: 20.319351, mean_q: 27.069593, mean_eps: 0.100000
 1102505/2000000: episode: 5630, duration: 11.918s, episode steps: 697, steps per second: 58, episode reward: 161.088, mean reward: 0.231 [-9.469, 100.000], mean action: 1.620 [0.000, 3.000], mean observation: 0.021 [-0.796, 1.000], loss: 0.852656, mean_absolute_error: 20.096696, mean_q: 27.138929, mean_eps: 0.100000
 1103442/2000000: episode: 5631, duration: 15.639s, episode steps: 937, steps per second: 60, episode reward: 115.247, mean reward: 0.123 [-19.033, 100.000], mean action: 1.264 [0.000, 3.000], mean observation: 0.068 [-0.719, 1.000], loss: 0.863207, mean_absolute_error: 20.071176, mean_q: 26.998226, mean_eps: 0.100000
 1103769/2000000: episode: 5632, duration: 5.249s, episode steps: 327, steps per second: 62, episode reward: 157.011, mean reward: 0.480 [-11.998, 100.000], mean action: 1.557 [0.000, 3.000], mean observation: 0.039 [-0.756, 1.000], loss: 1.020234, mean_absolute_error: 20.127242, mean_q: 26.974703, mean_eps: 0.100000
 1104769/2000000: episode: 5633, duration: 18.453s, episode steps: 1000, steps per second: 54, episode reward: -36.783, mean reward: -0.037 [-4.215, 6.427], mean action: 1.570 [0.000, 3.000], mean observation: 0.063 [-0.567, 0.944], loss: 0.901980, mean_absolute_error: 20.110786, mean_q: 27.066851, mean_eps: 0.100000
 1105310/2000000: episode: 5634, duration: 9.070s, episode steps: 541, steps per second: 60, episode reward: 166.205, mean reward: 0.307 [-3.978, 100.000], mean action: 1.466 [0.000, 3.000], mean observation: 0.018 [-0.635, 1.000], loss: 1.004176, mean_absolute_error: 20.377345, mean_q: 27.319610, mean_eps: 0.100000
 1105707/2000000: episode: 5635, duration: 6.452s, episode steps: 397, steps per second: 62, episode reward: 238.633, mean reward: 0.601 [-17.464, 100.000], mean action: 1.101 [0.000, 3.000], mean observation: 0.130 [-0.650, 1.000], loss: 0.823039, mean_absolute_error: 19.778662, mean_q: 26.673307, mean_eps: 0.100000
 1106371/2000000: episode: 5636, duration: 10.738s, episode steps: 664, steps per second: 62, episode reward: 161.096, mean reward: 0.243 [-3.975, 100.000], mean action: 1.283 [0.000, 3.000], mean observation: 0.029 [-0.731, 1.000], loss: 0.745033, mean_absolute_error: 20.202555, mean_q: 27.346788, mean_eps: 0.100000
 1107371/2000000: episode: 5637, duration: 18.022s, episode steps: 1000, steps per second: 55, episode reward: -66.265, mean reward: -0.066 [-4.277, 5.323], mean action: 1.721 [0.000, 3.000], mean observation: -0.026 [-0.727, 0.933], loss: 0.937086, mean_absolute_error: 20.252682, mean_q: 27.239922, mean_eps: 0.100000
 1108371/2000000: episode: 5638, duration: 18.040s, episode steps: 1000, steps per second: 55, episode reward: -64.764, mean reward: -0.065 [-4.430, 6.896], mean action: 1.550 [0.000, 3.000], mean observation: 0.081 [-0.608, 1.038], loss: 0.866090, mean_absolute_error: 20.082643, mean_q: 26.987957, mean_eps: 0.100000
 1108999/2000000: episode: 5639, duration: 10.353s, episode steps: 628, steps per second: 61, episode reward: 208.704, mean reward: 0.332 [-17.757, 100.000], mean action: 1.228 [0.000, 3.000], mean observation: 0.086 [-0.657, 1.000], loss: 0.890852, mean_absolute_error: 20.300312, mean_q: 27.418980, mean_eps: 0.100000
 1109918/2000000: episode: 5640, duration: 15.627s, episode steps: 919, steps per second: 59, episode reward: 75.834, mean reward: 0.083 [-19.846, 100.000], mean action: 1.609 [0.000, 3.000], mean observation: 0.094 [-0.700, 1.000], loss: 0.800525, mean_absolute_error: 20.168784, mean_q: 27.130442, mean_eps: 0.100000
 1110918/2000000: episode: 5641, duration: 17.245s, episode steps: 1000, steps per second: 58, episode reward: -45.706, mean reward: -0.046 [-4.985, 4.771], mean action: 1.698 [0.000, 3.000], mean observation: -0.016 [-0.711, 0.928], loss: 0.805816, mean_absolute_error: 20.205847, mean_q: 27.304115, mean_eps: 0.100000
 1111390/2000000: episode: 5642, duration: 7.696s, episode steps: 472, steps per second: 61, episode reward: 145.370, mean reward: 0.308 [-9.005, 100.000], mean action: 1.528 [0.000, 3.000], mean observation: 0.045 [-0.752, 1.000], loss: 0.759056, mean_absolute_error: 19.992251, mean_q: 27.149514, mean_eps: 0.100000
 1112316/2000000: episode: 5643, duration: 15.765s, episode steps: 926, steps per second: 59, episode reward: 167.586, mean reward: 0.181 [-20.126, 100.000], mean action: 1.097 [0.000, 3.000], mean observation: 0.109 [-0.541, 1.000], loss: 0.828124, mean_absolute_error: 19.938366, mean_q: 26.742517, mean_eps: 0.100000
 1112838/2000000: episode: 5644, duration: 8.619s, episode steps: 522, steps per second: 61, episode reward: 172.489, mean reward: 0.330 [-19.812, 100.000], mean action: 1.008 [0.000, 3.000], mean observation: 0.103 [-0.842, 1.000], loss: 0.761582, mean_absolute_error: 20.084013, mean_q: 27.061523, mean_eps: 0.100000
 1113287/2000000: episode: 5645, duration: 7.379s, episode steps: 449, steps per second: 61, episode reward: 228.992, mean reward: 0.510 [-17.811, 100.000], mean action: 1.343 [0.000, 3.000], mean observation: 0.148 [-0.647, 1.028], loss: 0.852252, mean_absolute_error: 19.325172, mean_q: 26.142841, mean_eps: 0.100000
 1113691/2000000: episode: 5646, duration: 6.516s, episode steps: 404, steps per second: 62, episode reward: 222.170, mean reward: 0.550 [-18.146, 100.000], mean action: 1.161 [0.000, 3.000], mean observation: 0.145 [-0.805, 1.000], loss: 0.758751, mean_absolute_error: 19.734721, mean_q: 26.681244, mean_eps: 0.100000
 1114571/2000000: episode: 5647, duration: 15.578s, episode steps: 880, steps per second: 56, episode reward: 115.670, mean reward: 0.131 [-11.219, 100.000], mean action: 1.569 [0.000, 3.000], mean observation: 0.006 [-0.655, 1.000], loss: 0.775370, mean_absolute_error: 19.776915, mean_q: 26.762241, mean_eps: 0.100000
 1115179/2000000: episode: 5648, duration: 10.120s, episode steps: 608, steps per second: 60, episode reward: 172.407, mean reward: 0.284 [-19.739, 100.000], mean action: 1.298 [0.000, 3.000], mean observation: 0.120 [-0.783, 1.000], loss: 0.928995, mean_absolute_error: 19.855352, mean_q: 26.745746, mean_eps: 0.100000
 1115452/2000000: episode: 5649, duration: 4.349s, episode steps: 273, steps per second: 63, episode reward: 197.246, mean reward: 0.723 [-18.668, 100.000], mean action: 1.286 [0.000, 3.000], mean observation: 0.120 [-0.796, 1.000], loss: 0.816707, mean_absolute_error: 19.263819, mean_q: 26.102744, mean_eps: 0.100000
 1116128/2000000: episode: 5650, duration: 11.179s, episode steps: 676, steps per second: 60, episode reward: 195.730, mean reward: 0.290 [-17.875, 100.000], mean action: 1.296 [0.000, 3.000], mean observation: 0.083 [-0.608, 1.000], loss: 0.820295, mean_absolute_error: 19.778341, mean_q: 26.798893, mean_eps: 0.100000
 1116756/2000000: episode: 5651, duration: 10.754s, episode steps: 628, steps per second: 58, episode reward: 166.756, mean reward: 0.266 [-19.247, 100.000], mean action: 1.417 [0.000, 3.000], mean observation: 0.050 [-0.847, 1.000], loss: 0.839135, mean_absolute_error: 20.164320, mean_q: 27.141222, mean_eps: 0.100000
 1117756/2000000: episode: 5652, duration: 16.989s, episode steps: 1000, steps per second: 59, episode reward: -86.223, mean reward: -0.086 [-4.532, 4.842], mean action: 1.547 [0.000, 3.000], mean observation: -0.006 [-0.674, 0.943], loss: 0.803589, mean_absolute_error: 20.215886, mean_q: 27.181402, mean_eps: 0.100000
 1118481/2000000: episode: 5653, duration: 12.158s, episode steps: 725, steps per second: 60, episode reward: 149.246, mean reward: 0.206 [-7.233, 100.000], mean action: 1.486 [0.000, 3.000], mean observation: 0.058 [-0.679, 1.000], loss: 0.795446, mean_absolute_error: 19.939548, mean_q: 26.737788, mean_eps: 0.100000
 1119134/2000000: episode: 5654, duration: 11.144s, episode steps: 653, steps per second: 59, episode reward: 114.022, mean reward: 0.175 [-10.569, 100.000], mean action: 1.766 [0.000, 3.000], mean observation: 0.011 [-0.708, 1.000], loss: 0.867370, mean_absolute_error: 20.508949, mean_q: 27.507097, mean_eps: 0.100000
 1119776/2000000: episode: 5655, duration: 10.540s, episode steps: 642, steps per second: 61, episode reward: 140.663, mean reward: 0.219 [-12.238, 100.000], mean action: 1.461 [0.000, 3.000], mean observation: 0.059 [-0.780, 1.000], loss: 0.795986, mean_absolute_error: 19.574512, mean_q: 26.434156, mean_eps: 0.100000
 1120624/2000000: episode: 5656, duration: 14.977s, episode steps: 848, steps per second: 57, episode reward: 152.520, mean reward: 0.180 [-10.310, 100.000], mean action: 1.535 [0.000, 3.000], mean observation: 0.024 [-0.706, 1.000], loss: 0.815436, mean_absolute_error: 19.878691, mean_q: 26.813691, mean_eps: 0.100000
 1121624/2000000: episode: 5657, duration: 17.346s, episode steps: 1000, steps per second: 58, episode reward: 1.230, mean reward: 0.001 [-19.119, 11.839], mean action: 1.591 [0.000, 3.000], mean observation: 0.059 [-0.708, 1.000], loss: 0.856323, mean_absolute_error: 19.886564, mean_q: 26.509976, mean_eps: 0.100000
 1122577/2000000: episode: 5658, duration: 16.108s, episode steps: 953, steps per second: 59, episode reward: 59.525, mean reward: 0.062 [-10.844, 100.000], mean action: 1.487 [0.000, 3.000], mean observation: 0.001 [-0.714, 1.000], loss: 0.794304, mean_absolute_error: 19.835780, mean_q: 26.876839, mean_eps: 0.100000
 1123577/2000000: episode: 5659, duration: 18.167s, episode steps: 1000, steps per second: 55, episode reward: -54.102, mean reward: -0.054 [-4.960, 12.483], mean action: 1.611 [0.000, 3.000], mean observation: -0.005 [-0.734, 1.000], loss: 0.813134, mean_absolute_error: 19.816331, mean_q: 26.793281, mean_eps: 0.100000
 1124247/2000000: episode: 5660, duration: 11.604s, episode steps: 670, steps per second: 58, episode reward: 174.297, mean reward: 0.260 [-13.809, 100.000], mean action: 1.469 [0.000, 3.000], mean observation: 0.047 [-0.684, 1.000], loss: 0.769853, mean_absolute_error: 20.047074, mean_q: 26.985362, mean_eps: 0.100000
 1124720/2000000: episode: 5661, duration: 7.862s, episode steps: 473, steps per second: 60, episode reward: 204.212, mean reward: 0.432 [-8.821, 100.000], mean action: 1.548 [0.000, 3.000], mean observation: 0.055 [-0.627, 1.000], loss: 0.855054, mean_absolute_error: 19.988201, mean_q: 26.881397, mean_eps: 0.100000
 1125491/2000000: episode: 5662, duration: 13.540s, episode steps: 771, steps per second: 57, episode reward: 135.728, mean reward: 0.176 [-19.491, 100.000], mean action: 1.462 [0.000, 3.000], mean observation: 0.077 [-0.719, 1.000], loss: 0.850656, mean_absolute_error: 20.184419, mean_q: 27.120781, mean_eps: 0.100000
 1126491/2000000: episode: 5663, duration: 16.384s, episode steps: 1000, steps per second: 61, episode reward: -75.891, mean reward: -0.076 [-11.782, 14.112], mean action: 1.645 [0.000, 3.000], mean observation: 0.057 [-0.674, 1.000], loss: 0.777064, mean_absolute_error: 20.092193, mean_q: 26.914658, mean_eps: 0.100000
 1126960/2000000: episode: 5664, duration: 7.858s, episode steps: 469, steps per second: 60, episode reward: 200.670, mean reward: 0.428 [-17.830, 100.000], mean action: 0.972 [0.000, 3.000], mean observation: 0.097 [-0.730, 1.000], loss: 0.761574, mean_absolute_error: 19.840862, mean_q: 26.896462, mean_eps: 0.100000
 1127960/2000000: episode: 5665, duration: 16.567s, episode steps: 1000, steps per second: 60, episode reward: -39.748, mean reward: -0.040 [-4.643, 5.356], mean action: 1.861 [0.000, 3.000], mean observation: -0.017 [-0.758, 0.937], loss: 0.843910, mean_absolute_error: 19.768418, mean_q: 26.717559, mean_eps: 0.100000
 1128383/2000000: episode: 5666, duration: 6.759s, episode steps: 423, steps per second: 63, episode reward: 214.037, mean reward: 0.506 [-8.501, 100.000], mean action: 1.513 [0.000, 3.000], mean observation: 0.097 [-0.542, 1.045], loss: 0.929094, mean_absolute_error: 20.121967, mean_q: 26.885352, mean_eps: 0.100000
 1129383/2000000: episode: 5667, duration: 18.667s, episode steps: 1000, steps per second: 54, episode reward: -67.331, mean reward: -0.067 [-18.860, 21.648], mean action: 1.505 [0.000, 3.000], mean observation: 0.030 [-0.678, 1.000], loss: 0.838811, mean_absolute_error: 19.830727, mean_q: 26.713887, mean_eps: 0.100000
 1130118/2000000: episode: 5668, duration: 12.419s, episode steps: 735, steps per second: 59, episode reward: 181.889, mean reward: 0.247 [-19.517, 100.000], mean action: 1.261 [0.000, 3.000], mean observation: 0.086 [-0.541, 1.011], loss: 0.852238, mean_absolute_error: 19.776652, mean_q: 26.633974, mean_eps: 0.100000
 1131118/2000000: episode: 5669, duration: 17.399s, episode steps: 1000, steps per second: 57, episode reward: -32.351, mean reward: -0.032 [-3.717, 4.776], mean action: 1.661 [0.000, 3.000], mean observation: 0.013 [-0.693, 0.926], loss: 0.774771, mean_absolute_error: 19.787488, mean_q: 26.598523, mean_eps: 0.100000
 1131212/2000000: episode: 5670, duration: 1.477s, episode steps: 94, steps per second: 64, episode reward: -150.248, mean reward: -1.598 [-100.000, 5.491], mean action: 1.532 [0.000, 3.000], mean observation: -0.165 [-1.138, 3.587], loss: 0.576328, mean_absolute_error: 19.423093, mean_q: 26.395664, mean_eps: 0.100000
 1131371/2000000: episode: 5671, duration: 2.497s, episode steps: 159, steps per second: 64, episode reward: 10.625, mean reward: 0.067 [-100.000, 14.318], mean action: 1.780 [0.000, 3.000], mean observation: 0.094 [-1.040, 1.000], loss: 0.699200, mean_absolute_error: 19.687291, mean_q: 26.639865, mean_eps: 0.100000
 1132336/2000000: episode: 5672, duration: 16.708s, episode steps: 965, steps per second: 58, episode reward: 121.602, mean reward: 0.126 [-10.667, 100.000], mean action: 1.509 [0.000, 3.000], mean observation: 0.030 [-0.624, 1.009], loss: 0.881322, mean_absolute_error: 19.643664, mean_q: 26.360472, mean_eps: 0.100000
 1133037/2000000: episode: 5673, duration: 11.958s, episode steps: 701, steps per second: 59, episode reward: 115.953, mean reward: 0.165 [-18.662, 100.000], mean action: 1.541 [0.000, 3.000], mean observation: 0.020 [-0.831, 1.000], loss: 0.867595, mean_absolute_error: 19.554109, mean_q: 26.130493, mean_eps: 0.100000
 1133435/2000000: episode: 5674, duration: 6.581s, episode steps: 398, steps per second: 60, episode reward: 204.412, mean reward: 0.514 [-8.274, 100.000], mean action: 1.508 [0.000, 3.000], mean observation: 0.037 [-0.716, 1.000], loss: 0.763123, mean_absolute_error: 19.244703, mean_q: 26.009639, mean_eps: 0.100000
 1133935/2000000: episode: 5675, duration: 8.157s, episode steps: 500, steps per second: 61, episode reward: 219.544, mean reward: 0.439 [-13.200, 100.000], mean action: 1.418 [0.000, 3.000], mean observation: 0.101 [-0.646, 1.119], loss: 0.890219, mean_absolute_error: 19.371173, mean_q: 26.150914, mean_eps: 0.100000
 1134408/2000000: episode: 5676, duration: 7.638s, episode steps: 473, steps per second: 62, episode reward: 227.250, mean reward: 0.480 [-9.758, 100.000], mean action: 1.431 [0.000, 3.000], mean observation: 0.104 [-0.611, 1.096], loss: 0.750628, mean_absolute_error: 19.920078, mean_q: 26.809014, mean_eps: 0.100000
 1135408/2000000: episode: 5677, duration: 17.775s, episode steps: 1000, steps per second: 56, episode reward: 7.248, mean reward: 0.007 [-8.617, 14.732], mean action: 1.656 [0.000, 3.000], mean observation: 0.018 [-0.610, 1.000], loss: 0.842853, mean_absolute_error: 19.427589, mean_q: 26.077252, mean_eps: 0.100000
 1136087/2000000: episode: 5678, duration: 11.249s, episode steps: 679, steps per second: 60, episode reward: 126.696, mean reward: 0.187 [-8.184, 100.000], mean action: 1.594 [0.000, 3.000], mean observation: 0.022 [-0.601, 1.000], loss: 0.783405, mean_absolute_error: 19.904898, mean_q: 26.782580, mean_eps: 0.100000
 1137087/2000000: episode: 5679, duration: 16.719s, episode steps: 1000, steps per second: 60, episode reward: -70.603, mean reward: -0.071 [-4.976, 4.885], mean action: 1.729 [0.000, 3.000], mean observation: -0.013 [-0.679, 0.951], loss: 0.793859, mean_absolute_error: 19.454333, mean_q: 26.194324, mean_eps: 0.100000
 1137882/2000000: episode: 5680, duration: 13.944s, episode steps: 795, steps per second: 57, episode reward: 175.716, mean reward: 0.221 [-18.659, 100.000], mean action: 1.010 [0.000, 3.000], mean observation: 0.121 [-0.645, 1.000], loss: 0.802809, mean_absolute_error: 19.771605, mean_q: 26.778637, mean_eps: 0.100000
 1138137/2000000: episode: 5681, duration: 4.372s, episode steps: 255, steps per second: 58, episode reward: 197.057, mean reward: 0.773 [-10.334, 100.000], mean action: 1.024 [0.000, 3.000], mean observation: 0.099 [-0.763, 1.000], loss: 0.780814, mean_absolute_error: 19.309761, mean_q: 25.914451, mean_eps: 0.100000
 1139096/2000000: episode: 5682, duration: 16.451s, episode steps: 959, steps per second: 58, episode reward: 121.711, mean reward: 0.127 [-4.847, 100.000], mean action: 1.565 [0.000, 3.000], mean observation: 0.016 [-0.706, 1.000], loss: 0.774415, mean_absolute_error: 19.539598, mean_q: 26.276510, mean_eps: 0.100000
 1139493/2000000: episode: 5683, duration: 6.439s, episode steps: 397, steps per second: 62, episode reward: 177.779, mean reward: 0.448 [-8.914, 100.000], mean action: 1.169 [0.000, 3.000], mean observation: 0.113 [-0.738, 1.000], loss: 0.874936, mean_absolute_error: 19.270476, mean_q: 26.067041, mean_eps: 0.100000
 1140251/2000000: episode: 5684, duration: 12.576s, episode steps: 758, steps per second: 60, episode reward: 146.877, mean reward: 0.194 [-3.741, 100.000], mean action: 1.493 [0.000, 3.000], mean observation: 0.045 [-0.676, 1.002], loss: 0.791789, mean_absolute_error: 19.316074, mean_q: 26.112254, mean_eps: 0.100000
 1140398/2000000: episode: 5685, duration: 2.298s, episode steps: 147, steps per second: 64, episode reward: -14.802, mean reward: -0.101 [-100.000, 16.461], mean action: 1.483 [0.000, 3.000], mean observation: 0.020 [-0.687, 1.000], loss: 0.809174, mean_absolute_error: 20.355508, mean_q: 27.546067, mean_eps: 0.100000
 1140648/2000000: episode: 5686, duration: 3.951s, episode steps: 250, steps per second: 63, episode reward: 195.286, mean reward: 0.781 [-10.299, 100.000], mean action: 1.408 [0.000, 3.000], mean observation: 0.059 [-0.663, 1.000], loss: 0.689525, mean_absolute_error: 19.458699, mean_q: 26.402928, mean_eps: 0.100000
 1140784/2000000: episode: 5687, duration: 2.151s, episode steps: 136, steps per second: 63, episode reward: -4.908, mean reward: -0.036 [-100.000, 12.001], mean action: 1.772 [0.000, 3.000], mean observation: 0.054 [-0.809, 1.120], loss: 0.654280, mean_absolute_error: 18.609792, mean_q: 25.254988, mean_eps: 0.100000
 1141784/2000000: episode: 5688, duration: 16.950s, episode steps: 1000, steps per second: 59, episode reward: -28.369, mean reward: -0.028 [-12.393, 11.373], mean action: 1.724 [0.000, 3.000], mean observation: 0.012 [-0.792, 1.000], loss: 0.846789, mean_absolute_error: 19.523071, mean_q: 26.385490, mean_eps: 0.100000
 1142145/2000000: episode: 5689, duration: 5.833s, episode steps: 361, steps per second: 62, episode reward: 191.029, mean reward: 0.529 [-17.328, 100.000], mean action: 1.302 [0.000, 3.000], mean observation: 0.098 [-0.737, 1.000], loss: 0.743238, mean_absolute_error: 19.682639, mean_q: 26.610657, mean_eps: 0.100000
 1142757/2000000: episode: 5690, duration: 10.181s, episode steps: 612, steps per second: 60, episode reward: 184.326, mean reward: 0.301 [-3.892, 100.000], mean action: 1.600 [0.000, 3.000], mean observation: 0.043 [-0.764, 1.000], loss: 0.751200, mean_absolute_error: 19.307534, mean_q: 26.116676, mean_eps: 0.100000
 1143757/2000000: episode: 5691, duration: 16.387s, episode steps: 1000, steps per second: 61, episode reward: -32.048, mean reward: -0.032 [-5.593, 5.242], mean action: 1.762 [0.000, 3.000], mean observation: -0.008 [-0.636, 0.962], loss: 0.850889, mean_absolute_error: 19.188895, mean_q: 25.798485, mean_eps: 0.100000
 1143974/2000000: episode: 5692, duration: 3.351s, episode steps: 217, steps per second: 65, episode reward: -118.527, mean reward: -0.546 [-100.000, 13.698], mean action: 1.991 [0.000, 3.000], mean observation: 0.120 [-3.755, 1.332], loss: 0.826954, mean_absolute_error: 18.507928, mean_q: 24.971148, mean_eps: 0.100000
 1144505/2000000: episode: 5693, duration: 8.552s, episode steps: 531, steps per second: 62, episode reward: 191.993, mean reward: 0.362 [-9.152, 100.000], mean action: 1.384 [0.000, 3.000], mean observation: 0.112 [-0.694, 1.065], loss: 0.808549, mean_absolute_error: 19.023856, mean_q: 25.636174, mean_eps: 0.100000
 1145097/2000000: episode: 5694, duration: 9.789s, episode steps: 592, steps per second: 60, episode reward: 135.905, mean reward: 0.230 [-17.397, 100.000], mean action: 1.566 [0.000, 3.000], mean observation: 0.022 [-0.637, 1.000], loss: 0.812122, mean_absolute_error: 19.576579, mean_q: 26.043515, mean_eps: 0.100000
 1145559/2000000: episode: 5695, duration: 7.641s, episode steps: 462, steps per second: 60, episode reward: 178.156, mean reward: 0.386 [-17.589, 100.000], mean action: 1.528 [0.000, 3.000], mean observation: 0.045 [-0.589, 1.000], loss: 0.861034, mean_absolute_error: 19.325843, mean_q: 26.060406, mean_eps: 0.100000
 1146559/2000000: episode: 5696, duration: 17.865s, episode steps: 1000, steps per second: 56, episode reward: -26.971, mean reward: -0.027 [-5.015, 8.160], mean action: 1.651 [0.000, 3.000], mean observation: 0.047 [-0.820, 1.184], loss: 0.821334, mean_absolute_error: 19.262124, mean_q: 25.873402, mean_eps: 0.100000
 1146962/2000000: episode: 5697, duration: 6.785s, episode steps: 403, steps per second: 59, episode reward: 174.585, mean reward: 0.433 [-8.710, 100.000], mean action: 1.449 [0.000, 3.000], mean observation: 0.038 [-0.641, 1.000], loss: 0.754676, mean_absolute_error: 19.562372, mean_q: 26.322083, mean_eps: 0.100000
 1147514/2000000: episode: 5698, duration: 8.813s, episode steps: 552, steps per second: 63, episode reward: -66.799, mean reward: -0.121 [-100.000, 11.966], mean action: 1.716 [0.000, 3.000], mean observation: 0.052 [-0.733, 1.000], loss: 0.780205, mean_absolute_error: 19.426094, mean_q: 26.273310, mean_eps: 0.100000
 1148296/2000000: episode: 5699, duration: 13.721s, episode steps: 782, steps per second: 57, episode reward: 120.800, mean reward: 0.154 [-10.554, 100.000], mean action: 1.463 [0.000, 3.000], mean observation: 0.053 [-0.697, 1.000], loss: 0.813240, mean_absolute_error: 19.664177, mean_q: 26.474451, mean_eps: 0.100000
 1149124/2000000: episode: 5700, duration: 13.839s, episode steps: 828, steps per second: 60, episode reward: -148.433, mean reward: -0.179 [-100.000, 9.732], mean action: 1.903 [0.000, 3.000], mean observation: 0.057 [-0.850, 1.031], loss: 0.744793, mean_absolute_error: 19.499080, mean_q: 26.399121, mean_eps: 0.100000
 1149576/2000000: episode: 5701, duration: 7.357s, episode steps: 452, steps per second: 61, episode reward: 173.607, mean reward: 0.384 [-7.057, 100.000], mean action: 1.520 [0.000, 3.000], mean observation: 0.030 [-0.699, 1.000], loss: 0.757050, mean_absolute_error: 19.188819, mean_q: 26.004210, mean_eps: 0.100000
 1150110/2000000: episode: 5702, duration: 8.786s, episode steps: 534, steps per second: 61, episode reward: 158.494, mean reward: 0.297 [-10.436, 100.000], mean action: 1.558 [0.000, 3.000], mean observation: 0.027 [-0.769, 1.000], loss: 0.769077, mean_absolute_error: 19.467456, mean_q: 26.263952, mean_eps: 0.100000
 1150910/2000000: episode: 5703, duration: 13.385s, episode steps: 800, steps per second: 60, episode reward: 163.693, mean reward: 0.205 [-11.587, 100.000], mean action: 1.704 [0.000, 3.000], mean observation: 0.023 [-0.654, 1.000], loss: 0.765307, mean_absolute_error: 19.533013, mean_q: 26.372115, mean_eps: 0.100000
 1151123/2000000: episode: 5704, duration: 3.276s, episode steps: 213, steps per second: 65, episode reward: 237.751, mean reward: 1.116 [-10.659, 100.000], mean action: 1.502 [0.000, 3.000], mean observation: 0.080 [-0.683, 1.038], loss: 0.669545, mean_absolute_error: 18.834921, mean_q: 25.502229, mean_eps: 0.100000
 1151910/2000000: episode: 5705, duration: 13.502s, episode steps: 787, steps per second: 58, episode reward: 125.929, mean reward: 0.160 [-3.963, 100.000], mean action: 1.851 [0.000, 3.000], mean observation: -0.003 [-0.615, 1.000], loss: 0.726540, mean_absolute_error: 19.614360, mean_q: 26.330279, mean_eps: 0.100000
 1152910/2000000: episode: 5706, duration: 17.692s, episode steps: 1000, steps per second: 57, episode reward: -21.234, mean reward: -0.021 [-19.271, 20.281], mean action: 1.800 [0.000, 3.000], mean observation: 0.047 [-0.727, 1.000], loss: 0.818609, mean_absolute_error: 19.449222, mean_q: 26.297168, mean_eps: 0.100000
 1153824/2000000: episode: 5707, duration: 15.800s, episode steps: 914, steps per second: 58, episode reward: 125.903, mean reward: 0.138 [-4.208, 100.000], mean action: 1.519 [0.000, 3.000], mean observation: 0.041 [-0.687, 1.000], loss: 0.824417, mean_absolute_error: 19.351302, mean_q: 26.123954, mean_eps: 0.100000
 1154513/2000000: episode: 5708, duration: 11.546s, episode steps: 689, steps per second: 60, episode reward: 190.178, mean reward: 0.276 [-9.832, 100.000], mean action: 1.287 [0.000, 3.000], mean observation: 0.062 [-0.632, 1.000], loss: 0.851597, mean_absolute_error: 19.646610, mean_q: 26.062943, mean_eps: 0.100000
 1154758/2000000: episode: 5709, duration: 3.815s, episode steps: 245, steps per second: 64, episode reward: 243.708, mean reward: 0.995 [-9.830, 100.000], mean action: 1.433 [0.000, 3.000], mean observation: 0.116 [-0.809, 1.121], loss: 0.772044, mean_absolute_error: 19.264294, mean_q: 26.086354, mean_eps: 0.100000
 1155448/2000000: episode: 5710, duration: 11.295s, episode steps: 690, steps per second: 61, episode reward: 175.418, mean reward: 0.254 [-19.160, 100.000], mean action: 0.864 [0.000, 3.000], mean observation: 0.152 [-1.502, 1.000], loss: 0.802506, mean_absolute_error: 18.892525, mean_q: 25.513680, mean_eps: 0.100000
 1156448/2000000: episode: 5711, duration: 16.602s, episode steps: 1000, steps per second: 60, episode reward: -32.457, mean reward: -0.032 [-4.327, 4.621], mean action: 1.769 [0.000, 3.000], mean observation: -0.009 [-0.743, 0.934], loss: 0.699050, mean_absolute_error: 19.383671, mean_q: 26.213603, mean_eps: 0.100000
 1157448/2000000: episode: 5712, duration: 17.265s, episode steps: 1000, steps per second: 58, episode reward: 28.463, mean reward: 0.028 [-18.784, 22.944], mean action: 1.151 [0.000, 3.000], mean observation: 0.139 [-0.711, 1.078], loss: 0.833276, mean_absolute_error: 19.146217, mean_q: 25.713097, mean_eps: 0.100000
 1158448/2000000: episode: 5713, duration: 17.323s, episode steps: 1000, steps per second: 58, episode reward: -32.069, mean reward: -0.032 [-4.303, 11.123], mean action: 1.806 [0.000, 3.000], mean observation: -0.010 [-0.760, 1.000], loss: 0.721127, mean_absolute_error: 19.671454, mean_q: 26.499867, mean_eps: 0.100000
 1159448/2000000: episode: 5714, duration: 18.978s, episode steps: 1000, steps per second: 53, episode reward: -32.421, mean reward: -0.032 [-5.049, 5.155], mean action: 1.947 [0.000, 3.000], mean observation: -0.015 [-0.765, 0.930], loss: 0.784038, mean_absolute_error: 19.235789, mean_q: 25.981776, mean_eps: 0.100000
 1159827/2000000: episode: 5715, duration: 6.094s, episode steps: 379, steps per second: 62, episode reward: 211.121, mean reward: 0.557 [-10.214, 100.000], mean action: 1.298 [0.000, 3.000], mean observation: 0.037 [-0.732, 1.000], loss: 0.764377, mean_absolute_error: 19.054403, mean_q: 25.795167, mean_eps: 0.100000
 1160239/2000000: episode: 5716, duration: 6.698s, episode steps: 412, steps per second: 62, episode reward: 201.319, mean reward: 0.489 [-11.345, 100.000], mean action: 1.323 [0.000, 3.000], mean observation: 0.066 [-0.761, 1.004], loss: 0.709098, mean_absolute_error: 19.341502, mean_q: 26.176033, mean_eps: 0.100000
 1161239/2000000: episode: 5717, duration: 16.774s, episode steps: 1000, steps per second: 60, episode reward: -145.937, mean reward: -0.146 [-4.759, 6.351], mean action: 1.920 [0.000, 3.000], mean observation: -0.017 [-0.837, 0.975], loss: 0.779209, mean_absolute_error: 19.189132, mean_q: 25.969036, mean_eps: 0.100000
 1161532/2000000: episode: 5718, duration: 4.710s, episode steps: 293, steps per second: 62, episode reward: 157.237, mean reward: 0.537 [-13.370, 100.000], mean action: 1.597 [0.000, 3.000], mean observation: 0.102 [-0.697, 1.000], loss: 0.824899, mean_absolute_error: 19.518480, mean_q: 26.323517, mean_eps: 0.100000
 1161914/2000000: episode: 5719, duration: 6.220s, episode steps: 382, steps per second: 61, episode reward: 178.338, mean reward: 0.467 [-5.098, 100.000], mean action: 1.579 [0.000, 3.000], mean observation: 0.018 [-0.624, 1.000], loss: 0.843976, mean_absolute_error: 19.297231, mean_q: 25.856954, mean_eps: 0.100000
 1162914/2000000: episode: 5720, duration: 16.906s, episode steps: 1000, steps per second: 59, episode reward: 84.010, mean reward: 0.084 [-17.711, 14.743], mean action: 1.357 [0.000, 3.000], mean observation: 0.123 [-1.488, 1.000], loss: 0.730123, mean_absolute_error: 18.927273, mean_q: 25.340751, mean_eps: 0.100000
 1163669/2000000: episode: 5721, duration: 13.018s, episode steps: 755, steps per second: 58, episode reward: -120.325, mean reward: -0.159 [-100.000, 23.950], mean action: 1.799 [0.000, 3.000], mean observation: 0.047 [-0.842, 1.565], loss: 0.724555, mean_absolute_error: 19.131917, mean_q: 25.672116, mean_eps: 0.100000
 1164011/2000000: episode: 5722, duration: 5.419s, episode steps: 342, steps per second: 63, episode reward: 213.693, mean reward: 0.625 [-17.605, 100.000], mean action: 1.006 [0.000, 3.000], mean observation: 0.138 [-0.664, 1.000], loss: 0.767913, mean_absolute_error: 19.055040, mean_q: 25.668898, mean_eps: 0.100000
 1164518/2000000: episode: 5723, duration: 8.301s, episode steps: 507, steps per second: 61, episode reward: 204.228, mean reward: 0.403 [-15.641, 100.000], mean action: 1.010 [0.000, 3.000], mean observation: 0.161 [-0.710, 1.000], loss: 0.721492, mean_absolute_error: 19.049626, mean_q: 25.765199, mean_eps: 0.100000
 1164763/2000000: episode: 5724, duration: 3.820s, episode steps: 245, steps per second: 64, episode reward: 186.874, mean reward: 0.763 [-17.501, 100.000], mean action: 0.935 [0.000, 3.000], mean observation: 0.150 [-0.673, 1.000], loss: 0.739587, mean_absolute_error: 19.537992, mean_q: 26.379717, mean_eps: 0.100000
 1165071/2000000: episode: 5725, duration: 4.932s, episode steps: 308, steps per second: 62, episode reward: 215.684, mean reward: 0.700 [-10.952, 100.000], mean action: 1.510 [0.000, 3.000], mean observation: 0.022 [-0.718, 1.000], loss: 0.752853, mean_absolute_error: 19.545378, mean_q: 26.355130, mean_eps: 0.100000
 1165362/2000000: episode: 5726, duration: 4.580s, episode steps: 291, steps per second: 64, episode reward: 249.482, mean reward: 0.857 [-19.193, 100.000], mean action: 1.227 [0.000, 3.000], mean observation: 0.157 [-0.890, 1.238], loss: 0.695072, mean_absolute_error: 19.395412, mean_q: 26.248722, mean_eps: 0.100000
 1166029/2000000: episode: 5727, duration: 11.076s, episode steps: 667, steps per second: 60, episode reward: 200.280, mean reward: 0.300 [-17.643, 100.000], mean action: 1.339 [0.000, 3.000], mean observation: 0.127 [-0.976, 1.368], loss: 0.717237, mean_absolute_error: 19.116402, mean_q: 25.918487, mean_eps: 0.100000
 1166629/2000000: episode: 5728, duration: 10.137s, episode steps: 600, steps per second: 59, episode reward: 166.135, mean reward: 0.277 [-17.347, 100.000], mean action: 1.422 [0.000, 3.000], mean observation: 0.044 [-0.659, 1.000], loss: 0.743501, mean_absolute_error: 19.433436, mean_q: 26.298375, mean_eps: 0.100000
 1166953/2000000: episode: 5729, duration: 5.215s, episode steps: 324, steps per second: 62, episode reward: 191.750, mean reward: 0.592 [-3.275, 100.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.059 [-0.650, 1.000], loss: 0.770200, mean_absolute_error: 19.010457, mean_q: 25.697101, mean_eps: 0.100000
 1167224/2000000: episode: 5730, duration: 4.271s, episode steps: 271, steps per second: 63, episode reward: -362.371, mean reward: -1.337 [-100.000, 29.458], mean action: 1.852 [0.000, 3.000], mean observation: 0.091 [-1.291, 1.923], loss: 0.712288, mean_absolute_error: 19.591246, mean_q: 26.309790, mean_eps: 0.100000
 1168181/2000000: episode: 5731, duration: 16.730s, episode steps: 957, steps per second: 57, episode reward: 65.746, mean reward: 0.069 [-4.279, 100.000], mean action: 1.578 [0.000, 3.000], mean observation: 0.009 [-0.657, 1.000], loss: 0.761062, mean_absolute_error: 19.293468, mean_q: 25.976135, mean_eps: 0.100000
 1168319/2000000: episode: 5732, duration: 2.148s, episode steps: 138, steps per second: 64, episode reward: 0.203, mean reward: 0.001 [-100.000, 18.296], mean action: 1.275 [0.000, 3.000], mean observation: 0.043 [-0.899, 1.649], loss: 0.754508, mean_absolute_error: 19.595838, mean_q: 26.552777, mean_eps: 0.100000
 1168748/2000000: episode: 5733, duration: 6.929s, episode steps: 429, steps per second: 62, episode reward: 174.128, mean reward: 0.406 [-18.353, 100.000], mean action: 0.942 [0.000, 3.000], mean observation: 0.191 [-1.213, 1.000], loss: 0.831959, mean_absolute_error: 19.040060, mean_q: 25.751829, mean_eps: 0.100000
 1169748/2000000: episode: 5734, duration: 17.356s, episode steps: 1000, steps per second: 58, episode reward: -94.312, mean reward: -0.094 [-5.175, 4.851], mean action: 1.741 [0.000, 3.000], mean observation: -0.010 [-0.724, 0.930], loss: 0.699216, mean_absolute_error: 19.065728, mean_q: 25.755275, mean_eps: 0.100000
 1170748/2000000: episode: 5735, duration: 17.019s, episode steps: 1000, steps per second: 59, episode reward: -72.808, mean reward: -0.073 [-13.076, 38.834], mean action: 1.949 [0.000, 3.000], mean observation: 0.010 [-0.971, 1.115], loss: 0.661741, mean_absolute_error: 19.178056, mean_q: 25.994549, mean_eps: 0.100000
 1170944/2000000: episode: 5736, duration: 3.148s, episode steps: 196, steps per second: 62, episode reward: 216.077, mean reward: 1.102 [-9.933, 100.000], mean action: 1.352 [0.000, 3.000], mean observation: 0.049 [-0.768, 1.000], loss: 0.733337, mean_absolute_error: 18.838357, mean_q: 25.105727, mean_eps: 0.100000
 1171944/2000000: episode: 5737, duration: 18.153s, episode steps: 1000, steps per second: 55, episode reward: -86.112, mean reward: -0.086 [-5.329, 4.904], mean action: 1.698 [0.000, 3.000], mean observation: -0.007 [-0.642, 0.937], loss: 0.816744, mean_absolute_error: 19.277537, mean_q: 25.996807, mean_eps: 0.100000
 1172189/2000000: episode: 5738, duration: 3.957s, episode steps: 245, steps per second: 62, episode reward: 227.546, mean reward: 0.929 [-17.260, 100.000], mean action: 1.171 [0.000, 3.000], mean observation: 0.154 [-0.740, 1.000], loss: 0.698301, mean_absolute_error: 19.255618, mean_q: 25.996206, mean_eps: 0.100000
 1172351/2000000: episode: 5739, duration: 2.535s, episode steps: 162, steps per second: 64, episode reward: -54.594, mean reward: -0.337 [-100.000, 9.244], mean action: 1.852 [0.000, 3.000], mean observation: -0.037 [-1.627, 1.000], loss: 0.726144, mean_absolute_error: 19.316458, mean_q: 26.179415, mean_eps: 0.100000
 1173133/2000000: episode: 5740, duration: 13.780s, episode steps: 782, steps per second: 57, episode reward: 125.401, mean reward: 0.160 [-10.883, 100.000], mean action: 1.556 [0.000, 3.000], mean observation: 0.029 [-0.670, 1.000], loss: 0.763231, mean_absolute_error: 19.146964, mean_q: 25.830929, mean_eps: 0.100000
 1173391/2000000: episode: 5741, duration: 4.037s, episode steps: 258, steps per second: 64, episode reward: 272.703, mean reward: 1.057 [-11.830, 100.000], mean action: 1.244 [0.000, 3.000], mean observation: 0.146 [-0.762, 1.133], loss: 0.872292, mean_absolute_error: 18.968086, mean_q: 25.502988, mean_eps: 0.100000
 1173585/2000000: episode: 5742, duration: 3.072s, episode steps: 194, steps per second: 63, episode reward: 23.659, mean reward: 0.122 [-100.000, 21.802], mean action: 1.603 [0.000, 3.000], mean observation: 0.080 [-0.824, 1.268], loss: 0.847073, mean_absolute_error: 19.753816, mean_q: 26.602757, mean_eps: 0.100000
 1174585/2000000: episode: 5743, duration: 16.446s, episode steps: 1000, steps per second: 61, episode reward: 39.076, mean reward: 0.039 [-22.905, 15.422], mean action: 1.418 [0.000, 3.000], mean observation: 0.213 [-0.710, 1.000], loss: 0.805251, mean_absolute_error: 18.501491, mean_q: 24.964217, mean_eps: 0.100000
 1175096/2000000: episode: 5744, duration: 8.450s, episode steps: 511, steps per second: 60, episode reward: 151.777, mean reward: 0.297 [-17.581, 100.000], mean action: 1.464 [0.000, 3.000], mean observation: 0.037 [-0.653, 1.000], loss: 0.663726, mean_absolute_error: 18.830641, mean_q: 25.437678, mean_eps: 0.100000
 1176096/2000000: episode: 5745, duration: 17.228s, episode steps: 1000, steps per second: 58, episode reward: -49.294, mean reward: -0.049 [-5.180, 5.496], mean action: 1.665 [0.000, 3.000], mean observation: 0.005 [-0.736, 0.925], loss: 0.777855, mean_absolute_error: 19.117149, mean_q: 25.723936, mean_eps: 0.100000
 1177096/2000000: episode: 5746, duration: 17.335s, episode steps: 1000, steps per second: 58, episode reward: -49.354, mean reward: -0.049 [-5.383, 5.246], mean action: 1.622 [0.000, 3.000], mean observation: 0.003 [-0.561, 0.938], loss: 0.786480, mean_absolute_error: 19.307406, mean_q: 26.092966, mean_eps: 0.100000
 1178096/2000000: episode: 5747, duration: 17.565s, episode steps: 1000, steps per second: 57, episode reward: -56.871, mean reward: -0.057 [-5.124, 5.176], mean action: 1.617 [0.000, 3.000], mean observation: -0.016 [-0.733, 0.941], loss: 0.753297, mean_absolute_error: 19.149565, mean_q: 25.887235, mean_eps: 0.100000
 1178569/2000000: episode: 5748, duration: 7.872s, episode steps: 473, steps per second: 60, episode reward: 182.472, mean reward: 0.386 [-4.895, 100.000], mean action: 1.512 [0.000, 3.000], mean observation: 0.047 [-0.709, 1.000], loss: 0.643012, mean_absolute_error: 18.778668, mean_q: 25.461898, mean_eps: 0.100000
 1179538/2000000: episode: 5749, duration: 17.500s, episode steps: 969, steps per second: 55, episode reward: 113.532, mean reward: 0.117 [-8.466, 100.000], mean action: 1.534 [0.000, 3.000], mean observation: 0.030 [-0.577, 1.000], loss: 0.747466, mean_absolute_error: 19.356883, mean_q: 26.121875, mean_eps: 0.100000
 1180538/2000000: episode: 5750, duration: 17.570s, episode steps: 1000, steps per second: 57, episode reward: -63.321, mean reward: -0.063 [-4.434, 5.109], mean action: 1.612 [0.000, 3.000], mean observation: -0.003 [-0.672, 0.972], loss: 0.722800, mean_absolute_error: 18.534566, mean_q: 25.068293, mean_eps: 0.100000
 1180992/2000000: episode: 5751, duration: 7.503s, episode steps: 454, steps per second: 61, episode reward: 137.565, mean reward: 0.303 [-17.653, 100.000], mean action: 1.081 [0.000, 3.000], mean observation: 0.153 [-0.880, 1.000], loss: 0.683709, mean_absolute_error: 18.223981, mean_q: 24.566660, mean_eps: 0.100000
 1181254/2000000: episode: 5752, duration: 4.152s, episode steps: 262, steps per second: 63, episode reward: 227.946, mean reward: 0.870 [-18.074, 100.000], mean action: 1.019 [0.000, 3.000], mean observation: 0.164 [-1.001, 1.000], loss: 0.729042, mean_absolute_error: 19.320764, mean_q: 26.166745, mean_eps: 0.100000
 1181467/2000000: episode: 5753, duration: 3.341s, episode steps: 213, steps per second: 64, episode reward: 212.730, mean reward: 0.999 [-11.113, 100.000], mean action: 1.507 [0.000, 3.000], mean observation: 0.064 [-0.710, 1.000], loss: 0.668156, mean_absolute_error: 18.963763, mean_q: 25.659626, mean_eps: 0.100000
 1182467/2000000: episode: 5754, duration: 17.116s, episode steps: 1000, steps per second: 58, episode reward: -115.469, mean reward: -0.115 [-4.643, 4.653], mean action: 1.716 [0.000, 3.000], mean observation: 0.001 [-0.663, 0.977], loss: 0.778845, mean_absolute_error: 18.282183, mean_q: 24.669963, mean_eps: 0.100000
 1182613/2000000: episode: 5755, duration: 2.329s, episode steps: 146, steps per second: 63, episode reward: 7.193, mean reward: 0.049 [-100.000, 18.933], mean action: 1.452 [0.000, 3.000], mean observation: 0.041 [-0.772, 1.013], loss: 0.718971, mean_absolute_error: 18.766446, mean_q: 25.371134, mean_eps: 0.100000
 1183117/2000000: episode: 5756, duration: 8.402s, episode steps: 504, steps per second: 60, episode reward: 163.501, mean reward: 0.324 [-3.602, 100.000], mean action: 1.560 [0.000, 3.000], mean observation: 0.025 [-0.650, 1.000], loss: 0.792976, mean_absolute_error: 18.269562, mean_q: 24.534099, mean_eps: 0.100000
 1183789/2000000: episode: 5757, duration: 11.352s, episode steps: 672, steps per second: 59, episode reward: 177.803, mean reward: 0.265 [-18.702, 100.000], mean action: 1.007 [0.000, 3.000], mean observation: 0.100 [-0.637, 1.000], loss: 0.783439, mean_absolute_error: 18.374993, mean_q: 24.778504, mean_eps: 0.100000
 1184789/2000000: episode: 5758, duration: 17.139s, episode steps: 1000, steps per second: 58, episode reward: -54.981, mean reward: -0.055 [-4.327, 4.495], mean action: 1.751 [0.000, 3.000], mean observation: -0.004 [-0.757, 0.930], loss: 0.788460, mean_absolute_error: 18.617877, mean_q: 25.158041, mean_eps: 0.100000
 1185789/2000000: episode: 5759, duration: 16.665s, episode steps: 1000, steps per second: 60, episode reward: -58.451, mean reward: -0.058 [-4.417, 4.096], mean action: 1.584 [0.000, 3.000], mean observation: -0.003 [-0.741, 0.928], loss: 0.718956, mean_absolute_error: 18.423293, mean_q: 24.841841, mean_eps: 0.100000
 1185953/2000000: episode: 5760, duration: 2.539s, episode steps: 164, steps per second: 65, episode reward: -12.683, mean reward: -0.077 [-100.000, 14.820], mean action: 1.738 [0.000, 3.000], mean observation: 0.012 [-0.683, 1.238], loss: 0.821315, mean_absolute_error: 18.694462, mean_q: 25.340530, mean_eps: 0.100000
 1186953/2000000: episode: 5761, duration: 16.439s, episode steps: 1000, steps per second: 61, episode reward: -112.799, mean reward: -0.113 [-5.267, 4.936], mean action: 1.792 [0.000, 3.000], mean observation: -0.004 [-0.768, 0.945], loss: 0.767825, mean_absolute_error: 18.505015, mean_q: 24.999536, mean_eps: 0.100000
 1187953/2000000: episode: 5762, duration: 18.219s, episode steps: 1000, steps per second: 55, episode reward: -36.575, mean reward: -0.037 [-8.455, 5.158], mean action: 1.670 [0.000, 3.000], mean observation: 0.011 [-0.770, 1.070], loss: 0.773618, mean_absolute_error: 18.875226, mean_q: 25.466651, mean_eps: 0.100000
 1188493/2000000: episode: 5763, duration: 8.971s, episode steps: 540, steps per second: 60, episode reward: 172.276, mean reward: 0.319 [-4.591, 100.000], mean action: 1.509 [0.000, 3.000], mean observation: 0.034 [-0.683, 1.002], loss: 0.702806, mean_absolute_error: 18.073223, mean_q: 24.520996, mean_eps: 0.100000
 1188805/2000000: episode: 5764, duration: 4.892s, episode steps: 312, steps per second: 64, episode reward: -67.024, mean reward: -0.215 [-100.000, 12.229], mean action: 1.692 [0.000, 3.000], mean observation: 0.030 [-1.176, 1.000], loss: 0.681816, mean_absolute_error: 18.548839, mean_q: 25.123689, mean_eps: 0.100000
 1189281/2000000: episode: 5765, duration: 7.492s, episode steps: 476, steps per second: 64, episode reward: 238.985, mean reward: 0.502 [-19.113, 100.000], mean action: 0.960 [0.000, 3.000], mean observation: 0.157 [-0.688, 1.000], loss: 0.657614, mean_absolute_error: 18.460605, mean_q: 25.003040, mean_eps: 0.100000
 1189513/2000000: episode: 5766, duration: 3.617s, episode steps: 232, steps per second: 64, episode reward: 233.306, mean reward: 1.006 [-7.444, 100.000], mean action: 1.259 [0.000, 3.000], mean observation: 0.095 [-0.782, 1.026], loss: 0.828527, mean_absolute_error: 18.520650, mean_q: 25.048624, mean_eps: 0.100000
 1189847/2000000: episode: 5767, duration: 5.269s, episode steps: 334, steps per second: 63, episode reward: 156.917, mean reward: 0.470 [-12.008, 100.000], mean action: 1.060 [0.000, 3.000], mean observation: 0.171 [-0.919, 1.000], loss: 0.728942, mean_absolute_error: 18.835004, mean_q: 25.436008, mean_eps: 0.100000
 1190368/2000000: episode: 5768, duration: 8.470s, episode steps: 521, steps per second: 62, episode reward: 177.511, mean reward: 0.341 [-18.170, 100.000], mean action: 1.228 [0.000, 3.000], mean observation: 0.118 [-0.701, 1.000], loss: 0.681611, mean_absolute_error: 18.815519, mean_q: 25.252791, mean_eps: 0.100000
 1190699/2000000: episode: 5769, duration: 5.286s, episode steps: 331, steps per second: 63, episode reward: 208.641, mean reward: 0.630 [-17.636, 100.000], mean action: 1.363 [0.000, 3.000], mean observation: 0.050 [-0.642, 1.000], loss: 0.679695, mean_absolute_error: 18.972406, mean_q: 25.715782, mean_eps: 0.100000
 1191699/2000000: episode: 5770, duration: 17.649s, episode steps: 1000, steps per second: 57, episode reward: -43.387, mean reward: -0.043 [-4.437, 5.092], mean action: 1.821 [0.000, 3.000], mean observation: -0.005 [-0.621, 0.952], loss: 0.603479, mean_absolute_error: 18.364536, mean_q: 24.686797, mean_eps: 0.100000
 1192209/2000000: episode: 5771, duration: 8.315s, episode steps: 510, steps per second: 61, episode reward: 177.080, mean reward: 0.347 [-8.257, 100.000], mean action: 1.584 [0.000, 3.000], mean observation: 0.077 [-0.707, 1.000], loss: 0.743423, mean_absolute_error: 18.806144, mean_q: 25.245717, mean_eps: 0.100000
 1192687/2000000: episode: 5772, duration: 7.701s, episode steps: 478, steps per second: 62, episode reward: 214.866, mean reward: 0.450 [-18.066, 100.000], mean action: 1.523 [0.000, 3.000], mean observation: 0.079 [-0.720, 1.241], loss: 0.769933, mean_absolute_error: 18.253602, mean_q: 24.556487, mean_eps: 0.100000
 1193591/2000000: episode: 5773, duration: 15.465s, episode steps: 904, steps per second: 58, episode reward: 117.207, mean reward: 0.130 [-10.295, 100.000], mean action: 1.747 [0.000, 3.000], mean observation: 0.006 [-0.663, 1.000], loss: 0.655392, mean_absolute_error: 18.678205, mean_q: 25.180955, mean_eps: 0.100000
 1193975/2000000: episode: 5774, duration: 6.169s, episode steps: 384, steps per second: 62, episode reward: -287.224, mean reward: -0.748 [-100.000, 30.220], mean action: 1.935 [0.000, 3.000], mean observation: 0.065 [-0.719, 1.850], loss: 0.764613, mean_absolute_error: 18.551664, mean_q: 24.991380, mean_eps: 0.100000
 1194104/2000000: episode: 5775, duration: 2.055s, episode steps: 129, steps per second: 63, episode reward: 5.275, mean reward: 0.041 [-100.000, 13.561], mean action: 1.760 [0.000, 3.000], mean observation: 0.049 [-0.730, 1.000], loss: 0.651429, mean_absolute_error: 18.508362, mean_q: 25.049124, mean_eps: 0.100000
 1194524/2000000: episode: 5776, duration: 6.736s, episode steps: 420, steps per second: 62, episode reward: 197.644, mean reward: 0.471 [-3.860, 100.000], mean action: 1.529 [0.000, 3.000], mean observation: 0.073 [-0.618, 1.000], loss: 0.810399, mean_absolute_error: 19.138897, mean_q: 25.742159, mean_eps: 0.100000
 1194892/2000000: episode: 5777, duration: 5.946s, episode steps: 368, steps per second: 62, episode reward: 245.903, mean reward: 0.668 [-10.305, 100.000], mean action: 1.318 [0.000, 3.000], mean observation: 0.120 [-0.700, 1.021], loss: 0.685143, mean_absolute_error: 18.656132, mean_q: 25.250493, mean_eps: 0.100000
 1195104/2000000: episode: 5778, duration: 3.597s, episode steps: 212, steps per second: 59, episode reward: 199.970, mean reward: 0.943 [-12.437, 100.000], mean action: 2.151 [0.000, 3.000], mean observation: 0.038 [-0.860, 1.000], loss: 0.574993, mean_absolute_error: 18.091240, mean_q: 24.517285, mean_eps: 0.100000
 1195630/2000000: episode: 5779, duration: 8.619s, episode steps: 526, steps per second: 61, episode reward: 218.373, mean reward: 0.415 [-18.567, 100.000], mean action: 0.928 [0.000, 3.000], mean observation: 0.108 [-1.562, 1.000], loss: 0.715097, mean_absolute_error: 18.415833, mean_q: 24.896662, mean_eps: 0.100000
 1196453/2000000: episode: 5780, duration: 14.317s, episode steps: 823, steps per second: 57, episode reward: 153.019, mean reward: 0.186 [-11.161, 100.000], mean action: 1.392 [0.000, 3.000], mean observation: 0.035 [-0.786, 1.279], loss: 0.733602, mean_absolute_error: 18.952487, mean_q: 25.411652, mean_eps: 0.100000
 1196905/2000000: episode: 5781, duration: 7.083s, episode steps: 452, steps per second: 64, episode reward: 214.256, mean reward: 0.474 [-17.453, 100.000], mean action: 0.615 [0.000, 3.000], mean observation: 0.174 [-0.707, 1.000], loss: 0.836830, mean_absolute_error: 18.839239, mean_q: 25.401549, mean_eps: 0.100000
 1197029/2000000: episode: 5782, duration: 1.953s, episode steps: 124, steps per second: 64, episode reward: -13.413, mean reward: -0.108 [-100.000, 13.012], mean action: 1.831 [0.000, 3.000], mean observation: 0.058 [-1.263, 1.000], loss: 0.561117, mean_absolute_error: 18.603756, mean_q: 25.227931, mean_eps: 0.100000
 1198029/2000000: episode: 5783, duration: 16.407s, episode steps: 1000, steps per second: 61, episode reward: 20.626, mean reward: 0.021 [-17.600, 22.057], mean action: 1.445 [0.000, 3.000], mean observation: 0.031 [-0.786, 1.000], loss: 0.741995, mean_absolute_error: 18.860682, mean_q: 25.395833, mean_eps: 0.100000
 1198320/2000000: episode: 5784, duration: 4.597s, episode steps: 291, steps per second: 63, episode reward: 195.030, mean reward: 0.670 [-3.081, 100.000], mean action: 1.271 [0.000, 3.000], mean observation: 0.038 [-0.627, 1.000], loss: 0.753087, mean_absolute_error: 18.480268, mean_q: 24.901941, mean_eps: 0.100000
 1199320/2000000: episode: 5785, duration: 17.081s, episode steps: 1000, steps per second: 59, episode reward: -11.549, mean reward: -0.012 [-8.815, 13.202], mean action: 1.592 [0.000, 3.000], mean observation: -0.013 [-0.741, 1.000], loss: 0.761344, mean_absolute_error: 18.794502, mean_q: 25.357522, mean_eps: 0.100000
 1199846/2000000: episode: 5786, duration: 8.590s, episode steps: 526, steps per second: 61, episode reward: 201.253, mean reward: 0.383 [-4.434, 100.000], mean action: 1.416 [0.000, 3.000], mean observation: 0.059 [-0.549, 1.000], loss: 0.733196, mean_absolute_error: 18.961001, mean_q: 25.663655, mean_eps: 0.100000
 1200122/2000000: episode: 5787, duration: 4.449s, episode steps: 276, steps per second: 62, episode reward: 185.083, mean reward: 0.671 [-11.568, 100.000], mean action: 1.438 [0.000, 3.000], mean observation: 0.100 [-0.725, 1.000], loss: 0.732437, mean_absolute_error: 18.288781, mean_q: 24.757384, mean_eps: 0.100000
 1200568/2000000: episode: 5788, duration: 7.219s, episode steps: 446, steps per second: 62, episode reward: 191.500, mean reward: 0.429 [-17.380, 100.000], mean action: 1.217 [0.000, 3.000], mean observation: 0.053 [-0.572, 1.000], loss: 0.722705, mean_absolute_error: 19.014003, mean_q: 25.612912, mean_eps: 0.100000
 1200924/2000000: episode: 5789, duration: 5.762s, episode steps: 356, steps per second: 62, episode reward: 183.000, mean reward: 0.514 [-18.677, 100.000], mean action: 1.160 [0.000, 3.000], mean observation: 0.071 [-0.717, 1.000], loss: 0.702162, mean_absolute_error: 18.156636, mean_q: 24.525415, mean_eps: 0.100000
 1201727/2000000: episode: 5790, duration: 14.077s, episode steps: 803, steps per second: 57, episode reward: 135.896, mean reward: 0.169 [-19.297, 100.000], mean action: 1.512 [0.000, 3.000], mean observation: 0.007 [-0.738, 1.000], loss: 0.629489, mean_absolute_error: 18.718423, mean_q: 25.334335, mean_eps: 0.100000
 1202727/2000000: episode: 5791, duration: 17.605s, episode steps: 1000, steps per second: 57, episode reward: -35.209, mean reward: -0.035 [-4.784, 5.602], mean action: 1.738 [0.000, 3.000], mean observation: -0.004 [-0.518, 1.001], loss: 0.738360, mean_absolute_error: 18.760944, mean_q: 25.355699, mean_eps: 0.100000
 1202860/2000000: episode: 5792, duration: 2.133s, episode steps: 133, steps per second: 62, episode reward: -18.949, mean reward: -0.142 [-100.000, 12.954], mean action: 1.406 [0.000, 3.000], mean observation: 0.027 [-0.671, 1.000], loss: 0.678486, mean_absolute_error: 18.309493, mean_q: 24.815903, mean_eps: 0.100000
 1203226/2000000: episode: 5793, duration: 5.784s, episode steps: 366, steps per second: 63, episode reward: 236.386, mean reward: 0.646 [-10.894, 100.000], mean action: 1.347 [0.000, 3.000], mean observation: 0.163 [-0.694, 1.000], loss: 0.632561, mean_absolute_error: 18.487456, mean_q: 24.876141, mean_eps: 0.100000
 1203436/2000000: episode: 5794, duration: 3.340s, episode steps: 210, steps per second: 63, episode reward: 226.019, mean reward: 1.076 [-2.907, 100.000], mean action: 1.214 [0.000, 3.000], mean observation: 0.149 [-0.777, 1.000], loss: 0.614880, mean_absolute_error: 18.411795, mean_q: 24.960927, mean_eps: 0.100000
 1204436/2000000: episode: 5795, duration: 18.185s, episode steps: 1000, steps per second: 55, episode reward: -33.012, mean reward: -0.033 [-4.558, 6.838], mean action: 1.656 [0.000, 3.000], mean observation: 0.001 [-0.668, 1.012], loss: 0.720768, mean_absolute_error: 18.965007, mean_q: 25.652908, mean_eps: 0.100000
 1204539/2000000: episode: 5796, duration: 1.611s, episode steps: 103, steps per second: 64, episode reward: -43.756, mean reward: -0.425 [-100.000, 14.190], mean action: 1.165 [0.000, 3.000], mean observation: 0.012 [-0.880, 1.000], loss: 0.881375, mean_absolute_error: 17.910341, mean_q: 24.189972, mean_eps: 0.100000
 1204638/2000000: episode: 5797, duration: 1.586s, episode steps: 99, steps per second: 62, episode reward: -30.725, mean reward: -0.310 [-100.000, 18.847], mean action: 1.828 [0.000, 3.000], mean observation: 0.061 [-0.912, 1.000], loss: 0.595972, mean_absolute_error: 17.580514, mean_q: 23.844845, mean_eps: 0.100000
 1204739/2000000: episode: 5798, duration: 1.590s, episode steps: 101, steps per second: 64, episode reward: -3.728, mean reward: -0.037 [-100.000, 21.213], mean action: 1.921 [0.000, 3.000], mean observation: -0.000 [-0.849, 1.000], loss: 0.807634, mean_absolute_error: 18.044681, mean_q: 24.419433, mean_eps: 0.100000
 1205739/2000000: episode: 5799, duration: 16.387s, episode steps: 1000, steps per second: 61, episode reward: -34.226, mean reward: -0.034 [-4.354, 4.888], mean action: 1.702 [0.000, 3.000], mean observation: 0.001 [-0.664, 0.942], loss: 0.731307, mean_absolute_error: 18.886144, mean_q: 25.388495, mean_eps: 0.100000
 1206300/2000000: episode: 5800, duration: 8.975s, episode steps: 561, steps per second: 63, episode reward: 234.849, mean reward: 0.419 [-17.986, 100.000], mean action: 0.620 [0.000, 3.000], mean observation: 0.204 [-0.861, 1.000], loss: 0.630091, mean_absolute_error: 18.546059, mean_q: 25.085264, mean_eps: 0.100000
 1206442/2000000: episode: 5801, duration: 2.259s, episode steps: 142, steps per second: 63, episode reward: -12.508, mean reward: -0.088 [-100.000, 11.367], mean action: 1.599 [0.000, 3.000], mean observation: 0.068 [-1.502, 1.000], loss: 0.738753, mean_absolute_error: 18.830729, mean_q: 24.900349, mean_eps: 0.100000
 1206699/2000000: episode: 5802, duration: 4.076s, episode steps: 257, steps per second: 63, episode reward: 222.705, mean reward: 0.867 [-10.222, 100.000], mean action: 1.409 [0.000, 3.000], mean observation: 0.023 [-0.698, 1.000], loss: 0.610285, mean_absolute_error: 18.889144, mean_q: 25.562027, mean_eps: 0.100000
 1206824/2000000: episode: 5803, duration: 1.964s, episode steps: 125, steps per second: 64, episode reward: -29.999, mean reward: -0.240 [-100.000, 14.990], mean action: 1.472 [0.000, 3.000], mean observation: 0.046 [-0.866, 1.000], loss: 0.770716, mean_absolute_error: 18.823758, mean_q: 25.348358, mean_eps: 0.100000
 1207643/2000000: episode: 5804, duration: 14.087s, episode steps: 819, steps per second: 58, episode reward: 127.173, mean reward: 0.155 [-4.466, 100.000], mean action: 1.514 [0.000, 3.000], mean observation: 0.019 [-0.566, 1.000], loss: 0.643550, mean_absolute_error: 18.806980, mean_q: 25.443597, mean_eps: 0.100000
 1207830/2000000: episode: 5805, duration: 2.940s, episode steps: 187, steps per second: 64, episode reward: -354.101, mean reward: -1.894 [-100.000, 5.199], mean action: 1.877 [0.000, 3.000], mean observation: 0.236 [-1.124, 2.065], loss: 0.816551, mean_absolute_error: 18.277253, mean_q: 24.361171, mean_eps: 0.100000
 1208650/2000000: episode: 5806, duration: 13.611s, episode steps: 820, steps per second: 60, episode reward: 226.688, mean reward: 0.276 [-17.997, 100.000], mean action: 1.071 [0.000, 3.000], mean observation: 0.146 [-0.910, 1.394], loss: 0.654540, mean_absolute_error: 18.771449, mean_q: 25.385708, mean_eps: 0.100000
 1209650/2000000: episode: 5807, duration: 17.296s, episode steps: 1000, steps per second: 58, episode reward: -62.243, mean reward: -0.062 [-4.910, 5.182], mean action: 1.646 [0.000, 3.000], mean observation: 0.002 [-0.582, 0.941], loss: 0.729062, mean_absolute_error: 18.809118, mean_q: 25.349935, mean_eps: 0.100000
 1210115/2000000: episode: 5808, duration: 7.702s, episode steps: 465, steps per second: 60, episode reward: 208.375, mean reward: 0.448 [-17.374, 100.000], mean action: 1.280 [0.000, 3.000], mean observation: 0.068 [-0.608, 1.000], loss: 0.694287, mean_absolute_error: 18.471434, mean_q: 24.982583, mean_eps: 0.100000
 1210240/2000000: episode: 5809, duration: 2.006s, episode steps: 125, steps per second: 62, episode reward: -4.503, mean reward: -0.036 [-100.000, 12.263], mean action: 1.928 [0.000, 3.000], mean observation: 0.034 [-0.682, 1.000], loss: 0.592575, mean_absolute_error: 19.514878, mean_q: 26.403697, mean_eps: 0.100000
 1210679/2000000: episode: 5810, duration: 7.107s, episode steps: 439, steps per second: 62, episode reward: 195.068, mean reward: 0.444 [-11.189, 100.000], mean action: 1.674 [0.000, 3.000], mean observation: 0.053 [-0.636, 1.000], loss: 0.667757, mean_absolute_error: 18.846055, mean_q: 25.502837, mean_eps: 0.100000
 1211640/2000000: episode: 5811, duration: 16.823s, episode steps: 961, steps per second: 57, episode reward: 129.691, mean reward: 0.135 [-19.440, 100.000], mean action: 1.490 [0.000, 3.000], mean observation: 0.022 [-0.693, 1.000], loss: 0.769377, mean_absolute_error: 18.874245, mean_q: 25.279984, mean_eps: 0.100000
 1211785/2000000: episode: 5812, duration: 2.316s, episode steps: 145, steps per second: 63, episode reward: -42.425, mean reward: -0.293 [-100.000, 21.693], mean action: 1.690 [0.000, 3.000], mean observation: 0.029 [-1.071, 1.000], loss: 0.768629, mean_absolute_error: 19.135161, mean_q: 25.565226, mean_eps: 0.100000
 1211880/2000000: episode: 5813, duration: 1.502s, episode steps: 95, steps per second: 63, episode reward: -9.569, mean reward: -0.101 [-100.000, 22.250], mean action: 1.274 [0.000, 3.000], mean observation: 0.024 [-1.113, 1.302], loss: 0.625629, mean_absolute_error: 19.084739, mean_q: 25.816869, mean_eps: 0.100000
 1212288/2000000: episode: 5814, duration: 6.585s, episode steps: 408, steps per second: 62, episode reward: -306.749, mean reward: -0.752 [-100.000, 10.966], mean action: 1.936 [0.000, 3.000], mean observation: 0.143 [-2.202, 2.970], loss: 0.698006, mean_absolute_error: 18.488187, mean_q: 24.978367, mean_eps: 0.100000
 1213288/2000000: episode: 5815, duration: 16.137s, episode steps: 1000, steps per second: 62, episode reward: 50.328, mean reward: 0.050 [-19.506, 22.733], mean action: 0.935 [0.000, 3.000], mean observation: 0.170 [-0.715, 1.000], loss: 0.760272, mean_absolute_error: 18.791106, mean_q: 25.324410, mean_eps: 0.100000
 1213967/2000000: episode: 5816, duration: 11.404s, episode steps: 679, steps per second: 60, episode reward: 156.820, mean reward: 0.231 [-11.732, 100.000], mean action: 1.471 [0.000, 3.000], mean observation: 0.041 [-0.649, 1.000], loss: 0.729413, mean_absolute_error: 19.266959, mean_q: 25.422149, mean_eps: 0.100000
 1214202/2000000: episode: 5817, duration: 3.723s, episode steps: 235, steps per second: 63, episode reward: 185.697, mean reward: 0.790 [-3.085, 100.000], mean action: 0.885 [0.000, 3.000], mean observation: 0.126 [-0.923, 1.000], loss: 0.725568, mean_absolute_error: 17.989684, mean_q: 23.984014, mean_eps: 0.100000
 1214544/2000000: episode: 5818, duration: 5.416s, episode steps: 342, steps per second: 63, episode reward: 184.264, mean reward: 0.539 [-17.561, 100.000], mean action: 1.047 [0.000, 3.000], mean observation: 0.157 [-0.761, 1.000], loss: 0.636956, mean_absolute_error: 18.695414, mean_q: 25.168347, mean_eps: 0.100000
 1214912/2000000: episode: 5819, duration: 5.880s, episode steps: 368, steps per second: 63, episode reward: 206.133, mean reward: 0.560 [-19.742, 100.000], mean action: 0.899 [0.000, 3.000], mean observation: 0.123 [-1.164, 1.000], loss: 0.700171, mean_absolute_error: 18.902054, mean_q: 25.359183, mean_eps: 0.100000
 1215912/2000000: episode: 5820, duration: 16.262s, episode steps: 1000, steps per second: 61, episode reward: 74.464, mean reward: 0.074 [-19.428, 22.349], mean action: 0.983 [0.000, 3.000], mean observation: 0.133 [-0.805, 1.000], loss: 0.776133, mean_absolute_error: 18.444143, mean_q: 24.758220, mean_eps: 0.100000
 1216272/2000000: episode: 5821, duration: 5.899s, episode steps: 360, steps per second: 61, episode reward: 224.457, mean reward: 0.623 [-17.843, 100.000], mean action: 1.264 [0.000, 3.000], mean observation: 0.160 [-0.716, 1.000], loss: 0.700517, mean_absolute_error: 18.941392, mean_q: 25.325701, mean_eps: 0.100000
 1216868/2000000: episode: 5822, duration: 10.069s, episode steps: 596, steps per second: 59, episode reward: 155.849, mean reward: 0.261 [-4.019, 100.000], mean action: 1.331 [0.000, 3.000], mean observation: 0.043 [-0.554, 1.000], loss: 0.661097, mean_absolute_error: 19.019635, mean_q: 25.513688, mean_eps: 0.100000
 1217812/2000000: episode: 5823, duration: 16.051s, episode steps: 944, steps per second: 59, episode reward: 169.486, mean reward: 0.180 [-24.172, 100.000], mean action: 1.891 [0.000, 3.000], mean observation: 0.134 [-0.766, 1.012], loss: 0.693397, mean_absolute_error: 18.570295, mean_q: 25.107420, mean_eps: 0.100000
 1218363/2000000: episode: 5824, duration: 9.089s, episode steps: 551, steps per second: 61, episode reward: 209.905, mean reward: 0.381 [-17.579, 100.000], mean action: 1.049 [0.000, 3.000], mean observation: 0.082 [-0.696, 1.000], loss: 0.677409, mean_absolute_error: 19.365399, mean_q: 26.210383, mean_eps: 0.100000
 1218691/2000000: episode: 5825, duration: 5.247s, episode steps: 328, steps per second: 63, episode reward: 233.008, mean reward: 0.710 [-11.626, 100.000], mean action: 1.567 [0.000, 3.000], mean observation: 0.070 [-0.605, 1.000], loss: 0.830558, mean_absolute_error: 18.976850, mean_q: 25.569425, mean_eps: 0.100000
 1218990/2000000: episode: 5826, duration: 4.748s, episode steps: 299, steps per second: 63, episode reward: -22.535, mean reward: -0.075 [-100.000, 10.794], mean action: 1.799 [0.000, 3.000], mean observation: 0.064 [-0.633, 1.000], loss: 0.659598, mean_absolute_error: 18.354800, mean_q: 24.790264, mean_eps: 0.100000
 1219761/2000000: episode: 5827, duration: 12.603s, episode steps: 771, steps per second: 61, episode reward: 226.266, mean reward: 0.293 [-19.199, 100.000], mean action: 1.306 [0.000, 3.000], mean observation: 0.213 [-0.714, 1.000], loss: 0.684132, mean_absolute_error: 18.778335, mean_q: 25.388929, mean_eps: 0.100000
 1220036/2000000: episode: 5828, duration: 4.274s, episode steps: 275, steps per second: 64, episode reward: 221.415, mean reward: 0.805 [-9.150, 100.000], mean action: 0.862 [0.000, 3.000], mean observation: 0.103 [-0.754, 1.000], loss: 0.763447, mean_absolute_error: 19.632865, mean_q: 26.456283, mean_eps: 0.100000
 1220328/2000000: episode: 5829, duration: 4.683s, episode steps: 292, steps per second: 62, episode reward: 210.735, mean reward: 0.722 [-10.532, 100.000], mean action: 1.253 [0.000, 3.000], mean observation: 0.151 [-0.680, 1.000], loss: 0.703908, mean_absolute_error: 19.082731, mean_q: 25.304443, mean_eps: 0.100000
 1221118/2000000: episode: 5830, duration: 13.013s, episode steps: 790, steps per second: 61, episode reward: 154.518, mean reward: 0.196 [-20.807, 100.000], mean action: 0.992 [0.000, 3.000], mean observation: 0.199 [-0.787, 1.000], loss: 0.786335, mean_absolute_error: 18.717192, mean_q: 25.242426, mean_eps: 0.100000
 1221410/2000000: episode: 5831, duration: 4.615s, episode steps: 292, steps per second: 63, episode reward: -589.513, mean reward: -2.019 [-100.000, 5.270], mean action: 1.798 [0.000, 3.000], mean observation: 0.183 [-0.564, 2.476], loss: 0.696391, mean_absolute_error: 19.486226, mean_q: 26.323565, mean_eps: 0.100000
 1221667/2000000: episode: 5832, duration: 4.062s, episode steps: 257, steps per second: 63, episode reward: 225.216, mean reward: 0.876 [-8.968, 100.000], mean action: 1.237 [0.000, 3.000], mean observation: 0.085 [-0.665, 1.000], loss: 0.696307, mean_absolute_error: 18.176074, mean_q: 24.570998, mean_eps: 0.100000
 1221887/2000000: episode: 5833, duration: 3.822s, episode steps: 220, steps per second: 58, episode reward: 202.501, mean reward: 0.920 [-8.028, 100.000], mean action: 1.523 [0.000, 3.000], mean observation: 0.018 [-0.676, 1.000], loss: 0.754120, mean_absolute_error: 18.903427, mean_q: 25.546893, mean_eps: 0.100000
 1222160/2000000: episode: 5834, duration: 4.344s, episode steps: 273, steps per second: 63, episode reward: 202.507, mean reward: 0.742 [-11.438, 100.000], mean action: 0.846 [0.000, 3.000], mean observation: 0.107 [-0.623, 1.000], loss: 0.672913, mean_absolute_error: 18.713617, mean_q: 25.306712, mean_eps: 0.100000
 1222534/2000000: episode: 5835, duration: 5.984s, episode steps: 374, steps per second: 63, episode reward: 199.927, mean reward: 0.535 [-17.511, 100.000], mean action: 1.110 [0.000, 3.000], mean observation: 0.032 [-1.041, 1.000], loss: 0.775910, mean_absolute_error: 19.231294, mean_q: 25.938833, mean_eps: 0.100000
 1222651/2000000: episode: 5836, duration: 1.818s, episode steps: 117, steps per second: 64, episode reward: -30.378, mean reward: -0.260 [-100.000, 14.357], mean action: 1.239 [0.000, 3.000], mean observation: 0.024 [-0.714, 1.000], loss: 0.581199, mean_absolute_error: 18.282757, mean_q: 24.752748, mean_eps: 0.100000
 1222803/2000000: episode: 5837, duration: 2.406s, episode steps: 152, steps per second: 63, episode reward: -3.525, mean reward: -0.023 [-100.000, 22.105], mean action: 1.211 [0.000, 3.000], mean observation: -0.010 [-0.681, 1.000], loss: 0.605954, mean_absolute_error: 19.083884, mean_q: 25.859505, mean_eps: 0.100000
 1223159/2000000: episode: 5838, duration: 5.741s, episode steps: 356, steps per second: 62, episode reward: 212.518, mean reward: 0.597 [-10.840, 100.000], mean action: 1.306 [0.000, 3.000], mean observation: 0.065 [-0.580, 1.000], loss: 0.823030, mean_absolute_error: 19.132546, mean_q: 25.862892, mean_eps: 0.100000
 1223291/2000000: episode: 5839, duration: 2.092s, episode steps: 132, steps per second: 63, episode reward: -668.876, mean reward: -5.067 [-100.000, 2.397], mean action: 2.008 [1.000, 3.000], mean observation: 0.499 [-0.347, 3.442], loss: 0.794501, mean_absolute_error: 18.999571, mean_q: 25.376953, mean_eps: 0.100000
 1223708/2000000: episode: 5840, duration: 6.730s, episode steps: 417, steps per second: 62, episode reward: 204.188, mean reward: 0.490 [-4.518, 100.000], mean action: 1.103 [0.000, 3.000], mean observation: 0.055 [-0.704, 1.000], loss: 0.584698, mean_absolute_error: 18.436621, mean_q: 25.009861, mean_eps: 0.100000
 1223940/2000000: episode: 5841, duration: 3.891s, episode steps: 232, steps per second: 60, episode reward: 253.469, mean reward: 1.093 [-2.667, 100.000], mean action: 1.578 [0.000, 3.000], mean observation: 0.024 [-0.760, 1.000], loss: 0.669223, mean_absolute_error: 18.874570, mean_q: 25.473752, mean_eps: 0.100000
 1224231/2000000: episode: 5842, duration: 4.696s, episode steps: 291, steps per second: 62, episode reward: 211.767, mean reward: 0.728 [-10.736, 100.000], mean action: 1.131 [0.000, 3.000], mean observation: 0.060 [-0.646, 1.000], loss: 0.725024, mean_absolute_error: 19.031962, mean_q: 25.689565, mean_eps: 0.100000
 1224344/2000000: episode: 5843, duration: 1.849s, episode steps: 113, steps per second: 61, episode reward: 0.269, mean reward: 0.002 [-100.000, 11.371], mean action: 1.681 [0.000, 3.000], mean observation: -0.005 [-0.710, 1.000], loss: 0.713303, mean_absolute_error: 19.766079, mean_q: 26.743353, mean_eps: 0.100000
 1224724/2000000: episode: 5844, duration: 6.176s, episode steps: 380, steps per second: 62, episode reward: 223.873, mean reward: 0.589 [-10.270, 100.000], mean action: 1.497 [0.000, 3.000], mean observation: 0.088 [-0.599, 1.000], loss: 0.634291, mean_absolute_error: 18.479098, mean_q: 25.043524, mean_eps: 0.100000
 1225265/2000000: episode: 5845, duration: 8.935s, episode steps: 541, steps per second: 61, episode reward: 193.960, mean reward: 0.359 [-17.427, 100.000], mean action: 1.333 [0.000, 3.000], mean observation: 0.037 [-0.773, 1.000], loss: 0.639528, mean_absolute_error: 19.006123, mean_q: 25.739813, mean_eps: 0.100000
 1225694/2000000: episode: 5846, duration: 6.879s, episode steps: 429, steps per second: 62, episode reward: 179.306, mean reward: 0.418 [-17.415, 100.000], mean action: 1.026 [0.000, 3.000], mean observation: 0.214 [-0.695, 1.000], loss: 0.819507, mean_absolute_error: 18.859516, mean_q: 25.427941, mean_eps: 0.100000
 1226642/2000000: episode: 5847, duration: 16.060s, episode steps: 948, steps per second: 59, episode reward: 130.202, mean reward: 0.137 [-20.701, 100.000], mean action: 1.835 [0.000, 3.000], mean observation: 0.154 [-0.566, 1.000], loss: 0.734185, mean_absolute_error: 19.165850, mean_q: 25.842767, mean_eps: 0.100000
 1226849/2000000: episode: 5848, duration: 3.254s, episode steps: 207, steps per second: 64, episode reward: -111.238, mean reward: -0.537 [-100.000, 11.981], mean action: 1.662 [0.000, 3.000], mean observation: 0.080 [-1.090, 1.105], loss: 0.667861, mean_absolute_error: 19.777042, mean_q: 26.192092, mean_eps: 0.100000
 1227117/2000000: episode: 5849, duration: 4.222s, episode steps: 268, steps per second: 63, episode reward: 226.605, mean reward: 0.846 [-11.622, 100.000], mean action: 1.366 [0.000, 3.000], mean observation: 0.078 [-0.659, 1.000], loss: 0.891416, mean_absolute_error: 19.053525, mean_q: 25.310511, mean_eps: 0.100000
 1227241/2000000: episode: 5850, duration: 1.941s, episode steps: 124, steps per second: 64, episode reward: -33.399, mean reward: -0.269 [-100.000, 13.601], mean action: 1.758 [0.000, 3.000], mean observation: -0.010 [-0.810, 1.179], loss: 0.643946, mean_absolute_error: 20.425189, mean_q: 27.076987, mean_eps: 0.100000
 1227414/2000000: episode: 5851, duration: 2.662s, episode steps: 173, steps per second: 65, episode reward: 2.988, mean reward: 0.017 [-100.000, 20.105], mean action: 1.572 [0.000, 3.000], mean observation: 0.015 [-1.113, 1.010], loss: 0.727322, mean_absolute_error: 19.459941, mean_q: 26.288848, mean_eps: 0.100000
 1227518/2000000: episode: 5852, duration: 1.629s, episode steps: 104, steps per second: 64, episode reward: -3.724, mean reward: -0.036 [-100.000, 18.104], mean action: 1.981 [0.000, 3.000], mean observation: 0.000 [-0.769, 1.000], loss: 0.770792, mean_absolute_error: 18.570153, mean_q: 25.046390, mean_eps: 0.100000
 1227998/2000000: episode: 5853, duration: 7.854s, episode steps: 480, steps per second: 61, episode reward: 203.687, mean reward: 0.424 [-18.380, 100.000], mean action: 1.317 [0.000, 3.000], mean observation: 0.153 [-0.989, 1.000], loss: 0.675260, mean_absolute_error: 19.144328, mean_q: 25.648301, mean_eps: 0.100000
 1228490/2000000: episode: 5854, duration: 8.107s, episode steps: 492, steps per second: 61, episode reward: 164.303, mean reward: 0.334 [-4.306, 100.000], mean action: 1.510 [0.000, 3.000], mean observation: 0.025 [-0.592, 1.000], loss: 0.757395, mean_absolute_error: 19.505838, mean_q: 26.244889, mean_eps: 0.100000
 1228649/2000000: episode: 5855, duration: 2.509s, episode steps: 159, steps per second: 63, episode reward: -60.739, mean reward: -0.382 [-100.000, 24.509], mean action: 1.730 [0.000, 3.000], mean observation: 0.059 [-0.836, 1.022], loss: 0.625399, mean_absolute_error: 18.571010, mean_q: 25.091690, mean_eps: 0.100000
 1228903/2000000: episode: 5856, duration: 3.989s, episode steps: 254, steps per second: 64, episode reward: 197.238, mean reward: 0.777 [-3.264, 100.000], mean action: 1.169 [0.000, 3.000], mean observation: 0.107 [-0.636, 1.000], loss: 0.656918, mean_absolute_error: 18.724179, mean_q: 25.281403, mean_eps: 0.100000
 1229243/2000000: episode: 5857, duration: 5.417s, episode steps: 340, steps per second: 63, episode reward: 225.694, mean reward: 0.664 [-17.438, 100.000], mean action: 0.679 [0.000, 3.000], mean observation: 0.127 [-0.697, 1.000], loss: 0.669742, mean_absolute_error: 18.872594, mean_q: 25.173311, mean_eps: 0.100000
 1229742/2000000: episode: 5858, duration: 8.288s, episode steps: 499, steps per second: 60, episode reward: 188.919, mean reward: 0.379 [-11.867, 100.000], mean action: 1.649 [0.000, 3.000], mean observation: 0.025 [-0.531, 1.000], loss: 0.829712, mean_absolute_error: 19.257470, mean_q: 25.914761, mean_eps: 0.100000
 1230156/2000000: episode: 5859, duration: 6.787s, episode steps: 414, steps per second: 61, episode reward: 196.810, mean reward: 0.475 [-8.554, 100.000], mean action: 1.372 [0.000, 3.000], mean observation: 0.021 [-0.702, 1.000], loss: 0.721647, mean_absolute_error: 18.818868, mean_q: 25.425838, mean_eps: 0.100000
 1230611/2000000: episode: 5860, duration: 7.501s, episode steps: 455, steps per second: 61, episode reward: 236.339, mean reward: 0.519 [-18.872, 100.000], mean action: 1.204 [0.000, 3.000], mean observation: 0.141 [-0.800, 1.000], loss: 0.645898, mean_absolute_error: 18.845905, mean_q: 25.515261, mean_eps: 0.100000
 1230891/2000000: episode: 5861, duration: 4.401s, episode steps: 280, steps per second: 64, episode reward: 229.791, mean reward: 0.821 [-12.854, 100.000], mean action: 1.004 [0.000, 3.000], mean observation: 0.089 [-1.242, 1.000], loss: 0.702155, mean_absolute_error: 18.841410, mean_q: 25.491433, mean_eps: 0.100000
 1231136/2000000: episode: 5862, duration: 3.879s, episode steps: 245, steps per second: 63, episode reward: 209.434, mean reward: 0.855 [-2.886, 100.000], mean action: 1.143 [0.000, 3.000], mean observation: 0.041 [-0.657, 1.000], loss: 0.688576, mean_absolute_error: 18.764526, mean_q: 25.150333, mean_eps: 0.100000
 1231842/2000000: episode: 5863, duration: 12.337s, episode steps: 706, steps per second: 57, episode reward: 189.639, mean reward: 0.269 [-18.512, 100.000], mean action: 1.057 [0.000, 3.000], mean observation: 0.105 [-0.733, 1.000], loss: 0.641017, mean_absolute_error: 19.091636, mean_q: 25.863609, mean_eps: 0.100000
 1232016/2000000: episode: 5864, duration: 2.754s, episode steps: 174, steps per second: 63, episode reward: 10.566, mean reward: 0.061 [-100.000, 33.880], mean action: 1.609 [0.000, 3.000], mean observation: -0.024 [-1.329, 1.007], loss: 0.870112, mean_absolute_error: 18.929696, mean_q: 25.547789, mean_eps: 0.100000
 1232700/2000000: episode: 5865, duration: 11.893s, episode steps: 684, steps per second: 58, episode reward: 144.467, mean reward: 0.211 [-17.441, 100.000], mean action: 1.270 [0.000, 3.000], mean observation: 0.030 [-0.604, 1.000], loss: 0.712077, mean_absolute_error: 19.481428, mean_q: 26.222854, mean_eps: 0.100000
 1232940/2000000: episode: 5866, duration: 3.847s, episode steps: 240, steps per second: 62, episode reward: 213.720, mean reward: 0.891 [-4.649, 100.000], mean action: 1.292 [0.000, 3.000], mean observation: 0.085 [-0.643, 1.000], loss: 0.643773, mean_absolute_error: 18.891092, mean_q: 25.497216, mean_eps: 0.100000
 1233232/2000000: episode: 5867, duration: 4.670s, episode steps: 292, steps per second: 63, episode reward: 200.114, mean reward: 0.685 [-18.662, 100.000], mean action: 0.979 [0.000, 3.000], mean observation: 0.148 [-0.864, 1.000], loss: 0.702108, mean_absolute_error: 18.698173, mean_q: 25.312737, mean_eps: 0.100000
 1233643/2000000: episode: 5868, duration: 6.734s, episode steps: 411, steps per second: 61, episode reward: 230.341, mean reward: 0.560 [-17.401, 100.000], mean action: 1.479 [0.000, 3.000], mean observation: 0.123 [-0.522, 1.000], loss: 0.666195, mean_absolute_error: 18.685727, mean_q: 25.288914, mean_eps: 0.100000
 1234213/2000000: episode: 5869, duration: 9.526s, episode steps: 570, steps per second: 60, episode reward: 214.565, mean reward: 0.376 [-19.905, 100.000], mean action: 1.181 [0.000, 3.000], mean observation: 0.128 [-0.571, 1.000], loss: 0.603918, mean_absolute_error: 18.843850, mean_q: 25.514675, mean_eps: 0.100000
 1234483/2000000: episode: 5870, duration: 4.228s, episode steps: 270, steps per second: 64, episode reward: 184.650, mean reward: 0.684 [-17.598, 100.000], mean action: 1.063 [0.000, 3.000], mean observation: 0.151 [-1.077, 1.000], loss: 0.757136, mean_absolute_error: 18.976942, mean_q: 25.624576, mean_eps: 0.100000
 1234785/2000000: episode: 5871, duration: 4.949s, episode steps: 302, steps per second: 61, episode reward: 243.630, mean reward: 0.807 [-17.645, 100.000], mean action: 1.179 [0.000, 3.000], mean observation: 0.121 [-0.630, 1.000], loss: 0.816880, mean_absolute_error: 18.688854, mean_q: 25.237226, mean_eps: 0.100000
 1235420/2000000: episode: 5872, duration: 10.882s, episode steps: 635, steps per second: 58, episode reward: 155.756, mean reward: 0.245 [-18.769, 100.000], mean action: 1.276 [0.000, 3.000], mean observation: 0.056 [-0.707, 1.000], loss: 0.698766, mean_absolute_error: 18.951038, mean_q: 25.522237, mean_eps: 0.100000
 1235769/2000000: episode: 5873, duration: 5.657s, episode steps: 349, steps per second: 62, episode reward: 211.177, mean reward: 0.605 [-17.875, 100.000], mean action: 1.166 [0.000, 3.000], mean observation: 0.067 [-0.680, 1.000], loss: 0.686042, mean_absolute_error: 19.131729, mean_q: 25.728556, mean_eps: 0.100000
 1236169/2000000: episode: 5874, duration: 6.391s, episode steps: 400, steps per second: 63, episode reward: 220.617, mean reward: 0.552 [-18.306, 100.000], mean action: 0.965 [0.000, 3.000], mean observation: 0.152 [-0.664, 1.000], loss: 0.726723, mean_absolute_error: 18.675495, mean_q: 25.180941, mean_eps: 0.100000
 1236797/2000000: episode: 5875, duration: 10.300s, episode steps: 628, steps per second: 61, episode reward: 147.698, mean reward: 0.235 [-5.431, 100.000], mean action: 1.634 [0.000, 3.000], mean observation: 0.035 [-0.705, 1.000], loss: 0.732120, mean_absolute_error: 18.513145, mean_q: 24.990594, mean_eps: 0.100000
 1237246/2000000: episode: 5876, duration: 7.343s, episode steps: 449, steps per second: 61, episode reward: 187.871, mean reward: 0.418 [-17.766, 100.000], mean action: 1.577 [0.000, 3.000], mean observation: 0.048 [-0.596, 1.000], loss: 0.690882, mean_absolute_error: 19.245402, mean_q: 25.985376, mean_eps: 0.100000
 1237525/2000000: episode: 5877, duration: 4.435s, episode steps: 279, steps per second: 63, episode reward: -18.023, mean reward: -0.065 [-100.000, 22.301], mean action: 1.559 [0.000, 3.000], mean observation: 0.042 [-0.645, 1.000], loss: 0.639823, mean_absolute_error: 19.054009, mean_q: 25.825031, mean_eps: 0.100000
 1238163/2000000: episode: 5878, duration: 10.577s, episode steps: 638, steps per second: 60, episode reward: -188.658, mean reward: -0.296 [-100.000, 19.201], mean action: 1.713 [0.000, 3.000], mean observation: 0.173 [-1.850, 4.127], loss: 0.694370, mean_absolute_error: 18.862378, mean_q: 25.330036, mean_eps: 0.100000
 1238269/2000000: episode: 5879, duration: 1.696s, episode steps: 106, steps per second: 63, episode reward: -67.423, mean reward: -0.636 [-100.000, 10.935], mean action: 1.481 [0.000, 3.000], mean observation: 0.041 [-0.972, 2.873], loss: 0.715736, mean_absolute_error: 19.803483, mean_q: 26.769955, mean_eps: 0.100000
 1238419/2000000: episode: 5880, duration: 2.281s, episode steps: 150, steps per second: 66, episode reward: -10.124, mean reward: -0.067 [-100.000, 13.944], mean action: 1.553 [0.000, 3.000], mean observation: -0.019 [-0.723, 1.000], loss: 1.039860, mean_absolute_error: 19.389282, mean_q: 26.072342, mean_eps: 0.100000
 1238602/2000000: episode: 5881, duration: 3.015s, episode steps: 183, steps per second: 61, episode reward: 209.991, mean reward: 1.147 [-10.853, 100.000], mean action: 1.191 [0.000, 3.000], mean observation: 0.028 [-1.250, 1.000], loss: 0.906668, mean_absolute_error: 20.778167, mean_q: 27.249283, mean_eps: 0.100000
 1239005/2000000: episode: 5882, duration: 6.477s, episode steps: 403, steps per second: 62, episode reward: -78.199, mean reward: -0.194 [-100.000, 18.227], mean action: 1.831 [0.000, 3.000], mean observation: -0.022 [-0.725, 1.000], loss: 0.719477, mean_absolute_error: 18.664204, mean_q: 25.252146, mean_eps: 0.100000
 1239674/2000000: episode: 5883, duration: 11.488s, episode steps: 669, steps per second: 58, episode reward: 170.211, mean reward: 0.254 [-17.604, 100.000], mean action: 1.474 [0.000, 3.000], mean observation: 0.044 [-0.755, 1.000], loss: 0.753124, mean_absolute_error: 18.988998, mean_q: 25.659114, mean_eps: 0.100000
 1240580/2000000: episode: 5884, duration: 15.526s, episode steps: 906, steps per second: 58, episode reward: 147.316, mean reward: 0.163 [-18.183, 100.000], mean action: 1.381 [0.000, 3.000], mean observation: 0.071 [-0.827, 1.005], loss: 0.694882, mean_absolute_error: 19.101659, mean_q: 25.840819, mean_eps: 0.100000
 1240677/2000000: episode: 5885, duration: 1.574s, episode steps: 97, steps per second: 62, episode reward: -417.381, mean reward: -4.303 [-100.000, 1.659], mean action: 1.845 [0.000, 3.000], mean observation: 0.385 [-0.535, 1.909], loss: 0.682160, mean_absolute_error: 19.466652, mean_q: 26.347153, mean_eps: 0.100000
 1240774/2000000: episode: 5886, duration: 1.531s, episode steps: 97, steps per second: 63, episode reward: -431.077, mean reward: -4.444 [-100.000, 2.189], mean action: 1.897 [0.000, 3.000], mean observation: 0.279 [-0.893, 2.428], loss: 0.745980, mean_absolute_error: 18.786525, mean_q: 25.401179, mean_eps: 0.100000
 1241044/2000000: episode: 5887, duration: 4.329s, episode steps: 270, steps per second: 62, episode reward: -33.955, mean reward: -0.126 [-100.000, 13.270], mean action: 1.619 [0.000, 3.000], mean observation: 0.077 [-0.678, 1.000], loss: 0.856145, mean_absolute_error: 18.646585, mean_q: 25.077107, mean_eps: 0.100000
 1241581/2000000: episode: 5888, duration: 8.826s, episode steps: 537, steps per second: 61, episode reward: 211.055, mean reward: 0.393 [-4.144, 100.000], mean action: 1.484 [0.000, 3.000], mean observation: 0.140 [-0.473, 1.105], loss: 0.661552, mean_absolute_error: 19.126105, mean_q: 25.835516, mean_eps: 0.100000
 1241697/2000000: episode: 5889, duration: 1.837s, episode steps: 116, steps per second: 63, episode reward: 10.861, mean reward: 0.094 [-100.000, 12.669], mean action: 1.879 [0.000, 3.000], mean observation: 0.027 [-0.731, 1.000], loss: 0.853262, mean_absolute_error: 19.518227, mean_q: 26.109172, mean_eps: 0.100000
 1242033/2000000: episode: 5890, duration: 5.289s, episode steps: 336, steps per second: 64, episode reward: 199.634, mean reward: 0.594 [-18.744, 100.000], mean action: 1.098 [0.000, 3.000], mean observation: 0.097 [-0.914, 1.000], loss: 0.676414, mean_absolute_error: 19.708838, mean_q: 26.653410, mean_eps: 0.100000
 1242612/2000000: episode: 5891, duration: 9.830s, episode steps: 579, steps per second: 59, episode reward: 164.041, mean reward: 0.283 [-9.777, 100.000], mean action: 1.598 [0.000, 3.000], mean observation: 0.005 [-0.643, 1.000], loss: 0.672294, mean_absolute_error: 19.395968, mean_q: 26.143660, mean_eps: 0.100000
 1243207/2000000: episode: 5892, duration: 9.969s, episode steps: 595, steps per second: 60, episode reward: 175.645, mean reward: 0.295 [-24.081, 100.000], mean action: 1.313 [0.000, 3.000], mean observation: 0.048 [-0.578, 1.000], loss: 0.684018, mean_absolute_error: 18.725411, mean_q: 25.242624, mean_eps: 0.100000
 1243713/2000000: episode: 5893, duration: 8.512s, episode steps: 506, steps per second: 59, episode reward: 144.917, mean reward: 0.286 [-10.815, 100.000], mean action: 1.419 [0.000, 3.000], mean observation: 0.061 [-0.659, 1.000], loss: 0.694544, mean_absolute_error: 19.539971, mean_q: 26.339087, mean_eps: 0.100000
 1244468/2000000: episode: 5894, duration: 12.596s, episode steps: 755, steps per second: 60, episode reward: 121.183, mean reward: 0.161 [-18.259, 100.000], mean action: 1.437 [0.000, 3.000], mean observation: 0.025 [-0.822, 1.000], loss: 0.780005, mean_absolute_error: 19.119966, mean_q: 25.729953, mean_eps: 0.100000
 1244591/2000000: episode: 5895, duration: 1.947s, episode steps: 123, steps per second: 63, episode reward: -543.591, mean reward: -4.419 [-100.000, 1.614], mean action: 1.789 [0.000, 3.000], mean observation: 0.432 [-0.462, 2.712], loss: 0.549221, mean_absolute_error: 19.176358, mean_q: 25.943499, mean_eps: 0.100000
 1244935/2000000: episode: 5896, duration: 5.385s, episode steps: 344, steps per second: 64, episode reward: 234.050, mean reward: 0.680 [-17.189, 100.000], mean action: 0.953 [0.000, 3.000], mean observation: 0.145 [-0.611, 1.000], loss: 0.698027, mean_absolute_error: 19.841950, mean_q: 26.800500, mean_eps: 0.100000
 1245393/2000000: episode: 5897, duration: 7.422s, episode steps: 458, steps per second: 62, episode reward: 199.853, mean reward: 0.436 [-18.365, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.193 [-0.697, 1.000], loss: 0.708485, mean_absolute_error: 19.455851, mean_q: 26.191510, mean_eps: 0.100000
 1245817/2000000: episode: 5898, duration: 6.948s, episode steps: 424, steps per second: 61, episode reward: 192.488, mean reward: 0.454 [-20.103, 100.000], mean action: 1.382 [0.000, 3.000], mean observation: 0.068 [-1.026, 1.012], loss: 0.676404, mean_absolute_error: 19.139010, mean_q: 25.854436, mean_eps: 0.100000
 1246255/2000000: episode: 5899, duration: 7.123s, episode steps: 438, steps per second: 61, episode reward: 161.577, mean reward: 0.369 [-9.112, 100.000], mean action: 1.982 [0.000, 3.000], mean observation: 0.024 [-0.787, 1.051], loss: 0.691613, mean_absolute_error: 19.812882, mean_q: 26.773036, mean_eps: 0.100000
 1246591/2000000: episode: 5900, duration: 5.388s, episode steps: 336, steps per second: 62, episode reward: 191.503, mean reward: 0.570 [-17.398, 100.000], mean action: 1.488 [0.000, 3.000], mean observation: 0.032 [-0.668, 1.000], loss: 0.629439, mean_absolute_error: 19.227398, mean_q: 26.018970, mean_eps: 0.100000
 1246722/2000000: episode: 5901, duration: 2.075s, episode steps: 131, steps per second: 63, episode reward: -11.991, mean reward: -0.092 [-100.000, 11.688], mean action: 1.557 [0.000, 3.000], mean observation: 0.050 [-0.701, 2.115], loss: 0.565267, mean_absolute_error: 20.931703, mean_q: 27.877014, mean_eps: 0.100000
 1246982/2000000: episode: 5902, duration: 4.040s, episode steps: 260, steps per second: 64, episode reward: -0.159, mean reward: -0.001 [-100.000, 10.739], mean action: 1.565 [0.000, 3.000], mean observation: 0.087 [-1.047, 1.027], loss: 0.719480, mean_absolute_error: 20.555368, mean_q: 27.750347, mean_eps: 0.100000
 1247492/2000000: episode: 5903, duration: 8.354s, episode steps: 510, steps per second: 61, episode reward: 217.077, mean reward: 0.426 [-17.900, 100.000], mean action: 0.790 [0.000, 3.000], mean observation: 0.102 [-0.801, 1.000], loss: 0.738993, mean_absolute_error: 19.340845, mean_q: 26.074358, mean_eps: 0.100000
 1248492/2000000: episode: 5904, duration: 16.659s, episode steps: 1000, steps per second: 60, episode reward: -53.121, mean reward: -0.053 [-4.722, 4.595], mean action: 1.496 [0.000, 3.000], mean observation: -0.013 [-0.608, 0.960], loss: 0.731678, mean_absolute_error: 19.172123, mean_q: 25.863243, mean_eps: 0.100000
 1248792/2000000: episode: 5905, duration: 4.799s, episode steps: 300, steps per second: 63, episode reward: 197.557, mean reward: 0.659 [-17.606, 100.000], mean action: 0.913 [0.000, 3.000], mean observation: 0.121 [-0.714, 1.000], loss: 0.684447, mean_absolute_error: 19.726983, mean_q: 26.447580, mean_eps: 0.100000
 1249210/2000000: episode: 5906, duration: 6.781s, episode steps: 418, steps per second: 62, episode reward: 212.539, mean reward: 0.508 [-17.379, 100.000], mean action: 1.261 [0.000, 3.000], mean observation: 0.104 [-0.687, 1.000], loss: 0.623345, mean_absolute_error: 19.453129, mean_q: 26.289368, mean_eps: 0.100000
 1249541/2000000: episode: 5907, duration: 5.270s, episode steps: 331, steps per second: 63, episode reward: 176.624, mean reward: 0.534 [-17.250, 100.000], mean action: 2.027 [0.000, 3.000], mean observation: 0.056 [-0.660, 1.000], loss: 1.044301, mean_absolute_error: 19.439432, mean_q: 25.755383, mean_eps: 0.100000
 1249844/2000000: episode: 5908, duration: 4.783s, episode steps: 303, steps per second: 63, episode reward: 227.910, mean reward: 0.752 [-17.417, 100.000], mean action: 1.020 [0.000, 3.000], mean observation: 0.150 [-0.699, 1.000], loss: 0.578675, mean_absolute_error: 19.459615, mean_q: 26.193198, mean_eps: 0.100000
 1250102/2000000: episode: 5909, duration: 4.097s, episode steps: 258, steps per second: 63, episode reward: 216.126, mean reward: 0.838 [-17.483, 100.000], mean action: 1.229 [0.000, 3.000], mean observation: 0.114 [-1.135, 1.000], loss: 0.663098, mean_absolute_error: 18.888974, mean_q: 25.495376, mean_eps: 0.100000
 1250234/2000000: episode: 5910, duration: 2.065s, episode steps: 132, steps per second: 64, episode reward: -34.615, mean reward: -0.262 [-100.000, 14.532], mean action: 1.856 [0.000, 3.000], mean observation: 0.025 [-1.047, 1.000], loss: 0.671804, mean_absolute_error: 20.466805, mean_q: 27.707662, mean_eps: 0.100000
 1251049/2000000: episode: 5911, duration: 13.669s, episode steps: 815, steps per second: 60, episode reward: 150.429, mean reward: 0.185 [-6.082, 100.000], mean action: 1.701 [0.000, 3.000], mean observation: 0.016 [-0.683, 1.000], loss: 0.685310, mean_absolute_error: 19.637120, mean_q: 26.480048, mean_eps: 0.100000
 1251602/2000000: episode: 5912, duration: 9.490s, episode steps: 553, steps per second: 58, episode reward: 190.653, mean reward: 0.345 [-5.224, 100.000], mean action: 1.627 [0.000, 3.000], mean observation: 0.082 [-0.546, 1.000], loss: 0.762798, mean_absolute_error: 19.916019, mean_q: 26.863661, mean_eps: 0.100000
 1251980/2000000: episode: 5913, duration: 6.033s, episode steps: 378, steps per second: 63, episode reward: 236.230, mean reward: 0.625 [-18.141, 100.000], mean action: 1.138 [0.000, 3.000], mean observation: 0.092 [-0.750, 1.000], loss: 0.646874, mean_absolute_error: 19.028867, mean_q: 25.737421, mean_eps: 0.100000
 1252265/2000000: episode: 5914, duration: 4.619s, episode steps: 285, steps per second: 62, episode reward: 201.993, mean reward: 0.709 [-17.340, 100.000], mean action: 0.965 [0.000, 3.000], mean observation: 0.131 [-1.331, 1.000], loss: 0.589548, mean_absolute_error: 19.735910, mean_q: 26.713308, mean_eps: 0.100000
 1252550/2000000: episode: 5915, duration: 4.579s, episode steps: 285, steps per second: 62, episode reward: -460.489, mean reward: -1.616 [-100.000, 4.762], mean action: 1.937 [0.000, 3.000], mean observation: -0.064 [-1.828, 2.138], loss: 0.833656, mean_absolute_error: 19.447744, mean_q: 26.237085, mean_eps: 0.100000
 1252943/2000000: episode: 5916, duration: 6.305s, episode steps: 393, steps per second: 62, episode reward: -89.326, mean reward: -0.227 [-100.000, 17.413], mean action: 1.896 [0.000, 3.000], mean observation: 0.167 [-1.532, 2.526], loss: 0.768032, mean_absolute_error: 18.933010, mean_q: 25.483953, mean_eps: 0.100000
 1253338/2000000: episode: 5917, duration: 6.625s, episode steps: 395, steps per second: 60, episode reward: 216.961, mean reward: 0.549 [-9.193, 100.000], mean action: 1.689 [0.000, 3.000], mean observation: 0.049 [-0.596, 1.000], loss: 0.716935, mean_absolute_error: 19.152646, mean_q: 25.851647, mean_eps: 0.100000
 1253847/2000000: episode: 5918, duration: 8.754s, episode steps: 509, steps per second: 58, episode reward: 207.787, mean reward: 0.408 [-17.400, 100.000], mean action: 1.297 [0.000, 3.000], mean observation: 0.120 [-0.664, 1.000], loss: 0.596537, mean_absolute_error: 19.336955, mean_q: 26.183630, mean_eps: 0.100000
 1254588/2000000: episode: 5919, duration: 12.589s, episode steps: 741, steps per second: 59, episode reward: 132.594, mean reward: 0.179 [-17.410, 100.000], mean action: 1.382 [0.000, 3.000], mean observation: 0.018 [-0.605, 1.000], loss: 0.681825, mean_absolute_error: 19.493550, mean_q: 26.203062, mean_eps: 0.100000
 1254996/2000000: episode: 5920, duration: 6.995s, episode steps: 408, steps per second: 58, episode reward: 230.530, mean reward: 0.565 [-3.940, 100.000], mean action: 1.659 [0.000, 3.000], mean observation: 0.076 [-0.688, 1.000], loss: 0.705117, mean_absolute_error: 18.399350, mean_q: 24.846017, mean_eps: 0.100000
 1255528/2000000: episode: 5921, duration: 8.581s, episode steps: 532, steps per second: 62, episode reward: 206.612, mean reward: 0.388 [-19.889, 100.000], mean action: 0.688 [0.000, 3.000], mean observation: 0.140 [-0.938, 1.000], loss: 0.602475, mean_absolute_error: 19.097404, mean_q: 25.852128, mean_eps: 0.100000
 1255885/2000000: episode: 5922, duration: 5.838s, episode steps: 357, steps per second: 61, episode reward: 212.977, mean reward: 0.597 [-4.592, 100.000], mean action: 1.720 [0.000, 3.000], mean observation: 0.002 [-0.727, 1.000], loss: 0.699244, mean_absolute_error: 19.856348, mean_q: 26.717419, mean_eps: 0.100000
 1256233/2000000: episode: 5923, duration: 5.668s, episode steps: 348, steps per second: 61, episode reward: 202.859, mean reward: 0.583 [-20.415, 100.000], mean action: 1.049 [0.000, 3.000], mean observation: 0.108 [-0.918, 1.000], loss: 0.720637, mean_absolute_error: 19.069220, mean_q: 25.765825, mean_eps: 0.100000
 1256448/2000000: episode: 5924, duration: 3.440s, episode steps: 215, steps per second: 62, episode reward: 199.449, mean reward: 0.928 [-7.331, 100.000], mean action: 1.405 [0.000, 3.000], mean observation: 0.095 [-0.903, 1.000], loss: 0.646331, mean_absolute_error: 20.304236, mean_q: 27.457272, mean_eps: 0.100000
 1256558/2000000: episode: 5925, duration: 1.773s, episode steps: 110, steps per second: 62, episode reward: -39.840, mean reward: -0.362 [-100.000, 20.056], mean action: 1.755 [0.000, 3.000], mean observation: 0.007 [-0.792, 1.000], loss: 0.704216, mean_absolute_error: 19.497399, mean_q: 26.377649, mean_eps: 0.100000
 1256667/2000000: episode: 5926, duration: 1.707s, episode steps: 109, steps per second: 64, episode reward: 15.009, mean reward: 0.138 [-100.000, 14.466], mean action: 1.972 [0.000, 3.000], mean observation: -0.005 [-0.795, 1.000], loss: 0.600553, mean_absolute_error: 20.223762, mean_q: 27.394829, mean_eps: 0.100000
 1257274/2000000: episode: 5927, duration: 10.121s, episode steps: 607, steps per second: 60, episode reward: 199.346, mean reward: 0.328 [-14.422, 100.000], mean action: 1.359 [0.000, 3.000], mean observation: 0.048 [-0.762, 1.000], loss: 0.724891, mean_absolute_error: 19.750639, mean_q: 26.655644, mean_eps: 0.100000
 1257488/2000000: episode: 5928, duration: 3.420s, episode steps: 214, steps per second: 63, episode reward: -600.467, mean reward: -2.806 [-100.000, 4.067], mean action: 1.813 [0.000, 3.000], mean observation: 0.425 [-1.022, 3.381], loss: 0.598821, mean_absolute_error: 19.724749, mean_q: 26.627470, mean_eps: 0.100000
 1257598/2000000: episode: 5929, duration: 1.797s, episode steps: 110, steps per second: 61, episode reward: 5.047, mean reward: 0.046 [-100.000, 20.189], mean action: 1.964 [0.000, 3.000], mean observation: 0.015 [-0.729, 1.000], loss: 0.515542, mean_absolute_error: 19.881153, mean_q: 26.626366, mean_eps: 0.100000
 1258017/2000000: episode: 5930, duration: 6.803s, episode steps: 419, steps per second: 62, episode reward: 150.484, mean reward: 0.359 [-11.337, 100.000], mean action: 1.594 [0.000, 3.000], mean observation: 0.039 [-0.765, 1.000], loss: 0.772766, mean_absolute_error: 18.876913, mean_q: 25.468632, mean_eps: 0.100000
 1258256/2000000: episode: 5931, duration: 4.280s, episode steps: 239, steps per second: 56, episode reward: -18.233, mean reward: -0.076 [-100.000, 17.113], mean action: 1.774 [0.000, 3.000], mean observation: 0.096 [-1.119, 1.051], loss: 0.641157, mean_absolute_error: 20.085971, mean_q: 27.139884, mean_eps: 0.100000
 1258929/2000000: episode: 5932, duration: 11.172s, episode steps: 673, steps per second: 60, episode reward: 110.963, mean reward: 0.165 [-10.029, 100.000], mean action: 1.942 [0.000, 3.000], mean observation: 0.139 [-1.067, 2.229], loss: 0.753980, mean_absolute_error: 19.762821, mean_q: 26.669109, mean_eps: 0.100000
 1259270/2000000: episode: 5933, duration: 5.481s, episode steps: 341, steps per second: 62, episode reward: 210.143, mean reward: 0.616 [-19.282, 100.000], mean action: 1.370 [0.000, 3.000], mean observation: 0.061 [-0.702, 1.000], loss: 0.703769, mean_absolute_error: 18.697186, mean_q: 25.226598, mean_eps: 0.100000
 1259521/2000000: episode: 5934, duration: 4.050s, episode steps: 251, steps per second: 62, episode reward: 233.576, mean reward: 0.931 [-17.468, 100.000], mean action: 1.223 [0.000, 3.000], mean observation: 0.105 [-0.704, 1.000], loss: 0.975039, mean_absolute_error: 19.675132, mean_q: 26.539258, mean_eps: 0.100000
 1260066/2000000: episode: 5935, duration: 8.959s, episode steps: 545, steps per second: 61, episode reward: 180.004, mean reward: 0.330 [-17.911, 100.000], mean action: 1.312 [0.000, 3.000], mean observation: 0.058 [-0.542, 1.000], loss: 0.673504, mean_absolute_error: 19.307507, mean_q: 26.090523, mean_eps: 0.100000
 1260356/2000000: episode: 5936, duration: 4.612s, episode steps: 290, steps per second: 63, episode reward: 211.198, mean reward: 0.728 [-2.939, 100.000], mean action: 1.393 [0.000, 3.000], mean observation: 0.035 [-0.681, 1.000], loss: 0.835954, mean_absolute_error: 19.539463, mean_q: 26.314295, mean_eps: 0.100000
 1260798/2000000: episode: 5937, duration: 7.442s, episode steps: 442, steps per second: 59, episode reward: 174.879, mean reward: 0.396 [-4.234, 100.000], mean action: 1.554 [0.000, 3.000], mean observation: 0.025 [-0.611, 1.000], loss: 0.684713, mean_absolute_error: 19.868474, mean_q: 26.691211, mean_eps: 0.100000
 1261115/2000000: episode: 5938, duration: 4.954s, episode steps: 317, steps per second: 64, episode reward: 208.604, mean reward: 0.658 [-17.400, 100.000], mean action: 1.196 [0.000, 3.000], mean observation: 0.094 [-0.672, 1.000], loss: 0.931014, mean_absolute_error: 20.552219, mean_q: 27.336023, mean_eps: 0.100000
 1261855/2000000: episode: 5939, duration: 12.358s, episode steps: 740, steps per second: 60, episode reward: 180.973, mean reward: 0.245 [-17.453, 100.000], mean action: 1.147 [0.000, 3.000], mean observation: 0.113 [-0.683, 1.000], loss: 0.676935, mean_absolute_error: 19.783226, mean_q: 26.709320, mean_eps: 0.100000
 1261959/2000000: episode: 5940, duration: 1.650s, episode steps: 104, steps per second: 63, episode reward: -16.189, mean reward: -0.156 [-100.000, 11.883], mean action: 1.894 [0.000, 3.000], mean observation: 0.021 [-0.861, 1.000], loss: 0.744287, mean_absolute_error: 19.373299, mean_q: 25.886153, mean_eps: 0.100000
 1262925/2000000: episode: 5941, duration: 16.520s, episode steps: 966, steps per second: 58, episode reward: 115.734, mean reward: 0.120 [-20.684, 100.000], mean action: 1.324 [0.000, 3.000], mean observation: 0.049 [-0.696, 1.000], loss: 0.664194, mean_absolute_error: 19.784032, mean_q: 26.744573, mean_eps: 0.100000
 1263175/2000000: episode: 5942, duration: 4.001s, episode steps: 250, steps per second: 62, episode reward: -189.144, mean reward: -0.757 [-100.000, 15.543], mean action: 1.896 [0.000, 3.000], mean observation: -0.050 [-1.180, 3.029], loss: 0.857175, mean_absolute_error: 19.092555, mean_q: 25.554432, mean_eps: 0.100000
 1263332/2000000: episode: 5943, duration: 2.521s, episode steps: 157, steps per second: 62, episode reward: -415.958, mean reward: -2.649 [-100.000, 2.211], mean action: 1.924 [0.000, 3.000], mean observation: 0.287 [-0.564, 1.814], loss: 0.772940, mean_absolute_error: 19.976053, mean_q: 26.980829, mean_eps: 0.100000
 1264332/2000000: episode: 5944, duration: 17.361s, episode steps: 1000, steps per second: 58, episode reward: 1.331, mean reward: 0.001 [-18.843, 22.861], mean action: 1.481 [0.000, 3.000], mean observation: 0.032 [-0.738, 1.000], loss: 0.738628, mean_absolute_error: 19.796008, mean_q: 26.693637, mean_eps: 0.100000
 1265332/2000000: episode: 5945, duration: 17.508s, episode steps: 1000, steps per second: 57, episode reward: 12.443, mean reward: 0.012 [-20.576, 20.999], mean action: 1.777 [0.000, 3.000], mean observation: 0.131 [-0.577, 1.000], loss: 0.659017, mean_absolute_error: 19.748920, mean_q: 26.620393, mean_eps: 0.100000
 1265534/2000000: episode: 5946, duration: 3.275s, episode steps: 202, steps per second: 62, episode reward: 213.804, mean reward: 1.058 [-3.040, 100.000], mean action: 1.599 [0.000, 3.000], mean observation: 0.128 [-0.717, 1.000], loss: 0.854158, mean_absolute_error: 20.571359, mean_q: 27.689663, mean_eps: 0.100000
 1266375/2000000: episode: 5947, duration: 14.053s, episode steps: 841, steps per second: 60, episode reward: -623.859, mean reward: -0.742 [-100.000, 4.876], mean action: 1.679 [0.000, 3.000], mean observation: 0.040 [-0.735, 2.074], loss: 0.676656, mean_absolute_error: 19.562507, mean_q: 26.405418, mean_eps: 0.100000
 1266992/2000000: episode: 5948, duration: 10.197s, episode steps: 617, steps per second: 61, episode reward: 186.414, mean reward: 0.302 [-17.454, 100.000], mean action: 1.512 [0.000, 3.000], mean observation: 0.016 [-0.835, 1.000], loss: 0.649227, mean_absolute_error: 19.643389, mean_q: 26.532663, mean_eps: 0.100000
 1267312/2000000: episode: 5949, duration: 5.104s, episode steps: 320, steps per second: 63, episode reward: 208.245, mean reward: 0.651 [-17.902, 100.000], mean action: 0.912 [0.000, 3.000], mean observation: 0.094 [-0.948, 1.000], loss: 0.742749, mean_absolute_error: 20.034249, mean_q: 26.459226, mean_eps: 0.100000
 1268058/2000000: episode: 5950, duration: 13.299s, episode steps: 746, steps per second: 56, episode reward: 146.240, mean reward: 0.196 [-17.923, 100.000], mean action: 1.649 [0.000, 3.000], mean observation: 0.028 [-0.601, 1.000], loss: 0.696972, mean_absolute_error: 20.121394, mean_q: 27.117807, mean_eps: 0.100000
 1268966/2000000: episode: 5951, duration: 16.153s, episode steps: 908, steps per second: 56, episode reward: 107.779, mean reward: 0.119 [-18.929, 100.000], mean action: 1.674 [0.000, 3.000], mean observation: 0.097 [-0.787, 1.000], loss: 0.746193, mean_absolute_error: 20.064029, mean_q: 27.086679, mean_eps: 0.100000
 1269298/2000000: episode: 5952, duration: 5.351s, episode steps: 332, steps per second: 62, episode reward: 213.721, mean reward: 0.644 [-9.027, 100.000], mean action: 1.021 [0.000, 3.000], mean observation: 0.131 [-0.801, 1.000], loss: 0.618134, mean_absolute_error: 19.448274, mean_q: 26.275311, mean_eps: 0.100000
 1269937/2000000: episode: 5953, duration: 10.617s, episode steps: 639, steps per second: 60, episode reward: 157.486, mean reward: 0.246 [-17.584, 100.000], mean action: 1.480 [0.000, 3.000], mean observation: 0.010 [-0.702, 1.000], loss: 0.723851, mean_absolute_error: 19.725669, mean_q: 26.618486, mean_eps: 0.100000
 1270385/2000000: episode: 5954, duration: 7.208s, episode steps: 448, steps per second: 62, episode reward: 200.481, mean reward: 0.448 [-3.933, 100.000], mean action: 1.460 [0.000, 3.000], mean observation: 0.010 [-0.761, 1.000], loss: 0.697357, mean_absolute_error: 19.872569, mean_q: 26.829541, mean_eps: 0.100000
 1270735/2000000: episode: 5955, duration: 5.598s, episode steps: 350, steps per second: 63, episode reward: 199.610, mean reward: 0.570 [-19.671, 100.000], mean action: 1.057 [0.000, 3.000], mean observation: 0.078 [-0.815, 1.000], loss: 0.665819, mean_absolute_error: 20.152851, mean_q: 27.241150, mean_eps: 0.100000
 1270981/2000000: episode: 5956, duration: 3.942s, episode steps: 246, steps per second: 62, episode reward: -11.064, mean reward: -0.045 [-100.000, 20.931], mean action: 1.715 [0.000, 3.000], mean observation: 0.014 [-0.575, 1.000], loss: 0.648445, mean_absolute_error: 20.093431, mean_q: 27.148536, mean_eps: 0.100000
 1271135/2000000: episode: 5957, duration: 2.536s, episode steps: 154, steps per second: 61, episode reward: -58.982, mean reward: -0.383 [-100.000, 17.551], mean action: 1.903 [0.000, 3.000], mean observation: 0.038 [-1.797, 1.000], loss: 0.761064, mean_absolute_error: 20.980124, mean_q: 28.021354, mean_eps: 0.100000
 1271536/2000000: episode: 5958, duration: 6.690s, episode steps: 401, steps per second: 60, episode reward: 218.454, mean reward: 0.545 [-9.397, 100.000], mean action: 1.147 [0.000, 3.000], mean observation: 0.061 [-0.714, 1.000], loss: 0.664038, mean_absolute_error: 19.555857, mean_q: 26.418055, mean_eps: 0.100000
 1271784/2000000: episode: 5959, duration: 3.977s, episode steps: 248, steps per second: 62, episode reward: 219.444, mean reward: 0.885 [-17.685, 100.000], mean action: 1.121 [0.000, 3.000], mean observation: 0.141 [-0.974, 1.000], loss: 0.810733, mean_absolute_error: 19.156059, mean_q: 25.844253, mean_eps: 0.100000
 1272130/2000000: episode: 5960, duration: 5.622s, episode steps: 346, steps per second: 62, episode reward: 216.640, mean reward: 0.626 [-17.445, 100.000], mean action: 1.009 [0.000, 3.000], mean observation: 0.139 [-1.019, 1.000], loss: 0.772736, mean_absolute_error: 19.928732, mean_q: 26.876876, mean_eps: 0.100000
 1272460/2000000: episode: 5961, duration: 5.285s, episode steps: 330, steps per second: 62, episode reward: 231.388, mean reward: 0.701 [-11.701, 100.000], mean action: 1.297 [0.000, 3.000], mean observation: 0.120 [-0.696, 1.000], loss: 0.621763, mean_absolute_error: 19.581769, mean_q: 26.475689, mean_eps: 0.100000
 1272983/2000000: episode: 5962, duration: 8.493s, episode steps: 523, steps per second: 62, episode reward: 222.919, mean reward: 0.426 [-17.916, 100.000], mean action: 1.073 [0.000, 3.000], mean observation: 0.120 [-0.526, 1.000], loss: 0.644019, mean_absolute_error: 19.829222, mean_q: 26.795687, mean_eps: 0.100000
 1273117/2000000: episode: 5963, duration: 2.125s, episode steps: 134, steps per second: 63, episode reward: -20.380, mean reward: -0.152 [-100.000, 14.819], mean action: 1.716 [0.000, 3.000], mean observation: 0.019 [-0.729, 1.402], loss: 0.653524, mean_absolute_error: 19.524203, mean_q: 26.351057, mean_eps: 0.100000
 1273432/2000000: episode: 5964, duration: 4.973s, episode steps: 315, steps per second: 63, episode reward: -11.752, mean reward: -0.037 [-100.000, 13.265], mean action: 1.610 [0.000, 3.000], mean observation: 0.025 [-0.631, 1.000], loss: 0.658447, mean_absolute_error: 20.222306, mean_q: 27.284483, mean_eps: 0.100000
 1274083/2000000: episode: 5965, duration: 10.467s, episode steps: 651, steps per second: 62, episode reward: 189.690, mean reward: 0.291 [-19.372, 100.000], mean action: 1.840 [0.000, 3.000], mean observation: 0.169 [-0.635, 1.000], loss: 0.713243, mean_absolute_error: 19.729717, mean_q: 26.619309, mean_eps: 0.100000
 1274307/2000000: episode: 5966, duration: 3.507s, episode steps: 224, steps per second: 64, episode reward: 218.979, mean reward: 0.978 [-3.199, 100.000], mean action: 1.312 [0.000, 3.000], mean observation: 0.048 [-0.652, 1.000], loss: 0.566353, mean_absolute_error: 20.439330, mean_q: 27.572200, mean_eps: 0.100000
 1274547/2000000: episode: 5967, duration: 3.788s, episode steps: 240, steps per second: 63, episode reward: -1.866, mean reward: -0.008 [-100.000, 16.118], mean action: 1.721 [0.000, 3.000], mean observation: 0.074 [-1.030, 1.000], loss: 0.680205, mean_absolute_error: 19.970502, mean_q: 26.934645, mean_eps: 0.100000
 1274668/2000000: episode: 5968, duration: 1.941s, episode steps: 121, steps per second: 62, episode reward: 8.077, mean reward: 0.067 [-100.000, 17.526], mean action: 1.628 [0.000, 3.000], mean observation: 0.005 [-0.885, 1.000], loss: 0.514092, mean_absolute_error: 19.596697, mean_q: 26.531290, mean_eps: 0.100000
 1275137/2000000: episode: 5969, duration: 7.794s, episode steps: 469, steps per second: 60, episode reward: 199.270, mean reward: 0.425 [-17.777, 100.000], mean action: 1.087 [0.000, 3.000], mean observation: 0.081 [-0.527, 1.000], loss: 0.753572, mean_absolute_error: 19.967672, mean_q: 26.864862, mean_eps: 0.100000
 1275343/2000000: episode: 5970, duration: 3.227s, episode steps: 206, steps per second: 64, episode reward: -59.185, mean reward: -0.287 [-100.000, 17.955], mean action: 1.825 [0.000, 3.000], mean observation: 0.024 [-1.293, 1.000], loss: 0.657295, mean_absolute_error: 19.056403, mean_q: 25.734487, mean_eps: 0.100000
 1275593/2000000: episode: 5971, duration: 3.954s, episode steps: 250, steps per second: 63, episode reward: 199.284, mean reward: 0.797 [-11.606, 100.000], mean action: 1.040 [0.000, 3.000], mean observation: 0.108 [-1.588, 1.000], loss: 0.700608, mean_absolute_error: 20.489097, mean_q: 27.634430, mean_eps: 0.100000
 1276302/2000000: episode: 5972, duration: 12.204s, episode steps: 709, steps per second: 58, episode reward: 217.862, mean reward: 0.307 [-18.388, 100.000], mean action: 0.913 [0.000, 3.000], mean observation: 0.177 [-0.779, 1.289], loss: 0.724977, mean_absolute_error: 20.251284, mean_q: 27.322039, mean_eps: 0.100000
 1276719/2000000: episode: 5973, duration: 6.652s, episode steps: 417, steps per second: 63, episode reward: 195.003, mean reward: 0.468 [-9.070, 100.000], mean action: 1.141 [0.000, 3.000], mean observation: 0.050 [-0.563, 1.000], loss: 0.623646, mean_absolute_error: 19.540201, mean_q: 26.317006, mean_eps: 0.100000
 1276964/2000000: episode: 5974, duration: 3.874s, episode steps: 245, steps per second: 63, episode reward: 206.096, mean reward: 0.841 [-3.128, 100.000], mean action: 1.265 [0.000, 3.000], mean observation: 0.072 [-0.982, 1.000], loss: 0.692651, mean_absolute_error: 20.151580, mean_q: 27.183404, mean_eps: 0.100000
 1277113/2000000: episode: 5975, duration: 2.373s, episode steps: 149, steps per second: 63, episode reward: -76.151, mean reward: -0.511 [-100.000, 23.931], mean action: 1.772 [0.000, 3.000], mean observation: 0.051 [-1.906, 1.000], loss: 0.717810, mean_absolute_error: 19.763754, mean_q: 26.742207, mean_eps: 0.100000
 1278113/2000000: episode: 5976, duration: 16.759s, episode steps: 1000, steps per second: 60, episode reward: 32.516, mean reward: 0.033 [-20.818, 21.908], mean action: 1.430 [0.000, 3.000], mean observation: 0.167 [-1.193, 2.395], loss: 0.798684, mean_absolute_error: 19.930740, mean_q: 26.768104, mean_eps: 0.100000
 1278314/2000000: episode: 5977, duration: 3.127s, episode steps: 201, steps per second: 64, episode reward: -0.450, mean reward: -0.002 [-100.000, 16.898], mean action: 1.572 [0.000, 3.000], mean observation: -0.006 [-0.892, 1.000], loss: 0.773487, mean_absolute_error: 19.914754, mean_q: 26.492640, mean_eps: 0.100000
 1278644/2000000: episode: 5978, duration: 5.320s, episode steps: 330, steps per second: 62, episode reward: 179.238, mean reward: 0.543 [-8.526, 100.000], mean action: 1.527 [0.000, 3.000], mean observation: 0.063 [-0.806, 1.000], loss: 0.759246, mean_absolute_error: 19.426475, mean_q: 26.199348, mean_eps: 0.100000
 1279018/2000000: episode: 5979, duration: 5.988s, episode steps: 374, steps per second: 62, episode reward: 217.940, mean reward: 0.583 [-8.595, 100.000], mean action: 0.842 [0.000, 3.000], mean observation: 0.132 [-1.181, 1.000], loss: 0.704086, mean_absolute_error: 20.066190, mean_q: 27.117614, mean_eps: 0.100000
 1279157/2000000: episode: 5980, duration: 2.191s, episode steps: 139, steps per second: 63, episode reward: -53.512, mean reward: -0.385 [-100.000, 21.476], mean action: 1.547 [0.000, 3.000], mean observation: 0.011 [-0.826, 1.000], loss: 0.504225, mean_absolute_error: 19.297183, mean_q: 26.112326, mean_eps: 0.100000
 1279298/2000000: episode: 5981, duration: 2.233s, episode steps: 141, steps per second: 63, episode reward: -29.217, mean reward: -0.207 [-100.000, 12.356], mean action: 1.709 [0.000, 3.000], mean observation: -0.077 [-1.132, 1.000], loss: 0.690074, mean_absolute_error: 19.803408, mean_q: 26.715526, mean_eps: 0.100000
 1279454/2000000: episode: 5982, duration: 2.448s, episode steps: 156, steps per second: 64, episode reward: 31.923, mean reward: 0.205 [-100.000, 15.109], mean action: 1.859 [0.000, 3.000], mean observation: 0.070 [-0.788, 1.000], loss: 0.680870, mean_absolute_error: 19.611175, mean_q: 26.480095, mean_eps: 0.100000
 1280070/2000000: episode: 5983, duration: 9.919s, episode steps: 616, steps per second: 62, episode reward: 167.100, mean reward: 0.271 [-9.976, 100.000], mean action: 1.541 [0.000, 3.000], mean observation: 0.016 [-0.819, 1.000], loss: 0.786811, mean_absolute_error: 20.079277, mean_q: 26.974572, mean_eps: 0.100000
 1281070/2000000: episode: 5984, duration: 16.554s, episode steps: 1000, steps per second: 60, episode reward: -79.650, mean reward: -0.080 [-7.302, 5.192], mean action: 1.637 [0.000, 3.000], mean observation: 0.098 [-0.845, 1.580], loss: 0.716063, mean_absolute_error: 20.382407, mean_q: 27.504106, mean_eps: 0.100000
 1281314/2000000: episode: 5985, duration: 3.885s, episode steps: 244, steps per second: 63, episode reward: 215.010, mean reward: 0.881 [-12.386, 100.000], mean action: 1.512 [0.000, 3.000], mean observation: 0.051 [-1.069, 1.000], loss: 0.760610, mean_absolute_error: 19.741664, mean_q: 26.646563, mean_eps: 0.100000
 1281670/2000000: episode: 5986, duration: 5.712s, episode steps: 356, steps per second: 62, episode reward: 229.605, mean reward: 0.645 [-7.174, 100.000], mean action: 1.174 [0.000, 3.000], mean observation: 0.104 [-0.612, 1.000], loss: 0.633570, mean_absolute_error: 21.325960, mean_q: 28.684071, mean_eps: 0.100000
 1281808/2000000: episode: 5987, duration: 2.174s, episode steps: 138, steps per second: 63, episode reward: -31.676, mean reward: -0.230 [-100.000, 21.661], mean action: 1.551 [0.000, 3.000], mean observation: 0.022 [-0.705, 1.000], loss: 0.692538, mean_absolute_error: 19.617434, mean_q: 26.450342, mean_eps: 0.100000
 1281953/2000000: episode: 5988, duration: 2.301s, episode steps: 145, steps per second: 63, episode reward: -47.433, mean reward: -0.327 [-100.000, 21.363], mean action: 1.717 [0.000, 3.000], mean observation: 0.021 [-0.749, 1.618], loss: 0.581992, mean_absolute_error: 20.838619, mean_q: 28.100667, mean_eps: 0.100000
 1282247/2000000: episode: 5989, duration: 4.759s, episode steps: 294, steps per second: 62, episode reward: -18.864, mean reward: -0.064 [-100.000, 18.401], mean action: 1.762 [0.000, 3.000], mean observation: 0.073 [-0.549, 1.000], loss: 0.789286, mean_absolute_error: 21.307274, mean_q: 28.634294, mean_eps: 0.100000
 1282621/2000000: episode: 5990, duration: 5.990s, episode steps: 374, steps per second: 62, episode reward: 227.785, mean reward: 0.609 [-24.095, 100.000], mean action: 0.810 [0.000, 3.000], mean observation: 0.112 [-1.221, 1.000], loss: 0.677390, mean_absolute_error: 20.571858, mean_q: 27.779799, mean_eps: 0.100000
 1282898/2000000: episode: 5991, duration: 4.287s, episode steps: 277, steps per second: 65, episode reward: 231.710, mean reward: 0.836 [-12.628, 100.000], mean action: 1.242 [0.000, 3.000], mean observation: 0.046 [-0.714, 1.000], loss: 0.624154, mean_absolute_error: 20.420691, mean_q: 27.587225, mean_eps: 0.100000
 1283132/2000000: episode: 5992, duration: 3.666s, episode steps: 234, steps per second: 64, episode reward: -326.305, mean reward: -1.394 [-100.000, 18.738], mean action: 1.573 [0.000, 3.000], mean observation: 0.054 [-2.068, 1.000], loss: 0.698596, mean_absolute_error: 20.595149, mean_q: 27.707696, mean_eps: 0.100000
 1283295/2000000: episode: 5993, duration: 2.555s, episode steps: 163, steps per second: 64, episode reward: -35.930, mean reward: -0.220 [-100.000, 15.943], mean action: 1.417 [0.000, 3.000], mean observation: -0.006 [-0.647, 1.000], loss: 0.628184, mean_absolute_error: 20.690321, mean_q: 27.202249, mean_eps: 0.100000
 1283660/2000000: episode: 5994, duration: 5.794s, episode steps: 365, steps per second: 63, episode reward: 204.403, mean reward: 0.560 [-18.883, 100.000], mean action: 1.260 [0.000, 3.000], mean observation: 0.091 [-0.687, 1.000], loss: 0.559045, mean_absolute_error: 20.852892, mean_q: 28.150927, mean_eps: 0.100000
 1284142/2000000: episode: 5995, duration: 7.744s, episode steps: 482, steps per second: 62, episode reward: 228.701, mean reward: 0.474 [-17.651, 100.000], mean action: 0.934 [0.000, 3.000], mean observation: 0.143 [-1.339, 1.000], loss: 0.658799, mean_absolute_error: 21.167181, mean_q: 28.489035, mean_eps: 0.100000
 1284394/2000000: episode: 5996, duration: 3.978s, episode steps: 252, steps per second: 63, episode reward: 227.408, mean reward: 0.902 [-9.903, 100.000], mean action: 1.349 [0.000, 3.000], mean observation: 0.058 [-1.091, 1.000], loss: 0.605774, mean_absolute_error: 20.523949, mean_q: 27.685341, mean_eps: 0.100000
 1284766/2000000: episode: 5997, duration: 5.962s, episode steps: 372, steps per second: 62, episode reward: 182.449, mean reward: 0.490 [-4.788, 100.000], mean action: 1.457 [0.000, 3.000], mean observation: 0.061 [-0.770, 1.000], loss: 0.743804, mean_absolute_error: 20.335541, mean_q: 27.382468, mean_eps: 0.100000
 1285006/2000000: episode: 5998, duration: 3.812s, episode steps: 240, steps per second: 63, episode reward: -20.846, mean reward: -0.087 [-100.000, 15.485], mean action: 1.550 [0.000, 3.000], mean observation: 0.052 [-0.760, 1.000], loss: 0.586253, mean_absolute_error: 20.126312, mean_q: 27.175589, mean_eps: 0.100000
 1285494/2000000: episode: 5999, duration: 7.896s, episode steps: 488, steps per second: 62, episode reward: 226.011, mean reward: 0.463 [-17.730, 100.000], mean action: 1.283 [0.000, 3.000], mean observation: 0.141 [-0.757, 1.226], loss: 0.623322, mean_absolute_error: 20.421074, mean_q: 27.550388, mean_eps: 0.100000
 1285804/2000000: episode: 6000, duration: 4.929s, episode steps: 310, steps per second: 63, episode reward: 211.959, mean reward: 0.684 [-17.392, 100.000], mean action: 1.106 [0.000, 3.000], mean observation: 0.114 [-1.249, 1.000], loss: 0.557823, mean_absolute_error: 20.359666, mean_q: 27.527445, mean_eps: 0.100000
 1285940/2000000: episode: 6001, duration: 2.170s, episode steps: 136, steps per second: 63, episode reward: -35.256, mean reward: -0.259 [-100.000, 11.658], mean action: 1.441 [0.000, 3.000], mean observation: -0.086 [-1.240, 1.000], loss: 0.623445, mean_absolute_error: 20.541677, mean_q: 27.775668, mean_eps: 0.100000
 1286088/2000000: episode: 6002, duration: 2.391s, episode steps: 148, steps per second: 62, episode reward: -7.927, mean reward: -0.054 [-100.000, 21.103], mean action: 1.358 [0.000, 3.000], mean observation: 0.030 [-0.803, 1.000], loss: 0.859815, mean_absolute_error: 20.085530, mean_q: 27.013496, mean_eps: 0.100000
 1286235/2000000: episode: 6003, duration: 2.337s, episode steps: 147, steps per second: 63, episode reward: -21.348, mean reward: -0.145 [-100.000, 14.333], mean action: 1.830 [0.000, 3.000], mean observation: 0.000 [-0.916, 1.000], loss: 0.677026, mean_absolute_error: 20.124556, mean_q: 27.193425, mean_eps: 0.100000
 1286526/2000000: episode: 6004, duration: 4.556s, episode steps: 291, steps per second: 64, episode reward: 213.487, mean reward: 0.734 [-10.296, 100.000], mean action: 1.062 [0.000, 3.000], mean observation: 0.125 [-0.661, 1.000], loss: 0.803855, mean_absolute_error: 20.679090, mean_q: 27.885649, mean_eps: 0.100000
 1286735/2000000: episode: 6005, duration: 3.260s, episode steps: 209, steps per second: 64, episode reward: -36.294, mean reward: -0.174 [-100.000, 19.645], mean action: 1.589 [0.000, 3.000], mean observation: 0.030 [-0.792, 1.000], loss: 1.032063, mean_absolute_error: 21.106985, mean_q: 28.336289, mean_eps: 0.100000
 1287083/2000000: episode: 6006, duration: 5.640s, episode steps: 348, steps per second: 62, episode reward: 237.071, mean reward: 0.681 [-17.600, 100.000], mean action: 1.207 [0.000, 3.000], mean observation: 0.106 [-0.709, 1.000], loss: 0.564115, mean_absolute_error: 20.482404, mean_q: 27.685635, mean_eps: 0.100000
 1287469/2000000: episode: 6007, duration: 6.236s, episode steps: 386, steps per second: 62, episode reward: 238.096, mean reward: 0.617 [-18.100, 100.000], mean action: 1.199 [0.000, 3.000], mean observation: 0.076 [-0.940, 1.000], loss: 0.775087, mean_absolute_error: 20.825343, mean_q: 28.061120, mean_eps: 0.100000
 1287806/2000000: episode: 6008, duration: 5.441s, episode steps: 337, steps per second: 62, episode reward: 213.116, mean reward: 0.632 [-7.374, 100.000], mean action: 1.279 [0.000, 3.000], mean observation: 0.091 [-0.709, 1.000], loss: 0.666983, mean_absolute_error: 20.602919, mean_q: 27.792980, mean_eps: 0.100000
 1288526/2000000: episode: 6009, duration: 11.861s, episode steps: 720, steps per second: 61, episode reward: 215.183, mean reward: 0.299 [-19.426, 100.000], mean action: 0.867 [0.000, 3.000], mean observation: 0.114 [-0.738, 1.000], loss: 0.726446, mean_absolute_error: 20.513540, mean_q: 27.622541, mean_eps: 0.100000
 1288954/2000000: episode: 6010, duration: 6.806s, episode steps: 428, steps per second: 63, episode reward: 241.569, mean reward: 0.564 [-17.339, 100.000], mean action: 1.056 [0.000, 3.000], mean observation: 0.162 [-0.847, 1.000], loss: 0.692028, mean_absolute_error: 21.164920, mean_q: 28.443106, mean_eps: 0.100000
 1289319/2000000: episode: 6011, duration: 5.864s, episode steps: 365, steps per second: 62, episode reward: -75.237, mean reward: -0.206 [-100.000, 11.680], mean action: 1.679 [0.000, 3.000], mean observation: -0.009 [-0.547, 1.000], loss: 0.816653, mean_absolute_error: 19.997382, mean_q: 26.928413, mean_eps: 0.100000
 1289551/2000000: episode: 6012, duration: 3.661s, episode steps: 232, steps per second: 63, episode reward: -46.435, mean reward: -0.200 [-100.000, 19.037], mean action: 1.974 [0.000, 3.000], mean observation: -0.016 [-0.711, 1.000], loss: 0.636754, mean_absolute_error: 20.802021, mean_q: 27.975937, mean_eps: 0.100000
 1290551/2000000: episode: 6013, duration: 16.772s, episode steps: 1000, steps per second: 60, episode reward: 37.669, mean reward: 0.038 [-19.772, 23.015], mean action: 1.053 [0.000, 3.000], mean observation: 0.163 [-0.625, 1.000], loss: 0.644769, mean_absolute_error: 20.823487, mean_q: 28.054733, mean_eps: 0.100000
 1291066/2000000: episode: 6014, duration: 8.341s, episode steps: 515, steps per second: 62, episode reward: 146.126, mean reward: 0.284 [-17.883, 100.000], mean action: 1.798 [0.000, 3.000], mean observation: 0.009 [-0.714, 1.000], loss: 0.746945, mean_absolute_error: 20.433121, mean_q: 27.575564, mean_eps: 0.100000
 1291328/2000000: episode: 6015, duration: 4.166s, episode steps: 262, steps per second: 63, episode reward: -435.346, mean reward: -1.662 [-100.000, 4.653], mean action: 1.691 [0.000, 3.000], mean observation: 0.019 [-2.330, 1.055], loss: 0.763604, mean_absolute_error: 21.208384, mean_q: 28.252445, mean_eps: 0.100000
 1291447/2000000: episode: 6016, duration: 1.890s, episode steps: 119, steps per second: 63, episode reward: -63.658, mean reward: -0.535 [-100.000, 12.425], mean action: 1.613 [0.000, 3.000], mean observation: 0.041 [-1.515, 1.000], loss: 0.610096, mean_absolute_error: 19.396066, mean_q: 26.206875, mean_eps: 0.100000
 1291752/2000000: episode: 6017, duration: 4.870s, episode steps: 305, steps per second: 63, episode reward: 199.544, mean reward: 0.654 [-9.210, 100.000], mean action: 1.259 [0.000, 3.000], mean observation: 0.085 [-0.823, 1.000], loss: 0.790776, mean_absolute_error: 21.486995, mean_q: 28.661152, mean_eps: 0.100000
 1292137/2000000: episode: 6018, duration: 6.268s, episode steps: 385, steps per second: 61, episode reward: 209.517, mean reward: 0.544 [-9.940, 100.000], mean action: 1.208 [0.000, 3.000], mean observation: 0.066 [-0.558, 1.003], loss: 0.605161, mean_absolute_error: 20.136966, mean_q: 27.219197, mean_eps: 0.100000
 1292438/2000000: episode: 6019, duration: 4.764s, episode steps: 301, steps per second: 63, episode reward: 212.249, mean reward: 0.705 [-6.522, 100.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.144 [-0.877, 1.000], loss: 0.854189, mean_absolute_error: 20.599218, mean_q: 27.654519, mean_eps: 0.100000
 1292833/2000000: episode: 6020, duration: 6.434s, episode steps: 395, steps per second: 61, episode reward: 185.355, mean reward: 0.469 [-9.914, 100.000], mean action: 1.210 [0.000, 3.000], mean observation: 0.044 [-0.656, 1.000], loss: 0.659100, mean_absolute_error: 21.368964, mean_q: 28.675024, mean_eps: 0.100000
 1293044/2000000: episode: 6021, duration: 3.333s, episode steps: 211, steps per second: 63, episode reward: 184.496, mean reward: 0.874 [-3.246, 100.000], mean action: 1.299 [0.000, 3.000], mean observation: 0.041 [-0.769, 1.000], loss: 0.542182, mean_absolute_error: 21.018833, mean_q: 28.247436, mean_eps: 0.100000
 1293229/2000000: episode: 6022, duration: 2.913s, episode steps: 185, steps per second: 64, episode reward: -34.977, mean reward: -0.189 [-100.000, 7.839], mean action: 1.259 [0.000, 3.000], mean observation: 0.068 [-1.211, 1.000], loss: 0.668518, mean_absolute_error: 21.856741, mean_q: 29.508674, mean_eps: 0.100000
 1293358/2000000: episode: 6023, duration: 2.026s, episode steps: 129, steps per second: 64, episode reward: -23.931, mean reward: -0.186 [-100.000, 13.797], mean action: 1.899 [0.000, 3.000], mean observation: 0.082 [-1.437, 1.000], loss: 0.822709, mean_absolute_error: 20.264799, mean_q: 27.187933, mean_eps: 0.100000
 1293719/2000000: episode: 6024, duration: 5.745s, episode steps: 361, steps per second: 63, episode reward: 189.434, mean reward: 0.525 [-18.420, 100.000], mean action: 0.972 [0.000, 3.000], mean observation: 0.129 [-0.791, 1.000], loss: 0.820871, mean_absolute_error: 21.147089, mean_q: 28.493873, mean_eps: 0.100000
 1293910/2000000: episode: 6025, duration: 2.997s, episode steps: 191, steps per second: 64, episode reward: -5.997, mean reward: -0.031 [-100.000, 23.643], mean action: 1.691 [0.000, 3.000], mean observation: 0.023 [-1.385, 1.000], loss: 0.547725, mean_absolute_error: 21.114352, mean_q: 28.514684, mean_eps: 0.100000
 1294183/2000000: episode: 6026, duration: 4.454s, episode steps: 273, steps per second: 61, episode reward: 219.640, mean reward: 0.805 [-12.021, 100.000], mean action: 1.066 [0.000, 3.000], mean observation: 0.083 [-0.812, 1.000], loss: 0.582158, mean_absolute_error: 20.550643, mean_q: 27.705354, mean_eps: 0.100000
 1294349/2000000: episode: 6027, duration: 2.878s, episode steps: 166, steps per second: 58, episode reward: -34.637, mean reward: -0.209 [-100.000, 14.773], mean action: 1.542 [0.000, 3.000], mean observation: 0.030 [-0.804, 1.000], loss: 0.919276, mean_absolute_error: 20.521890, mean_q: 27.573477, mean_eps: 0.100000
 1294799/2000000: episode: 6028, duration: 7.369s, episode steps: 450, steps per second: 61, episode reward: 163.094, mean reward: 0.362 [-18.828, 100.000], mean action: 1.276 [0.000, 3.000], mean observation: 0.041 [-0.624, 1.000], loss: 0.888968, mean_absolute_error: 20.056183, mean_q: 26.983085, mean_eps: 0.100000
 1295151/2000000: episode: 6029, duration: 5.597s, episode steps: 352, steps per second: 63, episode reward: -119.364, mean reward: -0.339 [-100.000, 19.104], mean action: 1.173 [0.000, 3.000], mean observation: 0.149 [-0.795, 1.000], loss: 0.624881, mean_absolute_error: 20.793085, mean_q: 28.046193, mean_eps: 0.100000
 1295491/2000000: episode: 6030, duration: 5.473s, episode steps: 340, steps per second: 62, episode reward: -39.294, mean reward: -0.116 [-100.000, 11.134], mean action: 1.841 [0.000, 3.000], mean observation: -0.019 [-0.637, 1.000], loss: 0.703621, mean_absolute_error: 21.133700, mean_q: 28.428860, mean_eps: 0.100000
 1295804/2000000: episode: 6031, duration: 4.997s, episode steps: 313, steps per second: 63, episode reward: 157.061, mean reward: 0.502 [-19.690, 100.000], mean action: 2.217 [0.000, 3.000], mean observation: 0.161 [-0.773, 1.000], loss: 0.765294, mean_absolute_error: 21.817184, mean_q: 29.394837, mean_eps: 0.100000
 1296238/2000000: episode: 6032, duration: 7.040s, episode steps: 434, steps per second: 62, episode reward: 242.264, mean reward: 0.558 [-3.377, 100.000], mean action: 1.051 [0.000, 3.000], mean observation: 0.084 [-0.719, 1.000], loss: 0.764712, mean_absolute_error: 21.193480, mean_q: 28.576192, mean_eps: 0.100000
 1296560/2000000: episode: 6033, duration: 5.146s, episode steps: 322, steps per second: 63, episode reward: -106.436, mean reward: -0.331 [-100.000, 14.081], mean action: 1.829 [0.000, 3.000], mean observation: -0.002 [-1.097, 1.000], loss: 0.641102, mean_absolute_error: 21.352855, mean_q: 28.799176, mean_eps: 0.100000
 1297334/2000000: episode: 6034, duration: 12.543s, episode steps: 774, steps per second: 62, episode reward: 190.569, mean reward: 0.246 [-24.824, 100.000], mean action: 0.904 [0.000, 3.000], mean observation: 0.107 [-0.793, 1.000], loss: 0.687414, mean_absolute_error: 20.779528, mean_q: 27.931884, mean_eps: 0.100000
 1297804/2000000: episode: 6035, duration: 7.619s, episode steps: 470, steps per second: 62, episode reward: 165.648, mean reward: 0.352 [-19.502, 100.000], mean action: 1.004 [0.000, 3.000], mean observation: 0.087 [-0.654, 1.000], loss: 0.597273, mean_absolute_error: 20.545126, mean_q: 27.737810, mean_eps: 0.100000
 1298159/2000000: episode: 6036, duration: 5.679s, episode steps: 355, steps per second: 63, episode reward: 244.562, mean reward: 0.689 [-19.245, 100.000], mean action: 1.065 [0.000, 3.000], mean observation: 0.091 [-1.019, 1.000], loss: 0.613521, mean_absolute_error: 20.653321, mean_q: 27.834723, mean_eps: 0.100000
 1298403/2000000: episode: 6037, duration: 3.843s, episode steps: 244, steps per second: 63, episode reward: 185.648, mean reward: 0.761 [-9.324, 100.000], mean action: 1.254 [0.000, 3.000], mean observation: 0.127 [-1.017, 1.000], loss: 0.811016, mean_absolute_error: 21.226288, mean_q: 28.612978, mean_eps: 0.100000
 1298635/2000000: episode: 6038, duration: 3.677s, episode steps: 232, steps per second: 63, episode reward: 7.307, mean reward: 0.031 [-100.000, 19.330], mean action: 1.797 [0.000, 3.000], mean observation: -0.002 [-0.583, 1.000], loss: 0.602968, mean_absolute_error: 20.738573, mean_q: 28.030147, mean_eps: 0.100000
 1299635/2000000: episode: 6039, duration: 16.602s, episode steps: 1000, steps per second: 60, episode reward: 52.814, mean reward: 0.053 [-20.881, 22.439], mean action: 1.886 [0.000, 3.000], mean observation: 0.152 [-0.733, 1.000], loss: 0.637185, mean_absolute_error: 20.319931, mean_q: 27.451781, mean_eps: 0.100000
 1300159/2000000: episode: 6040, duration: 8.486s, episode steps: 524, steps per second: 62, episode reward: 126.484, mean reward: 0.241 [-11.116, 100.000], mean action: 1.511 [0.000, 3.000], mean observation: 0.059 [-0.733, 1.000], loss: 0.684215, mean_absolute_error: 21.378633, mean_q: 28.767857, mean_eps: 0.100000
 1300507/2000000: episode: 6041, duration: 5.519s, episode steps: 348, steps per second: 63, episode reward: 211.062, mean reward: 0.607 [-9.376, 100.000], mean action: 1.422 [0.000, 3.000], mean observation: 0.033 [-0.619, 1.000], loss: 0.585133, mean_absolute_error: 21.427066, mean_q: 28.861397, mean_eps: 0.100000
 1300830/2000000: episode: 6042, duration: 5.271s, episode steps: 323, steps per second: 61, episode reward: 180.466, mean reward: 0.559 [-3.200, 100.000], mean action: 1.365 [0.000, 3.000], mean observation: 0.011 [-0.526, 1.000], loss: 0.615279, mean_absolute_error: 20.829943, mean_q: 28.096055, mean_eps: 0.100000
 1301397/2000000: episode: 6043, duration: 9.342s, episode steps: 567, steps per second: 61, episode reward: 201.645, mean reward: 0.356 [-24.666, 100.000], mean action: 1.129 [0.000, 3.000], mean observation: 0.117 [-0.446, 1.000], loss: 0.693627, mean_absolute_error: 20.542868, mean_q: 27.719662, mean_eps: 0.100000
 1301728/2000000: episode: 6044, duration: 5.264s, episode steps: 331, steps per second: 63, episode reward: 194.769, mean reward: 0.588 [-17.349, 100.000], mean action: 0.912 [0.000, 3.000], mean observation: 0.105 [-1.028, 1.000], loss: 0.707217, mean_absolute_error: 21.370914, mean_q: 28.788405, mean_eps: 0.100000
 1302208/2000000: episode: 6045, duration: 7.779s, episode steps: 480, steps per second: 62, episode reward: 209.298, mean reward: 0.436 [-24.271, 100.000], mean action: 1.352 [0.000, 3.000], mean observation: 0.130 [-1.227, 1.000], loss: 0.640134, mean_absolute_error: 20.312025, mean_q: 27.393315, mean_eps: 0.100000
 1302690/2000000: episode: 6046, duration: 7.854s, episode steps: 482, steps per second: 61, episode reward: 200.108, mean reward: 0.415 [-9.954, 100.000], mean action: 1.201 [0.000, 3.000], mean observation: 0.102 [-0.750, 1.000], loss: 0.609100, mean_absolute_error: 20.708437, mean_q: 27.924667, mean_eps: 0.100000
 1302966/2000000: episode: 6047, duration: 4.402s, episode steps: 276, steps per second: 63, episode reward: 194.434, mean reward: 0.704 [-8.394, 100.000], mean action: 1.399 [0.000, 3.000], mean observation: 0.038 [-0.558, 1.000], loss: 0.617953, mean_absolute_error: 21.304726, mean_q: 28.719816, mean_eps: 0.100000
 1303105/2000000: episode: 6048, duration: 2.177s, episode steps: 139, steps per second: 64, episode reward: -11.776, mean reward: -0.085 [-100.000, 21.336], mean action: 1.784 [0.000, 3.000], mean observation: 0.001 [-1.407, 1.000], loss: 0.667654, mean_absolute_error: 21.070434, mean_q: 28.396421, mean_eps: 0.100000
 1303356/2000000: episode: 6049, duration: 3.936s, episode steps: 251, steps per second: 64, episode reward: 9.024, mean reward: 0.036 [-100.000, 18.661], mean action: 1.378 [0.000, 3.000], mean observation: 0.031 [-1.268, 1.018], loss: 0.793450, mean_absolute_error: 21.110027, mean_q: 28.390115, mean_eps: 0.100000
 1303687/2000000: episode: 6050, duration: 5.279s, episode steps: 331, steps per second: 63, episode reward: 192.297, mean reward: 0.581 [-11.074, 100.000], mean action: 1.012 [0.000, 3.000], mean observation: 0.105 [-1.162, 1.000], loss: 0.607283, mean_absolute_error: 20.922805, mean_q: 28.238316, mean_eps: 0.100000
 1304687/2000000: episode: 6051, duration: 16.378s, episode steps: 1000, steps per second: 61, episode reward: 74.782, mean reward: 0.075 [-24.669, 12.034], mean action: 1.483 [0.000, 3.000], mean observation: 0.178 [-0.757, 1.000], loss: 0.732372, mean_absolute_error: 20.911494, mean_q: 28.177298, mean_eps: 0.100000
 1305058/2000000: episode: 6052, duration: 5.914s, episode steps: 371, steps per second: 63, episode reward: 217.470, mean reward: 0.586 [-3.445, 100.000], mean action: 1.555 [0.000, 3.000], mean observation: 0.031 [-0.687, 1.000], loss: 0.823631, mean_absolute_error: 20.698198, mean_q: 27.755459, mean_eps: 0.100000
 1305498/2000000: episode: 6053, duration: 7.120s, episode steps: 440, steps per second: 62, episode reward: 210.112, mean reward: 0.478 [-9.614, 100.000], mean action: 1.127 [0.000, 3.000], mean observation: 0.115 [-0.515, 1.000], loss: 0.613050, mean_absolute_error: 20.946610, mean_q: 28.218308, mean_eps: 0.100000
 1305686/2000000: episode: 6054, duration: 2.912s, episode steps: 188, steps per second: 65, episode reward: -41.009, mean reward: -0.218 [-100.000, 11.042], mean action: 1.644 [0.000, 3.000], mean observation: 0.085 [-1.537, 1.000], loss: 1.042104, mean_absolute_error: 21.235433, mean_q: 28.516774, mean_eps: 0.100000
 1305972/2000000: episode: 6055, duration: 4.550s, episode steps: 286, steps per second: 63, episode reward: 30.526, mean reward: 0.107 [-100.000, 16.963], mean action: 1.804 [0.000, 3.000], mean observation: 0.061 [-0.579, 1.000], loss: 0.685675, mean_absolute_error: 21.178117, mean_q: 28.556505, mean_eps: 0.100000
 1306136/2000000: episode: 6056, duration: 2.623s, episode steps: 164, steps per second: 63, episode reward: 14.158, mean reward: 0.086 [-100.000, 14.062], mean action: 1.787 [0.000, 3.000], mean observation: 0.058 [-0.788, 1.000], loss: 0.747142, mean_absolute_error: 20.812043, mean_q: 28.051400, mean_eps: 0.100000
 1306436/2000000: episode: 6057, duration: 4.800s, episode steps: 300, steps per second: 62, episode reward: 181.737, mean reward: 0.606 [-2.986, 100.000], mean action: 1.433 [0.000, 3.000], mean observation: 0.059 [-0.634, 1.000], loss: 0.768142, mean_absolute_error: 20.299140, mean_q: 27.345655, mean_eps: 0.100000
 1306560/2000000: episode: 6058, duration: 2.060s, episode steps: 124, steps per second: 60, episode reward: 13.072, mean reward: 0.105 [-100.000, 20.308], mean action: 2.016 [0.000, 3.000], mean observation: 0.082 [-0.714, 1.246], loss: 0.777238, mean_absolute_error: 20.821964, mean_q: 28.042118, mean_eps: 0.100000
 1306885/2000000: episode: 6059, duration: 5.186s, episode steps: 325, steps per second: 63, episode reward: 238.280, mean reward: 0.733 [-17.435, 100.000], mean action: 0.732 [0.000, 3.000], mean observation: 0.162 [-0.803, 1.000], loss: 0.728372, mean_absolute_error: 22.065367, mean_q: 29.641243, mean_eps: 0.100000
 1307210/2000000: episode: 6060, duration: 5.165s, episode steps: 325, steps per second: 63, episode reward: 213.129, mean reward: 0.656 [-17.440, 100.000], mean action: 0.978 [0.000, 3.000], mean observation: 0.112 [-0.898, 1.000], loss: 0.585930, mean_absolute_error: 20.697196, mean_q: 27.892949, mean_eps: 0.100000
 1307444/2000000: episode: 6061, duration: 3.702s, episode steps: 234, steps per second: 63, episode reward: 232.213, mean reward: 0.992 [-8.371, 100.000], mean action: 1.624 [0.000, 3.000], mean observation: 0.095 [-0.676, 1.000], loss: 0.672516, mean_absolute_error: 21.165071, mean_q: 28.528966, mean_eps: 0.100000
 1307793/2000000: episode: 6062, duration: 5.610s, episode steps: 349, steps per second: 62, episode reward: 228.914, mean reward: 0.656 [-9.736, 100.000], mean action: 1.080 [0.000, 3.000], mean observation: 0.091 [-0.599, 1.000], loss: 0.759207, mean_absolute_error: 21.113873, mean_q: 28.377941, mean_eps: 0.100000
 1308122/2000000: episode: 6063, duration: 5.176s, episode steps: 329, steps per second: 64, episode reward: 219.851, mean reward: 0.668 [-17.976, 100.000], mean action: 1.258 [0.000, 3.000], mean observation: 0.090 [-0.818, 1.000], loss: 0.612210, mean_absolute_error: 20.890604, mean_q: 28.083943, mean_eps: 0.100000
 1308257/2000000: episode: 6064, duration: 2.308s, episode steps: 135, steps per second: 59, episode reward: -10.948, mean reward: -0.081 [-100.000, 14.383], mean action: 1.919 [0.000, 3.000], mean observation: 0.076 [-0.702, 1.000], loss: 0.807066, mean_absolute_error: 22.010915, mean_q: 29.630758, mean_eps: 0.100000
 1308569/2000000: episode: 6065, duration: 4.971s, episode steps: 312, steps per second: 63, episode reward: 209.474, mean reward: 0.671 [-9.355, 100.000], mean action: 1.359 [0.000, 3.000], mean observation: 0.100 [-0.734, 1.000], loss: 0.650303, mean_absolute_error: 20.797765, mean_q: 28.050173, mean_eps: 0.100000
 1308987/2000000: episode: 6066, duration: 6.697s, episode steps: 418, steps per second: 62, episode reward: 172.162, mean reward: 0.412 [-20.101, 100.000], mean action: 1.378 [0.000, 3.000], mean observation: 0.091 [-0.872, 1.000], loss: 0.683884, mean_absolute_error: 21.631842, mean_q: 29.124015, mean_eps: 0.100000
 1309150/2000000: episode: 6067, duration: 2.582s, episode steps: 163, steps per second: 63, episode reward: -51.586, mean reward: -0.316 [-100.000, 12.814], mean action: 1.650 [0.000, 3.000], mean observation: 0.060 [-1.481, 1.000], loss: 0.861963, mean_absolute_error: 21.152411, mean_q: 28.414853, mean_eps: 0.100000
 1309270/2000000: episode: 6068, duration: 1.837s, episode steps: 120, steps per second: 65, episode reward: -29.356, mean reward: -0.245 [-100.000, 17.567], mean action: 1.350 [0.000, 3.000], mean observation: -0.012 [-0.770, 1.476], loss: 0.554746, mean_absolute_error: 20.442642, mean_q: 27.577415, mean_eps: 0.100000
 1309645/2000000: episode: 6069, duration: 5.973s, episode steps: 375, steps per second: 63, episode reward: 231.335, mean reward: 0.617 [-19.117, 100.000], mean action: 1.096 [0.000, 3.000], mean observation: 0.132 [-0.800, 1.015], loss: 0.622767, mean_absolute_error: 21.133646, mean_q: 28.350051, mean_eps: 0.100000
 1310225/2000000: episode: 6070, duration: 9.634s, episode steps: 580, steps per second: 60, episode reward: 159.705, mean reward: 0.275 [-19.580, 100.000], mean action: 1.469 [0.000, 3.000], mean observation: 0.103 [-0.602, 1.000], loss: 0.602041, mean_absolute_error: 20.925109, mean_q: 28.167398, mean_eps: 0.100000
 1310326/2000000: episode: 6071, duration: 1.572s, episode steps: 101, steps per second: 64, episode reward: -34.835, mean reward: -0.345 [-100.000, 17.543], mean action: 1.832 [0.000, 3.000], mean observation: 0.026 [-1.002, 1.000], loss: 0.701651, mean_absolute_error: 19.911448, mean_q: 26.824747, mean_eps: 0.100000
 1310730/2000000: episode: 6072, duration: 6.590s, episode steps: 404, steps per second: 61, episode reward: 190.937, mean reward: 0.473 [-18.463, 100.000], mean action: 1.030 [0.000, 3.000], mean observation: 0.128 [-0.697, 1.000], loss: 0.602519, mean_absolute_error: 20.507282, mean_q: 27.677340, mean_eps: 0.100000
 1311007/2000000: episode: 6073, duration: 4.447s, episode steps: 277, steps per second: 62, episode reward: 197.487, mean reward: 0.713 [-21.653, 100.000], mean action: 1.209 [0.000, 3.000], mean observation: 0.090 [-0.680, 1.000], loss: 0.654843, mean_absolute_error: 20.877590, mean_q: 28.146781, mean_eps: 0.100000
 1311413/2000000: episode: 6074, duration: 6.515s, episode steps: 406, steps per second: 62, episode reward: 190.185, mean reward: 0.468 [-20.140, 100.000], mean action: 1.079 [0.000, 3.000], mean observation: 0.070 [-0.593, 1.000], loss: 0.611225, mean_absolute_error: 20.537314, mean_q: 27.713490, mean_eps: 0.100000
 1311654/2000000: episode: 6075, duration: 3.836s, episode steps: 241, steps per second: 63, episode reward: 205.564, mean reward: 0.853 [-3.779, 100.000], mean action: 1.307 [0.000, 3.000], mean observation: 0.058 [-0.532, 1.000], loss: 0.839589, mean_absolute_error: 21.026024, mean_q: 28.258594, mean_eps: 0.100000
 1311880/2000000: episode: 6076, duration: 3.756s, episode steps: 226, steps per second: 60, episode reward: -11.469, mean reward: -0.051 [-100.000, 16.823], mean action: 1.708 [0.000, 3.000], mean observation: 0.012 [-0.658, 1.000], loss: 0.794293, mean_absolute_error: 21.281759, mean_q: 28.041778, mean_eps: 0.100000
 1312880/2000000: episode: 6077, duration: 16.901s, episode steps: 1000, steps per second: 59, episode reward: 110.055, mean reward: 0.110 [-20.412, 12.113], mean action: 1.048 [0.000, 3.000], mean observation: 0.166 [-0.738, 1.000], loss: 0.649302, mean_absolute_error: 20.983021, mean_q: 28.285765, mean_eps: 0.100000
 1312998/2000000: episode: 6078, duration: 1.877s, episode steps: 118, steps per second: 63, episode reward: -75.173, mean reward: -0.637 [-100.000, 12.233], mean action: 1.864 [0.000, 3.000], mean observation: 0.058 [-1.298, 1.000], loss: 0.575343, mean_absolute_error: 20.410267, mean_q: 27.561741, mean_eps: 0.100000
 1313363/2000000: episode: 6079, duration: 6.348s, episode steps: 365, steps per second: 57, episode reward: 206.151, mean reward: 0.565 [-20.798, 100.000], mean action: 1.093 [0.000, 3.000], mean observation: 0.104 [-0.629, 1.000], loss: 0.645886, mean_absolute_error: 20.419380, mean_q: 27.515185, mean_eps: 0.100000
 1313794/2000000: episode: 6080, duration: 6.921s, episode steps: 431, steps per second: 62, episode reward: 193.987, mean reward: 0.450 [-11.521, 100.000], mean action: 1.081 [0.000, 3.000], mean observation: 0.168 [-1.020, 1.000], loss: 0.590461, mean_absolute_error: 20.738285, mean_q: 27.998028, mean_eps: 0.100000
 1313891/2000000: episode: 6081, duration: 1.517s, episode steps: 97, steps per second: 64, episode reward: 0.581, mean reward: 0.006 [-100.000, 17.604], mean action: 1.773 [0.000, 3.000], mean observation: 0.009 [-1.438, 1.000], loss: 0.878163, mean_absolute_error: 22.265045, mean_q: 29.921912, mean_eps: 0.100000
 1314026/2000000: episode: 6082, duration: 2.165s, episode steps: 135, steps per second: 62, episode reward: -19.549, mean reward: -0.145 [-100.000, 22.032], mean action: 1.452 [0.000, 3.000], mean observation: 0.019 [-0.796, 1.000], loss: 0.808229, mean_absolute_error: 21.023336, mean_q: 28.253204, mean_eps: 0.100000
 1314140/2000000: episode: 6083, duration: 1.814s, episode steps: 114, steps per second: 63, episode reward: -69.483, mean reward: -0.609 [-100.000, 17.941], mean action: 1.351 [0.000, 3.000], mean observation: 0.034 [-2.771, 1.000], loss: 0.711721, mean_absolute_error: 20.583878, mean_q: 27.653120, mean_eps: 0.100000
 1314271/2000000: episode: 6084, duration: 2.102s, episode steps: 131, steps per second: 62, episode reward: -21.004, mean reward: -0.160 [-100.000, 18.835], mean action: 1.969 [0.000, 3.000], mean observation: -0.065 [-0.687, 1.098], loss: 0.666105, mean_absolute_error: 20.580064, mean_q: 27.719229, mean_eps: 0.100000
 1314529/2000000: episode: 6085, duration: 4.150s, episode steps: 258, steps per second: 62, episode reward: 210.527, mean reward: 0.816 [-3.249, 100.000], mean action: 1.581 [0.000, 3.000], mean observation: 0.068 [-0.642, 1.000], loss: 0.618426, mean_absolute_error: 21.284056, mean_q: 28.484450, mean_eps: 0.100000
 1314682/2000000: episode: 6086, duration: 2.432s, episode steps: 153, steps per second: 63, episode reward: 16.360, mean reward: 0.107 [-100.000, 12.278], mean action: 1.712 [0.000, 3.000], mean observation: 0.056 [-0.654, 1.000], loss: 0.681050, mean_absolute_error: 20.314036, mean_q: 27.333291, mean_eps: 0.100000
 1314945/2000000: episode: 6087, duration: 4.219s, episode steps: 263, steps per second: 62, episode reward: 205.034, mean reward: 0.780 [-2.998, 100.000], mean action: 1.034 [0.000, 3.000], mean observation: 0.076 [-0.634, 1.000], loss: 0.598236, mean_absolute_error: 21.188047, mean_q: 28.585108, mean_eps: 0.100000
 1315137/2000000: episode: 6088, duration: 3.002s, episode steps: 192, steps per second: 64, episode reward: -13.613, mean reward: -0.071 [-100.000, 8.708], mean action: 1.651 [0.000, 3.000], mean observation: -0.015 [-0.755, 1.016], loss: 0.769143, mean_absolute_error: 21.294053, mean_q: 28.685223, mean_eps: 0.100000
 1315396/2000000: episode: 6089, duration: 4.121s, episode steps: 259, steps per second: 63, episode reward: 221.697, mean reward: 0.856 [-3.072, 100.000], mean action: 1.614 [0.000, 3.000], mean observation: 0.097 [-0.617, 1.000], loss: 0.668388, mean_absolute_error: 21.153502, mean_q: 28.540999, mean_eps: 0.100000
 1315779/2000000: episode: 6090, duration: 6.135s, episode steps: 383, steps per second: 62, episode reward: 190.310, mean reward: 0.497 [-17.446, 100.000], mean action: 1.010 [0.000, 3.000], mean observation: 0.158 [-0.808, 1.000], loss: 0.827864, mean_absolute_error: 21.346362, mean_q: 28.602505, mean_eps: 0.100000
 1316213/2000000: episode: 6091, duration: 7.083s, episode steps: 434, steps per second: 61, episode reward: 189.810, mean reward: 0.437 [-19.054, 100.000], mean action: 1.286 [0.000, 3.000], mean observation: 0.110 [-0.694, 1.000], loss: 0.718156, mean_absolute_error: 21.079082, mean_q: 28.378399, mean_eps: 0.100000
 1316671/2000000: episode: 6092, duration: 7.295s, episode steps: 458, steps per second: 63, episode reward: 247.217, mean reward: 0.540 [-17.989, 100.000], mean action: 1.004 [0.000, 3.000], mean observation: 0.173 [-0.713, 1.026], loss: 0.666897, mean_absolute_error: 21.496925, mean_q: 28.897889, mean_eps: 0.100000
 1316833/2000000: episode: 6093, duration: 2.590s, episode steps: 162, steps per second: 63, episode reward: 17.990, mean reward: 0.111 [-100.000, 12.131], mean action: 1.988 [0.000, 3.000], mean observation: -0.021 [-1.576, 1.000], loss: 0.788480, mean_absolute_error: 20.748467, mean_q: 27.916611, mean_eps: 0.100000
 1316988/2000000: episode: 6094, duration: 2.459s, episode steps: 155, steps per second: 63, episode reward: -33.136, mean reward: -0.214 [-100.000, 16.335], mean action: 1.497 [0.000, 3.000], mean observation: 0.053 [-1.161, 1.094], loss: 0.581215, mean_absolute_error: 21.581893, mean_q: 29.010044, mean_eps: 0.100000
 1317127/2000000: episode: 6095, duration: 2.204s, episode steps: 139, steps per second: 63, episode reward: -11.612, mean reward: -0.084 [-100.000, 16.031], mean action: 1.914 [0.000, 3.000], mean observation: -0.047 [-0.686, 1.319], loss: 0.635232, mean_absolute_error: 20.705324, mean_q: 27.911924, mean_eps: 0.100000
 1317708/2000000: episode: 6096, duration: 9.547s, episode steps: 581, steps per second: 61, episode reward: 227.920, mean reward: 0.392 [-18.615, 100.000], mean action: 1.153 [0.000, 3.000], mean observation: 0.144 [-0.697, 1.000], loss: 0.647366, mean_absolute_error: 20.595816, mean_q: 27.711758, mean_eps: 0.100000
 1317869/2000000: episode: 6097, duration: 2.588s, episode steps: 161, steps per second: 62, episode reward: -2.339, mean reward: -0.015 [-100.000, 27.658], mean action: 1.758 [0.000, 3.000], mean observation: 0.050 [-0.603, 1.000], loss: 0.674285, mean_absolute_error: 20.573723, mean_q: 27.751016, mean_eps: 0.100000
 1318869/2000000: episode: 6098, duration: 16.005s, episode steps: 1000, steps per second: 62, episode reward: 94.001, mean reward: 0.094 [-20.694, 21.406], mean action: 0.598 [0.000, 3.000], mean observation: 0.252 [-1.264, 1.000], loss: 0.756580, mean_absolute_error: 21.128378, mean_q: 28.225165, mean_eps: 0.100000
 1319296/2000000: episode: 6099, duration: 6.966s, episode steps: 427, steps per second: 61, episode reward: 217.637, mean reward: 0.510 [-17.788, 100.000], mean action: 0.977 [0.000, 3.000], mean observation: 0.113 [-0.645, 1.000], loss: 0.599095, mean_absolute_error: 21.807077, mean_q: 29.145103, mean_eps: 0.100000
 1319666/2000000: episode: 6100, duration: 6.087s, episode steps: 370, steps per second: 61, episode reward: 235.383, mean reward: 0.636 [-17.728, 100.000], mean action: 1.195 [0.000, 3.000], mean observation: 0.148 [-0.653, 1.000], loss: 0.719044, mean_absolute_error: 20.537845, mean_q: 27.518072, mean_eps: 0.100000
 1319900/2000000: episode: 6101, duration: 3.743s, episode steps: 234, steps per second: 63, episode reward: -25.145, mean reward: -0.107 [-100.000, 15.702], mean action: 1.859 [0.000, 3.000], mean observation: -0.011 [-0.935, 1.199], loss: 0.772018, mean_absolute_error: 20.703240, mean_q: 27.883208, mean_eps: 0.100000
 1320177/2000000: episode: 6102, duration: 4.491s, episode steps: 277, steps per second: 62, episode reward: 204.796, mean reward: 0.739 [-17.412, 100.000], mean action: 1.108 [0.000, 3.000], mean observation: 0.129 [-1.332, 1.000], loss: 0.633252, mean_absolute_error: 20.144236, mean_q: 27.188897, mean_eps: 0.100000
 1320498/2000000: episode: 6103, duration: 5.153s, episode steps: 321, steps per second: 62, episode reward: 211.658, mean reward: 0.659 [-8.592, 100.000], mean action: 1.037 [0.000, 3.000], mean observation: 0.078 [-0.738, 1.000], loss: 0.666142, mean_absolute_error: 20.487290, mean_q: 27.572082, mean_eps: 0.100000
 1320623/2000000: episode: 6104, duration: 1.974s, episode steps: 125, steps per second: 63, episode reward: -12.099, mean reward: -0.097 [-100.000, 12.007], mean action: 1.984 [0.000, 3.000], mean observation: -0.059 [-0.858, 1.000], loss: 0.724542, mean_absolute_error: 21.487029, mean_q: 28.931780, mean_eps: 0.100000
 1320706/2000000: episode: 6105, duration: 1.352s, episode steps: 83, steps per second: 61, episode reward: -42.554, mean reward: -0.513 [-100.000, 10.017], mean action: 1.964 [0.000, 3.000], mean observation: -0.005 [-0.971, 1.822], loss: 0.557982, mean_absolute_error: 20.139750, mean_q: 27.215452, mean_eps: 0.100000
 1321288/2000000: episode: 6106, duration: 9.562s, episode steps: 582, steps per second: 61, episode reward: 231.471, mean reward: 0.398 [-17.693, 100.000], mean action: 1.216 [0.000, 3.000], mean observation: 0.108 [-0.698, 1.017], loss: 0.679842, mean_absolute_error: 21.100029, mean_q: 28.366015, mean_eps: 0.100000
 1321395/2000000: episode: 6107, duration: 1.725s, episode steps: 107, steps per second: 62, episode reward: -23.658, mean reward: -0.221 [-100.000, 6.968], mean action: 1.907 [0.000, 3.000], mean observation: -0.101 [-0.903, 1.286], loss: 0.840137, mean_absolute_error: 20.615074, mean_q: 27.744922, mean_eps: 0.100000
 1322139/2000000: episode: 6108, duration: 12.259s, episode steps: 744, steps per second: 61, episode reward: 223.358, mean reward: 0.300 [-19.075, 100.000], mean action: 0.965 [0.000, 3.000], mean observation: 0.242 [-0.823, 1.000], loss: 0.705743, mean_absolute_error: 21.174572, mean_q: 28.415027, mean_eps: 0.100000
 1323139/2000000: episode: 6109, duration: 16.131s, episode steps: 1000, steps per second: 62, episode reward: 86.705, mean reward: 0.087 [-18.695, 22.529], mean action: 0.553 [0.000, 3.000], mean observation: 0.214 [-0.711, 1.000], loss: 0.632151, mean_absolute_error: 20.563646, mean_q: 27.676965, mean_eps: 0.100000
 1323349/2000000: episode: 6110, duration: 3.362s, episode steps: 210, steps per second: 62, episode reward: -9.648, mean reward: -0.046 [-100.000, 22.430], mean action: 1.729 [0.000, 3.000], mean observation: 0.057 [-0.858, 1.000], loss: 0.503607, mean_absolute_error: 20.921439, mean_q: 28.013653, mean_eps: 0.100000
 1324136/2000000: episode: 6111, duration: 12.845s, episode steps: 787, steps per second: 61, episode reward: 173.570, mean reward: 0.221 [-18.062, 100.000], mean action: 1.943 [0.000, 3.000], mean observation: 0.119 [-0.669, 1.000], loss: 0.689160, mean_absolute_error: 20.688914, mean_q: 27.831256, mean_eps: 0.100000
 1324850/2000000: episode: 6112, duration: 11.734s, episode steps: 714, steps per second: 61, episode reward: 183.621, mean reward: 0.257 [-18.769, 100.000], mean action: 1.732 [0.000, 3.000], mean observation: 0.083 [-0.779, 1.000], loss: 0.636390, mean_absolute_error: 20.926810, mean_q: 28.114163, mean_eps: 0.100000
 1325076/2000000: episode: 6113, duration: 3.607s, episode steps: 226, steps per second: 63, episode reward: -20.573, mean reward: -0.091 [-100.000, 18.509], mean action: 1.597 [0.000, 3.000], mean observation: 0.024 [-0.688, 1.000], loss: 0.535496, mean_absolute_error: 20.842714, mean_q: 28.154975, mean_eps: 0.100000
 1325256/2000000: episode: 6114, duration: 2.922s, episode steps: 180, steps per second: 62, episode reward: 162.697, mean reward: 0.904 [-14.605, 100.000], mean action: 1.944 [0.000, 3.000], mean observation: 0.147 [-0.845, 1.000], loss: 0.686914, mean_absolute_error: 21.470433, mean_q: 28.812559, mean_eps: 0.100000
 1325528/2000000: episode: 6115, duration: 4.370s, episode steps: 272, steps per second: 62, episode reward: -20.234, mean reward: -0.074 [-100.000, 22.900], mean action: 1.570 [0.000, 3.000], mean observation: 0.002 [-0.679, 1.013], loss: 0.873472, mean_absolute_error: 21.244926, mean_q: 28.539407, mean_eps: 0.100000
 1325689/2000000: episode: 6116, duration: 2.607s, episode steps: 161, steps per second: 62, episode reward: -42.184, mean reward: -0.262 [-100.000, 10.620], mean action: 1.745 [0.000, 3.000], mean observation: 0.086 [-0.759, 1.000], loss: 0.615721, mean_absolute_error: 21.460015, mean_q: 28.912617, mean_eps: 0.100000
 1326356/2000000: episode: 6117, duration: 11.321s, episode steps: 667, steps per second: 59, episode reward: 163.432, mean reward: 0.245 [-11.481, 100.000], mean action: 1.598 [0.000, 3.000], mean observation: 0.029 [-0.976, 1.000], loss: 0.668239, mean_absolute_error: 20.665183, mean_q: 27.858075, mean_eps: 0.100000
 1326754/2000000: episode: 6118, duration: 6.553s, episode steps: 398, steps per second: 61, episode reward: 240.833, mean reward: 0.605 [-17.674, 100.000], mean action: 0.937 [0.000, 3.000], mean observation: 0.187 [-0.613, 1.083], loss: 0.571523, mean_absolute_error: 21.339404, mean_q: 28.777879, mean_eps: 0.100000
 1326955/2000000: episode: 6119, duration: 3.207s, episode steps: 201, steps per second: 63, episode reward: -15.302, mean reward: -0.076 [-100.000, 24.012], mean action: 1.806 [0.000, 3.000], mean observation: -0.004 [-0.624, 1.000], loss: 0.651408, mean_absolute_error: 19.381931, mean_q: 26.178055, mean_eps: 0.100000
 1327098/2000000: episode: 6120, duration: 2.261s, episode steps: 143, steps per second: 63, episode reward: 7.656, mean reward: 0.054 [-100.000, 14.846], mean action: 1.741 [0.000, 3.000], mean observation: 0.050 [-0.821, 1.011], loss: 0.745513, mean_absolute_error: 20.983408, mean_q: 28.069811, mean_eps: 0.100000
 1327707/2000000: episode: 6121, duration: 10.498s, episode steps: 609, steps per second: 58, episode reward: 184.474, mean reward: 0.303 [-17.698, 100.000], mean action: 1.525 [0.000, 3.000], mean observation: 0.022 [-0.753, 1.000], loss: 0.700438, mean_absolute_error: 20.375586, mean_q: 27.466810, mean_eps: 0.100000
 1327843/2000000: episode: 6122, duration: 2.136s, episode steps: 136, steps per second: 64, episode reward: -23.858, mean reward: -0.175 [-100.000, 11.444], mean action: 1.978 [0.000, 3.000], mean observation: -0.054 [-0.691, 1.000], loss: 0.897856, mean_absolute_error: 20.093714, mean_q: 27.028781, mean_eps: 0.100000
 1328504/2000000: episode: 6123, duration: 11.040s, episode steps: 661, steps per second: 60, episode reward: 162.310, mean reward: 0.246 [-20.083, 100.000], mean action: 1.287 [0.000, 3.000], mean observation: 0.088 [-0.698, 1.000], loss: 0.628703, mean_absolute_error: 20.834192, mean_q: 28.024897, mean_eps: 0.100000
 1329085/2000000: episode: 6124, duration: 9.659s, episode steps: 581, steps per second: 60, episode reward: -434.264, mean reward: -0.747 [-100.000, 5.479], mean action: 1.697 [0.000, 3.000], mean observation: 0.166 [-0.681, 1.762], loss: 0.586661, mean_absolute_error: 20.779123, mean_q: 28.024956, mean_eps: 0.100000
 1329281/2000000: episode: 6125, duration: 3.117s, episode steps: 196, steps per second: 63, episode reward: -52.567, mean reward: -0.268 [-100.000, 18.282], mean action: 1.388 [0.000, 3.000], mean observation: 0.023 [-0.806, 1.000], loss: 0.691701, mean_absolute_error: 21.033310, mean_q: 28.236116, mean_eps: 0.100000
 1329847/2000000: episode: 6126, duration: 9.011s, episode steps: 566, steps per second: 63, episode reward: 219.825, mean reward: 0.388 [-18.166, 100.000], mean action: 0.806 [0.000, 3.000], mean observation: 0.207 [-1.129, 1.000], loss: 0.646956, mean_absolute_error: 20.848450, mean_q: 28.043875, mean_eps: 0.100000
 1330059/2000000: episode: 6127, duration: 3.401s, episode steps: 212, steps per second: 62, episode reward: 194.123, mean reward: 0.916 [-11.399, 100.000], mean action: 1.373 [0.000, 3.000], mean observation: 0.057 [-0.787, 1.000], loss: 0.527940, mean_absolute_error: 20.479685, mean_q: 27.556821, mean_eps: 0.100000
 1331059/2000000: episode: 6128, duration: 16.690s, episode steps: 1000, steps per second: 60, episode reward: 81.816, mean reward: 0.082 [-18.688, 21.701], mean action: 0.952 [0.000, 3.000], mean observation: 0.153 [-0.670, 1.000], loss: 0.605869, mean_absolute_error: 20.889396, mean_q: 28.148936, mean_eps: 0.100000
 1331201/2000000: episode: 6129, duration: 2.297s, episode steps: 142, steps per second: 62, episode reward: -6.139, mean reward: -0.043 [-100.000, 12.193], mean action: 1.887 [0.000, 3.000], mean observation: 0.082 [-0.747, 1.000], loss: 0.812333, mean_absolute_error: 21.596307, mean_q: 29.021674, mean_eps: 0.100000
 1331415/2000000: episode: 6130, duration: 3.358s, episode steps: 214, steps per second: 64, episode reward: -29.227, mean reward: -0.137 [-100.000, 12.262], mean action: 1.449 [0.000, 3.000], mean observation: 0.030 [-0.717, 1.000], loss: 0.635018, mean_absolute_error: 21.625579, mean_q: 28.833169, mean_eps: 0.100000
 1331773/2000000: episode: 6131, duration: 5.849s, episode steps: 358, steps per second: 61, episode reward: 207.277, mean reward: 0.579 [-4.675, 100.000], mean action: 1.369 [0.000, 3.000], mean observation: 0.053 [-0.750, 1.015], loss: 0.632681, mean_absolute_error: 20.972509, mean_q: 28.148928, mean_eps: 0.100000
 1331932/2000000: episode: 6132, duration: 2.801s, episode steps: 159, steps per second: 57, episode reward: -22.954, mean reward: -0.144 [-100.000, 13.090], mean action: 1.472 [0.000, 3.000], mean observation: 0.097 [-0.916, 1.320], loss: 0.690432, mean_absolute_error: 20.627473, mean_q: 27.819641, mean_eps: 0.100000
 1332187/2000000: episode: 6133, duration: 4.077s, episode steps: 255, steps per second: 63, episode reward: 246.495, mean reward: 0.967 [-11.903, 100.000], mean action: 1.341 [0.000, 3.000], mean observation: 0.144 [-0.675, 1.133], loss: 0.642802, mean_absolute_error: 21.016198, mean_q: 28.330348, mean_eps: 0.100000
 1332643/2000000: episode: 6134, duration: 7.462s, episode steps: 456, steps per second: 61, episode reward: 197.140, mean reward: 0.432 [-18.223, 100.000], mean action: 1.241 [0.000, 3.000], mean observation: 0.101 [-0.623, 1.000], loss: 0.649362, mean_absolute_error: 20.959039, mean_q: 28.202215, mean_eps: 0.100000
 1333153/2000000: episode: 6135, duration: 8.307s, episode steps: 510, steps per second: 61, episode reward: 215.750, mean reward: 0.423 [-13.446, 100.000], mean action: 0.798 [0.000, 3.000], mean observation: 0.130 [-0.666, 1.022], loss: 0.632239, mean_absolute_error: 21.329549, mean_q: 28.560520, mean_eps: 0.100000
 1333680/2000000: episode: 6136, duration: 8.401s, episode steps: 527, steps per second: 63, episode reward: 236.558, mean reward: 0.449 [-17.790, 100.000], mean action: 0.636 [0.000, 3.000], mean observation: 0.198 [-0.651, 1.003], loss: 0.674248, mean_absolute_error: 20.618201, mean_q: 27.738325, mean_eps: 0.100000
 1334012/2000000: episode: 6137, duration: 5.542s, episode steps: 332, steps per second: 60, episode reward: 228.027, mean reward: 0.687 [-8.307, 100.000], mean action: 1.196 [0.000, 3.000], mean observation: 0.047 [-0.530, 1.002], loss: 0.646587, mean_absolute_error: 20.186654, mean_q: 27.186487, mean_eps: 0.100000
 1334422/2000000: episode: 6138, duration: 6.677s, episode steps: 410, steps per second: 61, episode reward: -308.467, mean reward: -0.752 [-100.000, 4.866], mean action: 1.566 [0.000, 3.000], mean observation: 0.162 [-0.690, 1.465], loss: 0.818959, mean_absolute_error: 22.150377, mean_q: 29.682508, mean_eps: 0.100000
 1334493/2000000: episode: 6139, duration: 1.150s, episode steps: 71, steps per second: 62, episode reward: -76.235, mean reward: -1.074 [-100.000, 7.801], mean action: 1.972 [0.000, 3.000], mean observation: 0.049 [-1.036, 3.086], loss: 0.855693, mean_absolute_error: 20.705455, mean_q: 27.845856, mean_eps: 0.100000
 1335017/2000000: episode: 6140, duration: 8.392s, episode steps: 524, steps per second: 62, episode reward: 204.008, mean reward: 0.389 [-20.741, 100.000], mean action: 0.689 [0.000, 3.000], mean observation: 0.179 [-1.029, 1.000], loss: 0.621702, mean_absolute_error: 20.792537, mean_q: 28.000990, mean_eps: 0.100000
 1336017/2000000: episode: 6141, duration: 16.656s, episode steps: 1000, steps per second: 60, episode reward: 71.160, mean reward: 0.071 [-18.721, 12.053], mean action: 0.827 [0.000, 3.000], mean observation: 0.188 [-0.725, 1.000], loss: 0.668823, mean_absolute_error: 21.229316, mean_q: 28.615277, mean_eps: 0.100000
 1336195/2000000: episode: 6142, duration: 2.758s, episode steps: 178, steps per second: 65, episode reward: -14.272, mean reward: -0.080 [-100.000, 13.055], mean action: 1.680 [0.000, 3.000], mean observation: 0.048 [-0.670, 1.000], loss: 0.578146, mean_absolute_error: 21.271501, mean_q: 28.388585, mean_eps: 0.100000
 1336270/2000000: episode: 6143, duration: 1.204s, episode steps: 75, steps per second: 62, episode reward: -83.122, mean reward: -1.108 [-100.000, 15.456], mean action: 1.867 [0.000, 3.000], mean observation: 0.009 [-3.398, 1.000], loss: 0.821385, mean_absolute_error: 18.718141, mean_q: 25.163596, mean_eps: 0.100000
 1336787/2000000: episode: 6144, duration: 8.423s, episode steps: 517, steps per second: 61, episode reward: 212.405, mean reward: 0.411 [-19.845, 100.000], mean action: 0.950 [0.000, 3.000], mean observation: 0.093 [-0.694, 1.000], loss: 0.737890, mean_absolute_error: 21.326748, mean_q: 28.501104, mean_eps: 0.100000
 1337168/2000000: episode: 6145, duration: 6.248s, episode steps: 381, steps per second: 61, episode reward: 186.604, mean reward: 0.490 [-11.892, 100.000], mean action: 1.003 [0.000, 3.000], mean observation: 0.131 [-0.588, 1.000], loss: 0.752397, mean_absolute_error: 20.565070, mean_q: 27.610768, mean_eps: 0.100000
 1337952/2000000: episode: 6146, duration: 12.797s, episode steps: 784, steps per second: 61, episode reward: 214.738, mean reward: 0.274 [-18.509, 100.000], mean action: 0.893 [0.000, 3.000], mean observation: 0.102 [-1.151, 1.023], loss: 0.772736, mean_absolute_error: 21.495863, mean_q: 28.892648, mean_eps: 0.100000
 1338216/2000000: episode: 6147, duration: 4.239s, episode steps: 264, steps per second: 62, episode reward: 229.446, mean reward: 0.869 [-3.598, 100.000], mean action: 1.216 [0.000, 3.000], mean observation: 0.131 [-0.793, 1.000], loss: 0.788703, mean_absolute_error: 20.018014, mean_q: 26.835208, mean_eps: 0.100000
 1338320/2000000: episode: 6148, duration: 1.691s, episode steps: 104, steps per second: 61, episode reward: -7.878, mean reward: -0.076 [-100.000, 14.501], mean action: 1.413 [0.000, 3.000], mean observation: 0.028 [-0.915, 1.000], loss: 0.468621, mean_absolute_error: 20.672404, mean_q: 27.745629, mean_eps: 0.100000
 1338597/2000000: episode: 6149, duration: 4.500s, episode steps: 277, steps per second: 62, episode reward: 198.674, mean reward: 0.717 [-9.867, 100.000], mean action: 1.144 [0.000, 3.000], mean observation: 0.185 [-0.816, 1.000], loss: 0.732444, mean_absolute_error: 21.003583, mean_q: 28.254442, mean_eps: 0.100000
 1338910/2000000: episode: 6150, duration: 4.900s, episode steps: 313, steps per second: 64, episode reward: 240.675, mean reward: 0.769 [-15.483, 100.000], mean action: 1.425 [0.000, 3.000], mean observation: 0.137 [-0.710, 1.077], loss: 0.837870, mean_absolute_error: 22.089872, mean_q: 29.696092, mean_eps: 0.100000
 1339168/2000000: episode: 6151, duration: 4.170s, episode steps: 258, steps per second: 62, episode reward: 201.912, mean reward: 0.783 [-7.781, 100.000], mean action: 1.155 [0.000, 3.000], mean observation: 0.144 [-0.635, 1.000], loss: 0.798485, mean_absolute_error: 21.348638, mean_q: 28.429208, mean_eps: 0.100000
 1339465/2000000: episode: 6152, duration: 4.763s, episode steps: 297, steps per second: 62, episode reward: 228.760, mean reward: 0.770 [-7.824, 100.000], mean action: 1.354 [0.000, 3.000], mean observation: 0.109 [-0.727, 1.001], loss: 0.598780, mean_absolute_error: 21.348618, mean_q: 28.789290, mean_eps: 0.100000
 1339632/2000000: episode: 6153, duration: 2.631s, episode steps: 167, steps per second: 63, episode reward: -55.207, mean reward: -0.331 [-100.000, 12.933], mean action: 1.455 [0.000, 3.000], mean observation: 0.041 [-0.786, 1.000], loss: 0.871469, mean_absolute_error: 22.186319, mean_q: 29.234139, mean_eps: 0.100000
 1339865/2000000: episode: 6154, duration: 3.914s, episode steps: 233, steps per second: 60, episode reward: 208.014, mean reward: 0.893 [-10.732, 100.000], mean action: 1.588 [0.000, 3.000], mean observation: 0.065 [-0.700, 1.000], loss: 0.580558, mean_absolute_error: 21.174576, mean_q: 28.196554, mean_eps: 0.100000
 1340294/2000000: episode: 6155, duration: 6.866s, episode steps: 429, steps per second: 62, episode reward: 220.496, mean reward: 0.514 [-19.733, 100.000], mean action: 1.044 [0.000, 3.000], mean observation: 0.178 [-0.812, 1.000], loss: 0.605255, mean_absolute_error: 21.141130, mean_q: 28.459863, mean_eps: 0.100000
 1340457/2000000: episode: 6156, duration: 2.612s, episode steps: 163, steps per second: 62, episode reward: -23.215, mean reward: -0.142 [-100.000, 13.245], mean action: 1.871 [0.000, 3.000], mean observation: 0.006 [-0.753, 1.263], loss: 0.796638, mean_absolute_error: 20.015649, mean_q: 26.935651, mean_eps: 0.100000
 1341416/2000000: episode: 6157, duration: 16.998s, episode steps: 959, steps per second: 56, episode reward: 176.025, mean reward: 0.184 [-18.387, 100.000], mean action: 0.852 [0.000, 3.000], mean observation: 0.115 [-0.488, 1.000], loss: 0.737113, mean_absolute_error: 21.163099, mean_q: 28.365870, mean_eps: 0.100000
 1341606/2000000: episode: 6158, duration: 2.991s, episode steps: 190, steps per second: 64, episode reward: -64.605, mean reward: -0.340 [-100.000, 23.309], mean action: 1.658 [0.000, 3.000], mean observation: 0.028 [-0.590, 1.554], loss: 0.607696, mean_absolute_error: 20.377638, mean_q: 27.454826, mean_eps: 0.100000
 1341923/2000000: episode: 6159, duration: 5.032s, episode steps: 317, steps per second: 63, episode reward: -19.145, mean reward: -0.060 [-100.000, 14.804], mean action: 1.795 [0.000, 3.000], mean observation: 0.025 [-0.680, 1.000], loss: 0.636912, mean_absolute_error: 20.923382, mean_q: 28.161289, mean_eps: 0.100000
 1342325/2000000: episode: 6160, duration: 6.422s, episode steps: 402, steps per second: 63, episode reward: 222.631, mean reward: 0.554 [-19.983, 100.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.147 [-0.804, 1.000], loss: 0.674588, mean_absolute_error: 21.461856, mean_q: 28.886192, mean_eps: 0.100000
 1342737/2000000: episode: 6161, duration: 6.647s, episode steps: 412, steps per second: 62, episode reward: 143.446, mean reward: 0.348 [-13.598, 100.000], mean action: 1.774 [0.000, 3.000], mean observation: 0.094 [-0.710, 1.065], loss: 0.586559, mean_absolute_error: 21.299320, mean_q: 28.657338, mean_eps: 0.100000
 1343239/2000000: episode: 6162, duration: 8.119s, episode steps: 502, steps per second: 62, episode reward: 130.561, mean reward: 0.260 [-12.881, 100.000], mean action: 1.355 [0.000, 3.000], mean observation: 0.179 [-0.692, 1.185], loss: 0.614877, mean_absolute_error: 20.791350, mean_q: 27.991566, mean_eps: 0.100000
 1343825/2000000: episode: 6163, duration: 9.720s, episode steps: 586, steps per second: 60, episode reward: 172.491, mean reward: 0.294 [-9.035, 100.000], mean action: 1.263 [0.000, 3.000], mean observation: 0.016 [-0.556, 1.017], loss: 0.622812, mean_absolute_error: 21.085426, mean_q: 28.227616, mean_eps: 0.100000
 1344466/2000000: episode: 6164, duration: 10.331s, episode steps: 641, steps per second: 62, episode reward: 217.330, mean reward: 0.339 [-19.393, 100.000], mean action: 0.955 [0.000, 3.000], mean observation: 0.093 [-0.899, 1.000], loss: 0.714914, mean_absolute_error: 21.186189, mean_q: 28.374159, mean_eps: 0.100000
 1344581/2000000: episode: 6165, duration: 1.810s, episode steps: 115, steps per second: 64, episode reward: 34.655, mean reward: 0.301 [-100.000, 23.267], mean action: 1.913 [0.000, 3.000], mean observation: 0.007 [-0.717, 1.000], loss: 0.545876, mean_absolute_error: 21.174509, mean_q: 28.515970, mean_eps: 0.100000
 1344887/2000000: episode: 6166, duration: 4.871s, episode steps: 306, steps per second: 63, episode reward: 253.200, mean reward: 0.827 [-17.793, 100.000], mean action: 1.075 [0.000, 3.000], mean observation: 0.093 [-1.208, 1.000], loss: 0.803622, mean_absolute_error: 20.691677, mean_q: 27.508645, mean_eps: 0.100000
 1345544/2000000: episode: 6167, duration: 10.772s, episode steps: 657, steps per second: 61, episode reward: 199.023, mean reward: 0.303 [-19.058, 100.000], mean action: 0.822 [0.000, 3.000], mean observation: 0.212 [-1.124, 1.000], loss: 0.661181, mean_absolute_error: 21.161965, mean_q: 28.472403, mean_eps: 0.100000
 1345653/2000000: episode: 6168, duration: 1.738s, episode steps: 109, steps per second: 63, episode reward: -40.793, mean reward: -0.374 [-100.000, 13.452], mean action: 1.303 [0.000, 3.000], mean observation: 0.079 [-1.627, 1.000], loss: 0.496581, mean_absolute_error: 21.824935, mean_q: 29.425355, mean_eps: 0.100000
 1345825/2000000: episode: 6169, duration: 2.708s, episode steps: 172, steps per second: 64, episode reward: -8.774, mean reward: -0.051 [-100.000, 18.653], mean action: 1.500 [0.000, 3.000], mean observation: 0.008 [-0.811, 1.000], loss: 0.888794, mean_absolute_error: 21.897386, mean_q: 29.216266, mean_eps: 0.100000
 1346184/2000000: episode: 6170, duration: 5.818s, episode steps: 359, steps per second: 62, episode reward: 229.970, mean reward: 0.641 [-18.021, 100.000], mean action: 1.212 [0.000, 3.000], mean observation: 0.163 [-0.731, 1.202], loss: 0.774108, mean_absolute_error: 21.676306, mean_q: 29.142609, mean_eps: 0.100000
 1346735/2000000: episode: 6171, duration: 8.972s, episode steps: 551, steps per second: 61, episode reward: 201.506, mean reward: 0.366 [-18.047, 100.000], mean action: 0.971 [0.000, 3.000], mean observation: 0.114 [-0.624, 1.000], loss: 0.640466, mean_absolute_error: 20.971304, mean_q: 28.221772, mean_eps: 0.100000
 1346979/2000000: episode: 6172, duration: 3.863s, episode steps: 244, steps per second: 63, episode reward: -51.063, mean reward: -0.209 [-100.000, 11.696], mean action: 1.443 [0.000, 3.000], mean observation: 0.050 [-1.303, 1.000], loss: 0.880356, mean_absolute_error: 20.741074, mean_q: 27.819380, mean_eps: 0.100000
 1347353/2000000: episode: 6173, duration: 6.006s, episode steps: 374, steps per second: 62, episode reward: 206.001, mean reward: 0.551 [-17.388, 100.000], mean action: 1.136 [0.000, 3.000], mean observation: 0.179 [-0.812, 1.026], loss: 0.653955, mean_absolute_error: 21.131187, mean_q: 28.417305, mean_eps: 0.100000
 1347747/2000000: episode: 6174, duration: 6.342s, episode steps: 394, steps per second: 62, episode reward: 233.433, mean reward: 0.592 [-20.585, 100.000], mean action: 1.287 [0.000, 3.000], mean observation: 0.099 [-0.900, 1.000], loss: 0.744721, mean_absolute_error: 21.686345, mean_q: 29.059018, mean_eps: 0.100000
 1347947/2000000: episode: 6175, duration: 3.149s, episode steps: 200, steps per second: 64, episode reward: 245.795, mean reward: 1.229 [-6.994, 100.000], mean action: 1.465 [0.000, 3.000], mean observation: 0.132 [-0.768, 1.219], loss: 0.640633, mean_absolute_error: 21.702581, mean_q: 29.194115, mean_eps: 0.100000
 1348339/2000000: episode: 6176, duration: 6.288s, episode steps: 392, steps per second: 62, episode reward: 229.359, mean reward: 0.585 [-17.967, 100.000], mean action: 0.602 [0.000, 3.000], mean observation: 0.169 [-1.133, 1.000], loss: 0.869100, mean_absolute_error: 21.514356, mean_q: 28.874750, mean_eps: 0.100000
 1348520/2000000: episode: 6177, duration: 2.890s, episode steps: 181, steps per second: 63, episode reward: -48.826, mean reward: -0.270 [-100.000, 20.499], mean action: 1.967 [0.000, 3.000], mean observation: 0.015 [-1.989, 1.000], loss: 0.624065, mean_absolute_error: 21.974320, mean_q: 29.591058, mean_eps: 0.100000
 1348635/2000000: episode: 6178, duration: 1.840s, episode steps: 115, steps per second: 62, episode reward: 6.413, mean reward: 0.056 [-100.000, 14.003], mean action: 1.765 [0.000, 3.000], mean observation: -0.051 [-0.802, 1.000], loss: 0.434323, mean_absolute_error: 20.262596, mean_q: 27.374698, mean_eps: 0.100000
 1348719/2000000: episode: 6179, duration: 1.515s, episode steps: 84, steps per second: 55, episode reward: -51.335, mean reward: -0.611 [-100.000, 18.221], mean action: 1.643 [0.000, 3.000], mean observation: 0.002 [-0.905, 2.310], loss: 0.642518, mean_absolute_error: 21.722167, mean_q: 29.179300, mean_eps: 0.100000
 1348855/2000000: episode: 6180, duration: 2.204s, episode steps: 136, steps per second: 62, episode reward: -7.727, mean reward: -0.057 [-100.000, 14.905], mean action: 1.993 [0.000, 3.000], mean observation: -0.059 [-1.835, 1.000], loss: 0.607497, mean_absolute_error: 21.946057, mean_q: 29.516522, mean_eps: 0.100000
 1348939/2000000: episode: 6181, duration: 1.335s, episode steps: 84, steps per second: 63, episode reward: -61.747, mean reward: -0.735 [-100.000, 22.297], mean action: 1.869 [0.000, 3.000], mean observation: 0.112 [-0.835, 1.000], loss: 0.876079, mean_absolute_error: 22.234531, mean_q: 29.823815, mean_eps: 0.100000
 1349037/2000000: episode: 6182, duration: 1.586s, episode steps: 98, steps per second: 62, episode reward: -6.988, mean reward: -0.071 [-100.000, 14.059], mean action: 1.786 [0.000, 3.000], mean observation: 0.024 [-0.935, 1.000], loss: 0.728154, mean_absolute_error: 20.678466, mean_q: 27.699742, mean_eps: 0.100000
 1349206/2000000: episode: 6183, duration: 2.684s, episode steps: 169, steps per second: 63, episode reward: -49.608, mean reward: -0.294 [-100.000, 17.063], mean action: 1.568 [0.000, 3.000], mean observation: 0.020 [-0.566, 1.000], loss: 0.844529, mean_absolute_error: 21.228642, mean_q: 28.495936, mean_eps: 0.100000
 1349319/2000000: episode: 6184, duration: 1.766s, episode steps: 113, steps per second: 64, episode reward: -64.252, mean reward: -0.569 [-100.000, 6.865], mean action: 1.469 [0.000, 3.000], mean observation: -0.061 [-0.862, 2.974], loss: 0.495124, mean_absolute_error: 21.809903, mean_q: 29.339872, mean_eps: 0.100000
 1349948/2000000: episode: 6185, duration: 10.029s, episode steps: 629, steps per second: 63, episode reward: 256.358, mean reward: 0.408 [-19.254, 100.000], mean action: 0.809 [0.000, 3.000], mean observation: 0.203 [-0.692, 1.000], loss: 0.679863, mean_absolute_error: 21.518715, mean_q: 28.926454, mean_eps: 0.100000
 1350099/2000000: episode: 6186, duration: 2.393s, episode steps: 151, steps per second: 63, episode reward: -14.714, mean reward: -0.097 [-100.000, 21.241], mean action: 1.517 [0.000, 3.000], mean observation: 0.016 [-0.811, 1.000], loss: 0.476658, mean_absolute_error: 21.487226, mean_q: 28.941208, mean_eps: 0.100000
 1350387/2000000: episode: 6187, duration: 4.583s, episode steps: 288, steps per second: 63, episode reward: 213.765, mean reward: 0.742 [-18.097, 100.000], mean action: 1.014 [0.000, 3.000], mean observation: 0.125 [-0.626, 1.000], loss: 0.640458, mean_absolute_error: 22.706176, mean_q: 30.539798, mean_eps: 0.100000
 1350565/2000000: episode: 6188, duration: 2.852s, episode steps: 178, steps per second: 62, episode reward: -29.349, mean reward: -0.165 [-100.000, 20.582], mean action: 1.416 [0.000, 3.000], mean observation: 0.002 [-0.597, 1.000], loss: 0.575213, mean_absolute_error: 22.011617, mean_q: 29.611858, mean_eps: 0.100000
 1350694/2000000: episode: 6189, duration: 2.027s, episode steps: 129, steps per second: 64, episode reward: -16.079, mean reward: -0.125 [-100.000, 19.467], mean action: 1.450 [0.000, 3.000], mean observation: 0.034 [-1.823, 1.000], loss: 0.666433, mean_absolute_error: 22.456386, mean_q: 30.184166, mean_eps: 0.100000
 1350851/2000000: episode: 6190, duration: 2.460s, episode steps: 157, steps per second: 64, episode reward: -18.885, mean reward: -0.120 [-100.000, 10.286], mean action: 1.994 [0.000, 3.000], mean observation: -0.046 [-0.804, 1.462], loss: 0.626712, mean_absolute_error: 22.219715, mean_q: 29.728206, mean_eps: 0.100000
 1351014/2000000: episode: 6191, duration: 2.650s, episode steps: 163, steps per second: 62, episode reward: -32.236, mean reward: -0.198 [-100.000, 18.004], mean action: 1.325 [0.000, 3.000], mean observation: 0.044 [-0.901, 1.000], loss: 0.666887, mean_absolute_error: 20.137953, mean_q: 27.096435, mean_eps: 0.100000
 1351127/2000000: episode: 6192, duration: 1.787s, episode steps: 113, steps per second: 63, episode reward: -26.107, mean reward: -0.231 [-100.000, 16.851], mean action: 1.593 [0.000, 3.000], mean observation: -0.031 [-0.765, 2.027], loss: 0.548127, mean_absolute_error: 20.077475, mean_q: 27.042421, mean_eps: 0.100000
 1351545/2000000: episode: 6193, duration: 6.786s, episode steps: 418, steps per second: 62, episode reward: 227.018, mean reward: 0.543 [-17.775, 100.000], mean action: 1.354 [0.000, 3.000], mean observation: 0.061 [-0.906, 1.000], loss: 0.709143, mean_absolute_error: 21.637948, mean_q: 28.935524, mean_eps: 0.100000
 1351695/2000000: episode: 6194, duration: 2.316s, episode steps: 150, steps per second: 65, episode reward: -5.627, mean reward: -0.038 [-100.000, 21.573], mean action: 1.413 [0.000, 3.000], mean observation: 0.002 [-0.836, 1.244], loss: 0.828582, mean_absolute_error: 21.947725, mean_q: 29.207607, mean_eps: 0.100000
 1351838/2000000: episode: 6195, duration: 2.262s, episode steps: 143, steps per second: 63, episode reward: -41.901, mean reward: -0.293 [-100.000, 8.913], mean action: 1.797 [0.000, 3.000], mean observation: 0.105 [-0.630, 1.462], loss: 0.720602, mean_absolute_error: 21.029893, mean_q: 28.193760, mean_eps: 0.100000
 1352019/2000000: episode: 6196, duration: 2.953s, episode steps: 181, steps per second: 61, episode reward: -19.115, mean reward: -0.106 [-100.000, 19.705], mean action: 2.000 [0.000, 3.000], mean observation: -0.005 [-1.346, 1.000], loss: 0.472014, mean_absolute_error: 21.594069, mean_q: 29.120322, mean_eps: 0.100000
 1352150/2000000: episode: 6197, duration: 2.096s, episode steps: 131, steps per second: 62, episode reward: -59.542, mean reward: -0.455 [-100.000, 9.943], mean action: 1.893 [0.000, 3.000], mean observation: -0.002 [-0.649, 1.000], loss: 0.493880, mean_absolute_error: 20.721935, mean_q: 27.824259, mean_eps: 0.100000
 1352308/2000000: episode: 6198, duration: 2.498s, episode steps: 158, steps per second: 63, episode reward: -14.517, mean reward: -0.092 [-100.000, 11.713], mean action: 1.892 [0.000, 3.000], mean observation: 0.058 [-0.632, 1.970], loss: 0.636902, mean_absolute_error: 22.690482, mean_q: 30.537702, mean_eps: 0.100000
 1352719/2000000: episode: 6199, duration: 6.555s, episode steps: 411, steps per second: 63, episode reward: 196.640, mean reward: 0.478 [-10.405, 100.000], mean action: 1.151 [0.000, 3.000], mean observation: 0.079 [-0.566, 1.000], loss: 0.570815, mean_absolute_error: 21.470358, mean_q: 28.910398, mean_eps: 0.100000
 1352895/2000000: episode: 6200, duration: 2.792s, episode steps: 176, steps per second: 63, episode reward: -19.852, mean reward: -0.113 [-100.000, 19.966], mean action: 1.562 [0.000, 3.000], mean observation: 0.012 [-1.498, 1.000], loss: 0.577985, mean_absolute_error: 21.379664, mean_q: 28.805406, mean_eps: 0.100000
 1353000/2000000: episode: 6201, duration: 1.729s, episode steps: 105, steps per second: 61, episode reward: -100.736, mean reward: -0.959 [-100.000, 13.075], mean action: 1.410 [0.000, 3.000], mean observation: 0.071 [-1.370, 1.000], loss: 0.467123, mean_absolute_error: 21.812512, mean_q: 29.414168, mean_eps: 0.100000
 1353441/2000000: episode: 6202, duration: 7.231s, episode steps: 441, steps per second: 61, episode reward: -18.928, mean reward: -0.043 [-100.000, 13.041], mean action: 1.551 [0.000, 3.000], mean observation: 0.040 [-0.923, 1.085], loss: 0.667761, mean_absolute_error: 21.674262, mean_q: 29.162644, mean_eps: 0.100000
 1353594/2000000: episode: 6203, duration: 2.394s, episode steps: 153, steps per second: 64, episode reward: -32.155, mean reward: -0.210 [-100.000, 16.897], mean action: 1.843 [0.000, 3.000], mean observation: 0.036 [-1.304, 1.000], loss: 0.699232, mean_absolute_error: 21.672154, mean_q: 29.004803, mean_eps: 0.100000
 1354188/2000000: episode: 6204, duration: 10.025s, episode steps: 594, steps per second: 59, episode reward: 193.887, mean reward: 0.326 [-19.850, 100.000], mean action: 1.379 [0.000, 3.000], mean observation: 0.041 [-0.693, 1.000], loss: 0.692410, mean_absolute_error: 21.238376, mean_q: 28.423938, mean_eps: 0.100000
 1354381/2000000: episode: 6205, duration: 3.089s, episode steps: 193, steps per second: 62, episode reward: -2.997, mean reward: -0.016 [-100.000, 15.899], mean action: 1.808 [0.000, 3.000], mean observation: 0.014 [-0.563, 1.000], loss: 0.874563, mean_absolute_error: 22.474448, mean_q: 30.142797, mean_eps: 0.100000
 1354509/2000000: episode: 6206, duration: 2.020s, episode steps: 128, steps per second: 63, episode reward: -39.260, mean reward: -0.307 [-100.000, 11.185], mean action: 2.039 [0.000, 3.000], mean observation: -0.073 [-0.747, 2.462], loss: 0.546202, mean_absolute_error: 20.803177, mean_q: 28.024746, mean_eps: 0.100000
 1354674/2000000: episode: 6207, duration: 2.605s, episode steps: 165, steps per second: 63, episode reward: -31.975, mean reward: -0.194 [-100.000, 16.491], mean action: 1.721 [0.000, 3.000], mean observation: 0.006 [-1.577, 1.000], loss: 0.647846, mean_absolute_error: 21.732563, mean_q: 29.235559, mean_eps: 0.100000
 1354968/2000000: episode: 6208, duration: 4.733s, episode steps: 294, steps per second: 62, episode reward: 232.654, mean reward: 0.791 [-18.729, 100.000], mean action: 1.112 [0.000, 3.000], mean observation: 0.143 [-0.729, 1.000], loss: 0.548666, mean_absolute_error: 21.209727, mean_q: 28.566056, mean_eps: 0.100000
 1355070/2000000: episode: 6209, duration: 1.629s, episode steps: 102, steps per second: 63, episode reward: -37.123, mean reward: -0.364 [-100.000, 19.191], mean action: 1.343 [0.000, 3.000], mean observation: 0.049 [-1.266, 1.000], loss: 0.420177, mean_absolute_error: 22.552635, mean_q: 30.397899, mean_eps: 0.100000
 1355154/2000000: episode: 6210, duration: 1.352s, episode steps: 84, steps per second: 62, episode reward: -36.788, mean reward: -0.438 [-100.000, 15.954], mean action: 1.357 [0.000, 3.000], mean observation: -0.001 [-2.262, 1.000], loss: 0.487605, mean_absolute_error: 21.925854, mean_q: 29.566441, mean_eps: 0.100000
 1355388/2000000: episode: 6211, duration: 3.745s, episode steps: 234, steps per second: 62, episode reward: 237.261, mean reward: 1.014 [-6.807, 100.000], mean action: 1.444 [0.000, 3.000], mean observation: 0.056 [-0.998, 1.000], loss: 0.596528, mean_absolute_error: 21.782990, mean_q: 29.324526, mean_eps: 0.100000
 1355637/2000000: episode: 6212, duration: 3.925s, episode steps: 249, steps per second: 63, episode reward: -19.753, mean reward: -0.079 [-100.000, 16.238], mean action: 1.386 [0.000, 3.000], mean observation: 0.011 [-0.563, 1.100], loss: 0.653033, mean_absolute_error: 22.018503, mean_q: 29.637353, mean_eps: 0.100000
 1355965/2000000: episode: 6213, duration: 5.240s, episode steps: 328, steps per second: 63, episode reward: 174.433, mean reward: 0.532 [-5.938, 100.000], mean action: 1.732 [0.000, 3.000], mean observation: 0.051 [-0.657, 1.000], loss: 0.784814, mean_absolute_error: 21.007071, mean_q: 28.106249, mean_eps: 0.100000
 1356391/2000000: episode: 6214, duration: 7.147s, episode steps: 426, steps per second: 60, episode reward: 219.057, mean reward: 0.514 [-3.639, 100.000], mean action: 1.394 [0.000, 3.000], mean observation: 0.056 [-0.748, 1.000], loss: 0.676929, mean_absolute_error: 21.861789, mean_q: 29.335363, mean_eps: 0.100000
 1356576/2000000: episode: 6215, duration: 2.947s, episode steps: 185, steps per second: 63, episode reward: -16.918, mean reward: -0.091 [-100.000, 7.968], mean action: 1.768 [0.000, 3.000], mean observation: 0.064 [-1.277, 1.000], loss: 0.657479, mean_absolute_error: 20.875125, mean_q: 28.063722, mean_eps: 0.100000
 1356858/2000000: episode: 6216, duration: 4.445s, episode steps: 282, steps per second: 63, episode reward: 209.056, mean reward: 0.741 [-8.356, 100.000], mean action: 0.972 [0.000, 3.000], mean observation: 0.140 [-1.007, 1.000], loss: 0.716644, mean_absolute_error: 21.363654, mean_q: 28.611526, mean_eps: 0.100000
 1356978/2000000: episode: 6217, duration: 1.898s, episode steps: 120, steps per second: 63, episode reward: -51.781, mean reward: -0.432 [-100.000, 16.139], mean action: 2.025 [0.000, 3.000], mean observation: 0.002 [-0.783, 1.000], loss: 0.582441, mean_absolute_error: 22.800678, mean_q: 30.659229, mean_eps: 0.100000
 1357205/2000000: episode: 6218, duration: 3.566s, episode steps: 227, steps per second: 64, episode reward: -18.718, mean reward: -0.082 [-100.000, 22.009], mean action: 1.604 [0.000, 3.000], mean observation: 0.059 [-0.710, 1.000], loss: 0.602137, mean_absolute_error: 21.715977, mean_q: 29.156873, mean_eps: 0.100000
 1357328/2000000: episode: 6219, duration: 1.950s, episode steps: 123, steps per second: 63, episode reward: -10.167, mean reward: -0.083 [-100.000, 12.171], mean action: 1.789 [0.000, 3.000], mean observation: 0.011 [-0.634, 1.000], loss: 0.629049, mean_absolute_error: 21.726380, mean_q: 29.242586, mean_eps: 0.100000
 1357487/2000000: episode: 6220, duration: 2.509s, episode steps: 159, steps per second: 63, episode reward: -8.953, mean reward: -0.056 [-100.000, 17.257], mean action: 1.730 [0.000, 3.000], mean observation: 0.092 [-0.779, 1.004], loss: 0.777649, mean_absolute_error: 22.212071, mean_q: 29.621357, mean_eps: 0.100000
 1357748/2000000: episode: 6221, duration: 4.195s, episode steps: 261, steps per second: 62, episode reward: 247.427, mean reward: 0.948 [-8.785, 100.000], mean action: 1.613 [0.000, 3.000], mean observation: 0.041 [-1.236, 1.000], loss: 0.698268, mean_absolute_error: 21.834508, mean_q: 29.326228, mean_eps: 0.100000
 1357941/2000000: episode: 6222, duration: 3.101s, episode steps: 193, steps per second: 62, episode reward: -70.466, mean reward: -0.365 [-100.000, 7.563], mean action: 1.798 [0.000, 3.000], mean observation: 0.047 [-1.485, 1.000], loss: 0.688191, mean_absolute_error: 20.695943, mean_q: 27.820866, mean_eps: 0.100000
 1358182/2000000: episode: 6223, duration: 3.808s, episode steps: 241, steps per second: 63, episode reward: -42.038, mean reward: -0.174 [-100.000, 20.027], mean action: 1.568 [0.000, 3.000], mean observation: 0.009 [-0.668, 1.473], loss: 0.657310, mean_absolute_error: 21.209396, mean_q: 28.548403, mean_eps: 0.100000
 1358414/2000000: episode: 6224, duration: 3.656s, episode steps: 232, steps per second: 63, episode reward: 229.594, mean reward: 0.990 [-3.110, 100.000], mean action: 1.547 [0.000, 3.000], mean observation: 0.052 [-0.620, 1.013], loss: 0.648000, mean_absolute_error: 21.697753, mean_q: 29.155295, mean_eps: 0.100000
 1358501/2000000: episode: 6225, duration: 1.379s, episode steps: 87, steps per second: 63, episode reward: -30.364, mean reward: -0.349 [-100.000, 15.286], mean action: 1.931 [0.000, 3.000], mean observation: -0.004 [-0.866, 1.000], loss: 0.688045, mean_absolute_error: 23.328580, mean_q: 31.317629, mean_eps: 0.100000
 1358685/2000000: episode: 6226, duration: 2.896s, episode steps: 184, steps per second: 64, episode reward: -65.436, mean reward: -0.356 [-100.000, 15.929], mean action: 1.772 [0.000, 3.000], mean observation: 0.047 [-0.657, 1.000], loss: 0.758247, mean_absolute_error: 21.925595, mean_q: 29.369694, mean_eps: 0.100000
 1359685/2000000: episode: 6227, duration: 17.051s, episode steps: 1000, steps per second: 59, episode reward: -48.742, mean reward: -0.049 [-6.016, 5.258], mean action: 1.670 [0.000, 3.000], mean observation: 0.012 [-0.575, 0.946], loss: 0.677107, mean_absolute_error: 22.164730, mean_q: 29.694236, mean_eps: 0.100000
 1359942/2000000: episode: 6228, duration: 4.040s, episode steps: 257, steps per second: 64, episode reward: -28.387, mean reward: -0.110 [-100.000, 25.944], mean action: 1.720 [0.000, 3.000], mean observation: 0.022 [-0.607, 1.000], loss: 0.613928, mean_absolute_error: 21.452301, mean_q: 28.536759, mean_eps: 0.100000
 1360094/2000000: episode: 6229, duration: 2.378s, episode steps: 152, steps per second: 64, episode reward: -63.290, mean reward: -0.416 [-100.000, 10.272], mean action: 1.803 [0.000, 3.000], mean observation: 0.057 [-2.735, 1.000], loss: 0.469538, mean_absolute_error: 22.390896, mean_q: 30.178015, mean_eps: 0.100000
 1360500/2000000: episode: 6230, duration: 6.443s, episode steps: 406, steps per second: 63, episode reward: 205.539, mean reward: 0.506 [-10.439, 100.000], mean action: 1.138 [0.000, 3.000], mean observation: 0.039 [-0.510, 1.000], loss: 0.652798, mean_absolute_error: 21.615044, mean_q: 29.066194, mean_eps: 0.100000
 1360641/2000000: episode: 6231, duration: 2.232s, episode steps: 141, steps per second: 63, episode reward: -19.084, mean reward: -0.135 [-100.000, 10.650], mean action: 1.837 [0.000, 3.000], mean observation: 0.081 [-0.622, 1.395], loss: 0.578272, mean_absolute_error: 21.837565, mean_q: 29.360938, mean_eps: 0.100000
 1361074/2000000: episode: 6232, duration: 7.149s, episode steps: 433, steps per second: 61, episode reward: 173.986, mean reward: 0.402 [-11.073, 100.000], mean action: 1.818 [0.000, 3.000], mean observation: 0.073 [-0.671, 1.000], loss: 0.548927, mean_absolute_error: 22.424128, mean_q: 30.056534, mean_eps: 0.100000
 1361198/2000000: episode: 6233, duration: 1.961s, episode steps: 124, steps per second: 63, episode reward: -1.573, mean reward: -0.013 [-100.000, 18.377], mean action: 1.629 [0.000, 3.000], mean observation: 0.070 [-0.902, 1.000], loss: 0.937682, mean_absolute_error: 21.201786, mean_q: 28.443309, mean_eps: 0.100000
 1361386/2000000: episode: 6234, duration: 2.961s, episode steps: 188, steps per second: 63, episode reward: -29.659, mean reward: -0.158 [-100.000, 19.825], mean action: 1.734 [0.000, 3.000], mean observation: 0.004 [-1.045, 1.000], loss: 0.811874, mean_absolute_error: 21.901628, mean_q: 29.372606, mean_eps: 0.100000
 1361724/2000000: episode: 6235, duration: 5.487s, episode steps: 338, steps per second: 62, episode reward: 178.949, mean reward: 0.529 [-11.171, 100.000], mean action: 0.964 [0.000, 3.000], mean observation: 0.104 [-0.726, 1.000], loss: 0.707637, mean_absolute_error: 22.508818, mean_q: 30.154658, mean_eps: 0.100000
 1362079/2000000: episode: 6236, duration: 5.655s, episode steps: 355, steps per second: 63, episode reward: 246.896, mean reward: 0.695 [-19.301, 100.000], mean action: 1.003 [0.000, 3.000], mean observation: 0.186 [-0.600, 1.000], loss: 0.921861, mean_absolute_error: 22.454482, mean_q: 30.092823, mean_eps: 0.100000
 1362684/2000000: episode: 6237, duration: 9.846s, episode steps: 605, steps per second: 61, episode reward: 182.412, mean reward: 0.302 [-19.444, 100.000], mean action: 1.446 [0.000, 3.000], mean observation: 0.045 [-0.724, 1.000], loss: 0.669338, mean_absolute_error: 22.318693, mean_q: 29.987209, mean_eps: 0.100000
 1363038/2000000: episode: 6238, duration: 5.640s, episode steps: 354, steps per second: 63, episode reward: 218.653, mean reward: 0.618 [-13.323, 100.000], mean action: 2.150 [0.000, 3.000], mean observation: 0.146 [-0.784, 1.000], loss: 0.603449, mean_absolute_error: 21.795860, mean_q: 29.326173, mean_eps: 0.100000
 1363233/2000000: episode: 6239, duration: 3.069s, episode steps: 195, steps per second: 64, episode reward: 14.185, mean reward: 0.073 [-100.000, 22.902], mean action: 1.877 [0.000, 3.000], mean observation: -0.011 [-1.290, 1.000], loss: 0.766930, mean_absolute_error: 20.984104, mean_q: 28.098788, mean_eps: 0.100000
 1363714/2000000: episode: 6240, duration: 7.884s, episode steps: 481, steps per second: 61, episode reward: 195.311, mean reward: 0.406 [-18.121, 100.000], mean action: 1.087 [0.000, 3.000], mean observation: 0.073 [-0.589, 1.000], loss: 0.685349, mean_absolute_error: 21.767981, mean_q: 29.278293, mean_eps: 0.100000
 1363812/2000000: episode: 6241, duration: 1.556s, episode steps: 98, steps per second: 63, episode reward: -26.350, mean reward: -0.269 [-100.000, 26.300], mean action: 1.878 [0.000, 3.000], mean observation: -0.019 [-0.839, 1.000], loss: 0.519435, mean_absolute_error: 21.888764, mean_q: 29.503237, mean_eps: 0.100000
 1363931/2000000: episode: 6242, duration: 1.884s, episode steps: 119, steps per second: 63, episode reward: -38.786, mean reward: -0.326 [-100.000, 12.309], mean action: 1.706 [0.000, 3.000], mean observation: 0.127 [-1.629, 1.000], loss: 0.559359, mean_absolute_error: 22.039777, mean_q: 29.625899, mean_eps: 0.100000
 1364349/2000000: episode: 6243, duration: 6.583s, episode steps: 418, steps per second: 64, episode reward: -136.752, mean reward: -0.327 [-100.000, 14.346], mean action: 1.450 [0.000, 3.000], mean observation: 0.157 [-1.323, 1.043], loss: 0.719835, mean_absolute_error: 22.991008, mean_q: 30.893140, mean_eps: 0.100000
 1364485/2000000: episode: 6244, duration: 2.114s, episode steps: 136, steps per second: 64, episode reward: -4.205, mean reward: -0.031 [-100.000, 20.718], mean action: 1.713 [0.000, 3.000], mean observation: 0.068 [-0.810, 1.000], loss: 0.596069, mean_absolute_error: 22.263088, mean_q: 30.010200, mean_eps: 0.100000
 1364763/2000000: episode: 6245, duration: 4.322s, episode steps: 278, steps per second: 64, episode reward: -108.172, mean reward: -0.389 [-100.000, 25.688], mean action: 1.644 [0.000, 3.000], mean observation: 0.071 [-1.067, 1.000], loss: 0.561817, mean_absolute_error: 21.652123, mean_q: 29.105210, mean_eps: 0.100000
 1365305/2000000: episode: 6246, duration: 9.138s, episode steps: 542, steps per second: 59, episode reward: 218.081, mean reward: 0.402 [-17.821, 100.000], mean action: 0.919 [0.000, 3.000], mean observation: 0.174 [-0.628, 1.000], loss: 0.620925, mean_absolute_error: 22.004580, mean_q: 29.437601, mean_eps: 0.100000
 1365465/2000000: episode: 6247, duration: 2.484s, episode steps: 160, steps per second: 64, episode reward: -83.897, mean reward: -0.524 [-100.000, 12.613], mean action: 1.413 [0.000, 3.000], mean observation: 0.096 [-1.659, 1.000], loss: 0.702252, mean_absolute_error: 22.215023, mean_q: 29.885784, mean_eps: 0.100000
 1365762/2000000: episode: 6248, duration: 4.619s, episode steps: 297, steps per second: 64, episode reward: 228.228, mean reward: 0.768 [-8.112, 100.000], mean action: 1.128 [0.000, 3.000], mean observation: 0.088 [-0.684, 1.000], loss: 0.852641, mean_absolute_error: 22.599318, mean_q: 30.326562, mean_eps: 0.100000
 1366301/2000000: episode: 6249, duration: 8.651s, episode steps: 539, steps per second: 62, episode reward: -51.069, mean reward: -0.095 [-100.000, 13.352], mean action: 1.716 [0.000, 3.000], mean observation: 0.065 [-0.754, 1.286], loss: 0.690195, mean_absolute_error: 22.245152, mean_q: 29.902451, mean_eps: 0.100000
 1366440/2000000: episode: 6250, duration: 2.184s, episode steps: 139, steps per second: 64, episode reward: -31.305, mean reward: -0.225 [-100.000, 7.323], mean action: 1.791 [0.000, 3.000], mean observation: -0.068 [-0.716, 2.131], loss: 0.499755, mean_absolute_error: 22.112287, mean_q: 29.773719, mean_eps: 0.100000
 1366545/2000000: episode: 6251, duration: 1.680s, episode steps: 105, steps per second: 62, episode reward: -44.687, mean reward: -0.426 [-100.000, 15.238], mean action: 1.667 [0.000, 3.000], mean observation: 0.016 [-0.989, 1.000], loss: 0.560836, mean_absolute_error: 21.343560, mean_q: 28.675003, mean_eps: 0.100000
 1366704/2000000: episode: 6252, duration: 2.456s, episode steps: 159, steps per second: 65, episode reward: -25.056, mean reward: -0.158 [-100.000, 16.912], mean action: 1.214 [0.000, 3.000], mean observation: -0.005 [-0.637, 1.000], loss: 0.522356, mean_absolute_error: 23.088828, mean_q: 31.053041, mean_eps: 0.100000
 1367104/2000000: episode: 6253, duration: 6.404s, episode steps: 400, steps per second: 62, episode reward: 220.142, mean reward: 0.550 [-17.494, 100.000], mean action: 1.117 [0.000, 3.000], mean observation: 0.129 [-0.748, 1.003], loss: 0.646451, mean_absolute_error: 21.666184, mean_q: 29.147377, mean_eps: 0.100000
 1367194/2000000: episode: 6254, duration: 1.430s, episode steps: 90, steps per second: 63, episode reward: -70.870, mean reward: -0.787 [-100.000, 9.336], mean action: 1.811 [0.000, 3.000], mean observation: 0.028 [-1.704, 1.000], loss: 0.809939, mean_absolute_error: 23.232344, mean_q: 31.178123, mean_eps: 0.100000
 1367305/2000000: episode: 6255, duration: 1.738s, episode steps: 111, steps per second: 64, episode reward: -64.507, mean reward: -0.581 [-100.000, 13.347], mean action: 1.775 [0.000, 3.000], mean observation: -0.011 [-0.949, 1.772], loss: 0.712052, mean_absolute_error: 22.056088, mean_q: 29.580644, mean_eps: 0.100000
 1367716/2000000: episode: 6256, duration: 6.569s, episode steps: 411, steps per second: 63, episode reward: 229.868, mean reward: 0.559 [-5.208, 100.000], mean action: 1.577 [0.000, 3.000], mean observation: 0.020 [-0.780, 1.000], loss: 0.598487, mean_absolute_error: 21.612978, mean_q: 29.059262, mean_eps: 0.100000
 1367930/2000000: episode: 6257, duration: 3.382s, episode steps: 214, steps per second: 63, episode reward: -35.498, mean reward: -0.166 [-100.000, 11.930], mean action: 1.533 [0.000, 3.000], mean observation: 0.009 [-0.965, 1.000], loss: 0.679124, mean_absolute_error: 22.190547, mean_q: 29.835324, mean_eps: 0.100000
 1368027/2000000: episode: 6258, duration: 1.498s, episode steps: 97, steps per second: 65, episode reward: 4.355, mean reward: 0.045 [-100.000, 22.550], mean action: 1.794 [0.000, 3.000], mean observation: 0.029 [-0.833, 1.000], loss: 0.532981, mean_absolute_error: 22.354845, mean_q: 30.109686, mean_eps: 0.100000
 1368372/2000000: episode: 6259, duration: 5.617s, episode steps: 345, steps per second: 61, episode reward: 206.959, mean reward: 0.600 [-19.327, 100.000], mean action: 2.235 [0.000, 3.000], mean observation: 0.183 [-0.679, 1.108], loss: 0.613108, mean_absolute_error: 22.556956, mean_q: 30.317314, mean_eps: 0.100000
 1368602/2000000: episode: 6260, duration: 3.623s, episode steps: 230, steps per second: 63, episode reward: 205.039, mean reward: 0.891 [-4.128, 100.000], mean action: 1.587 [0.000, 3.000], mean observation: 0.114 [-0.677, 1.000], loss: 0.777116, mean_absolute_error: 21.827951, mean_q: 29.298202, mean_eps: 0.100000
 1368884/2000000: episode: 6261, duration: 4.912s, episode steps: 282, steps per second: 57, episode reward: 214.982, mean reward: 0.762 [-17.131, 100.000], mean action: 2.128 [0.000, 3.000], mean observation: 0.191 [-0.690, 1.087], loss: 0.766023, mean_absolute_error: 22.511698, mean_q: 29.985524, mean_eps: 0.100000
 1369311/2000000: episode: 6262, duration: 6.781s, episode steps: 427, steps per second: 63, episode reward: 210.241, mean reward: 0.492 [-20.120, 100.000], mean action: 0.756 [0.000, 3.000], mean observation: 0.181 [-0.746, 1.000], loss: 0.624230, mean_absolute_error: 22.481892, mean_q: 30.082297, mean_eps: 0.100000
 1369635/2000000: episode: 6263, duration: 5.093s, episode steps: 324, steps per second: 64, episode reward: 209.623, mean reward: 0.647 [-17.084, 100.000], mean action: 2.059 [0.000, 3.000], mean observation: 0.171 [-0.690, 1.000], loss: 0.637441, mean_absolute_error: 22.370904, mean_q: 30.068129, mean_eps: 0.100000
 1369789/2000000: episode: 6264, duration: 2.467s, episode steps: 154, steps per second: 62, episode reward: -53.973, mean reward: -0.350 [-100.000, 23.287], mean action: 1.292 [0.000, 3.000], mean observation: 0.048 [-1.328, 1.000], loss: 0.485592, mean_absolute_error: 22.552645, mean_q: 30.410236, mean_eps: 0.100000
 1370286/2000000: episode: 6265, duration: 8.094s, episode steps: 497, steps per second: 61, episode reward: -125.517, mean reward: -0.253 [-100.000, 175.559], mean action: 1.513 [0.000, 3.000], mean observation: 0.123 [-2.675, 2.893], loss: 0.736639, mean_absolute_error: 22.557354, mean_q: 30.153032, mean_eps: 0.100000
 1370391/2000000: episode: 6266, duration: 1.643s, episode steps: 105, steps per second: 64, episode reward: -52.380, mean reward: -0.499 [-100.000, 16.834], mean action: 1.695 [0.000, 3.000], mean observation: -0.012 [-0.885, 1.000], loss: 0.447142, mean_absolute_error: 21.043518, mean_q: 28.257248, mean_eps: 0.100000
 1370572/2000000: episode: 6267, duration: 2.883s, episode steps: 181, steps per second: 63, episode reward: -25.411, mean reward: -0.140 [-100.000, 17.928], mean action: 1.840 [0.000, 3.000], mean observation: 0.070 [-1.804, 1.000], loss: 0.999636, mean_absolute_error: 23.531853, mean_q: 31.536008, mean_eps: 0.100000
 1370785/2000000: episode: 6268, duration: 3.384s, episode steps: 213, steps per second: 63, episode reward: -22.252, mean reward: -0.104 [-100.000, 19.390], mean action: 1.272 [0.000, 3.000], mean observation: 0.038 [-0.832, 1.000], loss: 0.667225, mean_absolute_error: 22.213567, mean_q: 29.570704, mean_eps: 0.100000
 1371032/2000000: episode: 6269, duration: 4.073s, episode steps: 247, steps per second: 61, episode reward: 220.541, mean reward: 0.893 [-2.834, 100.000], mean action: 1.291 [0.000, 3.000], mean observation: 0.024 [-0.669, 1.000], loss: 0.753529, mean_absolute_error: 23.598140, mean_q: 31.698405, mean_eps: 0.100000
 1371165/2000000: episode: 6270, duration: 2.180s, episode steps: 133, steps per second: 61, episode reward: -10.822, mean reward: -0.081 [-100.000, 14.194], mean action: 1.752 [0.000, 3.000], mean observation: 0.008 [-0.730, 1.000], loss: 0.696955, mean_absolute_error: 22.763511, mean_q: 30.292634, mean_eps: 0.100000
 1371286/2000000: episode: 6271, duration: 1.908s, episode steps: 121, steps per second: 63, episode reward: -90.969, mean reward: -0.752 [-100.000, 10.594], mean action: 1.537 [0.000, 3.000], mean observation: 0.002 [-1.036, 3.705], loss: 0.457782, mean_absolute_error: 22.977451, mean_q: 30.930182, mean_eps: 0.100000
 1371520/2000000: episode: 6272, duration: 3.762s, episode steps: 234, steps per second: 62, episode reward: 221.236, mean reward: 0.945 [-20.403, 100.000], mean action: 1.068 [0.000, 3.000], mean observation: 0.160 [-0.875, 1.000], loss: 0.542121, mean_absolute_error: 22.003661, mean_q: 29.622461, mean_eps: 0.100000
 1371684/2000000: episode: 6273, duration: 2.758s, episode steps: 164, steps per second: 59, episode reward: -1.070, mean reward: -0.007 [-100.000, 10.465], mean action: 1.921 [0.000, 3.000], mean observation: 0.079 [-0.646, 1.000], loss: 0.552353, mean_absolute_error: 23.319074, mean_q: 31.280979, mean_eps: 0.100000
 1371869/2000000: episode: 6274, duration: 2.887s, episode steps: 185, steps per second: 64, episode reward: 231.642, mean reward: 1.252 [-2.388, 100.000], mean action: 1.243 [0.000, 3.000], mean observation: 0.061 [-0.656, 1.000], loss: 0.888404, mean_absolute_error: 23.848698, mean_q: 31.976995, mean_eps: 0.100000
 1372147/2000000: episode: 6275, duration: 4.358s, episode steps: 278, steps per second: 64, episode reward: 198.579, mean reward: 0.714 [-3.202, 100.000], mean action: 1.385 [0.000, 3.000], mean observation: 0.026 [-0.532, 1.000], loss: 0.720684, mean_absolute_error: 23.184068, mean_q: 31.170796, mean_eps: 0.100000
 1372307/2000000: episode: 6276, duration: 2.460s, episode steps: 160, steps per second: 65, episode reward: -34.246, mean reward: -0.214 [-100.000, 11.942], mean action: 1.706 [0.000, 3.000], mean observation: 0.027 [-0.759, 1.953], loss: 0.687823, mean_absolute_error: 22.327152, mean_q: 29.993647, mean_eps: 0.100000
 1372687/2000000: episode: 6277, duration: 6.043s, episode steps: 380, steps per second: 63, episode reward: 210.703, mean reward: 0.554 [-18.856, 100.000], mean action: 1.003 [0.000, 3.000], mean observation: 0.068 [-0.566, 1.000], loss: 0.771455, mean_absolute_error: 22.164037, mean_q: 29.631630, mean_eps: 0.100000
 1372903/2000000: episode: 6278, duration: 3.391s, episode steps: 216, steps per second: 64, episode reward: -30.453, mean reward: -0.141 [-100.000, 21.590], mean action: 1.454 [0.000, 3.000], mean observation: 0.034 [-0.668, 1.106], loss: 0.626689, mean_absolute_error: 24.122734, mean_q: 32.450523, mean_eps: 0.100000
 1373407/2000000: episode: 6279, duration: 8.166s, episode steps: 504, steps per second: 62, episode reward: 198.617, mean reward: 0.394 [-7.119, 100.000], mean action: 1.460 [0.000, 3.000], mean observation: 0.031 [-0.751, 1.000], loss: 0.719411, mean_absolute_error: 22.477122, mean_q: 30.203649, mean_eps: 0.100000
 1373527/2000000: episode: 6280, duration: 1.890s, episode steps: 120, steps per second: 63, episode reward: -12.041, mean reward: -0.100 [-100.000, 15.272], mean action: 1.717 [0.000, 3.000], mean observation: 0.101 [-0.883, 1.000], loss: 0.996583, mean_absolute_error: 22.048389, mean_q: 29.519422, mean_eps: 0.100000
 1373850/2000000: episode: 6281, duration: 5.162s, episode steps: 323, steps per second: 63, episode reward: 222.344, mean reward: 0.688 [-13.915, 100.000], mean action: 1.991 [0.000, 3.000], mean observation: 0.165 [-0.696, 1.000], loss: 0.741624, mean_absolute_error: 22.260295, mean_q: 29.657618, mean_eps: 0.100000
 1373950/2000000: episode: 6282, duration: 1.587s, episode steps: 100, steps per second: 63, episode reward: -35.117, mean reward: -0.351 [-100.000, 17.632], mean action: 1.790 [0.000, 3.000], mean observation: 0.124 [-1.044, 2.492], loss: 0.890830, mean_absolute_error: 25.706380, mean_q: 34.366465, mean_eps: 0.100000
 1374174/2000000: episode: 6283, duration: 3.472s, episode steps: 224, steps per second: 65, episode reward: 2.586, mean reward: 0.012 [-100.000, 18.926], mean action: 1.504 [0.000, 3.000], mean observation: 0.024 [-0.753, 1.000], loss: 0.981840, mean_absolute_error: 22.260721, mean_q: 29.583240, mean_eps: 0.100000
 1374298/2000000: episode: 6284, duration: 1.924s, episode steps: 124, steps per second: 64, episode reward: -41.276, mean reward: -0.333 [-100.000, 17.374], mean action: 1.831 [0.000, 3.000], mean observation: 0.028 [-1.453, 1.000], loss: 0.760865, mean_absolute_error: 22.339172, mean_q: 29.988562, mean_eps: 0.100000
 1374749/2000000: episode: 6285, duration: 7.159s, episode steps: 451, steps per second: 63, episode reward: 189.831, mean reward: 0.421 [-17.548, 100.000], mean action: 1.927 [0.000, 3.000], mean observation: 0.086 [-0.822, 1.000], loss: 0.658910, mean_absolute_error: 22.441865, mean_q: 30.097613, mean_eps: 0.100000
 1374873/2000000: episode: 6286, duration: 1.942s, episode steps: 124, steps per second: 64, episode reward: -39.822, mean reward: -0.321 [-100.000, 17.252], mean action: 1.371 [0.000, 3.000], mean observation: 0.012 [-1.432, 1.000], loss: 0.652628, mean_absolute_error: 23.828715, mean_q: 31.730379, mean_eps: 0.100000
 1375061/2000000: episode: 6287, duration: 2.955s, episode steps: 188, steps per second: 64, episode reward: 7.513, mean reward: 0.040 [-100.000, 19.514], mean action: 1.569 [0.000, 3.000], mean observation: 0.051 [-0.642, 1.000], loss: 0.562136, mean_absolute_error: 22.798471, mean_q: 30.692818, mean_eps: 0.100000
 1375238/2000000: episode: 6288, duration: 2.753s, episode steps: 177, steps per second: 64, episode reward: -6.061, mean reward: -0.034 [-100.000, 14.763], mean action: 1.548 [0.000, 3.000], mean observation: 0.076 [-0.703, 1.018], loss: 0.674085, mean_absolute_error: 22.536081, mean_q: 30.290935, mean_eps: 0.100000
 1375395/2000000: episode: 6289, duration: 2.449s, episode steps: 157, steps per second: 64, episode reward: -50.287, mean reward: -0.320 [-100.000, 13.375], mean action: 1.758 [0.000, 3.000], mean observation: 0.028 [-0.807, 1.000], loss: 0.726271, mean_absolute_error: 21.469848, mean_q: 28.802037, mean_eps: 0.100000
 1375558/2000000: episode: 6290, duration: 2.538s, episode steps: 163, steps per second: 64, episode reward: -48.608, mean reward: -0.298 [-100.000, 7.559], mean action: 1.454 [0.000, 3.000], mean observation: 0.078 [-1.114, 1.000], loss: 0.666145, mean_absolute_error: 23.537739, mean_q: 31.541107, mean_eps: 0.100000
 1376011/2000000: episode: 6291, duration: 7.158s, episode steps: 453, steps per second: 63, episode reward: 211.389, mean reward: 0.467 [-17.600, 100.000], mean action: 1.987 [0.000, 3.000], mean observation: 0.086 [-0.749, 1.000], loss: 0.664642, mean_absolute_error: 23.521984, mean_q: 31.548271, mean_eps: 0.100000
 1376364/2000000: episode: 6292, duration: 5.652s, episode steps: 353, steps per second: 62, episode reward: 233.483, mean reward: 0.661 [-19.573, 100.000], mean action: 1.147 [0.000, 3.000], mean observation: 0.151 [-0.739, 1.013], loss: 0.882312, mean_absolute_error: 22.499036, mean_q: 30.054424, mean_eps: 0.100000
 1376632/2000000: episode: 6293, duration: 4.280s, episode steps: 268, steps per second: 63, episode reward: 231.075, mean reward: 0.862 [-11.924, 100.000], mean action: 1.280 [0.000, 3.000], mean observation: 0.066 [-0.679, 1.000], loss: 0.666648, mean_absolute_error: 23.329499, mean_q: 31.338627, mean_eps: 0.100000
 1377189/2000000: episode: 6294, duration: 9.144s, episode steps: 557, steps per second: 61, episode reward: 190.910, mean reward: 0.343 [-18.315, 100.000], mean action: 1.409 [0.000, 3.000], mean observation: 0.060 [-0.552, 1.000], loss: 0.661741, mean_absolute_error: 23.048382, mean_q: 30.950635, mean_eps: 0.100000
 1377336/2000000: episode: 6295, duration: 2.297s, episode steps: 147, steps per second: 64, episode reward: -7.510, mean reward: -0.051 [-100.000, 10.370], mean action: 1.905 [0.000, 3.000], mean observation: 0.067 [-0.631, 1.000], loss: 0.699888, mean_absolute_error: 23.951797, mean_q: 32.181084, mean_eps: 0.100000
 1377473/2000000: episode: 6296, duration: 2.151s, episode steps: 137, steps per second: 64, episode reward: -26.036, mean reward: -0.190 [-100.000, 16.215], mean action: 1.869 [0.000, 3.000], mean observation: -0.005 [-0.607, 1.433], loss: 0.461888, mean_absolute_error: 22.489965, mean_q: 30.293712, mean_eps: 0.100000
 1377597/2000000: episode: 6297, duration: 1.956s, episode steps: 124, steps per second: 63, episode reward: -31.686, mean reward: -0.256 [-100.000, 13.203], mean action: 1.637 [0.000, 3.000], mean observation: 0.010 [-0.727, 1.000], loss: 0.519056, mean_absolute_error: 22.439755, mean_q: 30.196867, mean_eps: 0.100000
 1378234/2000000: episode: 6298, duration: 10.328s, episode steps: 637, steps per second: 62, episode reward: 186.932, mean reward: 0.293 [-19.035, 100.000], mean action: 1.347 [0.000, 3.000], mean observation: 0.054 [-0.680, 1.000], loss: 0.749608, mean_absolute_error: 22.964098, mean_q: 30.787786, mean_eps: 0.100000
 1378421/2000000: episode: 6299, duration: 2.953s, episode steps: 187, steps per second: 63, episode reward: 204.574, mean reward: 1.094 [-18.086, 100.000], mean action: 1.150 [0.000, 3.000], mean observation: 0.155 [-1.188, 1.000], loss: 0.782568, mean_absolute_error: 22.100669, mean_q: 29.637088, mean_eps: 0.100000
 1378987/2000000: episode: 6300, duration: 9.422s, episode steps: 566, steps per second: 60, episode reward: -40.471, mean reward: -0.072 [-100.000, 22.454], mean action: 1.786 [0.000, 3.000], mean observation: 0.018 [-0.705, 1.000], loss: 0.755599, mean_absolute_error: 22.936450, mean_q: 30.798224, mean_eps: 0.100000
 1379329/2000000: episode: 6301, duration: 5.515s, episode steps: 342, steps per second: 62, episode reward: 213.533, mean reward: 0.624 [-10.513, 100.000], mean action: 1.260 [0.000, 3.000], mean observation: 0.044 [-0.690, 1.000], loss: 0.742815, mean_absolute_error: 23.080600, mean_q: 30.932645, mean_eps: 0.100000
 1379471/2000000: episode: 6302, duration: 2.224s, episode steps: 142, steps per second: 64, episode reward: -17.655, mean reward: -0.124 [-100.000, 16.824], mean action: 1.634 [0.000, 3.000], mean observation: 0.085 [-0.710, 1.000], loss: 1.028629, mean_absolute_error: 22.397233, mean_q: 30.053663, mean_eps: 0.100000
 1379844/2000000: episode: 6303, duration: 5.974s, episode steps: 373, steps per second: 62, episode reward: 230.421, mean reward: 0.618 [-11.401, 100.000], mean action: 1.024 [0.000, 3.000], mean observation: 0.094 [-0.501, 1.000], loss: 0.810630, mean_absolute_error: 22.777144, mean_q: 30.577090, mean_eps: 0.100000
 1379939/2000000: episode: 6304, duration: 1.510s, episode steps: 95, steps per second: 63, episode reward: -65.838, mean reward: -0.693 [-100.000, 14.451], mean action: 1.063 [0.000, 3.000], mean observation: 0.001 [-2.163, 1.000], loss: 0.592118, mean_absolute_error: 22.377433, mean_q: 30.135224, mean_eps: 0.100000
 1380073/2000000: episode: 6305, duration: 2.150s, episode steps: 134, steps per second: 62, episode reward: -39.263, mean reward: -0.293 [-100.000, 9.249], mean action: 1.634 [0.000, 3.000], mean observation: 0.125 [-0.684, 1.489], loss: 0.632535, mean_absolute_error: 21.883318, mean_q: 29.451862, mean_eps: 0.100000
 1380166/2000000: episode: 6306, duration: 1.454s, episode steps: 93, steps per second: 64, episode reward: -44.151, mean reward: -0.475 [-100.000, 22.159], mean action: 1.645 [0.000, 3.000], mean observation: -0.007 [-1.627, 1.000], loss: 0.846281, mean_absolute_error: 23.029708, mean_q: 30.911590, mean_eps: 0.100000
 1380301/2000000: episode: 6307, duration: 2.131s, episode steps: 135, steps per second: 63, episode reward: -62.029, mean reward: -0.459 [-100.000, 16.847], mean action: 1.830 [0.000, 3.000], mean observation: 0.039 [-0.876, 1.000], loss: 0.571466, mean_absolute_error: 22.834953, mean_q: 30.722788, mean_eps: 0.100000
 1380435/2000000: episode: 6308, duration: 2.070s, episode steps: 134, steps per second: 65, episode reward: -54.369, mean reward: -0.406 [-100.000, 17.851], mean action: 1.201 [0.000, 3.000], mean observation: 0.007 [-0.805, 1.548], loss: 0.504492, mean_absolute_error: 22.508778, mean_q: 30.361152, mean_eps: 0.100000
 1380654/2000000: episode: 6309, duration: 3.477s, episode steps: 219, steps per second: 63, episode reward: -21.546, mean reward: -0.098 [-100.000, 7.453], mean action: 1.466 [0.000, 3.000], mean observation: -0.023 [-0.694, 1.078], loss: 0.739326, mean_absolute_error: 22.590908, mean_q: 30.374599, mean_eps: 0.100000
 1380865/2000000: episode: 6310, duration: 3.344s, episode steps: 211, steps per second: 63, episode reward: -25.393, mean reward: -0.120 [-100.000, 22.182], mean action: 1.716 [0.000, 3.000], mean observation: 0.005 [-0.541, 1.000], loss: 0.965028, mean_absolute_error: 23.184184, mean_q: 31.089750, mean_eps: 0.100000
 1381288/2000000: episode: 6311, duration: 6.934s, episode steps: 423, steps per second: 61, episode reward: 185.776, mean reward: 0.439 [-17.838, 100.000], mean action: 1.475 [0.000, 3.000], mean observation: 0.113 [-0.659, 1.000], loss: 0.643700, mean_absolute_error: 22.840102, mean_q: 30.743940, mean_eps: 0.100000
 1381564/2000000: episode: 6312, duration: 4.427s, episode steps: 276, steps per second: 62, episode reward: 214.087, mean reward: 0.776 [-7.818, 100.000], mean action: 1.757 [0.000, 3.000], mean observation: 0.161 [-0.694, 1.000], loss: 0.566703, mean_absolute_error: 23.219335, mean_q: 30.968984, mean_eps: 0.100000
 1381700/2000000: episode: 6313, duration: 2.173s, episode steps: 136, steps per second: 63, episode reward: -2.453, mean reward: -0.018 [-100.000, 11.160], mean action: 1.625 [0.000, 3.000], mean observation: 0.036 [-0.707, 1.000], loss: 0.510483, mean_absolute_error: 23.136311, mean_q: 31.149473, mean_eps: 0.100000
 1381849/2000000: episode: 6314, duration: 2.380s, episode steps: 149, steps per second: 63, episode reward: -50.393, mean reward: -0.338 [-100.000, 10.977], mean action: 1.926 [0.000, 3.000], mean observation: 0.013 [-0.799, 2.369], loss: 0.759610, mean_absolute_error: 22.941970, mean_q: 30.845845, mean_eps: 0.100000
 1381981/2000000: episode: 6315, duration: 2.096s, episode steps: 132, steps per second: 63, episode reward: -28.873, mean reward: -0.219 [-100.000, 13.490], mean action: 1.674 [0.000, 3.000], mean observation: 0.071 [-1.378, 1.000], loss: 0.535151, mean_absolute_error: 21.922034, mean_q: 29.523372, mean_eps: 0.100000
 1382081/2000000: episode: 6316, duration: 1.587s, episode steps: 100, steps per second: 63, episode reward: -58.448, mean reward: -0.584 [-100.000, 5.303], mean action: 2.090 [0.000, 3.000], mean observation: -0.093 [-0.832, 2.664], loss: 0.600363, mean_absolute_error: 24.085000, mean_q: 31.977957, mean_eps: 0.100000
 1382523/2000000: episode: 6317, duration: 7.144s, episode steps: 442, steps per second: 62, episode reward: 217.339, mean reward: 0.492 [-19.770, 100.000], mean action: 0.873 [0.000, 3.000], mean observation: 0.138 [-1.225, 1.000], loss: 0.602730, mean_absolute_error: 23.155444, mean_q: 31.159161, mean_eps: 0.100000
 1382603/2000000: episode: 6318, duration: 1.275s, episode steps: 80, steps per second: 63, episode reward: -45.071, mean reward: -0.563 [-100.000, 16.438], mean action: 2.000 [0.000, 3.000], mean observation: -0.007 [-0.946, 1.000], loss: 0.830213, mean_absolute_error: 21.534692, mean_q: 28.943484, mean_eps: 0.100000
 1382679/2000000: episode: 6319, duration: 1.190s, episode steps: 76, steps per second: 64, episode reward: -83.133, mean reward: -1.094 [-100.000, 11.412], mean action: 1.645 [0.000, 3.000], mean observation: 0.034 [-3.269, 1.000], loss: 0.677465, mean_absolute_error: 23.242253, mean_q: 31.146692, mean_eps: 0.100000
 1382900/2000000: episode: 6320, duration: 3.484s, episode steps: 221, steps per second: 63, episode reward: -69.122, mean reward: -0.313 [-100.000, 13.203], mean action: 1.498 [0.000, 3.000], mean observation: 0.022 [-2.599, 1.000], loss: 0.760691, mean_absolute_error: 22.570626, mean_q: 30.304960, mean_eps: 0.100000
 1383115/2000000: episode: 6321, duration: 3.376s, episode steps: 215, steps per second: 64, episode reward: -24.358, mean reward: -0.113 [-100.000, 10.930], mean action: 1.688 [0.000, 3.000], mean observation: 0.061 [-1.432, 1.012], loss: 0.509685, mean_absolute_error: 22.991991, mean_q: 30.954855, mean_eps: 0.100000
 1383443/2000000: episode: 6322, duration: 5.251s, episode steps: 328, steps per second: 62, episode reward: 189.921, mean reward: 0.579 [-17.855, 100.000], mean action: 1.195 [0.000, 3.000], mean observation: 0.076 [-0.538, 1.000], loss: 0.534016, mean_absolute_error: 22.629055, mean_q: 30.425407, mean_eps: 0.100000
 1383547/2000000: episode: 6323, duration: 1.634s, episode steps: 104, steps per second: 64, episode reward: -66.571, mean reward: -0.640 [-100.000, 24.279], mean action: 1.885 [0.000, 3.000], mean observation: 0.017 [-2.422, 1.000], loss: 0.674107, mean_absolute_error: 25.201289, mean_q: 33.819928, mean_eps: 0.100000
 1383682/2000000: episode: 6324, duration: 2.094s, episode steps: 135, steps per second: 64, episode reward: -9.973, mean reward: -0.074 [-100.000, 20.990], mean action: 1.778 [0.000, 3.000], mean observation: 0.074 [-0.702, 1.000], loss: 0.918003, mean_absolute_error: 23.786342, mean_q: 31.877421, mean_eps: 0.100000
 1384011/2000000: episode: 6325, duration: 5.233s, episode steps: 329, steps per second: 63, episode reward: 206.614, mean reward: 0.628 [-17.819, 100.000], mean action: 1.240 [0.000, 3.000], mean observation: 0.116 [-0.868, 1.000], loss: 0.745633, mean_absolute_error: 23.300686, mean_q: 31.111155, mean_eps: 0.100000
 1384232/2000000: episode: 6326, duration: 3.493s, episode steps: 221, steps per second: 63, episode reward: -22.065, mean reward: -0.100 [-100.000, 11.984], mean action: 1.900 [0.000, 3.000], mean observation: 0.014 [-0.894, 1.000], loss: 0.681044, mean_absolute_error: 24.090042, mean_q: 31.939882, mean_eps: 0.100000
 1384379/2000000: episode: 6327, duration: 2.298s, episode steps: 147, steps per second: 64, episode reward: -92.475, mean reward: -0.629 [-100.000, 10.028], mean action: 1.429 [0.000, 3.000], mean observation: 0.043 [-0.886, 1.000], loss: 0.633829, mean_absolute_error: 23.924032, mean_q: 31.858642, mean_eps: 0.100000
 1384514/2000000: episode: 6328, duration: 2.131s, episode steps: 135, steps per second: 63, episode reward: -2.651, mean reward: -0.020 [-100.000, 16.805], mean action: 1.704 [0.000, 3.000], mean observation: 0.081 [-0.879, 1.247], loss: 0.825002, mean_absolute_error: 22.708315, mean_q: 30.525741, mean_eps: 0.100000
 1384684/2000000: episode: 6329, duration: 2.676s, episode steps: 170, steps per second: 64, episode reward: -23.516, mean reward: -0.138 [-100.000, 10.310], mean action: 1.694 [0.000, 3.000], mean observation: 0.066 [-0.715, 1.000], loss: 0.868604, mean_absolute_error: 23.250460, mean_q: 31.169889, mean_eps: 0.100000
 1384906/2000000: episode: 6330, duration: 3.458s, episode steps: 222, steps per second: 64, episode reward: -39.395, mean reward: -0.177 [-100.000, 15.038], mean action: 1.428 [0.000, 3.000], mean observation: 0.093 [-1.242, 1.660], loss: 0.559118, mean_absolute_error: 24.105113, mean_q: 32.435821, mean_eps: 0.100000
 1385307/2000000: episode: 6331, duration: 6.528s, episode steps: 401, steps per second: 61, episode reward: -659.420, mean reward: -1.644 [-100.000, 5.070], mean action: 1.818 [0.000, 3.000], mean observation: 0.275 [-0.764, 4.207], loss: 0.613714, mean_absolute_error: 23.182512, mean_q: 31.173977, mean_eps: 0.100000
 1385454/2000000: episode: 6332, duration: 2.312s, episode steps: 147, steps per second: 64, episode reward: -48.724, mean reward: -0.331 [-100.000, 20.536], mean action: 1.782 [0.000, 3.000], mean observation: -0.043 [-1.140, 1.010], loss: 0.824742, mean_absolute_error: 23.128973, mean_q: 31.005392, mean_eps: 0.100000
 1385797/2000000: episode: 6333, duration: 5.492s, episode steps: 343, steps per second: 62, episode reward: -41.848, mean reward: -0.122 [-100.000, 21.269], mean action: 1.939 [0.000, 3.000], mean observation: 0.063 [-0.657, 1.026], loss: 0.788703, mean_absolute_error: 22.589722, mean_q: 30.231100, mean_eps: 0.100000
 1385959/2000000: episode: 6334, duration: 2.504s, episode steps: 162, steps per second: 65, episode reward: -38.745, mean reward: -0.239 [-100.000, 17.944], mean action: 2.006 [0.000, 3.000], mean observation: 0.106 [-1.564, 1.000], loss: 0.532887, mean_absolute_error: 22.707181, mean_q: 30.272397, mean_eps: 0.100000
 1386143/2000000: episode: 6335, duration: 3.023s, episode steps: 184, steps per second: 61, episode reward: -46.350, mean reward: -0.252 [-100.000, 17.456], mean action: 1.337 [0.000, 3.000], mean observation: 0.032 [-1.615, 1.000], loss: 0.773999, mean_absolute_error: 23.362661, mean_q: 31.308900, mean_eps: 0.100000
 1386955/2000000: episode: 6336, duration: 13.569s, episode steps: 812, steps per second: 60, episode reward: -610.191, mean reward: -0.751 [-100.000, 6.894], mean action: 1.656 [0.000, 3.000], mean observation: 0.219 [-1.181, 3.459], loss: 0.793049, mean_absolute_error: 22.556235, mean_q: 30.228236, mean_eps: 0.100000
 1387102/2000000: episode: 6337, duration: 2.300s, episode steps: 147, steps per second: 64, episode reward: -25.849, mean reward: -0.176 [-100.000, 16.089], mean action: 1.714 [0.000, 3.000], mean observation: 0.045 [-1.315, 1.000], loss: 0.880515, mean_absolute_error: 23.235480, mean_q: 31.009520, mean_eps: 0.100000
 1387246/2000000: episode: 6338, duration: 2.267s, episode steps: 144, steps per second: 64, episode reward: -13.591, mean reward: -0.094 [-100.000, 16.054], mean action: 1.875 [0.000, 3.000], mean observation: -0.038 [-1.672, 1.000], loss: 0.918392, mean_absolute_error: 23.664496, mean_q: 31.652304, mean_eps: 0.100000
 1387474/2000000: episode: 6339, duration: 3.559s, episode steps: 228, steps per second: 64, episode reward: -31.491, mean reward: -0.138 [-100.000, 22.311], mean action: 1.425 [0.000, 3.000], mean observation: 0.007 [-0.596, 1.000], loss: 0.792987, mean_absolute_error: 23.150444, mean_q: 31.063812, mean_eps: 0.100000
 1387638/2000000: episode: 6340, duration: 2.898s, episode steps: 164, steps per second: 57, episode reward: -9.050, mean reward: -0.055 [-100.000, 7.811], mean action: 1.360 [0.000, 3.000], mean observation: 0.101 [-0.743, 1.000], loss: 0.747529, mean_absolute_error: 22.981494, mean_q: 30.873372, mean_eps: 0.100000
 1387986/2000000: episode: 6341, duration: 5.869s, episode steps: 348, steps per second: 59, episode reward: 219.553, mean reward: 0.631 [-14.045, 100.000], mean action: 2.365 [0.000, 3.000], mean observation: 0.151 [-0.794, 1.000], loss: 0.626780, mean_absolute_error: 23.631798, mean_q: 31.747185, mean_eps: 0.100000
 1388114/2000000: episode: 6342, duration: 2.068s, episode steps: 128, steps per second: 62, episode reward: -5.145, mean reward: -0.040 [-100.000, 17.156], mean action: 1.945 [0.000, 3.000], mean observation: -0.075 [-0.841, 1.156], loss: 0.573834, mean_absolute_error: 22.539665, mean_q: 30.219236, mean_eps: 0.100000
 1388279/2000000: episode: 6343, duration: 2.565s, episode steps: 165, steps per second: 64, episode reward: -62.214, mean reward: -0.377 [-100.000, 23.085], mean action: 1.436 [0.000, 3.000], mean observation: 0.026 [-0.646, 1.000], loss: 0.802265, mean_absolute_error: 22.564801, mean_q: 30.299886, mean_eps: 0.100000
 1388792/2000000: episode: 6344, duration: 8.119s, episode steps: 513, steps per second: 63, episode reward: 143.178, mean reward: 0.279 [-13.512, 100.000], mean action: 1.616 [0.000, 3.000], mean observation: 0.181 [-0.757, 1.002], loss: 0.740880, mean_absolute_error: 23.376914, mean_q: 31.242342, mean_eps: 0.100000
 1388923/2000000: episode: 6345, duration: 2.111s, episode steps: 131, steps per second: 62, episode reward: 4.941, mean reward: 0.038 [-100.000, 11.687], mean action: 1.656 [0.000, 3.000], mean observation: 0.077 [-1.112, 1.000], loss: 0.813106, mean_absolute_error: 22.528882, mean_q: 29.999717, mean_eps: 0.100000
 1389508/2000000: episode: 6346, duration: 9.677s, episode steps: 585, steps per second: 60, episode reward: 200.555, mean reward: 0.343 [-17.571, 100.000], mean action: 0.918 [0.000, 3.000], mean observation: 0.191 [-0.676, 1.000], loss: 0.755351, mean_absolute_error: 23.151571, mean_q: 31.070067, mean_eps: 0.100000
 1389894/2000000: episode: 6347, duration: 6.065s, episode steps: 386, steps per second: 64, episode reward: -268.762, mean reward: -0.696 [-100.000, 5.709], mean action: 1.731 [0.000, 3.000], mean observation: 0.210 [-0.729, 1.977], loss: 0.675610, mean_absolute_error: 24.080546, mean_q: 32.179253, mean_eps: 0.100000
 1390113/2000000: episode: 6348, duration: 3.453s, episode steps: 219, steps per second: 63, episode reward: 20.968, mean reward: 0.096 [-100.000, 19.384], mean action: 1.530 [0.000, 3.000], mean observation: -0.020 [-0.733, 1.000], loss: 0.645017, mean_absolute_error: 23.460316, mean_q: 31.505336, mean_eps: 0.100000
 1390208/2000000: episode: 6349, duration: 1.491s, episode steps: 95, steps per second: 64, episode reward: -58.257, mean reward: -0.613 [-100.000, 12.294], mean action: 1.979 [0.000, 3.000], mean observation: 0.098 [-0.973, 1.508], loss: 1.091493, mean_absolute_error: 23.840519, mean_q: 31.885539, mean_eps: 0.100000
 1390387/2000000: episode: 6350, duration: 2.772s, episode steps: 179, steps per second: 65, episode reward: -74.857, mean reward: -0.418 [-100.000, 10.896], mean action: 1.760 [0.000, 3.000], mean observation: 0.032 [-0.733, 1.000], loss: 0.508678, mean_absolute_error: 23.160330, mean_q: 31.222167, mean_eps: 0.100000
 1390590/2000000: episode: 6351, duration: 3.140s, episode steps: 203, steps per second: 65, episode reward: -54.211, mean reward: -0.267 [-100.000, 20.647], mean action: 1.576 [0.000, 3.000], mean observation: 0.022 [-0.568, 1.327], loss: 0.773148, mean_absolute_error: 23.972893, mean_q: 32.124063, mean_eps: 0.100000
 1391177/2000000: episode: 6352, duration: 9.439s, episode steps: 587, steps per second: 62, episode reward: 198.536, mean reward: 0.338 [-17.767, 100.000], mean action: 1.162 [0.000, 3.000], mean observation: 0.075 [-0.643, 1.000], loss: 0.742446, mean_absolute_error: 23.644474, mean_q: 31.770605, mean_eps: 0.100000
 1391263/2000000: episode: 6353, duration: 1.336s, episode steps: 86, steps per second: 64, episode reward: -118.960, mean reward: -1.383 [-100.000, 6.341], mean action: 1.081 [0.000, 3.000], mean observation: -0.056 [-1.160, 3.921], loss: 0.803380, mean_absolute_error: 23.238122, mean_q: 31.195663, mean_eps: 0.100000
 1391342/2000000: episode: 6354, duration: 1.237s, episode steps: 79, steps per second: 64, episode reward: -81.750, mean reward: -1.035 [-100.000, 11.180], mean action: 1.582 [0.000, 3.000], mean observation: 0.064 [-0.989, 3.246], loss: 0.625365, mean_absolute_error: 24.620408, mean_q: 33.060570, mean_eps: 0.100000
 1391721/2000000: episode: 6355, duration: 5.934s, episode steps: 379, steps per second: 64, episode reward: 187.717, mean reward: 0.495 [-18.549, 100.000], mean action: 1.047 [0.000, 3.000], mean observation: 0.162 [-0.951, 1.000], loss: 0.732779, mean_absolute_error: 22.584037, mean_q: 30.271154, mean_eps: 0.100000
 1392066/2000000: episode: 6356, duration: 5.536s, episode steps: 345, steps per second: 62, episode reward: 223.164, mean reward: 0.647 [-9.735, 100.000], mean action: 1.536 [0.000, 3.000], mean observation: 0.006 [-0.661, 1.000], loss: 1.180307, mean_absolute_error: 24.017097, mean_q: 32.029657, mean_eps: 0.100000
 1392282/2000000: episode: 6357, duration: 3.346s, episode steps: 216, steps per second: 65, episode reward: 0.192, mean reward: 0.001 [-100.000, 19.055], mean action: 1.398 [0.000, 3.000], mean observation: 0.011 [-0.835, 1.000], loss: 0.729763, mean_absolute_error: 23.810548, mean_q: 31.925594, mean_eps: 0.100000
 1392442/2000000: episode: 6358, duration: 2.484s, episode steps: 160, steps per second: 64, episode reward: -58.809, mean reward: -0.368 [-100.000, 23.542], mean action: 1.212 [0.000, 3.000], mean observation: 0.003 [-1.208, 1.000], loss: 0.550612, mean_absolute_error: 23.679222, mean_q: 31.748337, mean_eps: 0.100000
 1392514/2000000: episode: 6359, duration: 1.136s, episode steps: 72, steps per second: 63, episode reward: -112.702, mean reward: -1.565 [-100.000, 7.790], mean action: 1.417 [0.000, 3.000], mean observation: 0.064 [-2.403, 1.000], loss: 0.660105, mean_absolute_error: 25.932819, mean_q: 34.185369, mean_eps: 0.100000
 1392652/2000000: episode: 6360, duration: 2.182s, episode steps: 138, steps per second: 63, episode reward: -29.668, mean reward: -0.215 [-100.000, 11.662], mean action: 1.457 [0.000, 3.000], mean observation: 0.083 [-1.615, 1.000], loss: 0.714957, mean_absolute_error: 23.392929, mean_q: 31.403473, mean_eps: 0.100000
 1392728/2000000: episode: 6361, duration: 1.318s, episode steps: 76, steps per second: 58, episode reward: -93.513, mean reward: -1.230 [-100.000, 17.981], mean action: 1.105 [0.000, 3.000], mean observation: -0.015 [-1.283, 1.000], loss: 1.007226, mean_absolute_error: 23.475812, mean_q: 31.434518, mean_eps: 0.100000
 1392813/2000000: episode: 6362, duration: 1.366s, episode steps: 85, steps per second: 62, episode reward: -114.526, mean reward: -1.347 [-100.000, 11.203], mean action: 0.859 [0.000, 3.000], mean observation: -0.032 [-1.207, 1.000], loss: 0.836156, mean_absolute_error: 24.793023, mean_q: 32.990886, mean_eps: 0.100000
 1393000/2000000: episode: 6363, duration: 2.962s, episode steps: 187, steps per second: 63, episode reward: 2.091, mean reward: 0.011 [-100.000, 13.639], mean action: 1.738 [0.000, 3.000], mean observation: -0.006 [-0.780, 1.000], loss: 1.092455, mean_absolute_error: 23.339004, mean_q: 31.114638, mean_eps: 0.100000
 1393078/2000000: episode: 6364, duration: 1.244s, episode steps: 78, steps per second: 63, episode reward: -107.287, mean reward: -1.375 [-100.000, 18.916], mean action: 1.974 [0.000, 3.000], mean observation: -0.028 [-1.261, 1.000], loss: 0.780189, mean_absolute_error: 23.268058, mean_q: 31.241860, mean_eps: 0.100000
 1393154/2000000: episode: 6365, duration: 1.232s, episode steps: 76, steps per second: 62, episode reward: -76.517, mean reward: -1.007 [-100.000, 12.393], mean action: 2.026 [0.000, 3.000], mean observation: 0.042 [-1.059, 2.930], loss: 0.878443, mean_absolute_error: 24.142193, mean_q: 32.315341, mean_eps: 0.100000
 1393392/2000000: episode: 6366, duration: 3.741s, episode steps: 238, steps per second: 64, episode reward: -12.560, mean reward: -0.053 [-100.000, 22.397], mean action: 1.500 [0.000, 3.000], mean observation: 0.026 [-0.544, 1.144], loss: 0.559507, mean_absolute_error: 24.075781, mean_q: 31.904867, mean_eps: 0.100000
 1393605/2000000: episode: 6367, duration: 3.500s, episode steps: 213, steps per second: 61, episode reward: 0.693, mean reward: 0.003 [-100.000, 18.437], mean action: 1.512 [0.000, 3.000], mean observation: 0.016 [-0.594, 1.000], loss: 0.737169, mean_absolute_error: 24.783436, mean_q: 33.205647, mean_eps: 0.100000
 1393758/2000000: episode: 6368, duration: 2.372s, episode steps: 153, steps per second: 64, episode reward: -38.387, mean reward: -0.251 [-100.000, 20.858], mean action: 1.399 [0.000, 3.000], mean observation: 0.020 [-1.286, 1.000], loss: 0.491751, mean_absolute_error: 24.039824, mean_q: 32.298190, mean_eps: 0.100000
 1393997/2000000: episode: 6369, duration: 3.770s, episode steps: 239, steps per second: 63, episode reward: -52.958, mean reward: -0.222 [-100.000, 17.746], mean action: 1.561 [0.000, 3.000], mean observation: 0.025 [-1.609, 1.000], loss: 0.564200, mean_absolute_error: 23.312872, mean_q: 31.341413, mean_eps: 0.100000
 1394086/2000000: episode: 6370, duration: 1.385s, episode steps: 89, steps per second: 64, episode reward: -48.633, mean reward: -0.546 [-100.000, 15.714], mean action: 1.652 [0.000, 3.000], mean observation: -0.015 [-0.899, 1.000], loss: 0.676374, mean_absolute_error: 24.124289, mean_q: 32.445446, mean_eps: 0.100000
 1394193/2000000: episode: 6371, duration: 1.692s, episode steps: 107, steps per second: 63, episode reward: -99.382, mean reward: -0.929 [-100.000, 13.934], mean action: 1.832 [0.000, 3.000], mean observation: 0.050 [-1.776, 1.000], loss: 0.507390, mean_absolute_error: 22.449797, mean_q: 30.181257, mean_eps: 0.100000
 1394324/2000000: episode: 6372, duration: 2.050s, episode steps: 131, steps per second: 64, episode reward: -44.782, mean reward: -0.342 [-100.000, 12.581], mean action: 1.885 [0.000, 3.000], mean observation: -0.066 [-0.854, 1.000], loss: 0.999276, mean_absolute_error: 24.076927, mean_q: 32.215796, mean_eps: 0.100000
 1394397/2000000: episode: 6373, duration: 1.185s, episode steps: 73, steps per second: 62, episode reward: -130.703, mean reward: -1.790 [-100.000, 7.459], mean action: 0.959 [0.000, 3.000], mean observation: 0.007 [-1.414, 1.000], loss: 0.655152, mean_absolute_error: 23.547666, mean_q: 31.567573, mean_eps: 0.100000
 1394465/2000000: episode: 6374, duration: 1.054s, episode steps: 68, steps per second: 65, episode reward: -139.098, mean reward: -2.046 [-100.000, 11.675], mean action: 0.647 [0.000, 3.000], mean observation: -0.029 [-1.652, 1.000], loss: 0.646046, mean_absolute_error: 25.431443, mean_q: 34.079629, mean_eps: 0.100000
 1394555/2000000: episode: 6375, duration: 1.378s, episode steps: 90, steps per second: 65, episode reward: -51.279, mean reward: -0.570 [-100.000, 16.208], mean action: 1.211 [0.000, 3.000], mean observation: -0.032 [-1.266, 1.000], loss: 0.619128, mean_absolute_error: 22.054624, mean_q: 29.628737, mean_eps: 0.100000
 1394632/2000000: episode: 6376, duration: 1.245s, episode steps: 77, steps per second: 62, episode reward: -62.667, mean reward: -0.814 [-100.000, 23.734], mean action: 1.961 [0.000, 3.000], mean observation: -0.053 [-1.141, 1.000], loss: 0.655854, mean_absolute_error: 23.123059, mean_q: 31.066269, mean_eps: 0.100000
 1395632/2000000: episode: 6377, duration: 16.391s, episode steps: 1000, steps per second: 61, episode reward: 84.768, mean reward: 0.085 [-18.954, 16.234], mean action: 1.073 [0.000, 3.000], mean observation: 0.216 [-0.993, 1.000], loss: 0.736357, mean_absolute_error: 24.208792, mean_q: 32.437124, mean_eps: 0.100000
 1395763/2000000: episode: 6378, duration: 2.096s, episode steps: 131, steps per second: 63, episode reward: -57.067, mean reward: -0.436 [-100.000, 14.009], mean action: 1.763 [0.000, 3.000], mean observation: 0.107 [-0.713, 1.433], loss: 0.741203, mean_absolute_error: 24.337956, mean_q: 32.584854, mean_eps: 0.100000
 1395976/2000000: episode: 6379, duration: 3.367s, episode steps: 213, steps per second: 63, episode reward: -34.210, mean reward: -0.161 [-100.000, 14.878], mean action: 1.784 [0.000, 3.000], mean observation: -0.002 [-0.717, 1.000], loss: 0.593570, mean_absolute_error: 23.310237, mean_q: 31.351721, mean_eps: 0.100000
 1396258/2000000: episode: 6380, duration: 4.473s, episode steps: 282, steps per second: 63, episode reward: 236.934, mean reward: 0.840 [-3.222, 100.000], mean action: 1.362 [0.000, 3.000], mean observation: 0.061 [-0.672, 1.000], loss: 0.779908, mean_absolute_error: 22.569535, mean_q: 30.273756, mean_eps: 0.100000
 1396372/2000000: episode: 6381, duration: 1.824s, episode steps: 114, steps per second: 62, episode reward: -39.412, mean reward: -0.346 [-100.000, 8.999], mean action: 1.868 [0.000, 3.000], mean observation: 0.090 [-2.400, 1.000], loss: 0.701311, mean_absolute_error: 22.477536, mean_q: 30.164035, mean_eps: 0.100000
 1396504/2000000: episode: 6382, duration: 2.064s, episode steps: 132, steps per second: 64, episode reward: -46.463, mean reward: -0.352 [-100.000, 17.504], mean action: 1.371 [0.000, 3.000], mean observation: 0.071 [-1.002, 1.000], loss: 0.796286, mean_absolute_error: 24.309019, mean_q: 32.514078, mean_eps: 0.100000
 1396639/2000000: episode: 6383, duration: 2.114s, episode steps: 135, steps per second: 64, episode reward: -100.808, mean reward: -0.747 [-100.000, 7.501], mean action: 1.178 [0.000, 3.000], mean observation: -0.022 [-1.255, 3.934], loss: 0.668855, mean_absolute_error: 23.133178, mean_q: 31.030549, mean_eps: 0.100000
 1396930/2000000: episode: 6384, duration: 4.767s, episode steps: 291, steps per second: 61, episode reward: 206.395, mean reward: 0.709 [-21.622, 100.000], mean action: 1.107 [0.000, 3.000], mean observation: 0.070 [-0.806, 1.000], loss: 0.636473, mean_absolute_error: 23.687966, mean_q: 31.707283, mean_eps: 0.100000
 1397058/2000000: episode: 6385, duration: 2.020s, episode steps: 128, steps per second: 63, episode reward: -62.924, mean reward: -0.492 [-100.000, 7.899], mean action: 1.758 [0.000, 3.000], mean observation: 0.080 [-0.794, 2.459], loss: 0.744348, mean_absolute_error: 23.905419, mean_q: 31.939399, mean_eps: 0.100000
 1397157/2000000: episode: 6386, duration: 1.572s, episode steps: 99, steps per second: 63, episode reward: -112.771, mean reward: -1.139 [-100.000, 17.988], mean action: 0.929 [0.000, 3.000], mean observation: 0.010 [-1.419, 1.000], loss: 1.310319, mean_absolute_error: 24.252226, mean_q: 32.301236, mean_eps: 0.100000
 1397279/2000000: episode: 6387, duration: 1.879s, episode steps: 122, steps per second: 65, episode reward: -35.341, mean reward: -0.290 [-100.000, 10.572], mean action: 1.352 [0.000, 3.000], mean observation: 0.076 [-0.849, 1.000], loss: 0.754078, mean_absolute_error: 23.716126, mean_q: 31.778115, mean_eps: 0.100000
 1397393/2000000: episode: 6388, duration: 1.778s, episode steps: 114, steps per second: 64, episode reward: -83.384, mean reward: -0.731 [-100.000, 12.759], mean action: 1.140 [0.000, 3.000], mean observation: -0.004 [-1.138, 3.318], loss: 0.582674, mean_absolute_error: 23.442585, mean_q: 31.446812, mean_eps: 0.100000
 1397618/2000000: episode: 6389, duration: 3.539s, episode steps: 225, steps per second: 64, episode reward: -36.645, mean reward: -0.163 [-100.000, 13.343], mean action: 1.502 [0.000, 3.000], mean observation: 0.006 [-1.916, 1.000], loss: 1.078027, mean_absolute_error: 24.201863, mean_q: 32.311036, mean_eps: 0.100000
 1397751/2000000: episode: 6390, duration: 2.071s, episode steps: 133, steps per second: 64, episode reward: -36.160, mean reward: -0.272 [-100.000, 24.141], mean action: 1.910 [0.000, 3.000], mean observation: 0.066 [-0.810, 1.000], loss: 0.634389, mean_absolute_error: 24.878606, mean_q: 33.407862, mean_eps: 0.100000
 1398209/2000000: episode: 6391, duration: 7.431s, episode steps: 458, steps per second: 62, episode reward: 159.762, mean reward: 0.349 [-17.783, 100.000], mean action: 1.225 [0.000, 3.000], mean observation: 0.157 [-0.913, 1.000], loss: 0.686161, mean_absolute_error: 23.946405, mean_q: 32.157997, mean_eps: 0.100000
 1398298/2000000: episode: 6392, duration: 1.412s, episode steps: 89, steps per second: 63, episode reward: -33.367, mean reward: -0.375 [-100.000, 21.671], mean action: 1.921 [0.000, 3.000], mean observation: 0.012 [-0.834, 2.151], loss: 0.789796, mean_absolute_error: 24.108094, mean_q: 32.324653, mean_eps: 0.100000
 1398438/2000000: episode: 6393, duration: 2.245s, episode steps: 140, steps per second: 62, episode reward: -75.925, mean reward: -0.542 [-100.000, 20.935], mean action: 1.343 [0.000, 3.000], mean observation: 0.060 [-1.038, 3.381], loss: 1.196125, mean_absolute_error: 25.199190, mean_q: 33.629568, mean_eps: 0.100000
 1398606/2000000: episode: 6394, duration: 2.647s, episode steps: 168, steps per second: 63, episode reward: -41.673, mean reward: -0.248 [-100.000, 8.087], mean action: 1.452 [0.000, 3.000], mean observation: 0.094 [-0.972, 1.000], loss: 0.624647, mean_absolute_error: 25.001511, mean_q: 33.575853, mean_eps: 0.100000
 1398702/2000000: episode: 6395, duration: 1.512s, episode steps: 96, steps per second: 63, episode reward: -45.819, mean reward: -0.477 [-100.000, 15.687], mean action: 1.979 [1.000, 3.000], mean observation: 0.094 [-0.972, 1.000], loss: 1.475439, mean_absolute_error: 24.286371, mean_q: 32.316580, mean_eps: 0.100000
 1399151/2000000: episode: 6396, duration: 7.252s, episode steps: 449, steps per second: 62, episode reward: 215.544, mean reward: 0.480 [-17.561, 100.000], mean action: 1.568 [0.000, 3.000], mean observation: 0.077 [-0.723, 1.000], loss: 0.727439, mean_absolute_error: 23.532299, mean_q: 31.544495, mean_eps: 0.100000
 1399226/2000000: episode: 6397, duration: 1.202s, episode steps: 75, steps per second: 62, episode reward: -113.366, mean reward: -1.512 [-100.000, 16.889], mean action: 1.093 [0.000, 3.000], mean observation: 0.018 [-1.339, 1.000], loss: 0.883405, mean_absolute_error: 25.812212, mean_q: 34.542901, mean_eps: 0.100000
 1399403/2000000: episode: 6398, duration: 2.758s, episode steps: 177, steps per second: 64, episode reward: -89.562, mean reward: -0.506 [-100.000, 6.521], mean action: 1.446 [0.000, 3.000], mean observation: 0.036 [-0.901, 1.000], loss: 0.736258, mean_absolute_error: 23.056022, mean_q: 30.937181, mean_eps: 0.100000
 1399675/2000000: episode: 6399, duration: 4.473s, episode steps: 272, steps per second: 61, episode reward: 233.354, mean reward: 0.858 [-17.761, 100.000], mean action: 0.816 [0.000, 3.000], mean observation: 0.164 [-0.939, 1.000], loss: 0.728422, mean_absolute_error: 24.063562, mean_q: 32.282748, mean_eps: 0.100000
 1400036/2000000: episode: 6400, duration: 5.914s, episode steps: 361, steps per second: 61, episode reward: 200.278, mean reward: 0.555 [-9.659, 100.000], mean action: 0.997 [0.000, 3.000], mean observation: 0.119 [-0.685, 1.000], loss: 0.826664, mean_absolute_error: 24.680603, mean_q: 32.643254, mean_eps: 0.100000
 1400113/2000000: episode: 6401, duration: 1.268s, episode steps: 77, steps per second: 61, episode reward: -78.437, mean reward: -1.019 [-100.000, 19.896], mean action: 1.065 [0.000, 3.000], mean observation: -0.012 [-1.201, 1.000], loss: 0.614147, mean_absolute_error: 24.163263, mean_q: 32.416098, mean_eps: 0.100000
 1400239/2000000: episode: 6402, duration: 1.922s, episode steps: 126, steps per second: 66, episode reward: -30.557, mean reward: -0.243 [-100.000, 14.651], mean action: 1.643 [0.000, 3.000], mean observation: 0.012 [-0.938, 1.977], loss: 0.512300, mean_absolute_error: 24.626589, mean_q: 33.067497, mean_eps: 0.100000
 1400440/2000000: episode: 6403, duration: 3.223s, episode steps: 201, steps per second: 62, episode reward: 13.378, mean reward: 0.067 [-100.000, 14.730], mean action: 1.711 [0.000, 3.000], mean observation: -0.010 [-0.663, 1.000], loss: 0.731762, mean_absolute_error: 23.929071, mean_q: 32.105594, mean_eps: 0.100000
 1400738/2000000: episode: 6404, duration: 4.979s, episode steps: 298, steps per second: 60, episode reward: 250.472, mean reward: 0.841 [-9.105, 100.000], mean action: 1.168 [0.000, 3.000], mean observation: 0.088 [-0.716, 1.000], loss: 0.756753, mean_absolute_error: 24.431669, mean_q: 32.713263, mean_eps: 0.100000
 1401089/2000000: episode: 6405, duration: 5.832s, episode steps: 351, steps per second: 60, episode reward: -154.599, mean reward: -0.440 [-100.000, 11.558], mean action: 1.689 [0.000, 3.000], mean observation: 0.064 [-1.258, 1.000], loss: 0.952956, mean_absolute_error: 24.481863, mean_q: 32.707846, mean_eps: 0.100000
 1401163/2000000: episode: 6406, duration: 1.150s, episode steps: 74, steps per second: 64, episode reward: -92.554, mean reward: -1.251 [-100.000, 22.070], mean action: 1.000 [0.000, 3.000], mean observation: 0.070 [-1.136, 3.345], loss: 1.049461, mean_absolute_error: 22.659394, mean_q: 30.267590, mean_eps: 0.100000
 1401394/2000000: episode: 6407, duration: 3.629s, episode steps: 231, steps per second: 64, episode reward: -9.061, mean reward: -0.039 [-100.000, 9.023], mean action: 1.463 [0.000, 3.000], mean observation: -0.006 [-0.586, 1.327], loss: 0.842052, mean_absolute_error: 23.762702, mean_q: 31.701824, mean_eps: 0.100000
 1401456/2000000: episode: 6408, duration: 1.041s, episode steps: 62, steps per second: 60, episode reward: -117.075, mean reward: -1.888 [-100.000, 16.738], mean action: 0.500 [0.000, 3.000], mean observation: -0.059 [-1.688, 1.000], loss: 0.800175, mean_absolute_error: 24.690469, mean_q: 33.073196, mean_eps: 0.100000
 1401630/2000000: episode: 6409, duration: 2.826s, episode steps: 174, steps per second: 62, episode reward: -51.856, mean reward: -0.298 [-100.000, 19.848], mean action: 1.362 [0.000, 3.000], mean observation: 0.028 [-0.857, 1.000], loss: 0.793616, mean_absolute_error: 23.681877, mean_q: 31.649996, mean_eps: 0.100000
 1401703/2000000: episode: 6410, duration: 1.131s, episode steps: 73, steps per second: 65, episode reward: -113.797, mean reward: -1.559 [-100.000, 18.072], mean action: 1.973 [0.000, 3.000], mean observation: -0.012 [-1.370, 1.000], loss: 0.521285, mean_absolute_error: 24.510205, mean_q: 32.906164, mean_eps: 0.100000
 1401857/2000000: episode: 6411, duration: 2.486s, episode steps: 154, steps per second: 62, episode reward: -33.673, mean reward: -0.219 [-100.000, 13.365], mean action: 1.838 [0.000, 3.000], mean observation: 0.053 [-0.750, 1.000], loss: 0.691592, mean_absolute_error: 23.995073, mean_q: 32.224337, mean_eps: 0.100000
 1402003/2000000: episode: 6412, duration: 2.329s, episode steps: 146, steps per second: 63, episode reward: -92.852, mean reward: -0.636 [-100.000, 18.253], mean action: 1.377 [0.000, 3.000], mean observation: 0.012 [-1.290, 1.000], loss: 0.639180, mean_absolute_error: 24.116622, mean_q: 32.239125, mean_eps: 0.100000
 1402335/2000000: episode: 6413, duration: 5.243s, episode steps: 332, steps per second: 63, episode reward: -83.373, mean reward: -0.251 [-100.000, 15.703], mean action: 1.377 [0.000, 3.000], mean observation: 0.035 [-1.242, 1.007], loss: 0.795357, mean_absolute_error: 25.072971, mean_q: 33.176515, mean_eps: 0.100000
 1402478/2000000: episode: 6414, duration: 2.250s, episode steps: 143, steps per second: 64, episode reward: -40.231, mean reward: -0.281 [-100.000, 12.306], mean action: 1.860 [0.000, 3.000], mean observation: 0.102 [-0.929, 1.000], loss: 0.555468, mean_absolute_error: 25.259324, mean_q: 33.926346, mean_eps: 0.100000
 1402646/2000000: episode: 6415, duration: 2.654s, episode steps: 168, steps per second: 63, episode reward: -58.260, mean reward: -0.347 [-100.000, 17.469], mean action: 1.726 [0.000, 3.000], mean observation: 0.047 [-0.961, 1.000], loss: 0.617009, mean_absolute_error: 23.213923, mean_q: 31.138205, mean_eps: 0.100000
 1402768/2000000: episode: 6416, duration: 1.921s, episode steps: 122, steps per second: 63, episode reward: -98.735, mean reward: -0.809 [-100.000, 9.628], mean action: 1.402 [0.000, 3.000], mean observation: 0.038 [-1.320, 1.008], loss: 0.802463, mean_absolute_error: 24.210675, mean_q: 32.432022, mean_eps: 0.100000
 1403007/2000000: episode: 6417, duration: 3.739s, episode steps: 239, steps per second: 64, episode reward: 236.100, mean reward: 0.988 [-2.910, 100.000], mean action: 1.180 [0.000, 3.000], mean observation: 0.073 [-0.690, 1.000], loss: 0.658850, mean_absolute_error: 23.784438, mean_q: 31.879489, mean_eps: 0.100000
 1403077/2000000: episode: 6418, duration: 1.129s, episode steps: 70, steps per second: 62, episode reward: -49.777, mean reward: -0.711 [-100.000, 16.023], mean action: 1.443 [0.000, 3.000], mean observation: 0.095 [-1.080, 1.000], loss: 0.386635, mean_absolute_error: 23.586323, mean_q: 31.692426, mean_eps: 0.100000
 1403274/2000000: episode: 6419, duration: 3.070s, episode steps: 197, steps per second: 64, episode reward: 6.658, mean reward: 0.034 [-100.000, 13.421], mean action: 1.766 [0.000, 3.000], mean observation: 0.010 [-0.665, 1.000], loss: 0.881753, mean_absolute_error: 24.221218, mean_q: 32.415035, mean_eps: 0.100000
 1403477/2000000: episode: 6420, duration: 3.192s, episode steps: 203, steps per second: 64, episode reward: -39.201, mean reward: -0.193 [-100.000, 16.196], mean action: 1.788 [0.000, 3.000], mean observation: -0.003 [-0.693, 1.000], loss: 0.727327, mean_absolute_error: 23.591070, mean_q: 31.635426, mean_eps: 0.100000
 1403711/2000000: episode: 6421, duration: 3.646s, episode steps: 234, steps per second: 64, episode reward: 212.394, mean reward: 0.908 [-7.551, 100.000], mean action: 0.970 [0.000, 3.000], mean observation: 0.069 [-0.842, 1.000], loss: 0.750972, mean_absolute_error: 24.069468, mean_q: 32.253495, mean_eps: 0.100000
 1403861/2000000: episode: 6422, duration: 2.395s, episode steps: 150, steps per second: 63, episode reward: -51.546, mean reward: -0.344 [-100.000, 11.929], mean action: 1.613 [0.000, 3.000], mean observation: 0.058 [-0.732, 1.365], loss: 1.093949, mean_absolute_error: 23.781759, mean_q: 31.749434, mean_eps: 0.100000
 1403947/2000000: episode: 6423, duration: 1.316s, episode steps: 86, steps per second: 65, episode reward: -84.822, mean reward: -0.986 [-100.000, 28.320], mean action: 1.628 [0.000, 3.000], mean observation: 0.013 [-1.216, 1.000], loss: 0.524263, mean_absolute_error: 23.506693, mean_q: 31.550993, mean_eps: 0.100000
 1404020/2000000: episode: 6424, duration: 1.168s, episode steps: 73, steps per second: 63, episode reward: -117.132, mean reward: -1.605 [-100.000, 26.749], mean action: 0.671 [0.000, 3.000], mean observation: -0.025 [-1.397, 1.000], loss: 0.629194, mean_absolute_error: 24.783712, mean_q: 33.248213, mean_eps: 0.100000
 1404203/2000000: episode: 6425, duration: 2.865s, episode steps: 183, steps per second: 64, episode reward: -62.767, mean reward: -0.343 [-100.000, 14.740], mean action: 1.585 [0.000, 3.000], mean observation: 0.027 [-0.738, 1.000], loss: 1.071241, mean_absolute_error: 24.475330, mean_q: 32.701907, mean_eps: 0.100000
 1404293/2000000: episode: 6426, duration: 1.442s, episode steps: 90, steps per second: 62, episode reward: -13.283, mean reward: -0.148 [-100.000, 22.471], mean action: 1.933 [0.000, 3.000], mean observation: -0.009 [-1.400, 1.000], loss: 0.702173, mean_absolute_error: 25.535673, mean_q: 34.097717, mean_eps: 0.100000
 1404456/2000000: episode: 6427, duration: 2.563s, episode steps: 163, steps per second: 64, episode reward: -45.234, mean reward: -0.278 [-100.000, 15.819], mean action: 1.583 [0.000, 3.000], mean observation: 0.006 [-0.759, 1.000], loss: 0.930279, mean_absolute_error: 23.197283, mean_q: 31.056707, mean_eps: 0.100000
 1404582/2000000: episode: 6428, duration: 1.998s, episode steps: 126, steps per second: 63, episode reward: -101.915, mean reward: -0.809 [-100.000, 22.972], mean action: 1.214 [0.000, 3.000], mean observation: 0.041 [-1.442, 1.016], loss: 0.658327, mean_absolute_error: 24.706827, mean_q: 33.166803, mean_eps: 0.100000
 1404692/2000000: episode: 6429, duration: 1.751s, episode steps: 110, steps per second: 63, episode reward: -29.645, mean reward: -0.270 [-100.000, 15.630], mean action: 1.936 [0.000, 3.000], mean observation: 0.092 [-0.814, 1.000], loss: 0.763160, mean_absolute_error: 24.328938, mean_q: 32.498928, mean_eps: 0.100000
 1404901/2000000: episode: 6430, duration: 3.312s, episode steps: 209, steps per second: 63, episode reward: -6.651, mean reward: -0.032 [-100.000, 21.965], mean action: 1.866 [0.000, 3.000], mean observation: 0.011 [-0.536, 1.000], loss: 0.730230, mean_absolute_error: 24.427334, mean_q: 32.636701, mean_eps: 0.100000
 1405084/2000000: episode: 6431, duration: 2.875s, episode steps: 183, steps per second: 64, episode reward: -55.338, mean reward: -0.302 [-100.000, 17.791], mean action: 1.514 [0.000, 3.000], mean observation: 0.026 [-0.909, 2.585], loss: 0.796976, mean_absolute_error: 24.248278, mean_q: 32.504148, mean_eps: 0.100000
 1405280/2000000: episode: 6432, duration: 3.142s, episode steps: 196, steps per second: 62, episode reward: -12.450, mean reward: -0.064 [-100.000, 14.905], mean action: 1.485 [0.000, 3.000], mean observation: -0.012 [-0.892, 1.000], loss: 0.691390, mean_absolute_error: 24.661374, mean_q: 32.920062, mean_eps: 0.100000
 1405444/2000000: episode: 6433, duration: 2.615s, episode steps: 164, steps per second: 63, episode reward: -42.738, mean reward: -0.261 [-100.000, 19.084], mean action: 1.939 [0.000, 3.000], mean observation: -0.005 [-0.842, 1.000], loss: 0.827814, mean_absolute_error: 24.238454, mean_q: 32.441194, mean_eps: 0.100000
 1405609/2000000: episode: 6434, duration: 2.611s, episode steps: 165, steps per second: 63, episode reward: -57.191, mean reward: -0.347 [-100.000, 17.835], mean action: 1.321 [0.000, 3.000], mean observation: 0.007 [-0.950, 2.648], loss: 0.767840, mean_absolute_error: 24.829230, mean_q: 33.280572, mean_eps: 0.100000
 1405984/2000000: episode: 6435, duration: 6.393s, episode steps: 375, steps per second: 59, episode reward: -180.981, mean reward: -0.483 [-100.000, 7.081], mean action: 1.224 [0.000, 3.000], mean observation: 0.038 [-1.708, 5.761], loss: 0.694800, mean_absolute_error: 24.083391, mean_q: 32.259351, mean_eps: 0.100000
 1406321/2000000: episode: 6436, duration: 5.520s, episode steps: 337, steps per second: 61, episode reward: 231.547, mean reward: 0.687 [-3.293, 100.000], mean action: 0.997 [0.000, 3.000], mean observation: 0.091 [-0.505, 1.000], loss: 0.825352, mean_absolute_error: 24.551924, mean_q: 32.813028, mean_eps: 0.100000
 1406485/2000000: episode: 6437, duration: 2.588s, episode steps: 164, steps per second: 63, episode reward: -12.096, mean reward: -0.074 [-100.000, 12.844], mean action: 1.713 [0.000, 3.000], mean observation: 0.019 [-0.894, 1.007], loss: 0.942085, mean_absolute_error: 24.420974, mean_q: 32.673341, mean_eps: 0.100000
 1406644/2000000: episode: 6438, duration: 2.486s, episode steps: 159, steps per second: 64, episode reward: -66.648, mean reward: -0.419 [-100.000, 11.256], mean action: 1.390 [0.000, 3.000], mean observation: 0.126 [-0.845, 1.501], loss: 0.867220, mean_absolute_error: 24.958950, mean_q: 33.378911, mean_eps: 0.100000
 1406891/2000000: episode: 6439, duration: 3.874s, episode steps: 247, steps per second: 64, episode reward: -16.763, mean reward: -0.068 [-100.000, 13.476], mean action: 1.761 [0.000, 3.000], mean observation: 0.011 [-0.667, 1.000], loss: 0.803784, mean_absolute_error: 24.306366, mean_q: 32.582273, mean_eps: 0.100000
 1407037/2000000: episode: 6440, duration: 2.365s, episode steps: 146, steps per second: 62, episode reward: -17.856, mean reward: -0.122 [-100.000, 15.688], mean action: 1.870 [0.000, 3.000], mean observation: 0.021 [-1.275, 1.000], loss: 0.594790, mean_absolute_error: 23.712494, mean_q: 31.850802, mean_eps: 0.100000
 1407118/2000000: episode: 6441, duration: 1.255s, episode steps: 81, steps per second: 65, episode reward: -60.087, mean reward: -0.742 [-100.000, 11.875], mean action: 1.407 [0.000, 3.000], mean observation: 0.088 [-2.869, 1.000], loss: 1.267222, mean_absolute_error: 24.866209, mean_q: 33.089101, mean_eps: 0.100000
 1407217/2000000: episode: 6442, duration: 1.540s, episode steps: 99, steps per second: 64, episode reward: -105.121, mean reward: -1.062 [-100.000, 16.926], mean action: 0.697 [0.000, 3.000], mean observation: 0.009 [-1.556, 3.581], loss: 0.734390, mean_absolute_error: 24.582098, mean_q: 32.940379, mean_eps: 0.100000
 1407314/2000000: episode: 6443, duration: 1.465s, episode steps: 97, steps per second: 66, episode reward: -76.070, mean reward: -0.784 [-100.000, 16.888], mean action: 1.062 [0.000, 3.000], mean observation: 0.111 [-1.095, 3.046], loss: 0.875653, mean_absolute_error: 25.649434, mean_q: 33.368923, mean_eps: 0.100000
 1407452/2000000: episode: 6444, duration: 2.206s, episode steps: 138, steps per second: 63, episode reward: 4.888, mean reward: 0.035 [-100.000, 18.718], mean action: 1.906 [0.000, 3.000], mean observation: 0.079 [-0.728, 1.000], loss: 0.569883, mean_absolute_error: 23.092494, mean_q: 31.020568, mean_eps: 0.100000
 1407616/2000000: episode: 6445, duration: 2.649s, episode steps: 164, steps per second: 62, episode reward: 216.583, mean reward: 1.321 [-7.967, 100.000], mean action: 1.280 [0.000, 3.000], mean observation: 0.072 [-0.865, 1.000], loss: 0.654823, mean_absolute_error: 24.467002, mean_q: 32.756901, mean_eps: 0.100000
 1407684/2000000: episode: 6446, duration: 1.125s, episode steps: 68, steps per second: 60, episode reward: -82.048, mean reward: -1.207 [-100.000, 18.246], mean action: 0.897 [0.000, 3.000], mean observation: -0.004 [-1.282, 1.000], loss: 0.971665, mean_absolute_error: 24.314302, mean_q: 32.547268, mean_eps: 0.100000
 1407746/2000000: episode: 6447, duration: 0.989s, episode steps: 62, steps per second: 63, episode reward: -75.446, mean reward: -1.217 [-100.000, 19.034], mean action: 0.790 [0.000, 3.000], mean observation: -0.051 [-1.463, 1.000], loss: 1.127188, mean_absolute_error: 23.606560, mean_q: 31.573554, mean_eps: 0.100000
 1408182/2000000: episode: 6448, duration: 7.157s, episode steps: 436, steps per second: 61, episode reward: 200.297, mean reward: 0.459 [-17.416, 100.000], mean action: 0.970 [0.000, 3.000], mean observation: 0.083 [-0.793, 1.000], loss: 0.810760, mean_absolute_error: 24.383432, mean_q: 32.670681, mean_eps: 0.100000
 1408335/2000000: episode: 6449, duration: 2.387s, episode steps: 153, steps per second: 64, episode reward: -57.961, mean reward: -0.379 [-100.000, 10.173], mean action: 1.686 [0.000, 3.000], mean observation: 0.082 [-0.794, 1.028], loss: 0.654216, mean_absolute_error: 23.608560, mean_q: 31.700728, mean_eps: 0.100000
 1408467/2000000: episode: 6450, duration: 2.101s, episode steps: 132, steps per second: 63, episode reward: -63.179, mean reward: -0.479 [-100.000, 7.438], mean action: 1.432 [0.000, 3.000], mean observation: 0.043 [-1.150, 1.000], loss: 0.651779, mean_absolute_error: 23.779471, mean_q: 31.951387, mean_eps: 0.100000
 1408738/2000000: episode: 6451, duration: 4.474s, episode steps: 271, steps per second: 61, episode reward: -241.235, mean reward: -0.890 [-100.000, 5.380], mean action: 1.804 [0.000, 3.000], mean observation: 0.080 [-0.891, 1.125], loss: 0.991448, mean_absolute_error: 24.461181, mean_q: 32.561823, mean_eps: 0.100000
 1408843/2000000: episode: 6452, duration: 1.624s, episode steps: 105, steps per second: 65, episode reward: -88.926, mean reward: -0.847 [-100.000, 10.128], mean action: 1.048 [0.000, 3.000], mean observation: 0.092 [-1.113, 3.467], loss: 0.660880, mean_absolute_error: 23.835172, mean_q: 31.411432, mean_eps: 0.100000
 1409031/2000000: episode: 6453, duration: 2.933s, episode steps: 188, steps per second: 64, episode reward: -273.335, mean reward: -1.454 [-100.000, 4.436], mean action: 1.713 [0.000, 3.000], mean observation: 0.067 [-0.854, 1.560], loss: 0.976634, mean_absolute_error: 25.220626, mean_q: 33.736770, mean_eps: 0.100000
 1409198/2000000: episode: 6454, duration: 2.606s, episode steps: 167, steps per second: 64, episode reward: -15.307, mean reward: -0.092 [-100.000, 12.458], mean action: 1.593 [0.000, 3.000], mean observation: 0.090 [-1.087, 1.000], loss: 0.804489, mean_absolute_error: 24.705367, mean_q: 32.868429, mean_eps: 0.100000
 1409338/2000000: episode: 6455, duration: 2.179s, episode steps: 140, steps per second: 64, episode reward: -28.657, mean reward: -0.205 [-100.000, 22.031], mean action: 1.464 [0.000, 3.000], mean observation: -0.005 [-0.801, 1.000], loss: 0.740649, mean_absolute_error: 23.790224, mean_q: 31.918461, mean_eps: 0.100000
 1409607/2000000: episode: 6456, duration: 4.218s, episode steps: 269, steps per second: 64, episode reward: -63.733, mean reward: -0.237 [-100.000, 17.757], mean action: 1.550 [0.000, 3.000], mean observation: 0.009 [-1.094, 1.000], loss: 0.751634, mean_absolute_error: 24.448554, mean_q: 32.804384, mean_eps: 0.100000
 1409703/2000000: episode: 6457, duration: 1.483s, episode steps: 96, steps per second: 65, episode reward: -105.266, mean reward: -1.097 [-100.000, 10.828], mean action: 0.969 [0.000, 3.000], mean observation: 0.089 [-3.641, 1.000], loss: 0.496831, mean_absolute_error: 24.977492, mean_q: 33.614832, mean_eps: 0.100000
 1409858/2000000: episode: 6458, duration: 2.424s, episode steps: 155, steps per second: 64, episode reward: -31.403, mean reward: -0.203 [-100.000, 12.880], mean action: 1.503 [0.000, 3.000], mean observation: 0.028 [-0.908, 1.000], loss: 0.969855, mean_absolute_error: 24.270299, mean_q: 32.289863, mean_eps: 0.100000
 1410003/2000000: episode: 6459, duration: 2.269s, episode steps: 145, steps per second: 64, episode reward: -17.875, mean reward: -0.123 [-100.000, 11.679], mean action: 1.828 [0.000, 3.000], mean observation: 0.048 [-0.888, 1.000], loss: 0.756161, mean_absolute_error: 23.959390, mean_q: 32.021024, mean_eps: 0.100000
 1410085/2000000: episode: 6460, duration: 1.322s, episode steps: 82, steps per second: 62, episode reward: -61.076, mean reward: -0.745 [-100.000, 12.199], mean action: 1.976 [0.000, 3.000], mean observation: -0.143 [-1.084, 2.764], loss: 1.374118, mean_absolute_error: 25.885742, mean_q: 34.530722, mean_eps: 0.100000
 1410863/2000000: episode: 6461, duration: 12.694s, episode steps: 778, steps per second: 61, episode reward: -417.023, mean reward: -0.536 [-100.000, 6.256], mean action: 1.790 [0.000, 3.000], mean observation: 0.340 [-2.141, 5.993], loss: 0.782539, mean_absolute_error: 24.242899, mean_q: 32.453569, mean_eps: 0.100000
 1411016/2000000: episode: 6462, duration: 2.465s, episode steps: 153, steps per second: 62, episode reward: -3.796, mean reward: -0.025 [-100.000, 21.820], mean action: 1.621 [0.000, 3.000], mean observation: -0.035 [-0.761, 1.000], loss: 0.767269, mean_absolute_error: 26.054576, mean_q: 34.801673, mean_eps: 0.100000
 1411449/2000000: episode: 6463, duration: 6.969s, episode steps: 433, steps per second: 62, episode reward: -11.423, mean reward: -0.026 [-100.000, 12.733], mean action: 1.559 [0.000, 3.000], mean observation: 0.039 [-0.853, 1.000], loss: 0.672264, mean_absolute_error: 25.218123, mean_q: 33.666282, mean_eps: 0.100000
 1411530/2000000: episode: 6464, duration: 1.277s, episode steps: 81, steps per second: 63, episode reward: -84.564, mean reward: -1.044 [-100.000, 17.899], mean action: 0.963 [0.000, 3.000], mean observation: -0.009 [-1.202, 1.000], loss: 0.659355, mean_absolute_error: 24.859846, mean_q: 32.881814, mean_eps: 0.100000
 1411895/2000000: episode: 6465, duration: 5.867s, episode steps: 365, steps per second: 62, episode reward: -221.347, mean reward: -0.606 [-100.000, 5.911], mean action: 1.310 [0.000, 3.000], mean observation: 0.068 [-2.151, 7.471], loss: 0.791362, mean_absolute_error: 25.068897, mean_q: 33.639993, mean_eps: 0.100000
 1411968/2000000: episode: 6466, duration: 1.218s, episode steps: 73, steps per second: 60, episode reward: -104.073, mean reward: -1.426 [-100.000, 17.016], mean action: 0.918 [0.000, 3.000], mean observation: -0.054 [-1.326, 4.311], loss: 0.533430, mean_absolute_error: 26.941740, mean_q: 35.383885, mean_eps: 0.100000
 1412044/2000000: episode: 6467, duration: 1.232s, episode steps: 76, steps per second: 62, episode reward: -32.202, mean reward: -0.424 [-100.000, 16.781], mean action: 1.605 [0.000, 3.000], mean observation: -0.149 [-1.073, 1.000], loss: 1.086863, mean_absolute_error: 28.421495, mean_q: 37.979867, mean_eps: 0.100000
 1412214/2000000: episode: 6468, duration: 2.685s, episode steps: 170, steps per second: 63, episode reward: -75.725, mean reward: -0.445 [-100.000, 10.383], mean action: 1.535 [0.000, 3.000], mean observation: 0.090 [-0.665, 1.463], loss: 0.655508, mean_absolute_error: 25.269599, mean_q: 33.906039, mean_eps: 0.100000
 1412379/2000000: episode: 6469, duration: 2.718s, episode steps: 165, steps per second: 61, episode reward: 7.562, mean reward: 0.046 [-100.000, 12.880], mean action: 1.958 [0.000, 3.000], mean observation: -0.005 [-0.604, 1.000], loss: 0.668502, mean_absolute_error: 24.992219, mean_q: 33.578111, mean_eps: 0.100000
 1412455/2000000: episode: 6470, duration: 1.226s, episode steps: 76, steps per second: 62, episode reward: -87.869, mean reward: -1.156 [-100.000, 24.301], mean action: 1.855 [0.000, 3.000], mean observation: -0.035 [-1.270, 1.000], loss: 0.482839, mean_absolute_error: 23.610525, mean_q: 31.788655, mean_eps: 0.100000
 1412561/2000000: episode: 6471, duration: 1.695s, episode steps: 106, steps per second: 63, episode reward: -53.880, mean reward: -0.508 [-100.000, 11.386], mean action: 1.425 [0.000, 3.000], mean observation: 0.056 [-1.565, 1.000], loss: 0.620626, mean_absolute_error: 24.222121, mean_q: 32.547438, mean_eps: 0.100000
 1412660/2000000: episode: 6472, duration: 1.578s, episode steps: 99, steps per second: 63, episode reward: -16.890, mean reward: -0.171 [-100.000, 8.758], mean action: 1.646 [0.000, 3.000], mean observation: -0.070 [-0.965, 1.308], loss: 0.781199, mean_absolute_error: 23.599247, mean_q: 31.637375, mean_eps: 0.100000
 1412795/2000000: episode: 6473, duration: 2.119s, episode steps: 135, steps per second: 64, episode reward: 1.655, mean reward: 0.012 [-100.000, 19.026], mean action: 1.437 [0.000, 3.000], mean observation: 0.012 [-0.902, 1.000], loss: 0.560549, mean_absolute_error: 24.711638, mean_q: 33.219083, mean_eps: 0.100000
 1412951/2000000: episode: 6474, duration: 2.430s, episode steps: 156, steps per second: 64, episode reward: -27.998, mean reward: -0.179 [-100.000, 12.043], mean action: 1.333 [0.000, 3.000], mean observation: 0.103 [-0.802, 2.984], loss: 0.855994, mean_absolute_error: 25.991745, mean_q: 34.814097, mean_eps: 0.100000
 1413170/2000000: episode: 6475, duration: 3.473s, episode steps: 219, steps per second: 63, episode reward: -16.436, mean reward: -0.075 [-100.000, 13.576], mean action: 1.534 [0.000, 3.000], mean observation: -0.009 [-0.956, 1.000], loss: 0.555558, mean_absolute_error: 24.859403, mean_q: 33.392778, mean_eps: 0.100000
 1413291/2000000: episode: 6476, duration: 1.905s, episode steps: 121, steps per second: 64, episode reward: -45.022, mean reward: -0.372 [-100.000, 18.044], mean action: 1.545 [0.000, 3.000], mean observation: -0.023 [-0.985, 1.000], loss: 0.507162, mean_absolute_error: 25.271661, mean_q: 33.691464, mean_eps: 0.100000
 1413428/2000000: episode: 6477, duration: 2.161s, episode steps: 137, steps per second: 63, episode reward: -78.388, mean reward: -0.572 [-100.000, 17.646], mean action: 1.234 [0.000, 3.000], mean observation: -0.004 [-1.044, 2.908], loss: 0.791535, mean_absolute_error: 24.925619, mean_q: 33.411266, mean_eps: 0.100000
 1413773/2000000: episode: 6478, duration: 5.494s, episode steps: 345, steps per second: 63, episode reward: 217.163, mean reward: 0.629 [-17.992, 100.000], mean action: 0.933 [0.000, 3.000], mean observation: 0.230 [-0.972, 1.263], loss: 0.759266, mean_absolute_error: 24.731882, mean_q: 32.825259, mean_eps: 0.100000
 1413862/2000000: episode: 6479, duration: 1.405s, episode steps: 89, steps per second: 63, episode reward: -74.298, mean reward: -0.835 [-100.000, 11.010], mean action: 1.472 [0.000, 3.000], mean observation: 0.078 [-1.434, 1.000], loss: 0.588913, mean_absolute_error: 24.883835, mean_q: 33.451694, mean_eps: 0.100000
 1413967/2000000: episode: 6480, duration: 1.623s, episode steps: 105, steps per second: 65, episode reward: -17.337, mean reward: -0.165 [-100.000, 17.054], mean action: 1.724 [0.000, 3.000], mean observation: -0.027 [-0.892, 1.000], loss: 0.558224, mean_absolute_error: 23.706982, mean_q: 31.649519, mean_eps: 0.100000
 1414077/2000000: episode: 6481, duration: 1.747s, episode steps: 110, steps per second: 63, episode reward: -39.979, mean reward: -0.363 [-100.000, 17.164], mean action: 1.873 [0.000, 3.000], mean observation: -0.003 [-0.920, 1.000], loss: 0.571763, mean_absolute_error: 24.057771, mean_q: 32.361489, mean_eps: 0.100000
 1414163/2000000: episode: 6482, duration: 1.307s, episode steps: 86, steps per second: 66, episode reward: -27.157, mean reward: -0.316 [-100.000, 11.408], mean action: 1.547 [0.000, 3.000], mean observation: -0.125 [-1.019, 2.834], loss: 0.746617, mean_absolute_error: 24.286382, mean_q: 32.538411, mean_eps: 0.100000
 1414267/2000000: episode: 6483, duration: 1.615s, episode steps: 104, steps per second: 64, episode reward: -72.815, mean reward: -0.700 [-100.000, 11.795], mean action: 1.010 [0.000, 3.000], mean observation: 0.077 [-1.154, 2.975], loss: 0.636513, mean_absolute_error: 24.931364, mean_q: 33.462474, mean_eps: 0.100000
 1414370/2000000: episode: 6484, duration: 1.616s, episode steps: 103, steps per second: 64, episode reward: -80.206, mean reward: -0.779 [-100.000, 12.261], mean action: 1.398 [0.000, 3.000], mean observation: 0.018 [-0.913, 1.000], loss: 1.446996, mean_absolute_error: 25.089033, mean_q: 33.333247, mean_eps: 0.100000
 1414771/2000000: episode: 6485, duration: 6.426s, episode steps: 401, steps per second: 62, episode reward: 215.512, mean reward: 0.537 [-17.950, 100.000], mean action: 1.177 [0.000, 3.000], mean observation: 0.201 [-0.812, 1.000], loss: 0.863184, mean_absolute_error: 24.760475, mean_q: 33.165377, mean_eps: 0.100000
 1415006/2000000: episode: 6486, duration: 3.676s, episode steps: 235, steps per second: 64, episode reward: 36.050, mean reward: 0.153 [-100.000, 12.195], mean action: 1.379 [0.000, 3.000], mean observation: 0.013 [-0.691, 1.021], loss: 0.725309, mean_absolute_error: 26.679922, mean_q: 35.779572, mean_eps: 0.100000
 1415209/2000000: episode: 6487, duration: 3.207s, episode steps: 203, steps per second: 63, episode reward: -9.955, mean reward: -0.049 [-100.000, 16.184], mean action: 1.596 [0.000, 3.000], mean observation: -0.004 [-0.877, 1.000], loss: 0.675309, mean_absolute_error: 24.944475, mean_q: 33.464419, mean_eps: 0.100000
 1415296/2000000: episode: 6488, duration: 1.358s, episode steps: 87, steps per second: 64, episode reward: -108.047, mean reward: -1.242 [-100.000, 11.042], mean action: 1.092 [0.000, 3.000], mean observation: 0.090 [-1.237, 4.266], loss: 0.570934, mean_absolute_error: 26.302250, mean_q: 35.326151, mean_eps: 0.100000
 1415631/2000000: episode: 6489, duration: 5.400s, episode steps: 335, steps per second: 62, episode reward: 230.961, mean reward: 0.689 [-18.320, 100.000], mean action: 1.484 [0.000, 3.000], mean observation: 0.034 [-0.698, 1.000], loss: 0.626605, mean_absolute_error: 24.719894, mean_q: 33.182544, mean_eps: 0.100000
 1415763/2000000: episode: 6490, duration: 2.093s, episode steps: 132, steps per second: 63, episode reward: -109.649, mean reward: -0.831 [-100.000, 7.964], mean action: 1.614 [0.000, 3.000], mean observation: 0.027 [-3.514, 1.000], loss: 0.738912, mean_absolute_error: 25.471595, mean_q: 33.887774, mean_eps: 0.100000
 1415840/2000000: episode: 6491, duration: 1.262s, episode steps: 77, steps per second: 61, episode reward: -98.886, mean reward: -1.284 [-100.000, 16.075], mean action: 1.961 [0.000, 3.000], mean observation: -0.014 [-1.280, 1.000], loss: 0.869112, mean_absolute_error: 25.199612, mean_q: 33.796375, mean_eps: 0.100000
 1415972/2000000: episode: 6492, duration: 2.118s, episode steps: 132, steps per second: 62, episode reward: -54.296, mean reward: -0.411 [-100.000, 17.210], mean action: 1.932 [0.000, 3.000], mean observation: -0.012 [-0.722, 1.000], loss: 0.682667, mean_absolute_error: 25.460228, mean_q: 33.868530, mean_eps: 0.100000
 1416042/2000000: episode: 6493, duration: 1.318s, episode steps: 70, steps per second: 53, episode reward: -85.171, mean reward: -1.217 [-100.000, 16.199], mean action: 1.986 [1.000, 3.000], mean observation: -0.012 [-1.350, 1.000], loss: 0.833066, mean_absolute_error: 24.954632, mean_q: 33.430556, mean_eps: 0.100000
 1416593/2000000: episode: 6494, duration: 8.805s, episode steps: 551, steps per second: 63, episode reward: 227.410, mean reward: 0.413 [-21.649, 100.000], mean action: 0.824 [0.000, 3.000], mean observation: 0.113 [-0.632, 1.000], loss: 0.938848, mean_absolute_error: 25.199667, mean_q: 33.561918, mean_eps: 0.100000
 1416758/2000000: episode: 6495, duration: 2.611s, episode steps: 165, steps per second: 63, episode reward: -303.511, mean reward: -1.839 [-100.000, 4.679], mean action: 1.927 [0.000, 3.000], mean observation: 0.081 [-0.728, 2.151], loss: 0.761532, mean_absolute_error: 26.052234, mean_q: 34.811604, mean_eps: 0.100000
 1416845/2000000: episode: 6496, duration: 1.365s, episode steps: 87, steps per second: 64, episode reward: -56.893, mean reward: -0.654 [-100.000, 12.971], mean action: 1.080 [0.000, 3.000], mean observation: -0.003 [-0.974, 1.000], loss: 0.964146, mean_absolute_error: 24.529867, mean_q: 32.798638, mean_eps: 0.100000
 1416956/2000000: episode: 6497, duration: 1.756s, episode steps: 111, steps per second: 63, episode reward: -75.088, mean reward: -0.676 [-100.000, 24.640], mean action: 1.532 [0.000, 3.000], mean observation: 0.056 [-1.382, 1.026], loss: 0.662933, mean_absolute_error: 26.639149, mean_q: 35.771032, mean_eps: 0.100000
 1417072/2000000: episode: 6498, duration: 1.852s, episode steps: 116, steps per second: 63, episode reward: -156.403, mean reward: -1.348 [-100.000, 7.066], mean action: 1.353 [0.000, 3.000], mean observation: -0.065 [-1.409, 4.491], loss: 0.667403, mean_absolute_error: 26.083853, mean_q: 34.025146, mean_eps: 0.100000
 1417145/2000000: episode: 6499, duration: 1.184s, episode steps: 73, steps per second: 62, episode reward: -41.161, mean reward: -0.564 [-100.000, 19.124], mean action: 1.384 [0.000, 3.000], mean observation: 0.063 [-1.156, 1.000], loss: 0.500229, mean_absolute_error: 24.931158, mean_q: 33.455424, mean_eps: 0.100000
 1417479/2000000: episode: 6500, duration: 5.269s, episode steps: 334, steps per second: 63, episode reward: 189.936, mean reward: 0.569 [-6.930, 100.000], mean action: 1.545 [0.000, 3.000], mean observation: 0.120 [-0.790, 1.000], loss: 0.634511, mean_absolute_error: 25.892005, mean_q: 34.694712, mean_eps: 0.100000
 1417824/2000000: episode: 6501, duration: 5.476s, episode steps: 345, steps per second: 63, episode reward: -107.789, mean reward: -0.312 [-100.000, 12.092], mean action: 1.548 [0.000, 3.000], mean observation: 0.041 [-3.364, 1.142], loss: 1.001549, mean_absolute_error: 25.506322, mean_q: 33.656536, mean_eps: 0.100000
 1418053/2000000: episode: 6502, duration: 3.583s, episode steps: 229, steps per second: 64, episode reward: -17.746, mean reward: -0.077 [-100.000, 14.811], mean action: 1.419 [0.000, 3.000], mean observation: 0.046 [-0.831, 1.016], loss: 0.870188, mean_absolute_error: 25.419760, mean_q: 34.065117, mean_eps: 0.100000
 1418166/2000000: episode: 6503, duration: 1.761s, episode steps: 113, steps per second: 64, episode reward: -67.172, mean reward: -0.594 [-100.000, 15.010], mean action: 1.903 [0.000, 3.000], mean observation: 0.108 [-0.874, 1.000], loss: 0.605484, mean_absolute_error: 25.435090, mean_q: 34.127772, mean_eps: 0.100000
 1418423/2000000: episode: 6504, duration: 4.043s, episode steps: 257, steps per second: 64, episode reward: -6.019, mean reward: -0.023 [-100.000, 10.756], mean action: 1.770 [0.000, 3.000], mean observation: -0.012 [-0.723, 1.085], loss: 0.720923, mean_absolute_error: 25.571740, mean_q: 34.304806, mean_eps: 0.100000
 1418489/2000000: episode: 6505, duration: 1.061s, episode steps: 66, steps per second: 62, episode reward: -102.585, mean reward: -1.554 [-100.000, 18.686], mean action: 1.015 [0.000, 3.000], mean observation: -0.046 [-1.508, 1.000], loss: 0.715169, mean_absolute_error: 27.214774, mean_q: 36.493763, mean_eps: 0.100000
 1418555/2000000: episode: 6506, duration: 1.003s, episode steps: 66, steps per second: 66, episode reward: -97.794, mean reward: -1.482 [-100.000, 7.446], mean action: 1.409 [0.000, 3.000], mean observation: 0.054 [-1.265, 4.158], loss: 0.752997, mean_absolute_error: 25.447825, mean_q: 34.102268, mean_eps: 0.100000
 1418619/2000000: episode: 6507, duration: 0.993s, episode steps: 64, steps per second: 64, episode reward: -134.079, mean reward: -2.095 [-100.000, 18.220], mean action: 0.516 [0.000, 3.000], mean observation: -0.068 [-1.731, 1.000], loss: 0.740602, mean_absolute_error: 24.961627, mean_q: 33.499517, mean_eps: 0.100000
 1418834/2000000: episode: 6508, duration: 3.406s, episode steps: 215, steps per second: 63, episode reward: -77.294, mean reward: -0.360 [-100.000, 9.110], mean action: 1.674 [0.000, 3.000], mean observation: 0.077 [-0.891, 1.000], loss: 0.666293, mean_absolute_error: 24.537342, mean_q: 32.927976, mean_eps: 0.100000
 1419226/2000000: episode: 6509, duration: 6.214s, episode steps: 392, steps per second: 63, episode reward: -69.233, mean reward: -0.177 [-100.000, 11.985], mean action: 1.383 [0.000, 3.000], mean observation: 0.072 [-1.115, 1.018], loss: 0.749063, mean_absolute_error: 25.003505, mean_q: 33.509665, mean_eps: 0.100000
 1419357/2000000: episode: 6510, duration: 2.074s, episode steps: 131, steps per second: 63, episode reward: -69.826, mean reward: -0.533 [-100.000, 16.679], mean action: 1.481 [0.000, 3.000], mean observation: 0.026 [-0.970, 1.000], loss: 0.817378, mean_absolute_error: 26.105863, mean_q: 34.063160, mean_eps: 0.100000
 1419516/2000000: episode: 6511, duration: 2.510s, episode steps: 159, steps per second: 63, episode reward: -86.563, mean reward: -0.544 [-100.000, 16.679], mean action: 1.528 [0.000, 3.000], mean observation: 0.029 [-0.751, 1.000], loss: 0.853300, mean_absolute_error: 25.759888, mean_q: 34.531586, mean_eps: 0.100000
 1419577/2000000: episode: 6512, duration: 0.996s, episode steps: 61, steps per second: 61, episode reward: -120.272, mean reward: -1.972 [-100.000, 14.209], mean action: 0.656 [0.000, 3.000], mean observation: -0.061 [-1.737, 1.000], loss: 0.562375, mean_absolute_error: 25.567244, mean_q: 34.391491, mean_eps: 0.100000
 1419687/2000000: episode: 6513, duration: 1.688s, episode steps: 110, steps per second: 65, episode reward: -61.460, mean reward: -0.559 [-100.000, 13.109], mean action: 1.227 [0.000, 3.000], mean observation: -0.001 [-0.907, 1.000], loss: 0.682897, mean_absolute_error: 26.453472, mean_q: 35.376721, mean_eps: 0.100000
 1419943/2000000: episode: 6514, duration: 4.024s, episode steps: 256, steps per second: 64, episode reward: -13.365, mean reward: -0.052 [-100.000, 13.122], mean action: 1.875 [0.000, 3.000], mean observation: 0.027 [-0.682, 1.007], loss: 0.783240, mean_absolute_error: 25.918779, mean_q: 34.645348, mean_eps: 0.100000
 1420020/2000000: episode: 6515, duration: 1.261s, episode steps: 77, steps per second: 61, episode reward: -120.394, mean reward: -1.564 [-100.000, 15.475], mean action: 1.740 [0.000, 3.000], mean observation: 0.005 [-1.339, 1.000], loss: 0.599540, mean_absolute_error: 24.812985, mean_q: 33.281168, mean_eps: 0.100000
 1420123/2000000: episode: 6516, duration: 1.895s, episode steps: 103, steps per second: 54, episode reward: -45.372, mean reward: -0.441 [-100.000, 13.921], mean action: 1.699 [0.000, 3.000], mean observation: 0.008 [-0.982, 1.000], loss: 0.393736, mean_absolute_error: 24.673868, mean_q: 33.177147, mean_eps: 0.100000
 1420283/2000000: episode: 6517, duration: 2.535s, episode steps: 160, steps per second: 63, episode reward: -21.744, mean reward: -0.136 [-100.000, 13.671], mean action: 1.469 [0.000, 3.000], mean observation: 0.017 [-0.776, 1.000], loss: 0.862910, mean_absolute_error: 25.938018, mean_q: 34.598454, mean_eps: 0.100000
 1420463/2000000: episode: 6518, duration: 2.790s, episode steps: 180, steps per second: 65, episode reward: -4.756, mean reward: -0.026 [-100.000, 24.098], mean action: 1.450 [0.000, 3.000], mean observation: 0.026 [-0.736, 1.003], loss: 1.038204, mean_absolute_error: 25.248434, mean_q: 33.710795, mean_eps: 0.100000
 1420625/2000000: episode: 6519, duration: 2.561s, episode steps: 162, steps per second: 63, episode reward: -18.037, mean reward: -0.111 [-100.000, 21.101], mean action: 1.691 [0.000, 3.000], mean observation: -0.005 [-0.784, 1.000], loss: 0.752791, mean_absolute_error: 25.347908, mean_q: 33.890725, mean_eps: 0.100000
 1420834/2000000: episode: 6520, duration: 3.289s, episode steps: 209, steps per second: 64, episode reward: -56.085, mean reward: -0.268 [-100.000, 12.054], mean action: 1.718 [0.000, 3.000], mean observation: 0.006 [-0.820, 1.000], loss: 0.586134, mean_absolute_error: 25.052184, mean_q: 33.682556, mean_eps: 0.100000
 1420924/2000000: episode: 6521, duration: 1.443s, episode steps: 90, steps per second: 62, episode reward: -134.203, mean reward: -1.491 [-100.000, 7.207], mean action: 1.667 [0.000, 3.000], mean observation: 0.018 [-4.381, 1.000], loss: 0.766930, mean_absolute_error: 27.194744, mean_q: 36.506218, mean_eps: 0.100000
 1420986/2000000: episode: 6522, duration: 0.986s, episode steps: 62, steps per second: 63, episode reward: -144.761, mean reward: -2.335 [-100.000, 17.398], mean action: 1.387 [0.000, 3.000], mean observation: -0.008 [-5.610, 1.000], loss: 1.144554, mean_absolute_error: 24.997937, mean_q: 33.348509, mean_eps: 0.100000
 1421047/2000000: episode: 6523, duration: 0.929s, episode steps: 61, steps per second: 66, episode reward: -110.360, mean reward: -1.809 [-100.000, 17.341], mean action: 0.705 [0.000, 3.000], mean observation: -0.053 [-1.676, 1.000], loss: 0.970499, mean_absolute_error: 25.662449, mean_q: 34.220802, mean_eps: 0.100000
 1421406/2000000: episode: 6524, duration: 5.724s, episode steps: 359, steps per second: 63, episode reward: 165.664, mean reward: 0.461 [-18.634, 100.000], mean action: 1.292 [0.000, 3.000], mean observation: 0.148 [-1.394, 1.000], loss: 0.709300, mean_absolute_error: 25.726233, mean_q: 34.500028, mean_eps: 0.100000
 1421503/2000000: episode: 6525, duration: 1.519s, episode steps: 97, steps per second: 64, episode reward: -90.648, mean reward: -0.935 [-100.000, 11.733], mean action: 1.928 [1.000, 3.000], mean observation: 0.068 [-1.620, 1.000], loss: 0.889261, mean_absolute_error: 24.860297, mean_q: 33.355872, mean_eps: 0.100000
 1421573/2000000: episode: 6526, duration: 1.131s, episode steps: 70, steps per second: 62, episode reward: -105.650, mean reward: -1.509 [-100.000, 20.661], mean action: 1.643 [0.000, 3.000], mean observation: 0.003 [-1.351, 1.000], loss: 0.702536, mean_absolute_error: 24.837806, mean_q: 33.305738, mean_eps: 0.100000
 1421885/2000000: episode: 6527, duration: 4.906s, episode steps: 312, steps per second: 64, episode reward: -33.401, mean reward: -0.107 [-100.000, 15.761], mean action: 1.747 [0.000, 3.000], mean observation: 0.006 [-0.722, 1.000], loss: 0.717196, mean_absolute_error: 26.282691, mean_q: 35.243637, mean_eps: 0.100000
 1422011/2000000: episode: 6528, duration: 1.940s, episode steps: 126, steps per second: 65, episode reward: -88.813, mean reward: -0.705 [-100.000, 10.306], mean action: 1.508 [0.000, 3.000], mean observation: 0.027 [-1.053, 1.000], loss: 1.232460, mean_absolute_error: 26.289231, mean_q: 35.087488, mean_eps: 0.100000
 1422081/2000000: episode: 6529, duration: 1.118s, episode steps: 70, steps per second: 63, episode reward: -131.552, mean reward: -1.879 [-100.000, 11.709], mean action: 1.086 [0.000, 3.000], mean observation: 0.015 [-3.708, 1.000], loss: 0.783435, mean_absolute_error: 24.242181, mean_q: 32.455079, mean_eps: 0.100000
 1422257/2000000: episode: 6530, duration: 2.726s, episode steps: 176, steps per second: 65, episode reward: -9.887, mean reward: -0.056 [-100.000, 14.918], mean action: 1.477 [0.000, 3.000], mean observation: 0.094 [-0.749, 1.146], loss: 1.033646, mean_absolute_error: 25.567101, mean_q: 34.037252, mean_eps: 0.100000
 1422478/2000000: episode: 6531, duration: 3.409s, episode steps: 221, steps per second: 65, episode reward: -171.778, mean reward: -0.777 [-100.000, 6.888], mean action: 1.805 [0.000, 3.000], mean observation: 0.132 [-2.012, 1.533], loss: 0.541293, mean_absolute_error: 25.621615, mean_q: 34.451829, mean_eps: 0.100000
 1422546/2000000: episode: 6532, duration: 1.070s, episode steps: 68, steps per second: 64, episode reward: -143.520, mean reward: -2.111 [-100.000, 17.070], mean action: 1.750 [0.000, 3.000], mean observation: -0.070 [-1.611, 1.000], loss: 0.986233, mean_absolute_error: 23.938221, mean_q: 32.022313, mean_eps: 0.100000
 1422658/2000000: episode: 6533, duration: 1.750s, episode steps: 112, steps per second: 64, episode reward: -13.216, mean reward: -0.118 [-100.000, 18.263], mean action: 1.420 [0.000, 3.000], mean observation: -0.025 [-1.045, 1.000], loss: 1.089510, mean_absolute_error: 26.517612, mean_q: 35.433195, mean_eps: 0.100000
 1422960/2000000: episode: 6534, duration: 4.767s, episode steps: 302, steps per second: 63, episode reward: -698.126, mean reward: -2.312 [-100.000, 3.919], mean action: 1.848 [0.000, 3.000], mean observation: 0.471 [-0.725, 4.598], loss: 0.732798, mean_absolute_error: 25.783915, mean_q: 34.543981, mean_eps: 0.100000
 1423174/2000000: episode: 6535, duration: 3.362s, episode steps: 214, steps per second: 64, episode reward: -8.062, mean reward: -0.038 [-100.000, 17.761], mean action: 1.537 [0.000, 3.000], mean observation: 0.038 [-0.655, 1.000], loss: 0.589941, mean_absolute_error: 25.796829, mean_q: 34.651576, mean_eps: 0.100000
 1423235/2000000: episode: 6536, duration: 0.962s, episode steps: 61, steps per second: 63, episode reward: -122.951, mean reward: -2.016 [-100.000, 16.619], mean action: 1.885 [0.000, 3.000], mean observation: -0.030 [-1.652, 1.000], loss: 1.059137, mean_absolute_error: 26.726343, mean_q: 35.436146, mean_eps: 0.100000
 1423298/2000000: episode: 6537, duration: 0.990s, episode steps: 63, steps per second: 64, episode reward: -130.001, mean reward: -2.064 [-100.000, 20.540], mean action: 1.571 [0.000, 3.000], mean observation: -0.007 [-1.596, 1.000], loss: 0.742871, mean_absolute_error: 24.435445, mean_q: 31.724822, mean_eps: 0.100000
 1423472/2000000: episode: 6538, duration: 2.741s, episode steps: 174, steps per second: 63, episode reward: -29.824, mean reward: -0.171 [-100.000, 21.516], mean action: 1.425 [0.000, 3.000], mean observation: -0.001 [-0.849, 1.000], loss: 0.571513, mean_absolute_error: 26.395487, mean_q: 34.982038, mean_eps: 0.100000
 1423621/2000000: episode: 6539, duration: 2.545s, episode steps: 149, steps per second: 59, episode reward: -27.653, mean reward: -0.186 [-100.000, 19.903], mean action: 1.617 [0.000, 3.000], mean observation: 0.002 [-0.917, 1.000], loss: 0.704454, mean_absolute_error: 24.483266, mean_q: 32.818037, mean_eps: 0.100000
 1423682/2000000: episode: 6540, duration: 0.932s, episode steps: 61, steps per second: 65, episode reward: -148.584, mean reward: -2.436 [-100.000, 17.289], mean action: 0.492 [0.000, 3.000], mean observation: -0.024 [-1.803, 1.000], loss: 0.627418, mean_absolute_error: 26.674909, mean_q: 35.800608, mean_eps: 0.100000
 1423784/2000000: episode: 6541, duration: 1.639s, episode steps: 102, steps per second: 62, episode reward: -75.269, mean reward: -0.738 [-100.000, 11.793], mean action: 1.353 [0.000, 3.000], mean observation: -0.030 [-1.112, 2.824], loss: 0.872664, mean_absolute_error: 25.313841, mean_q: 33.813657, mean_eps: 0.100000
 1423872/2000000: episode: 6542, duration: 1.419s, episode steps: 88, steps per second: 62, episode reward: -82.701, mean reward: -0.940 [-100.000, 15.992], mean action: 1.727 [0.000, 3.000], mean observation: -0.006 [-1.227, 3.211], loss: 0.905872, mean_absolute_error: 24.414601, mean_q: 32.663705, mean_eps: 0.100000
 1423980/2000000: episode: 6543, duration: 1.732s, episode steps: 108, steps per second: 62, episode reward: -116.833, mean reward: -1.082 [-100.000, 16.540], mean action: 0.981 [0.000, 3.000], mean observation: 0.027 [-1.606, 1.019], loss: 0.522347, mean_absolute_error: 24.580122, mean_q: 33.056483, mean_eps: 0.100000
 1424187/2000000: episode: 6544, duration: 3.233s, episode steps: 207, steps per second: 64, episode reward: -37.121, mean reward: -0.179 [-100.000, 20.574], mean action: 1.435 [0.000, 3.000], mean observation: 0.034 [-0.847, 1.000], loss: 0.573503, mean_absolute_error: 25.701793, mean_q: 34.477137, mean_eps: 0.100000
 1424249/2000000: episode: 6545, duration: 1.011s, episode steps: 62, steps per second: 61, episode reward: -113.771, mean reward: -1.835 [-100.000, 16.293], mean action: 1.371 [0.000, 3.000], mean observation: -0.022 [-1.618, 1.000], loss: 1.147736, mean_absolute_error: 26.165344, mean_q: 34.946536, mean_eps: 0.100000
 1424360/2000000: episode: 6546, duration: 1.746s, episode steps: 111, steps per second: 64, episode reward: -23.167, mean reward: -0.209 [-100.000, 11.842], mean action: 1.928 [0.000, 3.000], mean observation: 0.098 [-0.975, 1.000], loss: 0.773824, mean_absolute_error: 26.136371, mean_q: 34.334485, mean_eps: 0.100000
 1424520/2000000: episode: 6547, duration: 2.590s, episode steps: 160, steps per second: 62, episode reward: -39.619, mean reward: -0.248 [-100.000, 12.238], mean action: 1.681 [0.000, 3.000], mean observation: -0.017 [-0.714, 1.000], loss: 0.586064, mean_absolute_error: 26.217922, mean_q: 35.213671, mean_eps: 0.100000
 1424600/2000000: episode: 6548, duration: 1.291s, episode steps: 80, steps per second: 62, episode reward: -157.779, mean reward: -1.972 [-100.000, 7.520], mean action: 0.775 [0.000, 3.000], mean observation: 0.012 [-5.633, 1.000], loss: 0.666628, mean_absolute_error: 25.639111, mean_q: 34.241842, mean_eps: 0.100000
 1424815/2000000: episode: 6549, duration: 3.374s, episode steps: 215, steps per second: 64, episode reward: -117.409, mean reward: -0.546 [-100.000, 14.467], mean action: 1.326 [0.000, 3.000], mean observation: 0.100 [-4.759, 1.000], loss: 0.794702, mean_absolute_error: 26.228913, mean_q: 35.012103, mean_eps: 0.100000
 1424963/2000000: episode: 6550, duration: 2.387s, episode steps: 148, steps per second: 62, episode reward: -45.759, mean reward: -0.309 [-100.000, 13.249], mean action: 1.959 [0.000, 3.000], mean observation: 0.113 [-0.743, 1.038], loss: 1.179985, mean_absolute_error: 24.481934, mean_q: 32.732559, mean_eps: 0.100000
 1425070/2000000: episode: 6551, duration: 1.717s, episode steps: 107, steps per second: 62, episode reward: -67.714, mean reward: -0.633 [-100.000, 10.932], mean action: 1.729 [0.000, 3.000], mean observation: 0.097 [-1.204, 3.052], loss: 0.727831, mean_absolute_error: 26.505718, mean_q: 35.434672, mean_eps: 0.100000
 1425163/2000000: episode: 6552, duration: 1.731s, episode steps: 93, steps per second: 54, episode reward: -133.228, mean reward: -1.433 [-100.000, 6.834], mean action: 1.022 [0.000, 3.000], mean observation: -0.054 [-1.483, 4.850], loss: 0.565786, mean_absolute_error: 25.482238, mean_q: 34.242126, mean_eps: 0.100000
 1425231/2000000: episode: 6553, duration: 1.177s, episode steps: 68, steps per second: 58, episode reward: -136.482, mean reward: -2.007 [-100.000, 17.662], mean action: 0.750 [0.000, 3.000], mean observation: -0.001 [-1.596, 1.000], loss: 0.748891, mean_absolute_error: 27.190439, mean_q: 36.469258, mean_eps: 0.100000
 1425364/2000000: episode: 6554, duration: 2.130s, episode steps: 133, steps per second: 62, episode reward: -122.804, mean reward: -0.923 [-100.000, 17.499], mean action: 1.180 [0.000, 3.000], mean observation: -0.011 [-1.639, 1.000], loss: 0.794674, mean_absolute_error: 25.478903, mean_q: 34.154622, mean_eps: 0.100000
 1425469/2000000: episode: 6555, duration: 1.699s, episode steps: 105, steps per second: 62, episode reward: -91.379, mean reward: -0.870 [-100.000, 18.274], mean action: 0.819 [0.000, 3.000], mean observation: 0.022 [-1.504, 1.000], loss: 1.204374, mean_absolute_error: 27.177338, mean_q: 36.265550, mean_eps: 0.100000
 1425530/2000000: episode: 6556, duration: 0.933s, episode steps: 61, steps per second: 65, episode reward: -146.913, mean reward: -2.408 [-100.000, 8.205], mean action: 0.623 [0.000, 3.000], mean observation: -0.112 [-1.752, 6.189], loss: 1.431455, mean_absolute_error: 27.779194, mean_q: 36.919271, mean_eps: 0.100000
 1425735/2000000: episode: 6557, duration: 3.175s, episode steps: 205, steps per second: 65, episode reward: -15.699, mean reward: -0.077 [-100.000, 18.276], mean action: 1.278 [0.000, 3.000], mean observation: 0.026 [-0.662, 1.005], loss: 0.907544, mean_absolute_error: 26.543717, mean_q: 35.524239, mean_eps: 0.100000
 1425891/2000000: episode: 6558, duration: 2.473s, episode steps: 156, steps per second: 63, episode reward: -51.247, mean reward: -0.329 [-100.000, 10.001], mean action: 1.282 [0.000, 3.000], mean observation: 0.098 [-0.876, 1.000], loss: 1.010887, mean_absolute_error: 25.509661, mean_q: 34.130648, mean_eps: 0.100000
 1425962/2000000: episode: 6559, duration: 1.108s, episode steps: 71, steps per second: 64, episode reward: -164.539, mean reward: -2.317 [-100.000, 17.360], mean action: 1.197 [0.000, 3.000], mean observation: -0.022 [-1.737, 1.000], loss: 0.868201, mean_absolute_error: 25.530648, mean_q: 34.067019, mean_eps: 0.100000
 1426031/2000000: episode: 6560, duration: 1.069s, episode steps: 69, steps per second: 65, episode reward: -132.821, mean reward: -1.925 [-100.000, 16.532], mean action: 1.942 [0.000, 3.000], mean observation: 0.013 [-1.564, 1.000], loss: 0.954270, mean_absolute_error: 25.970388, mean_q: 34.760254, mean_eps: 0.100000
 1426185/2000000: episode: 6561, duration: 2.425s, episode steps: 154, steps per second: 64, episode reward: -66.904, mean reward: -0.434 [-100.000, 4.889], mean action: 1.539 [0.000, 3.000], mean observation: 0.064 [-0.742, 1.000], loss: 0.848950, mean_absolute_error: 25.837376, mean_q: 34.585276, mean_eps: 0.100000
 1426298/2000000: episode: 6562, duration: 1.753s, episode steps: 113, steps per second: 64, episode reward: -75.337, mean reward: -0.667 [-100.000, 9.325], mean action: 1.655 [0.000, 3.000], mean observation: 0.075 [-3.689, 1.001], loss: 0.788695, mean_absolute_error: 26.834096, mean_q: 35.960759, mean_eps: 0.100000
 1426399/2000000: episode: 6563, duration: 1.591s, episode steps: 101, steps per second: 63, episode reward: -104.209, mean reward: -1.032 [-100.000, 18.735], mean action: 0.604 [0.000, 3.000], mean observation: 0.056 [-1.518, 4.249], loss: 0.724500, mean_absolute_error: 26.646448, mean_q: 35.620730, mean_eps: 0.100000
 1426483/2000000: episode: 6564, duration: 1.310s, episode steps: 84, steps per second: 64, episode reward: -76.038, mean reward: -0.905 [-100.000, 22.104], mean action: 1.536 [0.000, 3.000], mean observation: 0.019 [-1.150, 1.000], loss: 0.495869, mean_absolute_error: 24.865784, mean_q: 33.387874, mean_eps: 0.100000
 1426629/2000000: episode: 6565, duration: 2.290s, episode steps: 146, steps per second: 64, episode reward: -31.449, mean reward: -0.215 [-100.000, 7.914], mean action: 1.699 [0.000, 3.000], mean observation: 0.071 [-0.797, 1.000], loss: 0.683535, mean_absolute_error: 26.640450, mean_q: 35.669874, mean_eps: 0.100000
 1426706/2000000: episode: 6566, duration: 1.217s, episode steps: 77, steps per second: 63, episode reward: -102.308, mean reward: -1.329 [-100.000, 17.322], mean action: 1.948 [0.000, 3.000], mean observation: -0.033 [-1.242, 1.000], loss: 0.794552, mean_absolute_error: 26.316225, mean_q: 35.184131, mean_eps: 0.100000
 1426893/2000000: episode: 6567, duration: 2.938s, episode steps: 187, steps per second: 64, episode reward: -28.998, mean reward: -0.155 [-100.000, 12.850], mean action: 1.556 [0.000, 3.000], mean observation: -0.010 [-0.706, 1.000], loss: 0.779944, mean_absolute_error: 25.737164, mean_q: 34.483416, mean_eps: 0.100000
 1426974/2000000: episode: 6568, duration: 1.266s, episode steps: 81, steps per second: 64, episode reward: -144.720, mean reward: -1.787 [-100.000, 17.192], mean action: 0.852 [0.000, 3.000], mean observation: -0.012 [-1.583, 1.000], loss: 0.774744, mean_absolute_error: 26.820044, mean_q: 35.648675, mean_eps: 0.100000
 1427105/2000000: episode: 6569, duration: 2.056s, episode steps: 131, steps per second: 64, episode reward: -72.726, mean reward: -0.555 [-100.000, 13.484], mean action: 1.168 [0.000, 3.000], mean observation: 0.002 [-1.139, 1.000], loss: 1.227961, mean_absolute_error: 25.480979, mean_q: 33.885152, mean_eps: 0.100000
 1427174/2000000: episode: 6570, duration: 1.081s, episode steps: 69, steps per second: 64, episode reward: -122.929, mean reward: -1.782 [-100.000, 13.494], mean action: 1.507 [0.000, 3.000], mean observation: 0.020 [-1.528, 1.000], loss: 1.098635, mean_absolute_error: 28.083429, mean_q: 37.584615, mean_eps: 0.100000
 1427468/2000000: episode: 6571, duration: 4.628s, episode steps: 294, steps per second: 64, episode reward: -86.253, mean reward: -0.293 [-100.000, 16.941], mean action: 1.520 [0.000, 3.000], mean observation: 0.009 [-1.253, 1.000], loss: 0.863600, mean_absolute_error: 25.515665, mean_q: 34.124873, mean_eps: 0.100000
 1427567/2000000: episode: 6572, duration: 1.540s, episode steps: 99, steps per second: 64, episode reward: -97.140, mean reward: -0.981 [-100.000, 11.910], mean action: 0.949 [0.000, 3.000], mean observation: 0.035 [-2.187, 1.000], loss: 0.648883, mean_absolute_error: 25.526675, mean_q: 33.399460, mean_eps: 0.100000
 1427630/2000000: episode: 6573, duration: 1.012s, episode steps: 63, steps per second: 62, episode reward: -136.502, mean reward: -2.167 [-100.000, 10.196], mean action: 1.143 [0.000, 3.000], mean observation: 0.019 [-5.417, 1.000], loss: 0.503957, mean_absolute_error: 25.081601, mean_q: 33.713014, mean_eps: 0.100000
 1427807/2000000: episode: 6574, duration: 2.745s, episode steps: 177, steps per second: 64, episode reward: -31.923, mean reward: -0.180 [-100.000, 17.549], mean action: 1.418 [0.000, 3.000], mean observation: 0.062 [-0.830, 1.001], loss: 0.525042, mean_absolute_error: 25.833283, mean_q: 34.687422, mean_eps: 0.100000
 1427958/2000000: episode: 6575, duration: 2.368s, episode steps: 151, steps per second: 64, episode reward: -52.375, mean reward: -0.347 [-100.000, 17.645], mean action: 1.642 [0.000, 3.000], mean observation: 0.057 [-1.036, 1.000], loss: 0.463347, mean_absolute_error: 24.988955, mean_q: 33.596302, mean_eps: 0.100000
 1428028/2000000: episode: 6576, duration: 1.110s, episode steps: 70, steps per second: 63, episode reward: -176.183, mean reward: -2.517 [-100.000, 6.878], mean action: 0.586 [0.000, 3.000], mean observation: -0.096 [-1.815, 7.608], loss: 0.619453, mean_absolute_error: 25.603798, mean_q: 34.381713, mean_eps: 0.100000
 1428108/2000000: episode: 6577, duration: 1.306s, episode steps: 80, steps per second: 61, episode reward: -164.332, mean reward: -2.054 [-100.000, 6.731], mean action: 1.488 [0.000, 3.000], mean observation: -0.066 [-1.580, 5.517], loss: 0.468745, mean_absolute_error: 25.705857, mean_q: 34.571745, mean_eps: 0.100000
 1428163/2000000: episode: 6578, duration: 0.877s, episode steps: 55, steps per second: 63, episode reward: -144.445, mean reward: -2.626 [-100.000, 7.662], mean action: 2.018 [0.000, 3.000], mean observation: -0.074 [-1.849, 1.000], loss: 0.707381, mean_absolute_error: 25.644649, mean_q: 34.369227, mean_eps: 0.100000
 1428225/2000000: episode: 6579, duration: 1.014s, episode steps: 62, steps per second: 61, episode reward: -97.758, mean reward: -1.577 [-100.000, 6.291], mean action: 2.032 [0.000, 3.000], mean observation: -0.155 [-1.239, 4.131], loss: 0.623753, mean_absolute_error: 25.934357, mean_q: 34.292051, mean_eps: 0.100000
 1428350/2000000: episode: 6580, duration: 1.945s, episode steps: 125, steps per second: 64, episode reward: -116.813, mean reward: -0.935 [-100.000, 12.360], mean action: 1.352 [0.000, 3.000], mean observation: 0.057 [-1.536, 1.000], loss: 1.238950, mean_absolute_error: 24.934887, mean_q: 33.268925, mean_eps: 0.100000
 1428511/2000000: episode: 6581, duration: 2.472s, episode steps: 161, steps per second: 65, episode reward: -45.328, mean reward: -0.282 [-100.000, 8.561], mean action: 1.323 [0.000, 3.000], mean observation: 0.090 [-0.812, 1.000], loss: 0.848654, mean_absolute_error: 25.485654, mean_q: 34.130282, mean_eps: 0.100000
 1428567/2000000: episode: 6582, duration: 0.878s, episode steps: 56, steps per second: 64, episode reward: -130.091, mean reward: -2.323 [-100.000, 16.845], mean action: 1.679 [0.000, 3.000], mean observation: -0.091 [-1.828, 1.000], loss: 0.557142, mean_absolute_error: 24.004666, mean_q: 31.952544, mean_eps: 0.100000
 1428701/2000000: episode: 6583, duration: 2.100s, episode steps: 134, steps per second: 64, episode reward: -55.167, mean reward: -0.412 [-100.000, 11.697], mean action: 1.940 [0.000, 3.000], mean observation: 0.103 [-0.764, 1.238], loss: 0.688600, mean_absolute_error: 26.335713, mean_q: 35.371276, mean_eps: 0.100000
 1428766/2000000: episode: 6584, duration: 1.016s, episode steps: 65, steps per second: 64, episode reward: -109.101, mean reward: -1.678 [-100.000, 16.571], mean action: 1.785 [0.000, 3.000], mean observation: -0.020 [-1.504, 1.000], loss: 0.481149, mean_absolute_error: 25.525494, mean_q: 34.325388, mean_eps: 0.100000
 1428830/2000000: episode: 6585, duration: 1.005s, episode steps: 64, steps per second: 64, episode reward: -114.607, mean reward: -1.791 [-100.000, 14.964], mean action: 1.438 [0.000, 3.000], mean observation: -0.049 [-1.655, 1.000], loss: 0.495850, mean_absolute_error: 25.184256, mean_q: 33.823236, mean_eps: 0.100000
 1429049/2000000: episode: 6586, duration: 3.478s, episode steps: 219, steps per second: 63, episode reward: -27.262, mean reward: -0.124 [-100.000, 17.523], mean action: 1.420 [0.000, 3.000], mean observation: -0.013 [-0.732, 1.000], loss: 0.652804, mean_absolute_error: 26.104418, mean_q: 35.040173, mean_eps: 0.100000
 1429117/2000000: episode: 6587, duration: 1.068s, episode steps: 68, steps per second: 64, episode reward: -85.966, mean reward: -1.264 [-100.000, 19.414], mean action: 1.676 [0.000, 3.000], mean observation: 0.008 [-1.232, 1.000], loss: 1.158107, mean_absolute_error: 26.936242, mean_q: 35.387208, mean_eps: 0.100000
 1429247/2000000: episode: 6588, duration: 1.992s, episode steps: 130, steps per second: 65, episode reward: -124.773, mean reward: -0.960 [-100.000, 7.318], mean action: 1.654 [0.000, 3.000], mean observation: -0.014 [-1.344, 4.722], loss: 0.696250, mean_absolute_error: 27.191353, mean_q: 36.196750, mean_eps: 0.100000
 1429351/2000000: episode: 6589, duration: 1.619s, episode steps: 104, steps per second: 64, episode reward: 2.073, mean reward: 0.020 [-100.000, 18.063], mean action: 1.904 [0.000, 3.000], mean observation: -0.017 [-0.906, 1.000], loss: 0.791641, mean_absolute_error: 27.501478, mean_q: 36.849328, mean_eps: 0.100000
 1429561/2000000: episode: 6590, duration: 3.308s, episode steps: 210, steps per second: 63, episode reward: -86.056, mean reward: -0.410 [-100.000, 10.631], mean action: 1.505 [0.000, 3.000], mean observation: 0.008 [-1.049, 3.328], loss: 0.559544, mean_absolute_error: 24.827936, mean_q: 33.345262, mean_eps: 0.100000
 1429647/2000000: episode: 6591, duration: 1.320s, episode steps: 86, steps per second: 65, episode reward: -86.861, mean reward: -1.010 [-100.000, 14.331], mean action: 1.035 [0.000, 3.000], mean observation: -0.022 [-1.183, 1.000], loss: 0.770658, mean_absolute_error: 24.357357, mean_q: 32.659942, mean_eps: 0.100000
 1429908/2000000: episode: 6592, duration: 4.159s, episode steps: 261, steps per second: 63, episode reward: -6.957, mean reward: -0.027 [-100.000, 20.727], mean action: 1.552 [0.000, 3.000], mean observation: -0.007 [-1.113, 1.000], loss: 0.644673, mean_absolute_error: 26.502654, mean_q: 35.550562, mean_eps: 0.100000
 1429980/2000000: episode: 6593, duration: 1.179s, episode steps: 72, steps per second: 61, episode reward: -163.329, mean reward: -2.268 [-100.000, 16.661], mean action: 1.792 [0.000, 3.000], mean observation: -0.039 [-1.764, 1.000], loss: 0.589237, mean_absolute_error: 27.327635, mean_q: 36.638697, mean_eps: 0.100000
 1430071/2000000: episode: 6594, duration: 1.428s, episode steps: 91, steps per second: 64, episode reward: -134.868, mean reward: -1.482 [-100.000, 17.684], mean action: 1.516 [0.000, 3.000], mean observation: 0.019 [-1.900, 1.014], loss: 0.811996, mean_absolute_error: 27.455433, mean_q: 36.700788, mean_eps: 0.100000
 1430129/2000000: episode: 6595, duration: 0.934s, episode steps: 58, steps per second: 62, episode reward: -126.621, mean reward: -2.183 [-100.000, 21.839], mean action: 1.810 [0.000, 3.000], mean observation: -0.055 [-1.782, 1.000], loss: 0.735866, mean_absolute_error: 25.121441, mean_q: 32.197814, mean_eps: 0.100000
 1430213/2000000: episode: 6596, duration: 1.298s, episode steps: 84, steps per second: 65, episode reward: -118.964, mean reward: -1.416 [-100.000, 18.132], mean action: 1.429 [0.000, 3.000], mean observation: -0.029 [-1.502, 1.000], loss: 1.010985, mean_absolute_error: 25.581820, mean_q: 33.922686, mean_eps: 0.100000
 1430282/2000000: episode: 6597, duration: 1.081s, episode steps: 69, steps per second: 64, episode reward: -94.398, mean reward: -1.368 [-100.000, 11.004], mean action: 1.406 [0.000, 3.000], mean observation: 0.015 [-3.714, 1.000], loss: 0.883785, mean_absolute_error: 25.332128, mean_q: 33.693286, mean_eps: 0.100000
 1430348/2000000: episode: 6598, duration: 1.040s, episode steps: 66, steps per second: 63, episode reward: -159.308, mean reward: -2.414 [-100.000, 21.070], mean action: 1.364 [0.000, 3.000], mean observation: -0.020 [-1.804, 1.000], loss: 0.468227, mean_absolute_error: 25.571460, mean_q: 34.344839, mean_eps: 0.100000
 1430436/2000000: episode: 6599, duration: 1.401s, episode steps: 88, steps per second: 63, episode reward: -98.406, mean reward: -1.118 [-100.000, 15.446], mean action: 1.409 [0.000, 3.000], mean observation: 0.080 [-1.108, 3.192], loss: 0.803769, mean_absolute_error: 28.391148, mean_q: 37.981620, mean_eps: 0.100000
 1430515/2000000: episode: 6600, duration: 1.259s, episode steps: 79, steps per second: 63, episode reward: -159.852, mean reward: -2.023 [-100.000, 16.834], mean action: 1.203 [0.000, 3.000], mean observation: -0.053 [-1.788, 1.000], loss: 0.580488, mean_absolute_error: 25.756841, mean_q: 34.582733, mean_eps: 0.100000
 1430625/2000000: episode: 6601, duration: 1.718s, episode steps: 110, steps per second: 64, episode reward: -64.928, mean reward: -0.590 [-100.000, 7.413], mean action: 1.682 [0.000, 3.000], mean observation: 0.096 [-1.616, 1.000], loss: 0.548025, mean_absolute_error: 26.084423, mean_q: 35.028708, mean_eps: 0.100000
 1430686/2000000: episode: 6602, duration: 0.927s, episode steps: 61, steps per second: 66, episode reward: -146.442, mean reward: -2.401 [-100.000, 16.470], mean action: 0.459 [0.000, 3.000], mean observation: -0.062 [-1.837, 1.000], loss: 0.917764, mean_absolute_error: 26.184466, mean_q: 34.968190, mean_eps: 0.100000
 1430857/2000000: episode: 6603, duration: 2.679s, episode steps: 171, steps per second: 64, episode reward: -43.429, mean reward: -0.254 [-100.000, 12.242], mean action: 1.374 [0.000, 3.000], mean observation: 0.094 [-1.253, 1.000], loss: 0.743018, mean_absolute_error: 26.991170, mean_q: 35.887150, mean_eps: 0.100000
 1431013/2000000: episode: 6604, duration: 2.413s, episode steps: 156, steps per second: 65, episode reward: -62.849, mean reward: -0.403 [-100.000, 13.781], mean action: 1.423 [0.000, 3.000], mean observation: 0.113 [-0.776, 1.000], loss: 1.153383, mean_absolute_error: 28.129070, mean_q: 37.471570, mean_eps: 0.100000
 1431239/2000000: episode: 6605, duration: 3.774s, episode steps: 226, steps per second: 60, episode reward: -310.597, mean reward: -1.374 [-100.000, 5.163], mean action: 1.677 [0.000, 3.000], mean observation: 0.123 [-0.597, 1.352], loss: 0.802521, mean_absolute_error: 25.775391, mean_q: 34.508734, mean_eps: 0.100000
 1431308/2000000: episode: 6606, duration: 1.127s, episode steps: 69, steps per second: 61, episode reward: -158.852, mean reward: -2.302 [-100.000, 17.568], mean action: 0.478 [0.000, 3.000], mean observation: -0.046 [-1.750, 1.000], loss: 0.581337, mean_absolute_error: 25.603676, mean_q: 34.257745, mean_eps: 0.100000
 1431393/2000000: episode: 6607, duration: 1.368s, episode steps: 85, steps per second: 62, episode reward: -161.199, mean reward: -1.896 [-100.000, 7.338], mean action: 0.941 [0.000, 3.000], mean observation: -0.064 [-1.593, 5.377], loss: 0.832836, mean_absolute_error: 25.103469, mean_q: 33.642779, mean_eps: 0.100000
 1431591/2000000: episode: 6608, duration: 3.055s, episode steps: 198, steps per second: 65, episode reward: -49.506, mean reward: -0.250 [-100.000, 16.563], mean action: 1.641 [0.000, 3.000], mean observation: 0.033 [-1.067, 1.000], loss: 0.669385, mean_absolute_error: 26.504508, mean_q: 35.569560, mean_eps: 0.100000
 1431885/2000000: episode: 6609, duration: 4.645s, episode steps: 294, steps per second: 63, episode reward: -102.813, mean reward: -0.350 [-100.000, 17.790], mean action: 1.704 [0.000, 3.000], mean observation: 0.015 [-4.232, 1.000], loss: 0.675171, mean_absolute_error: 26.538204, mean_q: 35.577978, mean_eps: 0.100000
 1431944/2000000: episode: 6610, duration: 0.934s, episode steps: 59, steps per second: 63, episode reward: -135.992, mean reward: -2.305 [-100.000, 22.041], mean action: 0.559 [0.000, 3.000], mean observation: -0.043 [-1.815, 1.000], loss: 0.496468, mean_absolute_error: 26.053814, mean_q: 35.059928, mean_eps: 0.100000
 1431997/2000000: episode: 6611, duration: 0.880s, episode steps: 53, steps per second: 60, episode reward: -125.117, mean reward: -2.361 [-100.000, 16.790], mean action: 0.302 [0.000, 3.000], mean observation: -0.096 [-1.884, 1.000], loss: 1.222944, mean_absolute_error: 28.207350, mean_q: 37.524162, mean_eps: 0.100000
 1432072/2000000: episode: 6612, duration: 1.180s, episode steps: 75, steps per second: 64, episode reward: -160.689, mean reward: -2.143 [-100.000, 16.837], mean action: 0.533 [0.000, 3.000], mean observation: -0.046 [-1.807, 1.000], loss: 0.835664, mean_absolute_error: 26.469768, mean_q: 35.459344, mean_eps: 0.100000
 1432138/2000000: episode: 6613, duration: 1.068s, episode steps: 66, steps per second: 62, episode reward: -158.797, mean reward: -2.406 [-100.000, 17.288], mean action: 0.485 [0.000, 3.000], mean observation: -0.073 [-1.821, 1.000], loss: 0.885480, mean_absolute_error: 26.854342, mean_q: 35.956774, mean_eps: 0.100000
 1432196/2000000: episode: 6614, duration: 0.932s, episode steps: 58, steps per second: 62, episode reward: -140.054, mean reward: -2.415 [-100.000, 17.272], mean action: 0.828 [0.000, 3.000], mean observation: -0.034 [-1.765, 1.000], loss: 0.983075, mean_absolute_error: 25.450913, mean_q: 34.026867, mean_eps: 0.100000
 1432255/2000000: episode: 6615, duration: 0.933s, episode steps: 59, steps per second: 63, episode reward: -129.173, mean reward: -2.189 [-100.000, 16.999], mean action: 0.559 [0.000, 3.000], mean observation: -0.064 [-1.823, 1.000], loss: 1.078375, mean_absolute_error: 25.572063, mean_q: 34.108547, mean_eps: 0.100000
 1432363/2000000: episode: 6616, duration: 1.672s, episode steps: 108, steps per second: 65, episode reward: -20.415, mean reward: -0.189 [-100.000, 12.565], mean action: 1.833 [0.000, 3.000], mean observation: 0.124 [-0.925, 1.128], loss: 0.694468, mean_absolute_error: 26.370713, mean_q: 35.246276, mean_eps: 0.100000
 1432572/2000000: episode: 6617, duration: 3.304s, episode steps: 209, steps per second: 63, episode reward: -117.593, mean reward: -0.563 [-100.000, 9.234], mean action: 1.378 [0.000, 3.000], mean observation: 0.104 [-1.413, 1.000], loss: 0.577807, mean_absolute_error: 26.215252, mean_q: 35.252660, mean_eps: 0.100000
 1432666/2000000: episode: 6618, duration: 1.500s, episode steps: 94, steps per second: 63, episode reward: -120.226, mean reward: -1.279 [-100.000, 14.574], mean action: 0.787 [0.000, 3.000], mean observation: 0.046 [-1.591, 1.000], loss: 0.963029, mean_absolute_error: 27.130487, mean_q: 36.288035, mean_eps: 0.100000
 1432762/2000000: episode: 6619, duration: 1.515s, episode steps: 96, steps per second: 63, episode reward: -128.711, mean reward: -1.341 [-100.000, 14.498], mean action: 1.958 [0.000, 3.000], mean observation: 0.053 [-4.765, 1.000], loss: 0.695530, mean_absolute_error: 25.518062, mean_q: 34.251561, mean_eps: 0.100000
 1432839/2000000: episode: 6620, duration: 1.216s, episode steps: 77, steps per second: 63, episode reward: -157.511, mean reward: -2.046 [-100.000, 18.205], mean action: 1.494 [0.000, 3.000], mean observation: -0.010 [-1.804, 1.000], loss: 0.757497, mean_absolute_error: 26.171474, mean_q: 35.057047, mean_eps: 0.100000
 1432895/2000000: episode: 6621, duration: 0.858s, episode steps: 56, steps per second: 65, episode reward: -137.986, mean reward: -2.464 [-100.000, 17.528], mean action: 1.089 [0.000, 3.000], mean observation: -0.031 [-1.859, 1.000], loss: 1.006133, mean_absolute_error: 25.891627, mean_q: 34.634432, mean_eps: 0.100000
 1432955/2000000: episode: 6622, duration: 0.932s, episode steps: 60, steps per second: 64, episode reward: -123.821, mean reward: -2.064 [-100.000, 8.077], mean action: 1.083 [0.000, 3.000], mean observation: 0.020 [-1.570, 5.313], loss: 1.190123, mean_absolute_error: 28.221253, mean_q: 37.703367, mean_eps: 0.100000
 1433035/2000000: episode: 6623, duration: 1.255s, episode steps: 80, steps per second: 64, episode reward: -144.696, mean reward: -1.809 [-100.000, 16.678], mean action: 0.988 [0.000, 3.000], mean observation: -0.002 [-1.714, 1.000], loss: 0.816941, mean_absolute_error: 26.131039, mean_q: 34.270508, mean_eps: 0.100000
 1433105/2000000: episode: 6624, duration: 1.128s, episode steps: 70, steps per second: 62, episode reward: -112.559, mean reward: -1.608 [-100.000, 6.437], mean action: 1.371 [0.000, 3.000], mean observation: -0.163 [-1.227, 3.575], loss: 0.692316, mean_absolute_error: 25.637895, mean_q: 34.391952, mean_eps: 0.100000
 1433244/2000000: episode: 6625, duration: 2.178s, episode steps: 139, steps per second: 64, episode reward: -61.244, mean reward: -0.441 [-100.000, 17.316], mean action: 1.388 [0.000, 3.000], mean observation: -0.010 [-1.151, 3.084], loss: 0.578201, mean_absolute_error: 27.107085, mean_q: 35.924302, mean_eps: 0.100000
 1433314/2000000: episode: 6626, duration: 1.125s, episode steps: 70, steps per second: 62, episode reward: -82.540, mean reward: -1.179 [-100.000, 14.378], mean action: 1.471 [0.000, 3.000], mean observation: -0.170 [-1.148, 2.983], loss: 0.739337, mean_absolute_error: 27.658388, mean_q: 37.051149, mean_eps: 0.100000
 1433401/2000000: episode: 6627, duration: 1.354s, episode steps: 87, steps per second: 64, episode reward: -90.390, mean reward: -1.039 [-100.000, 17.080], mean action: 0.954 [0.000, 3.000], mean observation: -0.063 [-1.139, 3.150], loss: 0.773472, mean_absolute_error: 27.541080, mean_q: 36.312106, mean_eps: 0.100000
 1433501/2000000: episode: 6628, duration: 1.612s, episode steps: 100, steps per second: 62, episode reward: -120.834, mean reward: -1.208 [-100.000, 19.867], mean action: 0.690 [0.000, 3.000], mean observation: 0.018 [-1.647, 1.000], loss: 1.007762, mean_absolute_error: 28.017807, mean_q: 37.425235, mean_eps: 0.100000
 1433629/2000000: episode: 6629, duration: 1.980s, episode steps: 128, steps per second: 65, episode reward: -127.317, mean reward: -0.995 [-100.000, 15.962], mean action: 1.211 [0.000, 3.000], mean observation: -0.038 [-1.499, 1.000], loss: 0.987354, mean_absolute_error: 27.427933, mean_q: 36.668333, mean_eps: 0.100000
 1433746/2000000: episode: 6630, duration: 1.826s, episode steps: 117, steps per second: 64, episode reward: -55.364, mean reward: -0.473 [-100.000, 39.315], mean action: 1.821 [0.000, 3.000], mean observation: -0.077 [-1.107, 1.130], loss: 0.773819, mean_absolute_error: 26.922167, mean_q: 36.087759, mean_eps: 0.100000
 1433864/2000000: episode: 6631, duration: 1.853s, episode steps: 118, steps per second: 64, episode reward: -138.011, mean reward: -1.170 [-100.000, 6.918], mean action: 1.042 [0.000, 3.000], mean observation: 0.006 [-1.625, 5.616], loss: 0.853937, mean_absolute_error: 25.840668, mean_q: 34.587705, mean_eps: 0.100000
 1434016/2000000: episode: 6632, duration: 2.441s, episode steps: 152, steps per second: 62, episode reward: -51.803, mean reward: -0.341 [-100.000, 7.062], mean action: 1.612 [0.000, 3.000], mean observation: 0.094 [-0.904, 1.000], loss: 0.817086, mean_absolute_error: 27.381243, mean_q: 36.365634, mean_eps: 0.100000
 1434211/2000000: episode: 6633, duration: 3.085s, episode steps: 195, steps per second: 63, episode reward: -14.489, mean reward: -0.074 [-100.000, 14.273], mean action: 1.841 [0.000, 3.000], mean observation: -0.007 [-0.578, 1.000], loss: 0.797570, mean_absolute_error: 27.836520, mean_q: 37.205810, mean_eps: 0.100000
 1434323/2000000: episode: 6634, duration: 1.757s, episode steps: 112, steps per second: 64, episode reward: 25.291, mean reward: 0.226 [-100.000, 17.612], mean action: 1.821 [0.000, 3.000], mean observation: -0.029 [-1.023, 1.000], loss: 0.838722, mean_absolute_error: 25.754967, mean_q: 34.528389, mean_eps: 0.100000
 1434407/2000000: episode: 6635, duration: 1.302s, episode steps: 84, steps per second: 65, episode reward: -130.900, mean reward: -1.558 [-100.000, 17.778], mean action: 1.167 [0.000, 3.000], mean observation: -0.015 [-1.647, 1.000], loss: 1.629335, mean_absolute_error: 26.362384, mean_q: 34.856444, mean_eps: 0.100000
 1434485/2000000: episode: 6636, duration: 1.237s, episode steps: 78, steps per second: 63, episode reward: -115.220, mean reward: -1.477 [-100.000, 13.530], mean action: 1.397 [0.000, 3.000], mean observation: 0.031 [-3.808, 1.000], loss: 0.682607, mean_absolute_error: 24.862529, mean_q: 33.132066, mean_eps: 0.100000
 1434549/2000000: episode: 6637, duration: 0.984s, episode steps: 64, steps per second: 65, episode reward: -134.772, mean reward: -2.106 [-100.000, 18.408], mean action: 0.625 [0.000, 3.000], mean observation: -0.052 [-1.725, 1.000], loss: 0.984766, mean_absolute_error: 24.422413, mean_q: 31.986007, mean_eps: 0.100000
 1434607/2000000: episode: 6638, duration: 0.876s, episode steps: 58, steps per second: 66, episode reward: -123.857, mean reward: -2.135 [-100.000, 16.329], mean action: 0.983 [0.000, 3.000], mean observation: -0.089 [-1.763, 1.000], loss: 0.850392, mean_absolute_error: 27.603487, mean_q: 36.956448, mean_eps: 0.100000
 1434708/2000000: episode: 6639, duration: 1.605s, episode steps: 101, steps per second: 63, episode reward: -114.468, mean reward: -1.133 [-100.000, 17.735], mean action: 0.733 [0.000, 3.000], mean observation: 0.041 [-1.638, 1.011], loss: 0.814756, mean_absolute_error: 27.474006, mean_q: 36.804776, mean_eps: 0.100000
 1434765/2000000: episode: 6640, duration: 0.912s, episode steps: 57, steps per second: 63, episode reward: -117.239, mean reward: -2.057 [-100.000, 17.474], mean action: 0.719 [0.000, 3.000], mean observation: 0.012 [-1.606, 5.423], loss: 1.031623, mean_absolute_error: 26.674578, mean_q: 35.739023, mean_eps: 0.100000
 1434897/2000000: episode: 6641, duration: 2.013s, episode steps: 132, steps per second: 66, episode reward: -86.723, mean reward: -0.657 [-100.000, 8.485], mean action: 1.424 [0.000, 3.000], mean observation: 0.092 [-0.793, 1.631], loss: 0.792388, mean_absolute_error: 26.405576, mean_q: 34.974604, mean_eps: 0.100000
 1434976/2000000: episode: 6642, duration: 1.224s, episode steps: 79, steps per second: 65, episode reward: -9.801, mean reward: -0.124 [-100.000, 18.436], mean action: 1.797 [0.000, 3.000], mean observation: 0.090 [-1.447, 1.000], loss: 0.610078, mean_absolute_error: 26.247656, mean_q: 35.283632, mean_eps: 0.100000
 1435060/2000000: episode: 6643, duration: 1.326s, episode steps: 84, steps per second: 63, episode reward: -54.270, mean reward: -0.646 [-100.000, 11.454], mean action: 1.940 [0.000, 3.000], mean observation: -0.080 [-0.958, 2.999], loss: 0.922338, mean_absolute_error: 27.854855, mean_q: 37.257367, mean_eps: 0.100000
 1435243/2000000: episode: 6644, duration: 2.861s, episode steps: 183, steps per second: 64, episode reward: -49.552, mean reward: -0.271 [-100.000, 12.099], mean action: 1.749 [0.000, 3.000], mean observation: 0.011 [-0.626, 1.648], loss: 0.765472, mean_absolute_error: 27.833823, mean_q: 37.089276, mean_eps: 0.100000
 1435541/2000000: episode: 6645, duration: 4.745s, episode steps: 298, steps per second: 63, episode reward: 177.654, mean reward: 0.596 [-17.417, 100.000], mean action: 1.242 [0.000, 3.000], mean observation: 0.121 [-0.555, 1.000], loss: 0.921566, mean_absolute_error: 26.925153, mean_q: 35.868717, mean_eps: 0.100000
 1435657/2000000: episode: 6646, duration: 1.804s, episode steps: 116, steps per second: 64, episode reward: -34.533, mean reward: -0.298 [-100.000, 8.646], mean action: 1.629 [0.000, 3.000], mean observation: 0.098 [-0.884, 1.000], loss: 0.775619, mean_absolute_error: 26.996670, mean_q: 36.163665, mean_eps: 0.100000
 1435726/2000000: episode: 6647, duration: 1.053s, episode steps: 69, steps per second: 66, episode reward: -100.247, mean reward: -1.453 [-100.000, 13.556], mean action: 0.928 [0.000, 3.000], mean observation: 0.032 [-1.330, 4.041], loss: 0.546408, mean_absolute_error: 27.357450, mean_q: 36.695879, mean_eps: 0.100000
 1435804/2000000: episode: 6648, duration: 1.242s, episode steps: 78, steps per second: 63, episode reward: -69.156, mean reward: -0.887 [-100.000, 8.378], mean action: 1.218 [0.000, 3.000], mean observation: 0.078 [-1.086, 3.118], loss: 1.129850, mean_absolute_error: 28.219408, mean_q: 37.670879, mean_eps: 0.100000
 1435965/2000000: episode: 6649, duration: 2.569s, episode steps: 161, steps per second: 63, episode reward: -17.279, mean reward: -0.107 [-100.000, 17.989], mean action: 1.658 [0.000, 3.000], mean observation: 0.002 [-0.995, 1.000], loss: 0.928100, mean_absolute_error: 26.098762, mean_q: 34.351441, mean_eps: 0.100000
 1436305/2000000: episode: 6650, duration: 5.329s, episode steps: 340, steps per second: 64, episode reward: 207.770, mean reward: 0.611 [-11.183, 100.000], mean action: 1.135 [0.000, 3.000], mean observation: 0.182 [-0.913, 1.000], loss: 0.658851, mean_absolute_error: 26.393966, mean_q: 35.348665, mean_eps: 0.100000
 1436405/2000000: episode: 6651, duration: 1.571s, episode steps: 100, steps per second: 64, episode reward: -164.719, mean reward: -1.647 [-100.000, 6.940], mean action: 1.880 [0.000, 3.000], mean observation: -0.051 [-1.597, 5.640], loss: 0.765167, mean_absolute_error: 26.929919, mean_q: 35.862153, mean_eps: 0.100000
 1436558/2000000: episode: 6652, duration: 2.386s, episode steps: 153, steps per second: 64, episode reward: -32.908, mean reward: -0.215 [-100.000, 26.136], mean action: 1.582 [0.000, 3.000], mean observation: -0.013 [-1.030, 1.000], loss: 0.821228, mean_absolute_error: 28.328016, mean_q: 37.945877, mean_eps: 0.100000
 1436627/2000000: episode: 6653, duration: 1.067s, episode steps: 69, steps per second: 65, episode reward: -152.207, mean reward: -2.206 [-100.000, 15.164], mean action: 0.478 [0.000, 3.000], mean observation: -0.009 [-1.729, 1.000], loss: 0.619503, mean_absolute_error: 27.969197, mean_q: 37.540487, mean_eps: 0.100000
 1436769/2000000: episode: 6654, duration: 2.229s, episode steps: 142, steps per second: 64, episode reward: -54.823, mean reward: -0.386 [-100.000, 14.546], mean action: 1.423 [0.000, 3.000], mean observation: 0.102 [-0.781, 2.030], loss: 0.672599, mean_absolute_error: 25.520459, mean_q: 33.972171, mean_eps: 0.100000
 1436847/2000000: episode: 6655, duration: 1.195s, episode steps: 78, steps per second: 65, episode reward: -169.350, mean reward: -2.171 [-100.000, 7.772], mean action: 0.795 [0.000, 3.000], mean observation: -0.005 [-5.934, 1.000], loss: 0.607217, mean_absolute_error: 27.360632, mean_q: 36.699254, mean_eps: 0.100000
 1436902/2000000: episode: 6656, duration: 0.877s, episode steps: 55, steps per second: 63, episode reward: -147.324, mean reward: -2.679 [-100.000, 7.693], mean action: 0.855 [0.000, 3.000], mean observation: -0.053 [-6.309, 1.000], loss: 0.771760, mean_absolute_error: 27.861481, mean_q: 37.293886, mean_eps: 0.100000
 1437000/2000000: episode: 6657, duration: 1.547s, episode steps: 98, steps per second: 63, episode reward: -56.468, mean reward: -0.576 [-100.000, 13.932], mean action: 1.908 [0.000, 3.000], mean observation: 0.055 [-0.876, 1.255], loss: 0.572243, mean_absolute_error: 27.843477, mean_q: 37.329752, mean_eps: 0.100000
 1437511/2000000: episode: 6658, duration: 8.552s, episode steps: 511, steps per second: 60, episode reward: -147.387, mean reward: -0.288 [-100.000, 10.755], mean action: 1.800 [0.000, 3.000], mean observation: 0.077 [-2.556, 1.000], loss: 0.652378, mean_absolute_error: 27.146485, mean_q: 36.291620, mean_eps: 0.100000
 1437662/2000000: episode: 6659, duration: 2.344s, episode steps: 151, steps per second: 64, episode reward: -65.034, mean reward: -0.431 [-100.000, 15.264], mean action: 1.305 [0.000, 3.000], mean observation: 0.054 [-1.006, 3.150], loss: 0.742421, mean_absolute_error: 26.230693, mean_q: 35.041876, mean_eps: 0.100000
 1437946/2000000: episode: 6660, duration: 4.493s, episode steps: 284, steps per second: 63, episode reward: -50.933, mean reward: -0.179 [-100.000, 12.024], mean action: 1.651 [0.000, 3.000], mean observation: 0.062 [-0.618, 1.000], loss: 1.056950, mean_absolute_error: 27.418385, mean_q: 36.474642, mean_eps: 0.100000
 1438597/2000000: episode: 6661, duration: 10.410s, episode steps: 651, steps per second: 63, episode reward: 181.675, mean reward: 0.279 [-19.270, 100.000], mean action: 1.935 [0.000, 3.000], mean observation: 0.257 [-0.794, 1.000], loss: 0.698071, mean_absolute_error: 26.813201, mean_q: 35.810808, mean_eps: 0.100000
 1438818/2000000: episode: 6662, duration: 3.595s, episode steps: 221, steps per second: 61, episode reward: -43.259, mean reward: -0.196 [-100.000, 10.777], mean action: 1.602 [0.000, 3.000], mean observation: -0.003 [-0.696, 1.362], loss: 0.613761, mean_absolute_error: 26.320274, mean_q: 35.311154, mean_eps: 0.100000
 1438899/2000000: episode: 6663, duration: 1.269s, episode steps: 81, steps per second: 64, episode reward: -107.906, mean reward: -1.332 [-100.000, 25.016], mean action: 0.765 [0.000, 3.000], mean observation: -0.004 [-1.561, 1.000], loss: 0.920728, mean_absolute_error: 26.776482, mean_q: 35.619627, mean_eps: 0.100000
 1438978/2000000: episode: 6664, duration: 1.237s, episode steps: 79, steps per second: 64, episode reward: -108.911, mean reward: -1.379 [-100.000, 14.400], mean action: 1.519 [0.000, 3.000], mean observation: -0.035 [-1.396, 1.000], loss: 0.788858, mean_absolute_error: 27.460668, mean_q: 36.544118, mean_eps: 0.100000
 1439467/2000000: episode: 6665, duration: 7.803s, episode steps: 489, steps per second: 63, episode reward: -233.006, mean reward: -0.476 [-100.000, 12.411], mean action: 1.593 [0.000, 3.000], mean observation: 0.111 [-2.444, 2.064], loss: 0.781176, mean_absolute_error: 27.302871, mean_q: 36.214249, mean_eps: 0.100000
 1439563/2000000: episode: 6666, duration: 1.495s, episode steps: 96, steps per second: 64, episode reward: -143.827, mean reward: -1.498 [-100.000, 6.718], mean action: 1.365 [0.000, 3.000], mean observation: -0.029 [-1.691, 1.277], loss: 1.109049, mean_absolute_error: 28.785049, mean_q: 38.233952, mean_eps: 0.100000
 1439666/2000000: episode: 6667, duration: 1.606s, episode steps: 103, steps per second: 64, episode reward: -81.254, mean reward: -0.789 [-100.000, 6.584], mean action: 1.029 [0.000, 3.000], mean observation: -0.008 [-1.314, 3.654], loss: 0.518670, mean_absolute_error: 28.240950, mean_q: 37.386970, mean_eps: 0.100000
 1439817/2000000: episode: 6668, duration: 2.468s, episode steps: 151, steps per second: 61, episode reward: -40.016, mean reward: -0.265 [-100.000, 13.246], mean action: 1.656 [0.000, 3.000], mean observation: 0.081 [-0.889, 1.524], loss: 0.861834, mean_absolute_error: 28.721645, mean_q: 37.918995, mean_eps: 0.100000
 1439882/2000000: episode: 6669, duration: 0.991s, episode steps: 65, steps per second: 66, episode reward: -112.959, mean reward: -1.738 [-100.000, 16.681], mean action: 0.831 [0.000, 3.000], mean observation: -0.060 [-1.508, 1.000], loss: 0.622837, mean_absolute_error: 27.517147, mean_q: 36.916924, mean_eps: 0.100000
 1439971/2000000: episode: 6670, duration: 1.353s, episode steps: 89, steps per second: 66, episode reward: -36.163, mean reward: -0.406 [-100.000, 12.534], mean action: 1.303 [0.000, 3.000], mean observation: -0.058 [-0.929, 1.000], loss: 0.912215, mean_absolute_error: 26.059454, mean_q: 34.838099, mean_eps: 0.100000
 1440111/2000000: episode: 6671, duration: 2.211s, episode steps: 140, steps per second: 63, episode reward: -9.474, mean reward: -0.068 [-100.000, 12.697], mean action: 1.657 [0.000, 3.000], mean observation: 0.089 [-1.496, 1.000], loss: 0.716001, mean_absolute_error: 27.272563, mean_q: 36.348959, mean_eps: 0.100000
 1440177/2000000: episode: 6672, duration: 1.058s, episode steps: 66, steps per second: 62, episode reward: -63.270, mean reward: -0.959 [-100.000, 14.624], mean action: 1.939 [0.000, 3.000], mean observation: 0.025 [-1.173, 1.000], loss: 0.865387, mean_absolute_error: 26.668412, mean_q: 34.992051, mean_eps: 0.100000
 1440301/2000000: episode: 6673, duration: 1.954s, episode steps: 124, steps per second: 63, episode reward: -83.043, mean reward: -0.670 [-100.000, 18.858], mean action: 1.992 [0.000, 3.000], mean observation: -0.014 [-1.266, 1.000], loss: 0.704490, mean_absolute_error: 28.731664, mean_q: 38.459749, mean_eps: 0.100000
 1440435/2000000: episode: 6674, duration: 2.060s, episode steps: 134, steps per second: 65, episode reward: -76.652, mean reward: -0.572 [-100.000, 19.587], mean action: 1.672 [0.000, 3.000], mean observation: 0.004 [-1.004, 1.000], loss: 1.206511, mean_absolute_error: 27.072172, mean_q: 36.030712, mean_eps: 0.100000
 1440502/2000000: episode: 6675, duration: 1.065s, episode steps: 67, steps per second: 63, episode reward: -119.027, mean reward: -1.777 [-100.000, 20.088], mean action: 1.955 [0.000, 3.000], mean observation: 0.004 [-1.528, 1.000], loss: 1.269389, mean_absolute_error: 25.148349, mean_q: 33.503660, mean_eps: 0.100000
 1440639/2000000: episode: 6676, duration: 2.118s, episode steps: 137, steps per second: 65, episode reward: -40.935, mean reward: -0.299 [-100.000, 18.165], mean action: 1.920 [0.000, 3.000], mean observation: -0.046 [-0.751, 1.540], loss: 0.755717, mean_absolute_error: 28.150512, mean_q: 37.457590, mean_eps: 0.100000
 1440728/2000000: episode: 6677, duration: 1.402s, episode steps: 89, steps per second: 63, episode reward: -56.773, mean reward: -0.638 [-100.000, 14.275], mean action: 1.270 [0.000, 3.000], mean observation: -0.069 [-1.083, 1.000], loss: 0.878838, mean_absolute_error: 27.877185, mean_q: 37.054379, mean_eps: 0.100000
 1440811/2000000: episode: 6678, duration: 1.314s, episode steps: 83, steps per second: 63, episode reward: -93.622, mean reward: -1.128 [-100.000, 23.828], mean action: 0.735 [0.000, 3.000], mean observation: -0.020 [-1.476, 1.000], loss: 0.871619, mean_absolute_error: 26.836847, mean_q: 35.935949, mean_eps: 0.100000
 1440976/2000000: episode: 6679, duration: 2.592s, episode steps: 165, steps per second: 64, episode reward: -20.447, mean reward: -0.124 [-100.000, 16.965], mean action: 1.430 [0.000, 3.000], mean observation: -0.029 [-0.893, 2.065], loss: 0.631315, mean_absolute_error: 26.913226, mean_q: 35.808858, mean_eps: 0.100000
 1441067/2000000: episode: 6680, duration: 1.472s, episode steps: 91, steps per second: 62, episode reward: -87.779, mean reward: -0.965 [-100.000, 18.121], mean action: 1.418 [0.000, 3.000], mean observation: 0.055 [-1.391, 3.635], loss: 0.956716, mean_absolute_error: 27.675600, mean_q: 37.050755, mean_eps: 0.100000
 1441298/2000000: episode: 6681, duration: 3.634s, episode steps: 231, steps per second: 64, episode reward: 182.966, mean reward: 0.792 [-14.475, 100.000], mean action: 1.121 [0.000, 3.000], mean observation: 0.083 [-0.881, 1.000], loss: 0.802620, mean_absolute_error: 27.766684, mean_q: 36.920047, mean_eps: 0.100000
 1441417/2000000: episode: 6682, duration: 1.857s, episode steps: 119, steps per second: 64, episode reward: -49.016, mean reward: -0.412 [-100.000, 13.464], mean action: 1.824 [0.000, 3.000], mean observation: 0.099 [-1.224, 2.640], loss: 0.690571, mean_absolute_error: 25.595214, mean_q: 34.281641, mean_eps: 0.100000
 1441473/2000000: episode: 6683, duration: 0.880s, episode steps: 56, steps per second: 64, episode reward: -129.422, mean reward: -2.311 [-100.000, 10.654], mean action: 1.304 [0.000, 3.000], mean observation: -0.052 [-5.752, 1.000], loss: 0.570954, mean_absolute_error: 25.986369, mean_q: 34.899585, mean_eps: 0.100000
 1441597/2000000: episode: 6684, duration: 1.915s, episode steps: 124, steps per second: 65, episode reward: -68.554, mean reward: -0.553 [-100.000, 14.682], mean action: 1.347 [0.000, 3.000], mean observation: 0.074 [-2.247, 1.030], loss: 0.838935, mean_absolute_error: 29.190153, mean_q: 38.654609, mean_eps: 0.100000
 1441722/2000000: episode: 6685, duration: 1.911s, episode steps: 125, steps per second: 65, episode reward: -142.921, mean reward: -1.143 [-100.000, 17.349], mean action: 1.680 [0.000, 3.000], mean observation: -0.016 [-1.645, 1.000], loss: 1.082284, mean_absolute_error: 27.062127, mean_q: 35.768567, mean_eps: 0.100000
 1441776/2000000: episode: 6686, duration: 0.872s, episode steps: 54, steps per second: 62, episode reward: -140.556, mean reward: -2.603 [-100.000, 11.097], mean action: 1.241 [0.000, 3.000], mean observation: -0.138 [-1.846, 2.657], loss: 1.130170, mean_absolute_error: 28.674869, mean_q: 38.247628, mean_eps: 0.100000
 1441919/2000000: episode: 6687, duration: 2.232s, episode steps: 143, steps per second: 64, episode reward: -28.006, mean reward: -0.196 [-100.000, 16.368], mean action: 1.979 [0.000, 3.000], mean observation: -0.008 [-0.933, 1.000], loss: 0.630194, mean_absolute_error: 28.072740, mean_q: 37.413239, mean_eps: 0.100000
 1442144/2000000: episode: 6688, duration: 3.583s, episode steps: 225, steps per second: 63, episode reward: -13.227, mean reward: -0.059 [-100.000, 23.991], mean action: 1.698 [0.000, 3.000], mean observation: 0.012 [-1.133, 1.000], loss: 0.732953, mean_absolute_error: 26.869366, mean_q: 36.031842, mean_eps: 0.100000
 1442234/2000000: episode: 6689, duration: 1.433s, episode steps: 90, steps per second: 63, episode reward: -41.898, mean reward: -0.466 [-100.000, 12.990], mean action: 1.911 [0.000, 3.000], mean observation: 0.098 [-1.061, 1.000], loss: 0.582017, mean_absolute_error: 27.474246, mean_q: 36.731669, mean_eps: 0.100000
 1442297/2000000: episode: 6690, duration: 0.998s, episode steps: 63, steps per second: 63, episode reward: -86.565, mean reward: -1.374 [-100.000, 22.006], mean action: 1.222 [0.000, 3.000], mean observation: 0.002 [-1.348, 4.014], loss: 0.863837, mean_absolute_error: 27.824707, mean_q: 36.645855, mean_eps: 0.100000
 1442540/2000000: episode: 6691, duration: 3.824s, episode steps: 243, steps per second: 64, episode reward: -4.174, mean reward: -0.017 [-100.000, 14.281], mean action: 1.683 [0.000, 3.000], mean observation: 0.015 [-0.615, 1.000], loss: 0.665847, mean_absolute_error: 27.188315, mean_q: 36.391477, mean_eps: 0.100000
 1442622/2000000: episode: 6692, duration: 1.295s, episode steps: 82, steps per second: 63, episode reward: -47.911, mean reward: -0.584 [-100.000, 17.916], mean action: 1.451 [0.000, 3.000], mean observation: -0.058 [-1.063, 1.000], loss: 1.036888, mean_absolute_error: 25.071282, mean_q: 33.180983, mean_eps: 0.100000
 1442780/2000000: episode: 6693, duration: 2.455s, episode steps: 158, steps per second: 64, episode reward: -51.652, mean reward: -0.327 [-100.000, 18.841], mean action: 1.443 [0.000, 3.000], mean observation: 0.013 [-1.222, 1.016], loss: 0.662706, mean_absolute_error: 26.645799, mean_q: 35.626059, mean_eps: 0.100000
 1442954/2000000: episode: 6694, duration: 2.741s, episode steps: 174, steps per second: 63, episode reward: -23.268, mean reward: -0.134 [-100.000, 18.998], mean action: 1.598 [0.000, 3.000], mean observation: -0.014 [-0.790, 1.000], loss: 0.712011, mean_absolute_error: 27.566808, mean_q: 36.931742, mean_eps: 0.100000
 1443020/2000000: episode: 6695, duration: 1.056s, episode steps: 66, steps per second: 62, episode reward: -116.193, mean reward: -1.761 [-100.000, 22.354], mean action: 0.697 [0.000, 3.000], mean observation: 0.003 [-1.518, 1.000], loss: 0.616644, mean_absolute_error: 27.484049, mean_q: 36.414484, mean_eps: 0.100000
 1443141/2000000: episode: 6696, duration: 1.913s, episode steps: 121, steps per second: 63, episode reward: -107.706, mean reward: -0.890 [-100.000, 7.329], mean action: 1.050 [0.000, 3.000], mean observation: -0.009 [-1.321, 4.441], loss: 0.767696, mean_absolute_error: 28.684115, mean_q: 38.229650, mean_eps: 0.100000
 1443247/2000000: episode: 6697, duration: 1.611s, episode steps: 106, steps per second: 66, episode reward: -92.519, mean reward: -0.873 [-100.000, 20.261], mean action: 1.632 [0.000, 3.000], mean observation: 0.034 [-1.434, 1.000], loss: 0.766846, mean_absolute_error: 27.564208, mean_q: 36.932166, mean_eps: 0.100000
 1443313/2000000: episode: 6698, duration: 1.067s, episode steps: 66, steps per second: 62, episode reward: -59.988, mean reward: -0.909 [-100.000, 13.992], mean action: 1.333 [0.000, 3.000], mean observation: -0.085 [-1.315, 1.000], loss: 0.711002, mean_absolute_error: 27.027902, mean_q: 36.003224, mean_eps: 0.100000
 1443405/2000000: episode: 6699, duration: 1.425s, episode steps: 92, steps per second: 65, episode reward: -138.718, mean reward: -1.508 [-100.000, 8.421], mean action: 1.935 [0.000, 3.000], mean observation: 0.010 [-4.914, 1.005], loss: 0.734742, mean_absolute_error: 28.313356, mean_q: 37.889462, mean_eps: 0.100000
 1443468/2000000: episode: 6700, duration: 0.991s, episode steps: 63, steps per second: 64, episode reward: -138.520, mean reward: -2.199 [-100.000, 10.584], mean action: 1.794 [0.000, 3.000], mean observation: -0.113 [-4.235, 1.000], loss: 0.839780, mean_absolute_error: 29.134682, mean_q: 38.949714, mean_eps: 0.100000
 1443646/2000000: episode: 6701, duration: 2.770s, episode steps: 178, steps per second: 64, episode reward: -35.105, mean reward: -0.197 [-100.000, 21.990], mean action: 1.500 [0.000, 3.000], mean observation: 0.066 [-0.861, 1.000], loss: 0.885446, mean_absolute_error: 26.634619, mean_q: 35.637260, mean_eps: 0.100000
 1443743/2000000: episode: 6702, duration: 1.495s, episode steps: 97, steps per second: 65, episode reward: -87.295, mean reward: -0.900 [-100.000, 11.525], mean action: 0.928 [0.000, 3.000], mean observation: -0.058 [-3.206, 1.000], loss: 0.666953, mean_absolute_error: 26.057004, mean_q: 34.724820, mean_eps: 0.100000
 1444091/2000000: episode: 6703, duration: 5.506s, episode steps: 348, steps per second: 63, episode reward: 189.924, mean reward: 0.546 [-17.445, 100.000], mean action: 0.925 [0.000, 3.000], mean observation: 0.166 [-0.805, 1.000], loss: 0.917945, mean_absolute_error: 27.710362, mean_q: 36.957716, mean_eps: 0.100000
 1444153/2000000: episode: 6704, duration: 1.327s, episode steps: 62, steps per second: 47, episode reward: -78.961, mean reward: -1.274 [-100.000, 15.311], mean action: 1.258 [0.000, 3.000], mean observation: 0.032 [-1.327, 3.802], loss: 0.674379, mean_absolute_error: 30.214483, mean_q: 40.405305, mean_eps: 0.100000
 1444305/2000000: episode: 6705, duration: 2.395s, episode steps: 152, steps per second: 63, episode reward: -29.050, mean reward: -0.191 [-100.000, 16.019], mean action: 1.895 [0.000, 3.000], mean observation: -0.011 [-0.935, 2.133], loss: 0.709967, mean_absolute_error: 28.109318, mean_q: 37.657058, mean_eps: 0.100000
 1444412/2000000: episode: 6706, duration: 1.693s, episode steps: 107, steps per second: 63, episode reward: -36.773, mean reward: -0.344 [-100.000, 15.872], mean action: 1.869 [0.000, 3.000], mean observation: 0.019 [-1.255, 1.000], loss: 0.916757, mean_absolute_error: 28.800234, mean_q: 38.264435, mean_eps: 0.100000
 1444576/2000000: episode: 6707, duration: 2.608s, episode steps: 164, steps per second: 63, episode reward: 9.480, mean reward: 0.058 [-100.000, 25.532], mean action: 1.701 [0.000, 3.000], mean observation: -0.005 [-0.728, 1.000], loss: 1.128495, mean_absolute_error: 28.361633, mean_q: 37.376165, mean_eps: 0.100000
 1444643/2000000: episode: 6708, duration: 1.082s, episode steps: 67, steps per second: 62, episode reward: -106.113, mean reward: -1.584 [-100.000, 16.543], mean action: 1.642 [0.000, 3.000], mean observation: -0.005 [-1.377, 1.000], loss: 0.504157, mean_absolute_error: 26.029876, mean_q: 34.928607, mean_eps: 0.100000
 1444822/2000000: episode: 6709, duration: 2.785s, episode steps: 179, steps per second: 64, episode reward: -33.215, mean reward: -0.186 [-100.000, 19.742], mean action: 1.514 [0.000, 3.000], mean observation: 0.011 [-0.770, 1.000], loss: 0.851702, mean_absolute_error: 27.930467, mean_q: 37.321289, mean_eps: 0.100000
 1444970/2000000: episode: 6710, duration: 2.307s, episode steps: 148, steps per second: 64, episode reward: -37.585, mean reward: -0.254 [-100.000, 10.857], mean action: 1.811 [0.000, 3.000], mean observation: 0.111 [-0.727, 1.312], loss: 1.106225, mean_absolute_error: 28.266174, mean_q: 36.761162, mean_eps: 0.100000
 1445092/2000000: episode: 6711, duration: 1.902s, episode steps: 122, steps per second: 64, episode reward: -51.552, mean reward: -0.423 [-100.000, 18.204], mean action: 1.910 [0.000, 3.000], mean observation: -0.067 [-0.992, 2.373], loss: 0.979040, mean_absolute_error: 25.550417, mean_q: 34.173665, mean_eps: 0.100000
 1445166/2000000: episode: 6712, duration: 1.159s, episode steps: 74, steps per second: 64, episode reward: -156.935, mean reward: -2.121 [-100.000, 19.460], mean action: 0.757 [0.000, 3.000], mean observation: -0.060 [-1.761, 1.000], loss: 0.822431, mean_absolute_error: 29.255576, mean_q: 38.385168, mean_eps: 0.100000
 1445291/2000000: episode: 6713, duration: 1.931s, episode steps: 125, steps per second: 65, episode reward: -90.115, mean reward: -0.721 [-100.000, 10.866], mean action: 1.352 [0.000, 3.000], mean observation: -0.030 [-1.134, 2.722], loss: 0.960002, mean_absolute_error: 27.325808, mean_q: 36.385837, mean_eps: 0.100000
 1445436/2000000: episode: 6714, duration: 2.355s, episode steps: 145, steps per second: 62, episode reward: -94.039, mean reward: -0.649 [-100.000, 9.880], mean action: 1.834 [0.000, 3.000], mean observation: 0.002 [-1.256, 3.940], loss: 0.846813, mean_absolute_error: 26.701253, mean_q: 35.764576, mean_eps: 0.100000
 1445865/2000000: episode: 6715, duration: 6.852s, episode steps: 429, steps per second: 63, episode reward: -171.923, mean reward: -0.401 [-100.000, 25.416], mean action: 1.606 [0.000, 3.000], mean observation: 0.131 [-1.762, 3.931], loss: 0.827915, mean_absolute_error: 27.784234, mean_q: 37.037326, mean_eps: 0.100000
 1446030/2000000: episode: 6716, duration: 2.567s, episode steps: 165, steps per second: 64, episode reward: 2.146, mean reward: 0.013 [-100.000, 19.300], mean action: 1.339 [0.000, 3.000], mean observation: -0.011 [-0.652, 1.000], loss: 0.843620, mean_absolute_error: 27.221549, mean_q: 36.431352, mean_eps: 0.100000
 1446532/2000000: episode: 6717, duration: 8.261s, episode steps: 502, steps per second: 61, episode reward: 200.996, mean reward: 0.400 [-21.530, 100.000], mean action: 0.892 [0.000, 3.000], mean observation: 0.220 [-0.884, 1.000], loss: 0.807319, mean_absolute_error: 27.108580, mean_q: 36.247840, mean_eps: 0.100000
 1446611/2000000: episode: 6718, duration: 1.266s, episode steps: 79, steps per second: 62, episode reward: -64.617, mean reward: -0.818 [-100.000, 17.820], mean action: 1.190 [0.000, 3.000], mean observation: -0.029 [-1.150, 1.000], loss: 0.599553, mean_absolute_error: 28.073306, mean_q: 37.720597, mean_eps: 0.100000
 1446793/2000000: episode: 6719, duration: 3.056s, episode steps: 182, steps per second: 60, episode reward: -43.204, mean reward: -0.237 [-100.000, 10.272], mean action: 1.527 [0.000, 3.000], mean observation: 0.091 [-1.241, 1.000], loss: 1.020824, mean_absolute_error: 29.072486, mean_q: 38.648693, mean_eps: 0.100000
 1446985/2000000: episode: 6720, duration: 2.990s, episode steps: 192, steps per second: 64, episode reward: -43.525, mean reward: -0.227 [-100.000, 13.968], mean action: 1.438 [0.000, 3.000], mean observation: 0.107 [-0.700, 1.358], loss: 1.023150, mean_absolute_error: 27.631121, mean_q: 36.673505, mean_eps: 0.100000
 1447079/2000000: episode: 6721, duration: 1.427s, episode steps: 94, steps per second: 66, episode reward: -93.652, mean reward: -0.996 [-100.000, 15.118], mean action: 1.840 [0.000, 3.000], mean observation: 0.085 [-3.447, 1.000], loss: 0.731702, mean_absolute_error: 26.153422, mean_q: 35.059372, mean_eps: 0.100000
 1447166/2000000: episode: 6722, duration: 1.381s, episode steps: 87, steps per second: 63, episode reward: -93.499, mean reward: -1.075 [-100.000, 18.832], mean action: 1.425 [0.000, 3.000], mean observation: -0.025 [-1.381, 1.000], loss: 1.253142, mean_absolute_error: 25.998331, mean_q: 34.304081, mean_eps: 0.100000
 1447248/2000000: episode: 6723, duration: 1.303s, episode steps: 82, steps per second: 63, episode reward: -114.573, mean reward: -1.397 [-100.000, 16.853], mean action: 2.012 [1.000, 3.000], mean observation: 0.014 [-4.156, 1.000], loss: 0.685008, mean_absolute_error: 27.527429, mean_q: 36.990140, mean_eps: 0.100000
 1447450/2000000: episode: 6724, duration: 3.189s, episode steps: 202, steps per second: 63, episode reward: -66.639, mean reward: -0.330 [-100.000, 12.583], mean action: 1.757 [0.000, 3.000], mean observation: 0.102 [-2.711, 1.000], loss: 1.075838, mean_absolute_error: 27.466324, mean_q: 36.513959, mean_eps: 0.100000
 1447575/2000000: episode: 6725, duration: 1.943s, episode steps: 125, steps per second: 64, episode reward: -103.496, mean reward: -0.828 [-100.000, 21.587], mean action: 2.008 [0.000, 3.000], mean observation: -0.006 [-1.389, 1.000], loss: 0.483908, mean_absolute_error: 27.100013, mean_q: 36.001275, mean_eps: 0.100000
 1447635/2000000: episode: 6726, duration: 0.927s, episode steps: 60, steps per second: 65, episode reward: -98.382, mean reward: -1.640 [-100.000, 21.302], mean action: 1.900 [0.000, 3.000], mean observation: -0.073 [-1.514, 1.000], loss: 0.518510, mean_absolute_error: 27.985534, mean_q: 37.417355, mean_eps: 0.100000
 1447710/2000000: episode: 6727, duration: 1.161s, episode steps: 75, steps per second: 65, episode reward: -14.445, mean reward: -0.193 [-100.000, 21.403], mean action: 1.827 [0.000, 3.000], mean observation: -0.121 [-1.143, 2.146], loss: 0.541308, mean_absolute_error: 27.331958, mean_q: 36.419708, mean_eps: 0.100000
 1447813/2000000: episode: 6728, duration: 1.653s, episode steps: 103, steps per second: 62, episode reward: -48.062, mean reward: -0.467 [-100.000, 11.128], mean action: 1.903 [0.000, 3.000], mean observation: 0.012 [-1.954, 1.000], loss: 1.068562, mean_absolute_error: 27.821357, mean_q: 37.184588, mean_eps: 0.100000
 1447885/2000000: episode: 6729, duration: 1.110s, episode steps: 72, steps per second: 65, episode reward: -68.326, mean reward: -0.949 [-100.000, 7.037], mean action: 1.778 [0.000, 3.000], mean observation: 0.074 [-1.044, 1.000], loss: 0.513763, mean_absolute_error: 26.096800, mean_q: 34.990847, mean_eps: 0.100000
 1447984/2000000: episode: 6730, duration: 1.533s, episode steps: 99, steps per second: 65, episode reward: -63.412, mean reward: -0.641 [-100.000, 22.425], mean action: 1.414 [0.000, 3.000], mean observation: -0.000 [-1.272, 1.000], loss: 0.806892, mean_absolute_error: 25.822322, mean_q: 34.550140, mean_eps: 0.100000
 1448057/2000000: episode: 6731, duration: 1.174s, episode steps: 73, steps per second: 62, episode reward: -55.022, mean reward: -0.754 [-100.000, 13.044], mean action: 1.534 [0.000, 3.000], mean observation: 0.112 [-1.020, 2.492], loss: 0.849363, mean_absolute_error: 27.806270, mean_q: 37.236817, mean_eps: 0.100000
 1448187/2000000: episode: 6732, duration: 2.026s, episode steps: 130, steps per second: 64, episode reward: -102.349, mean reward: -0.787 [-100.000, 15.398], mean action: 1.862 [0.000, 3.000], mean observation: -0.009 [-1.434, 1.000], loss: 0.790330, mean_absolute_error: 26.561219, mean_q: 35.592106, mean_eps: 0.100000
 1448399/2000000: episode: 6733, duration: 3.257s, episode steps: 212, steps per second: 65, episode reward: -185.978, mean reward: -0.877 [-100.000, 15.749], mean action: 1.939 [0.000, 3.000], mean observation: 0.112 [-0.838, 1.008], loss: 0.588768, mean_absolute_error: 27.762550, mean_q: 37.333088, mean_eps: 0.100000
 1448495/2000000: episode: 6734, duration: 1.491s, episode steps: 96, steps per second: 64, episode reward: -30.305, mean reward: -0.316 [-100.000, 17.375], mean action: 1.448 [0.000, 3.000], mean observation: 0.133 [-1.139, 1.000], loss: 0.980890, mean_absolute_error: 25.609644, mean_q: 33.808612, mean_eps: 0.100000
 1448556/2000000: episode: 6735, duration: 0.985s, episode steps: 61, steps per second: 62, episode reward: -85.384, mean reward: -1.400 [-100.000, 6.707], mean action: 2.016 [0.000, 3.000], mean observation: -0.175 [-3.973, 1.000], loss: 0.567395, mean_absolute_error: 27.911882, mean_q: 37.321497, mean_eps: 0.100000
 1448746/2000000: episode: 6736, duration: 2.986s, episode steps: 190, steps per second: 64, episode reward: -2.228, mean reward: -0.012 [-100.000, 25.520], mean action: 1.674 [0.000, 3.000], mean observation: -0.004 [-0.761, 1.000], loss: 0.572296, mean_absolute_error: 27.246680, mean_q: 36.564285, mean_eps: 0.100000
 1448874/2000000: episode: 6737, duration: 1.979s, episode steps: 128, steps per second: 65, episode reward: -15.960, mean reward: -0.125 [-100.000, 19.354], mean action: 1.703 [0.000, 3.000], mean observation: -0.018 [-0.863, 1.489], loss: 0.572284, mean_absolute_error: 28.490537, mean_q: 38.234561, mean_eps: 0.100000
 1449017/2000000: episode: 6738, duration: 2.219s, episode steps: 143, steps per second: 64, episode reward: -52.330, mean reward: -0.366 [-100.000, 17.462], mean action: 1.531 [0.000, 3.000], mean observation: -0.037 [-0.756, 1.913], loss: 0.526307, mean_absolute_error: 28.089790, mean_q: 37.591028, mean_eps: 0.100000
 1449093/2000000: episode: 6739, duration: 1.186s, episode steps: 76, steps per second: 64, episode reward: -111.134, mean reward: -1.462 [-100.000, 18.959], mean action: 0.684 [0.000, 3.000], mean observation: -0.056 [-1.482, 1.000], loss: 0.773112, mean_absolute_error: 28.407126, mean_q: 37.973266, mean_eps: 0.100000
 1449231/2000000: episode: 6740, duration: 2.104s, episode steps: 138, steps per second: 66, episode reward: -88.072, mean reward: -0.638 [-100.000, 15.442], mean action: 0.884 [0.000, 3.000], mean observation: 0.007 [-1.611, 1.015], loss: 0.639586, mean_absolute_error: 26.649497, mean_q: 35.744542, mean_eps: 0.100000
 1449293/2000000: episode: 6741, duration: 0.983s, episode steps: 62, steps per second: 63, episode reward: -103.997, mean reward: -1.677 [-100.000, 17.041], mean action: 0.790 [0.000, 3.000], mean observation: -0.050 [-1.588, 1.000], loss: 0.610172, mean_absolute_error: 28.575933, mean_q: 37.881011, mean_eps: 0.100000
 1449374/2000000: episode: 6742, duration: 1.245s, episode steps: 81, steps per second: 65, episode reward: -37.011, mean reward: -0.457 [-100.000, 13.780], mean action: 1.667 [0.000, 3.000], mean observation: 0.114 [-0.976, 1.431], loss: 0.404900, mean_absolute_error: 26.220598, mean_q: 35.222336, mean_eps: 0.100000
 1449498/2000000: episode: 6743, duration: 1.929s, episode steps: 124, steps per second: 64, episode reward: -15.655, mean reward: -0.126 [-100.000, 22.577], mean action: 1.581 [0.000, 3.000], mean observation: 0.000 [-0.803, 1.000], loss: 0.661758, mean_absolute_error: 28.229006, mean_q: 37.814004, mean_eps: 0.100000
 1449570/2000000: episode: 6744, duration: 1.108s, episode steps: 72, steps per second: 65, episode reward: -86.496, mean reward: -1.201 [-100.000, 6.713], mean action: 1.292 [0.000, 3.000], mean observation: -0.070 [-1.282, 3.282], loss: 1.494432, mean_absolute_error: 28.172429, mean_q: 37.290052, mean_eps: 0.100000
 1449708/2000000: episode: 6745, duration: 2.186s, episode steps: 138, steps per second: 63, episode reward: -43.572, mean reward: -0.316 [-100.000, 17.874], mean action: 1.754 [0.000, 3.000], mean observation: -0.011 [-1.135, 1.000], loss: 0.482732, mean_absolute_error: 27.001946, mean_q: 36.219427, mean_eps: 0.100000
 1449855/2000000: episode: 6746, duration: 2.306s, episode steps: 147, steps per second: 64, episode reward: -20.186, mean reward: -0.137 [-100.000, 18.735], mean action: 1.667 [0.000, 3.000], mean observation: -0.011 [-0.861, 1.000], loss: 0.975157, mean_absolute_error: 27.930641, mean_q: 37.070537, mean_eps: 0.100000
 1450208/2000000: episode: 6747, duration: 5.559s, episode steps: 353, steps per second: 63, episode reward: 251.022, mean reward: 0.711 [-8.399, 100.000], mean action: 1.244 [0.000, 3.000], mean observation: 0.055 [-0.729, 1.008], loss: 0.708834, mean_absolute_error: 28.060446, mean_q: 37.413715, mean_eps: 0.100000
 1450285/2000000: episode: 6748, duration: 1.248s, episode steps: 77, steps per second: 62, episode reward: -22.334, mean reward: -0.290 [-100.000, 10.930], mean action: 1.532 [0.000, 3.000], mean observation: 0.044 [-1.343, 1.000], loss: 1.505052, mean_absolute_error: 26.802305, mean_q: 35.251196, mean_eps: 0.100000
 1450356/2000000: episode: 6749, duration: 1.120s, episode steps: 71, steps per second: 63, episode reward: -54.564, mean reward: -0.769 [-100.000, 11.087], mean action: 1.127 [0.000, 3.000], mean observation: 0.004 [-1.218, 1.000], loss: 0.997157, mean_absolute_error: 29.942866, mean_q: 39.924271, mean_eps: 0.100000
 1450523/2000000: episode: 6750, duration: 2.622s, episode steps: 167, steps per second: 64, episode reward: 0.676, mean reward: 0.004 [-100.000, 14.479], mean action: 1.635 [0.000, 3.000], mean observation: 0.004 [-0.693, 1.000], loss: 0.903458, mean_absolute_error: 28.670400, mean_q: 38.223676, mean_eps: 0.100000
 1450587/2000000: episode: 6751, duration: 1.000s, episode steps: 64, steps per second: 64, episode reward: -83.404, mean reward: -1.303 [-100.000, 15.870], mean action: 1.344 [0.000, 3.000], mean observation: -0.185 [-1.468, 3.969], loss: 0.618169, mean_absolute_error: 26.406455, mean_q: 34.717436, mean_eps: 0.100000
 1450719/2000000: episode: 6752, duration: 2.034s, episode steps: 132, steps per second: 65, episode reward: -30.362, mean reward: -0.230 [-100.000, 12.439], mean action: 1.742 [0.000, 3.000], mean observation: 0.110 [-0.786, 1.353], loss: 0.893303, mean_absolute_error: 27.748503, mean_q: 35.882518, mean_eps: 0.100000
 1450911/2000000: episode: 6753, duration: 2.988s, episode steps: 192, steps per second: 64, episode reward: -12.966, mean reward: -0.068 [-100.000, 16.300], mean action: 1.844 [0.000, 3.000], mean observation: 0.008 [-0.536, 1.000], loss: 0.964989, mean_absolute_error: 28.330230, mean_q: 37.857051, mean_eps: 0.100000
 1451250/2000000: episode: 6754, duration: 5.441s, episode steps: 339, steps per second: 62, episode reward: -456.566, mean reward: -1.347 [-100.000, 5.356], mean action: 1.929 [0.000, 3.000], mean observation: 0.106 [-0.709, 1.857], loss: 0.891898, mean_absolute_error: 27.688444, mean_q: 36.988471, mean_eps: 0.100000
 1451331/2000000: episode: 6755, duration: 1.252s, episode steps: 81, steps per second: 65, episode reward: -93.955, mean reward: -1.160 [-100.000, 10.130], mean action: 1.654 [0.000, 3.000], mean observation: 0.082 [-1.269, 3.059], loss: 1.045947, mean_absolute_error: 27.217359, mean_q: 36.378714, mean_eps: 0.100000
 1451973/2000000: episode: 6756, duration: 10.367s, episode steps: 642, steps per second: 62, episode reward: 136.088, mean reward: 0.212 [-20.757, 100.000], mean action: 1.559 [0.000, 3.000], mean observation: 0.253 [-1.420, 1.000], loss: 0.925638, mean_absolute_error: 27.758717, mean_q: 37.002473, mean_eps: 0.100000
 1452364/2000000: episode: 6757, duration: 6.303s, episode steps: 391, steps per second: 62, episode reward: 208.256, mean reward: 0.533 [-9.328, 100.000], mean action: 1.284 [0.000, 3.000], mean observation: 0.183 [-0.726, 1.000], loss: 0.867399, mean_absolute_error: 28.081061, mean_q: 37.509000, mean_eps: 0.100000
 1452438/2000000: episode: 6758, duration: 1.201s, episode steps: 74, steps per second: 62, episode reward: -99.487, mean reward: -1.344 [-100.000, 16.667], mean action: 1.135 [0.000, 3.000], mean observation: 0.010 [-1.428, 1.000], loss: 0.558517, mean_absolute_error: 26.456244, mean_q: 35.240644, mean_eps: 0.100000
 1452542/2000000: episode: 6759, duration: 1.626s, episode steps: 104, steps per second: 64, episode reward: -9.868, mean reward: -0.095 [-100.000, 22.717], mean action: 1.481 [0.000, 3.000], mean observation: -0.046 [-0.870, 1.000], loss: 0.753126, mean_absolute_error: 28.226157, mean_q: 37.458607, mean_eps: 0.100000
 1452651/2000000: episode: 6760, duration: 1.669s, episode steps: 109, steps per second: 65, episode reward: -45.467, mean reward: -0.417 [-100.000, 16.706], mean action: 1.000 [0.000, 3.000], mean observation: -0.026 [-1.247, 1.000], loss: 0.737206, mean_absolute_error: 27.691535, mean_q: 37.092838, mean_eps: 0.100000
 1452950/2000000: episode: 6761, duration: 4.733s, episode steps: 299, steps per second: 63, episode reward: 168.113, mean reward: 0.562 [-19.284, 100.000], mean action: 0.953 [0.000, 3.000], mean observation: 0.169 [-1.797, 1.000], loss: 0.941396, mean_absolute_error: 27.755903, mean_q: 37.116901, mean_eps: 0.100000
 1453020/2000000: episode: 6762, duration: 1.134s, episode steps: 70, steps per second: 62, episode reward: -72.162, mean reward: -1.031 [-100.000, 13.876], mean action: 1.557 [0.000, 3.000], mean observation: -0.166 [-1.104, 3.103], loss: 0.524491, mean_absolute_error: 25.411596, mean_q: 34.206944, mean_eps: 0.100000
 1453136/2000000: episode: 6763, duration: 1.847s, episode steps: 116, steps per second: 63, episode reward: -3.302, mean reward: -0.028 [-100.000, 23.434], mean action: 1.750 [0.000, 3.000], mean observation: 0.088 [-1.014, 1.000], loss: 0.828433, mean_absolute_error: 29.021075, mean_q: 38.787114, mean_eps: 0.100000
 1453365/2000000: episode: 6764, duration: 3.614s, episode steps: 229, steps per second: 63, episode reward: 0.425, mean reward: 0.002 [-100.000, 18.824], mean action: 1.642 [0.000, 3.000], mean observation: -0.007 [-0.988, 1.000], loss: 0.745952, mean_absolute_error: 27.677860, mean_q: 36.616631, mean_eps: 0.100000
 1453418/2000000: episode: 6765, duration: 0.820s, episode steps: 53, steps per second: 65, episode reward: -112.702, mean reward: -2.126 [-100.000, 19.999], mean action: 1.226 [0.000, 3.000], mean observation: -0.183 [-1.632, 4.822], loss: 1.061053, mean_absolute_error: 28.572227, mean_q: 38.167745, mean_eps: 0.100000
 1453512/2000000: episode: 6766, duration: 1.505s, episode steps: 94, steps per second: 62, episode reward: -63.920, mean reward: -0.680 [-100.000, 16.700], mean action: 1.489 [0.000, 3.000], mean observation: 0.079 [-1.294, 1.000], loss: 1.161287, mean_absolute_error: 26.798181, mean_q: 35.616900, mean_eps: 0.100000
 1453581/2000000: episode: 6767, duration: 1.130s, episode steps: 69, steps per second: 61, episode reward: -74.826, mean reward: -1.084 [-100.000, 17.489], mean action: 1.217 [0.000, 3.000], mean observation: 0.042 [-1.260, 1.000], loss: 0.864064, mean_absolute_error: 28.370353, mean_q: 37.998612, mean_eps: 0.100000
 1453695/2000000: episode: 6768, duration: 1.751s, episode steps: 114, steps per second: 65, episode reward: -65.552, mean reward: -0.575 [-100.000, 18.716], mean action: 1.825 [0.000, 3.000], mean observation: -0.066 [-1.022, 1.000], loss: 0.946698, mean_absolute_error: 28.999247, mean_q: 38.349400, mean_eps: 0.100000
 1453833/2000000: episode: 6769, duration: 2.177s, episode steps: 138, steps per second: 63, episode reward: -24.866, mean reward: -0.180 [-100.000, 9.435], mean action: 1.623 [0.000, 3.000], mean observation: 0.065 [-2.653, 1.000], loss: 0.690391, mean_absolute_error: 27.485390, mean_q: 36.674228, mean_eps: 0.100000
 1453902/2000000: episode: 6770, duration: 1.232s, episode steps: 69, steps per second: 56, episode reward: -89.472, mean reward: -1.297 [-100.000, 21.223], mean action: 1.232 [0.000, 3.000], mean observation: -0.118 [-1.414, 1.000], loss: 0.888172, mean_absolute_error: 29.652296, mean_q: 39.321284, mean_eps: 0.100000
 1453958/2000000: episode: 6771, duration: 0.861s, episode steps: 56, steps per second: 65, episode reward: -98.837, mean reward: -1.765 [-100.000, 13.480], mean action: 0.804 [0.000, 3.000], mean observation: -0.071 [-1.649, 1.000], loss: 0.548181, mean_absolute_error: 28.726746, mean_q: 38.568563, mean_eps: 0.100000
 1454039/2000000: episode: 6772, duration: 1.267s, episode steps: 81, steps per second: 64, episode reward: -145.243, mean reward: -1.793 [-100.000, 9.566], mean action: 1.049 [0.000, 3.000], mean observation: -0.064 [-1.546, 4.669], loss: 0.882147, mean_absolute_error: 28.396034, mean_q: 37.960527, mean_eps: 0.100000
 1454093/2000000: episode: 6773, duration: 0.874s, episode steps: 54, steps per second: 62, episode reward: -145.085, mean reward: -2.687 [-100.000, 12.559], mean action: 0.722 [0.000, 3.000], mean observation: -0.165 [-1.751, 1.000], loss: 0.580023, mean_absolute_error: 25.080243, mean_q: 33.655685, mean_eps: 0.100000
 1454249/2000000: episode: 6774, duration: 2.433s, episode steps: 156, steps per second: 64, episode reward: -14.878, mean reward: -0.095 [-100.000, 18.678], mean action: 1.763 [0.000, 3.000], mean observation: 0.075 [-0.807, 1.000], loss: 0.912096, mean_absolute_error: 28.554916, mean_q: 38.127159, mean_eps: 0.100000
 1454316/2000000: episode: 6775, duration: 1.081s, episode steps: 67, steps per second: 62, episode reward: -99.472, mean reward: -1.485 [-100.000, 17.768], mean action: 1.881 [0.000, 3.000], mean observation: -0.012 [-1.427, 1.000], loss: 0.687502, mean_absolute_error: 28.792771, mean_q: 38.636182, mean_eps: 0.100000
 1454375/2000000: episode: 6776, duration: 0.950s, episode steps: 59, steps per second: 62, episode reward: -113.523, mean reward: -1.924 [-100.000, 21.879], mean action: 1.983 [1.000, 3.000], mean observation: -0.119 [-1.664, 1.000], loss: 0.611474, mean_absolute_error: 27.432702, mean_q: 36.754621, mean_eps: 0.100000
 1454437/2000000: episode: 6777, duration: 1.002s, episode steps: 62, steps per second: 62, episode reward: -42.617, mean reward: -0.687 [-100.000, 20.976], mean action: 1.694 [0.000, 3.000], mean observation: 0.017 [-1.288, 2.536], loss: 0.787782, mean_absolute_error: 29.205509, mean_q: 39.113139, mean_eps: 0.100000
 1454590/2000000: episode: 6778, duration: 2.363s, episode steps: 153, steps per second: 65, episode reward: 235.636, mean reward: 1.540 [-2.728, 100.000], mean action: 1.183 [0.000, 3.000], mean observation: 0.172 [-1.086, 1.000], loss: 0.888570, mean_absolute_error: 29.757297, mean_q: 39.523786, mean_eps: 0.100000
 1454694/2000000: episode: 6779, duration: 1.617s, episode steps: 104, steps per second: 64, episode reward: -63.906, mean reward: -0.614 [-100.000, 16.565], mean action: 1.692 [0.000, 3.000], mean observation: 0.064 [-1.330, 1.015], loss: 0.817596, mean_absolute_error: 28.413038, mean_q: 38.098106, mean_eps: 0.100000
 1454749/2000000: episode: 6780, duration: 0.874s, episode steps: 55, steps per second: 63, episode reward: -97.663, mean reward: -1.776 [-100.000, 22.914], mean action: 0.691 [0.000, 3.000], mean observation: -0.089 [-1.666, 1.000], loss: 1.228565, mean_absolute_error: 27.728010, mean_q: 36.897775, mean_eps: 0.100000
 1454971/2000000: episode: 6781, duration: 3.502s, episode steps: 222, steps per second: 63, episode reward: 4.260, mean reward: 0.019 [-100.000, 14.117], mean action: 1.653 [0.000, 3.000], mean observation: 0.007 [-0.738, 1.000], loss: 0.913507, mean_absolute_error: 28.602990, mean_q: 38.101431, mean_eps: 0.100000
 1455396/2000000: episode: 6782, duration: 6.902s, episode steps: 425, steps per second: 62, episode reward: 217.831, mean reward: 0.513 [-11.088, 100.000], mean action: 1.287 [0.000, 3.000], mean observation: 0.147 [-0.471, 1.000], loss: 0.956786, mean_absolute_error: 26.605569, mean_q: 35.135581, mean_eps: 0.100000
 1455746/2000000: episode: 6783, duration: 5.522s, episode steps: 350, steps per second: 63, episode reward: -125.045, mean reward: -0.357 [-100.000, 22.456], mean action: 1.654 [0.000, 3.000], mean observation: 0.079 [-1.880, 1.316], loss: 0.921015, mean_absolute_error: 28.523163, mean_q: 37.390534, mean_eps: 0.100000
 1455900/2000000: episode: 6784, duration: 2.439s, episode steps: 154, steps per second: 63, episode reward: -65.790, mean reward: -0.427 [-100.000, 9.219], mean action: 1.890 [0.000, 3.000], mean observation: 0.032 [-1.112, 1.000], loss: 0.630831, mean_absolute_error: 27.804391, mean_q: 37.306816, mean_eps: 0.100000
 1456075/2000000: episode: 6785, duration: 2.771s, episode steps: 175, steps per second: 63, episode reward: 0.416, mean reward: 0.002 [-100.000, 21.057], mean action: 1.800 [0.000, 3.000], mean observation: -0.011 [-0.606, 1.000], loss: 0.880329, mean_absolute_error: 27.979007, mean_q: 37.187467, mean_eps: 0.100000
 1456205/2000000: episode: 6786, duration: 2.097s, episode steps: 130, steps per second: 62, episode reward: -49.709, mean reward: -0.382 [-100.000, 18.879], mean action: 1.600 [0.000, 3.000], mean observation: 0.006 [-1.804, 1.000], loss: 0.647623, mean_absolute_error: 28.805619, mean_q: 38.621613, mean_eps: 0.100000
 1456357/2000000: episode: 6787, duration: 2.481s, episode steps: 152, steps per second: 61, episode reward: -27.118, mean reward: -0.178 [-100.000, 27.152], mean action: 1.914 [0.000, 3.000], mean observation: 0.019 [-0.709, 1.000], loss: 0.797757, mean_absolute_error: 27.853943, mean_q: 37.166074, mean_eps: 0.100000
 1456436/2000000: episode: 6788, duration: 1.258s, episode steps: 79, steps per second: 63, episode reward: -109.256, mean reward: -1.383 [-100.000, 11.287], mean action: 1.722 [0.000, 3.000], mean observation: 0.040 [-3.845, 1.000], loss: 0.690864, mean_absolute_error: 28.280840, mean_q: 37.924951, mean_eps: 0.100000
 1456510/2000000: episode: 6789, duration: 1.180s, episode steps: 74, steps per second: 63, episode reward: -127.353, mean reward: -1.721 [-100.000, 12.624], mean action: 1.216 [0.000, 3.000], mean observation: 0.080 [-1.332, 3.667], loss: 0.633877, mean_absolute_error: 26.321293, mean_q: 35.118494, mean_eps: 0.100000
 1456592/2000000: episode: 6790, duration: 1.308s, episode steps: 82, steps per second: 63, episode reward: -104.659, mean reward: -1.276 [-100.000, 16.679], mean action: 0.866 [0.000, 3.000], mean observation: -0.003 [-3.571, 1.000], loss: 0.870056, mean_absolute_error: 28.374683, mean_q: 37.978150, mean_eps: 0.100000
 1456753/2000000: episode: 6791, duration: 2.556s, episode steps: 161, steps per second: 63, episode reward: -10.729, mean reward: -0.067 [-100.000, 18.353], mean action: 1.714 [0.000, 3.000], mean observation: 0.075 [-0.814, 1.000], loss: 0.716020, mean_absolute_error: 28.673435, mean_q: 38.337886, mean_eps: 0.100000
 1456937/2000000: episode: 6792, duration: 2.889s, episode steps: 184, steps per second: 64, episode reward: -21.521, mean reward: -0.117 [-100.000, 10.833], mean action: 1.739 [0.000, 3.000], mean observation: 0.074 [-0.789, 1.116], loss: 0.661462, mean_absolute_error: 28.519784, mean_q: 38.165589, mean_eps: 0.100000
 1457188/2000000: episode: 6793, duration: 3.943s, episode steps: 251, steps per second: 64, episode reward: -182.422, mean reward: -0.727 [-100.000, 12.387], mean action: 1.351 [0.000, 3.000], mean observation: 0.146 [-1.406, 1.546], loss: 0.651390, mean_absolute_error: 29.635917, mean_q: 38.874848, mean_eps: 0.100000
 1457330/2000000: episode: 6794, duration: 2.247s, episode steps: 142, steps per second: 63, episode reward: -22.390, mean reward: -0.158 [-100.000, 9.577], mean action: 1.817 [0.000, 3.000], mean observation: 0.089 [-0.907, 1.000], loss: 0.872050, mean_absolute_error: 26.195233, mean_q: 34.827261, mean_eps: 0.100000
 1457569/2000000: episode: 6795, duration: 3.794s, episode steps: 239, steps per second: 63, episode reward: 0.339, mean reward: 0.001 [-100.000, 17.830], mean action: 1.653 [0.000, 3.000], mean observation: -0.016 [-0.904, 1.000], loss: 0.568798, mean_absolute_error: 26.912810, mean_q: 35.880389, mean_eps: 0.100000
 1457641/2000000: episode: 6796, duration: 1.141s, episode steps: 72, steps per second: 63, episode reward: -48.239, mean reward: -0.670 [-100.000, 13.874], mean action: 1.458 [0.000, 3.000], mean observation: 0.022 [-2.728, 1.000], loss: 1.432150, mean_absolute_error: 26.423574, mean_q: 35.096823, mean_eps: 0.100000
 1457729/2000000: episode: 6797, duration: 1.373s, episode steps: 88, steps per second: 64, episode reward: -96.111, mean reward: -1.092 [-100.000, 17.294], mean action: 1.909 [0.000, 3.000], mean observation: 0.047 [-1.282, 1.000], loss: 0.668288, mean_absolute_error: 29.377548, mean_q: 38.799629, mean_eps: 0.100000
 1457815/2000000: episode: 6798, duration: 1.308s, episode steps: 86, steps per second: 66, episode reward: -111.816, mean reward: -1.300 [-100.000, 11.393], mean action: 1.942 [0.000, 3.000], mean observation: 0.043 [-3.785, 1.000], loss: 0.673353, mean_absolute_error: 28.210654, mean_q: 37.836102, mean_eps: 0.100000
 1457885/2000000: episode: 6799, duration: 1.122s, episode steps: 70, steps per second: 62, episode reward: -90.872, mean reward: -1.298 [-100.000, 21.700], mean action: 1.571 [0.000, 3.000], mean observation: -0.037 [-1.409, 1.000], loss: 0.820413, mean_absolute_error: 25.856118, mean_q: 34.637810, mean_eps: 0.100000
 1457954/2000000: episode: 6800, duration: 1.059s, episode steps: 69, steps per second: 65, episode reward: -83.328, mean reward: -1.208 [-100.000, 12.780], mean action: 1.638 [0.000, 3.000], mean observation: -0.177 [-1.181, 3.707], loss: 0.882683, mean_absolute_error: 30.221791, mean_q: 40.470208, mean_eps: 0.100000
 1458036/2000000: episode: 6801, duration: 1.462s, episode steps: 82, steps per second: 56, episode reward: -41.947, mean reward: -0.512 [-100.000, 10.402], mean action: 1.659 [0.000, 3.000], mean observation: -0.139 [-1.030, 1.000], loss: 0.655830, mean_absolute_error: 30.095921, mean_q: 40.277107, mean_eps: 0.100000
 1458191/2000000: episode: 6802, duration: 2.473s, episode steps: 155, steps per second: 63, episode reward: -25.679, mean reward: -0.166 [-100.000, 16.877], mean action: 1.587 [0.000, 3.000], mean observation: -0.010 [-1.479, 1.000], loss: 0.802517, mean_absolute_error: 27.725413, mean_q: 37.035342, mean_eps: 0.100000
 1458279/2000000: episode: 6803, duration: 1.356s, episode steps: 88, steps per second: 65, episode reward: -62.967, mean reward: -0.716 [-100.000, 21.609], mean action: 1.568 [0.000, 3.000], mean observation: -0.021 [-1.328, 2.553], loss: 0.735127, mean_absolute_error: 26.652983, mean_q: 35.338622, mean_eps: 0.100000
 1458676/2000000: episode: 6804, duration: 6.377s, episode steps: 397, steps per second: 62, episode reward: -168.012, mean reward: -0.423 [-100.000, 21.600], mean action: 1.597 [0.000, 3.000], mean observation: 0.096 [-2.159, 1.587], loss: 0.893333, mean_absolute_error: 28.155768, mean_q: 37.623707, mean_eps: 0.100000
 1458734/2000000: episode: 6805, duration: 0.928s, episode steps: 58, steps per second: 63, episode reward: -52.988, mean reward: -0.914 [-100.000, 20.747], mean action: 0.845 [0.000, 3.000], mean observation: -0.053 [-1.406, 1.000], loss: 0.917387, mean_absolute_error: 26.304273, mean_q: 35.224600, mean_eps: 0.100000
 1458799/2000000: episode: 6806, duration: 1.002s, episode steps: 65, steps per second: 65, episode reward: -61.246, mean reward: -0.942 [-100.000, 25.171], mean action: 1.262 [0.000, 3.000], mean observation: -0.145 [-1.386, 1.000], loss: 0.508475, mean_absolute_error: 28.042693, mean_q: 37.657501, mean_eps: 0.100000
 1458853/2000000: episode: 6807, duration: 0.868s, episode steps: 54, steps per second: 62, episode reward: -95.907, mean reward: -1.776 [-100.000, 15.085], mean action: 1.426 [0.000, 3.000], mean observation: -0.208 [-4.605, 1.000], loss: 0.967447, mean_absolute_error: 29.071865, mean_q: 36.988378, mean_eps: 0.100000
 1458936/2000000: episode: 6808, duration: 1.289s, episode steps: 83, steps per second: 64, episode reward: -95.864, mean reward: -1.155 [-100.000, 18.535], mean action: 1.542 [0.000, 3.000], mean observation: 0.007 [-3.619, 1.000], loss: 0.921861, mean_absolute_error: 29.500274, mean_q: 39.430262, mean_eps: 0.100000
 1459035/2000000: episode: 6809, duration: 1.549s, episode steps: 99, steps per second: 64, episode reward: -53.594, mean reward: -0.541 [-100.000, 15.061], mean action: 1.646 [0.000, 3.000], mean observation: -0.055 [-0.959, 1.000], loss: 1.462574, mean_absolute_error: 29.592160, mean_q: 39.398524, mean_eps: 0.100000
 1459240/2000000: episode: 6810, duration: 3.282s, episode steps: 205, steps per second: 62, episode reward: -18.841, mean reward: -0.092 [-100.000, 20.574], mean action: 1.693 [0.000, 3.000], mean observation: 0.018 [-0.972, 1.000], loss: 0.679363, mean_absolute_error: 27.703371, mean_q: 37.020542, mean_eps: 0.100000
 1459664/2000000: episode: 6811, duration: 6.688s, episode steps: 424, steps per second: 63, episode reward: 165.666, mean reward: 0.391 [-21.917, 100.000], mean action: 1.307 [0.000, 3.000], mean observation: 0.145 [-1.233, 1.000], loss: 0.819750, mean_absolute_error: 28.775035, mean_q: 38.082439, mean_eps: 0.100000
 1459729/2000000: episode: 6812, duration: 1.058s, episode steps: 65, steps per second: 61, episode reward: -41.734, mean reward: -0.642 [-100.000, 17.026], mean action: 1.154 [0.000, 3.000], mean observation: 0.017 [-1.215, 2.708], loss: 0.781502, mean_absolute_error: 25.994162, mean_q: 34.331609, mean_eps: 0.100000
 1459888/2000000: episode: 6813, duration: 2.496s, episode steps: 159, steps per second: 64, episode reward: -28.281, mean reward: -0.178 [-100.000, 20.407], mean action: 1.572 [0.000, 3.000], mean observation: 0.011 [-0.679, 1.000], loss: 0.732590, mean_absolute_error: 27.126904, mean_q: 36.230668, mean_eps: 0.100000
 1459952/2000000: episode: 6814, duration: 1.070s, episode steps: 64, steps per second: 60, episode reward: -71.990, mean reward: -1.125 [-100.000, 15.724], mean action: 1.719 [0.000, 3.000], mean observation: -0.028 [-1.447, 1.000], loss: 0.409182, mean_absolute_error: 26.214837, mean_q: 35.287504, mean_eps: 0.100000
 1460006/2000000: episode: 6815, duration: 0.875s, episode steps: 54, steps per second: 62, episode reward: -139.261, mean reward: -2.579 [-100.000, 10.226], mean action: 1.000 [0.000, 3.000], mean observation: -0.175 [-5.461, 1.000], loss: 0.463735, mean_absolute_error: 29.161082, mean_q: 39.154476, mean_eps: 0.100000
 1460087/2000000: episode: 6816, duration: 1.237s, episode steps: 81, steps per second: 65, episode reward: -111.434, mean reward: -1.376 [-100.000, 12.209], mean action: 0.840 [0.000, 3.000], mean observation: 0.041 [-3.999, 1.000], loss: 0.838067, mean_absolute_error: 26.943682, mean_q: 36.004575, mean_eps: 0.100000
 1460423/2000000: episode: 6817, duration: 5.459s, episode steps: 336, steps per second: 62, episode reward: -116.675, mean reward: -0.347 [-100.000, 17.277], mean action: 1.756 [0.000, 3.000], mean observation: 0.051 [-1.400, 1.000], loss: 0.922516, mean_absolute_error: 28.332998, mean_q: 37.508179, mean_eps: 0.100000
 1460842/2000000: episode: 6818, duration: 6.689s, episode steps: 419, steps per second: 63, episode reward: -115.865, mean reward: -0.277 [-100.000, 14.437], mean action: 1.668 [0.000, 3.000], mean observation: 0.047 [-1.550, 1.000], loss: 1.013628, mean_absolute_error: 28.297979, mean_q: 37.643439, mean_eps: 0.100000
 1460968/2000000: episode: 6819, duration: 2.009s, episode steps: 126, steps per second: 63, episode reward: -14.414, mean reward: -0.114 [-100.000, 22.491], mean action: 1.833 [0.000, 3.000], mean observation: -0.031 [-0.915, 1.000], loss: 0.804824, mean_absolute_error: 27.034119, mean_q: 35.863540, mean_eps: 0.100000
 1461097/2000000: episode: 6820, duration: 2.069s, episode steps: 129, steps per second: 62, episode reward: -71.454, mean reward: -0.554 [-100.000, 10.155], mean action: 1.760 [0.000, 3.000], mean observation: -0.021 [-1.164, 2.854], loss: 0.980072, mean_absolute_error: 28.065120, mean_q: 37.419806, mean_eps: 0.100000
 1461190/2000000: episode: 6821, duration: 1.428s, episode steps: 93, steps per second: 65, episode reward: -104.189, mean reward: -1.120 [-100.000, 15.422], mean action: 0.731 [0.000, 3.000], mean observation: 0.052 [-1.434, 1.000], loss: 0.917236, mean_absolute_error: 28.705257, mean_q: 38.320162, mean_eps: 0.100000
 1461435/2000000: episode: 6822, duration: 4.058s, episode steps: 245, steps per second: 60, episode reward: -9.801, mean reward: -0.040 [-100.000, 15.939], mean action: 1.743 [0.000, 3.000], mean observation: 0.014 [-0.619, 1.000], loss: 1.077604, mean_absolute_error: 28.551287, mean_q: 37.748375, mean_eps: 0.100000
 1461506/2000000: episode: 6823, duration: 1.113s, episode steps: 71, steps per second: 64, episode reward: -126.692, mean reward: -1.784 [-100.000, 21.016], mean action: 0.873 [0.000, 3.000], mean observation: -0.027 [-1.460, 1.000], loss: 0.631127, mean_absolute_error: 30.279568, mean_q: 40.614393, mean_eps: 0.100000
 1461708/2000000: episode: 6824, duration: 3.175s, episode steps: 202, steps per second: 64, episode reward: -95.231, mean reward: -0.471 [-100.000, 10.329], mean action: 1.446 [0.000, 3.000], mean observation: 0.136 [-1.402, 4.187], loss: 0.795408, mean_absolute_error: 28.457109, mean_q: 38.036846, mean_eps: 0.100000
 1461833/2000000: episode: 6825, duration: 1.987s, episode steps: 125, steps per second: 63, episode reward: -58.619, mean reward: -0.469 [-100.000, 11.965], mean action: 1.680 [0.000, 3.000], mean observation: 0.045 [-0.794, 1.417], loss: 0.509699, mean_absolute_error: 27.298031, mean_q: 36.560602, mean_eps: 0.100000
 1461900/2000000: episode: 6826, duration: 1.095s, episode steps: 67, steps per second: 61, episode reward: -87.561, mean reward: -1.307 [-100.000, 12.173], mean action: 1.194 [0.000, 3.000], mean observation: -0.004 [-1.376, 1.000], loss: 0.574353, mean_absolute_error: 27.629009, mean_q: 37.072497, mean_eps: 0.100000
 1461958/2000000: episode: 6827, duration: 1.029s, episode steps: 58, steps per second: 56, episode reward: -70.409, mean reward: -1.214 [-100.000, 19.490], mean action: 0.828 [0.000, 3.000], mean observation: -0.068 [-1.488, 1.000], loss: 0.779672, mean_absolute_error: 27.775237, mean_q: 37.176911, mean_eps: 0.100000
 1462043/2000000: episode: 6828, duration: 1.313s, episode steps: 85, steps per second: 65, episode reward: -111.801, mean reward: -1.315 [-100.000, 12.512], mean action: 1.659 [0.000, 3.000], mean observation: 0.023 [-4.339, 1.000], loss: 1.531599, mean_absolute_error: 27.532732, mean_q: 36.177703, mean_eps: 0.100000
 1462108/2000000: episode: 6829, duration: 1.044s, episode steps: 65, steps per second: 62, episode reward: -93.580, mean reward: -1.440 [-100.000, 25.783], mean action: 2.000 [1.000, 3.000], mean observation: -0.078 [-1.465, 1.000], loss: 0.653612, mean_absolute_error: 31.093378, mean_q: 41.233789, mean_eps: 0.100000
 1462174/2000000: episode: 6830, duration: 1.054s, episode steps: 66, steps per second: 63, episode reward: -28.603, mean reward: -0.433 [-100.000, 17.177], mean action: 2.015 [1.000, 3.000], mean observation: -0.114 [-1.317, 2.440], loss: 0.963236, mean_absolute_error: 28.779574, mean_q: 38.399141, mean_eps: 0.100000
 1462394/2000000: episode: 6831, duration: 3.481s, episode steps: 220, steps per second: 63, episode reward: 186.919, mean reward: 0.850 [-17.343, 100.000], mean action: 1.341 [0.000, 3.000], mean observation: 0.154 [-1.332, 1.000], loss: 0.837433, mean_absolute_error: 27.561705, mean_q: 36.654956, mean_eps: 0.100000
 1462481/2000000: episode: 6832, duration: 1.363s, episode steps: 87, steps per second: 64, episode reward: -66.714, mean reward: -0.767 [-100.000, 9.559], mean action: 1.241 [0.000, 3.000], mean observation: 0.104 [-1.115, 2.528], loss: 0.528022, mean_absolute_error: 27.063202, mean_q: 36.267496, mean_eps: 0.100000
 1462785/2000000: episode: 6833, duration: 4.744s, episode steps: 304, steps per second: 64, episode reward: 205.739, mean reward: 0.677 [-12.075, 100.000], mean action: 2.010 [0.000, 3.000], mean observation: 0.144 [-0.724, 1.000], loss: 0.893273, mean_absolute_error: 28.890454, mean_q: 38.094828, mean_eps: 0.100000
 1462844/2000000: episode: 6834, duration: 0.927s, episode steps: 59, steps per second: 64, episode reward: -42.280, mean reward: -0.717 [-100.000, 26.549], mean action: 1.051 [0.000, 3.000], mean observation: -0.071 [-1.479, 1.000], loss: 0.568737, mean_absolute_error: 27.421050, mean_q: 36.777452, mean_eps: 0.100000
 1462903/2000000: episode: 6835, duration: 0.924s, episode steps: 59, steps per second: 64, episode reward: -76.829, mean reward: -1.302 [-100.000, 15.649], mean action: 0.881 [0.000, 3.000], mean observation: -0.064 [-1.475, 1.000], loss: 1.413844, mean_absolute_error: 29.456475, mean_q: 38.843919, mean_eps: 0.100000
 1463438/2000000: episode: 6836, duration: 8.600s, episode steps: 535, steps per second: 62, episode reward: -217.966, mean reward: -0.407 [-100.000, 19.524], mean action: 1.759 [0.000, 3.000], mean observation: 0.117 [-2.347, 2.003], loss: 0.842085, mean_absolute_error: 27.886557, mean_q: 37.251504, mean_eps: 0.100000
 1463498/2000000: episode: 6837, duration: 0.940s, episode steps: 60, steps per second: 64, episode reward: -106.575, mean reward: -1.776 [-100.000, 24.145], mean action: 1.967 [0.000, 3.000], mean observation: -0.169 [-4.383, 1.000], loss: 0.890863, mean_absolute_error: 27.864377, mean_q: 37.186976, mean_eps: 0.100000
 1463564/2000000: episode: 6838, duration: 1.063s, episode steps: 66, steps per second: 62, episode reward: -60.605, mean reward: -0.918 [-100.000, 13.598], mean action: 1.924 [0.000, 3.000], mean observation: 0.016 [-3.137, 1.000], loss: 0.707469, mean_absolute_error: 29.207023, mean_q: 39.132719, mean_eps: 0.100000
 1463726/2000000: episode: 6839, duration: 2.554s, episode steps: 162, steps per second: 63, episode reward: -36.327, mean reward: -0.224 [-100.000, 12.783], mean action: 2.019 [1.000, 3.000], mean observation: 0.005 [-1.105, 1.000], loss: 0.819143, mean_absolute_error: 29.483317, mean_q: 39.461651, mean_eps: 0.100000
 1463790/2000000: episode: 6840, duration: 1.023s, episode steps: 64, steps per second: 63, episode reward: -50.131, mean reward: -0.783 [-100.000, 24.058], mean action: 1.641 [0.000, 3.000], mean observation: -0.046 [-1.452, 1.000], loss: 1.097586, mean_absolute_error: 27.723305, mean_q: 36.513542, mean_eps: 0.100000
 1463853/2000000: episode: 6841, duration: 0.994s, episode steps: 63, steps per second: 63, episode reward: -109.033, mean reward: -1.731 [-100.000, 8.899], mean action: 1.222 [0.000, 3.000], mean observation: -0.115 [-1.446, 5.286], loss: 0.813681, mean_absolute_error: 28.910322, mean_q: 38.693414, mean_eps: 0.100000
 1463933/2000000: episode: 6842, duration: 1.232s, episode steps: 80, steps per second: 65, episode reward: -105.906, mean reward: -1.324 [-100.000, 23.068], mean action: 1.212 [0.000, 3.000], mean observation: -0.019 [-1.489, 1.000], loss: 0.714433, mean_absolute_error: 29.778390, mean_q: 39.922873, mean_eps: 0.100000
 1463995/2000000: episode: 6843, duration: 0.929s, episode steps: 62, steps per second: 67, episode reward: -57.139, mean reward: -0.922 [-100.000, 15.258], mean action: 1.516 [0.000, 3.000], mean observation: -0.069 [-1.372, 1.000], loss: 0.883735, mean_absolute_error: 30.305911, mean_q: 39.782514, mean_eps: 0.100000
 1464102/2000000: episode: 6844, duration: 1.663s, episode steps: 107, steps per second: 64, episode reward: 2.526, mean reward: 0.024 [-100.000, 17.833], mean action: 1.636 [0.000, 3.000], mean observation: -0.023 [-1.116, 1.000], loss: 0.770461, mean_absolute_error: 29.156570, mean_q: 38.968556, mean_eps: 0.100000
 1464205/2000000: episode: 6845, duration: 1.628s, episode steps: 103, steps per second: 63, episode reward: -39.842, mean reward: -0.387 [-100.000, 19.821], mean action: 1.379 [0.000, 3.000], mean observation: -0.036 [-1.146, 1.000], loss: 0.877779, mean_absolute_error: 28.365896, mean_q: 37.562472, mean_eps: 0.100000
 1464275/2000000: episode: 6846, duration: 1.080s, episode steps: 70, steps per second: 65, episode reward: -87.508, mean reward: -1.250 [-100.000, 23.086], mean action: 1.657 [0.000, 3.000], mean observation: -0.022 [-1.384, 1.000], loss: 1.178271, mean_absolute_error: 30.999764, mean_q: 41.218227, mean_eps: 0.100000
 1464355/2000000: episode: 6847, duration: 1.260s, episode steps: 80, steps per second: 63, episode reward: -77.687, mean reward: -0.971 [-100.000, 17.938], mean action: 1.438 [0.000, 3.000], mean observation: 0.023 [-1.387, 1.000], loss: 0.770932, mean_absolute_error: 28.734122, mean_q: 38.392057, mean_eps: 0.100000
 1464427/2000000: episode: 6848, duration: 1.135s, episode steps: 72, steps per second: 63, episode reward: -111.529, mean reward: -1.549 [-100.000, 22.079], mean action: 1.875 [0.000, 3.000], mean observation: -0.091 [-1.459, 1.000], loss: 0.735732, mean_absolute_error: 29.887075, mean_q: 39.788367, mean_eps: 0.100000
 1464484/2000000: episode: 6849, duration: 0.938s, episode steps: 57, steps per second: 61, episode reward: -64.483, mean reward: -1.131 [-100.000, 17.727], mean action: 2.053 [1.000, 3.000], mean observation: -0.132 [-1.472, 3.494], loss: 1.520136, mean_absolute_error: 27.042552, mean_q: 35.625874, mean_eps: 0.100000
 1464947/2000000: episode: 6850, duration: 7.373s, episode steps: 463, steps per second: 63, episode reward: -130.294, mean reward: -0.281 [-100.000, 9.877], mean action: 1.378 [0.000, 3.000], mean observation: 0.078 [-0.834, 1.000], loss: 0.686904, mean_absolute_error: 28.637738, mean_q: 38.158240, mean_eps: 0.100000
 1465076/2000000: episode: 6851, duration: 2.049s, episode steps: 129, steps per second: 63, episode reward: -48.765, mean reward: -0.378 [-100.000, 10.644], mean action: 1.930 [0.000, 3.000], mean observation: 0.093 [-0.923, 1.012], loss: 0.809836, mean_absolute_error: 28.662712, mean_q: 38.236763, mean_eps: 0.100000
 1465149/2000000: episode: 6852, duration: 1.201s, episode steps: 73, steps per second: 61, episode reward: -90.083, mean reward: -1.234 [-100.000, 17.105], mean action: 1.836 [0.000, 3.000], mean observation: -0.007 [-3.238, 1.000], loss: 1.096220, mean_absolute_error: 29.276125, mean_q: 38.927579, mean_eps: 0.100000
 1465212/2000000: episode: 6853, duration: 1.000s, episode steps: 63, steps per second: 63, episode reward: -41.679, mean reward: -0.662 [-100.000, 23.191], mean action: 1.873 [0.000, 3.000], mean observation: -0.034 [-1.331, 1.000], loss: 0.540660, mean_absolute_error: 28.797982, mean_q: 38.620393, mean_eps: 0.100000
 1465948/2000000: episode: 6854, duration: 11.985s, episode steps: 736, steps per second: 61, episode reward: 201.055, mean reward: 0.273 [-19.315, 100.000], mean action: 0.933 [0.000, 3.000], mean observation: 0.242 [-1.283, 1.000], loss: 0.768177, mean_absolute_error: 28.780279, mean_q: 38.350050, mean_eps: 0.100000
 1466014/2000000: episode: 6855, duration: 1.047s, episode steps: 66, steps per second: 63, episode reward: -133.352, mean reward: -2.020 [-100.000, 10.799], mean action: 0.667 [0.000, 3.000], mean observation: -0.008 [-4.719, 1.000], loss: 0.687472, mean_absolute_error: 27.811278, mean_q: 37.293984, mean_eps: 0.100000
 1466084/2000000: episode: 6856, duration: 1.117s, episode steps: 70, steps per second: 63, episode reward: -125.838, mean reward: -1.798 [-100.000, 19.931], mean action: 1.543 [0.000, 3.000], mean observation: -0.073 [-1.534, 1.000], loss: 0.590334, mean_absolute_error: 28.283039, mean_q: 37.702507, mean_eps: 0.100000
 1466175/2000000: episode: 6857, duration: 1.429s, episode steps: 91, steps per second: 64, episode reward: -104.223, mean reward: -1.145 [-100.000, 23.867], mean action: 2.000 [0.000, 3.000], mean observation: 0.019 [-4.317, 1.000], loss: 0.842543, mean_absolute_error: 28.991668, mean_q: 38.747790, mean_eps: 0.100000
 1466265/2000000: episode: 6858, duration: 1.427s, episode steps: 90, steps per second: 63, episode reward: -82.115, mean reward: -0.912 [-100.000, 16.012], mean action: 1.989 [0.000, 3.000], mean observation: -0.007 [-1.531, 1.000], loss: 0.930742, mean_absolute_error: 27.656688, mean_q: 36.868338, mean_eps: 0.100000
 1466630/2000000: episode: 6859, duration: 5.705s, episode steps: 365, steps per second: 64, episode reward: -71.910, mean reward: -0.197 [-100.000, 7.955], mean action: 1.559 [0.000, 3.000], mean observation: 0.043 [-0.985, 2.504], loss: 0.945830, mean_absolute_error: 28.519126, mean_q: 38.010995, mean_eps: 0.100000
 1466697/2000000: episode: 6860, duration: 1.089s, episode steps: 67, steps per second: 62, episode reward: -128.419, mean reward: -1.917 [-100.000, 9.869], mean action: 1.881 [0.000, 3.000], mean observation: -0.012 [-1.488, 1.000], loss: 0.915196, mean_absolute_error: 27.706855, mean_q: 36.263838, mean_eps: 0.100000
 1466976/2000000: episode: 6861, duration: 4.436s, episode steps: 279, steps per second: 63, episode reward: 189.806, mean reward: 0.680 [-17.301, 100.000], mean action: 2.176 [0.000, 3.000], mean observation: 0.189 [-0.736, 1.000], loss: 1.000030, mean_absolute_error: 29.187303, mean_q: 38.839324, mean_eps: 0.100000
 1467060/2000000: episode: 6862, duration: 1.389s, episode steps: 84, steps per second: 60, episode reward: -102.095, mean reward: -1.215 [-100.000, 19.515], mean action: 1.821 [0.000, 3.000], mean observation: 0.028 [-1.468, 1.000], loss: 0.917204, mean_absolute_error: 29.084327, mean_q: 38.747823, mean_eps: 0.100000
 1467132/2000000: episode: 6863, duration: 1.169s, episode steps: 72, steps per second: 62, episode reward: -114.097, mean reward: -1.585 [-100.000, 20.991], mean action: 1.917 [0.000, 3.000], mean observation: -0.039 [-3.824, 1.000], loss: 0.807587, mean_absolute_error: 28.042436, mean_q: 37.597954, mean_eps: 0.100000
 1467203/2000000: episode: 6864, duration: 1.129s, episode steps: 71, steps per second: 63, episode reward: -12.506, mean reward: -0.176 [-100.000, 16.220], mean action: 1.380 [0.000, 3.000], mean observation: 0.027 [-1.229, 1.000], loss: 1.040939, mean_absolute_error: 27.924478, mean_q: 37.356459, mean_eps: 0.100000
 1467281/2000000: episode: 6865, duration: 1.252s, episode steps: 78, steps per second: 62, episode reward: -111.514, mean reward: -1.430 [-100.000, 15.162], mean action: 1.372 [0.000, 3.000], mean observation: -0.061 [-1.517, 1.000], loss: 0.834229, mean_absolute_error: 26.513657, mean_q: 35.213964, mean_eps: 0.100000
 1467366/2000000: episode: 6866, duration: 1.334s, episode steps: 85, steps per second: 64, episode reward: -61.096, mean reward: -0.719 [-100.000, 20.418], mean action: 1.953 [0.000, 3.000], mean observation: -0.066 [-2.310, 1.000], loss: 0.644090, mean_absolute_error: 31.719402, mean_q: 42.509867, mean_eps: 0.100000
 1467430/2000000: episode: 6867, duration: 1.006s, episode steps: 64, steps per second: 64, episode reward: -112.674, mean reward: -1.761 [-100.000, 18.231], mean action: 1.875 [0.000, 3.000], mean observation: -0.042 [-1.574, 1.000], loss: 0.807500, mean_absolute_error: 30.815482, mean_q: 41.140321, mean_eps: 0.100000
 1467496/2000000: episode: 6868, duration: 1.048s, episode steps: 66, steps per second: 63, episode reward: -112.464, mean reward: -1.704 [-100.000, 15.146], mean action: 0.970 [0.000, 3.000], mean observation: -0.108 [-1.499, 1.000], loss: 1.010802, mean_absolute_error: 28.627990, mean_q: 37.520436, mean_eps: 0.100000
 1467569/2000000: episode: 6869, duration: 1.175s, episode steps: 73, steps per second: 62, episode reward: -102.661, mean reward: -1.406 [-100.000, 21.487], mean action: 0.863 [0.000, 3.000], mean observation: -0.031 [-1.456, 1.000], loss: 0.902452, mean_absolute_error: 29.677324, mean_q: 39.459856, mean_eps: 0.100000
 1467721/2000000: episode: 6870, duration: 2.351s, episode steps: 152, steps per second: 65, episode reward: -21.638, mean reward: -0.142 [-100.000, 9.148], mean action: 1.717 [0.000, 3.000], mean observation: 0.107 [-0.718, 1.118], loss: 0.813256, mean_absolute_error: 28.013120, mean_q: 37.421470, mean_eps: 0.100000
 1467794/2000000: episode: 6871, duration: 1.113s, episode steps: 73, steps per second: 66, episode reward: -62.979, mean reward: -0.863 [-100.000, 20.602], mean action: 1.973 [0.000, 3.000], mean observation: -0.047 [-1.254, 1.000], loss: 1.160033, mean_absolute_error: 32.736405, mean_q: 43.705051, mean_eps: 0.100000
 1467917/2000000: episode: 6872, duration: 1.912s, episode steps: 123, steps per second: 64, episode reward: -86.983, mean reward: -0.707 [-100.000, 25.314], mean action: 1.431 [0.000, 3.000], mean observation: -0.004 [-1.443, 1.000], loss: 0.677521, mean_absolute_error: 29.364231, mean_q: 39.077592, mean_eps: 0.100000
 1467995/2000000: episode: 6873, duration: 1.202s, episode steps: 78, steps per second: 65, episode reward: -108.610, mean reward: -1.392 [-100.000, 11.861], mean action: 1.462 [0.000, 3.000], mean observation: -0.000 [-1.506, 1.000], loss: 0.632477, mean_absolute_error: 27.973576, mean_q: 37.507043, mean_eps: 0.100000
 1468087/2000000: episode: 6874, duration: 1.424s, episode steps: 92, steps per second: 65, episode reward: -1.022, mean reward: -0.011 [-100.000, 10.822], mean action: 1.870 [0.000, 3.000], mean observation: 0.131 [-1.037, 1.451], loss: 0.776120, mean_absolute_error: 28.347017, mean_q: 37.782260, mean_eps: 0.100000
 1468148/2000000: episode: 6875, duration: 0.994s, episode steps: 61, steps per second: 61, episode reward: -87.902, mean reward: -1.441 [-100.000, 13.705], mean action: 1.934 [0.000, 3.000], mean observation: -0.022 [-1.466, 1.000], loss: 0.604206, mean_absolute_error: 29.674377, mean_q: 39.799346, mean_eps: 0.100000
 1468229/2000000: episode: 6876, duration: 1.311s, episode steps: 81, steps per second: 62, episode reward: -100.578, mean reward: -1.242 [-100.000, 20.413], mean action: 1.889 [0.000, 3.000], mean observation: 0.047 [-1.392, 1.000], loss: 0.713688, mean_absolute_error: 27.717322, mean_q: 36.649969, mean_eps: 0.100000
 1468291/2000000: episode: 6877, duration: 0.931s, episode steps: 62, steps per second: 67, episode reward: -81.464, mean reward: -1.314 [-100.000, 20.833], mean action: 1.968 [0.000, 3.000], mean observation: -0.073 [-1.497, 1.000], loss: 1.365742, mean_absolute_error: 32.846354, mean_q: 43.726808, mean_eps: 0.100000
 1469291/2000000: episode: 6878, duration: 16.307s, episode steps: 1000, steps per second: 61, episode reward: 88.308, mean reward: 0.088 [-20.098, 21.774], mean action: 0.807 [0.000, 3.000], mean observation: 0.231 [-1.145, 1.000], loss: 0.856897, mean_absolute_error: 29.078907, mean_q: 38.611791, mean_eps: 0.100000
 1469410/2000000: episode: 6879, duration: 1.863s, episode steps: 119, steps per second: 64, episode reward: -90.188, mean reward: -0.758 [-100.000, 22.557], mean action: 1.311 [0.000, 3.000], mean observation: -0.005 [-1.353, 1.000], loss: 0.701331, mean_absolute_error: 28.490655, mean_q: 38.041799, mean_eps: 0.100000
 1469545/2000000: episode: 6880, duration: 2.080s, episode steps: 135, steps per second: 65, episode reward: -56.819, mean reward: -0.421 [-100.000, 17.624], mean action: 1.304 [0.000, 3.000], mean observation: 0.101 [-0.917, 1.726], loss: 0.743711, mean_absolute_error: 29.341647, mean_q: 38.280910, mean_eps: 0.100000
 1469755/2000000: episode: 6881, duration: 3.276s, episode steps: 210, steps per second: 64, episode reward: -209.578, mean reward: -0.998 [-100.000, 4.559], mean action: 1.690 [0.000, 3.000], mean observation: 0.176 [-0.936, 1.001], loss: 0.735829, mean_absolute_error: 29.155408, mean_q: 39.012487, mean_eps: 0.100000
 1469854/2000000: episode: 6882, duration: 1.537s, episode steps: 99, steps per second: 64, episode reward: -94.701, mean reward: -0.957 [-100.000, 13.859], mean action: 1.071 [0.000, 3.000], mean observation: -0.041 [-1.453, 3.926], loss: 1.423738, mean_absolute_error: 30.536815, mean_q: 40.415354, mean_eps: 0.100000
 1469968/2000000: episode: 6883, duration: 1.787s, episode steps: 114, steps per second: 64, episode reward: -26.098, mean reward: -0.229 [-100.000, 23.870], mean action: 1.561 [0.000, 3.000], mean observation: -0.033 [-1.031, 1.000], loss: 0.700248, mean_absolute_error: 27.339682, mean_q: 36.588310, mean_eps: 0.100000
 1470052/2000000: episode: 6884, duration: 1.344s, episode steps: 84, steps per second: 63, episode reward: -102.953, mean reward: -1.226 [-100.000, 10.119], mean action: 0.655 [0.000, 3.000], mean observation: 0.034 [-1.457, 1.000], loss: 0.674997, mean_absolute_error: 30.542638, mean_q: 40.892448, mean_eps: 0.100000
 1470130/2000000: episode: 6885, duration: 1.207s, episode steps: 78, steps per second: 65, episode reward: -110.936, mean reward: -1.422 [-100.000, 21.755], mean action: 0.949 [0.000, 3.000], mean observation: -0.002 [-3.912, 1.000], loss: 0.818802, mean_absolute_error: 27.967474, mean_q: 37.183144, mean_eps: 0.100000
 1470224/2000000: episode: 6886, duration: 1.552s, episode steps: 94, steps per second: 61, episode reward: -35.245, mean reward: -0.375 [-100.000, 14.550], mean action: 1.330 [0.000, 3.000], mean observation: -0.090 [-1.919, 1.000], loss: 0.789490, mean_absolute_error: 31.343854, mean_q: 41.951472, mean_eps: 0.100000
 1470521/2000000: episode: 6887, duration: 4.725s, episode steps: 297, steps per second: 63, episode reward: -380.341, mean reward: -1.281 [-100.000, 5.259], mean action: 1.835 [0.000, 3.000], mean observation: 0.133 [-0.777, 1.724], loss: 1.047552, mean_absolute_error: 29.265626, mean_q: 38.699746, mean_eps: 0.100000
 1470595/2000000: episode: 6888, duration: 1.124s, episode steps: 74, steps per second: 66, episode reward: -92.000, mean reward: -1.243 [-100.000, 19.590], mean action: 0.905 [0.000, 3.000], mean observation: -0.033 [-1.484, 1.000], loss: 0.932717, mean_absolute_error: 30.850505, mean_q: 40.734213, mean_eps: 0.100000
 1470802/2000000: episode: 6889, duration: 3.239s, episode steps: 207, steps per second: 64, episode reward: -19.012, mean reward: -0.092 [-100.000, 12.797], mean action: 1.362 [0.000, 3.000], mean observation: 0.100 [-0.915, 1.139], loss: 1.056148, mean_absolute_error: 28.411297, mean_q: 37.656021, mean_eps: 0.100000
 1470948/2000000: episode: 6890, duration: 2.295s, episode steps: 146, steps per second: 64, episode reward: -4.178, mean reward: -0.029 [-100.000, 19.300], mean action: 1.664 [0.000, 3.000], mean observation: -0.038 [-0.701, 1.362], loss: 0.549984, mean_absolute_error: 30.373290, mean_q: 40.627339, mean_eps: 0.100000
 1471012/2000000: episode: 6891, duration: 1.042s, episode steps: 64, steps per second: 61, episode reward: -68.073, mean reward: -1.064 [-100.000, 21.324], mean action: 1.328 [0.000, 3.000], mean observation: -0.035 [-1.411, 1.000], loss: 0.453523, mean_absolute_error: 28.525108, mean_q: 38.322689, mean_eps: 0.100000
 1471077/2000000: episode: 6892, duration: 1.042s, episode steps: 65, steps per second: 62, episode reward: -109.906, mean reward: -1.691 [-100.000, 11.178], mean action: 1.108 [0.000, 3.000], mean observation: -0.188 [-1.468, 4.025], loss: 0.823399, mean_absolute_error: 26.992829, mean_q: 36.199481, mean_eps: 0.100000
 1471360/2000000: episode: 6893, duration: 4.398s, episode steps: 283, steps per second: 64, episode reward: 217.031, mean reward: 0.767 [-3.313, 100.000], mean action: 1.314 [0.000, 3.000], mean observation: 0.040 [-0.553, 1.000], loss: 1.122890, mean_absolute_error: 28.256950, mean_q: 37.555958, mean_eps: 0.100000
 1471438/2000000: episode: 6894, duration: 1.233s, episode steps: 78, steps per second: 63, episode reward: -51.245, mean reward: -0.657 [-100.000, 17.640], mean action: 1.526 [0.000, 3.000], mean observation: 0.039 [-2.168, 1.000], loss: 0.889354, mean_absolute_error: 31.090227, mean_q: 41.253198, mean_eps: 0.100000
 1471559/2000000: episode: 6895, duration: 1.873s, episode steps: 121, steps per second: 65, episode reward: -74.430, mean reward: -0.615 [-100.000, 17.632], mean action: 1.364 [0.000, 3.000], mean observation: -0.079 [-3.091, 1.000], loss: 0.751129, mean_absolute_error: 29.880419, mean_q: 39.314523, mean_eps: 0.100000
 1471652/2000000: episode: 6896, duration: 1.454s, episode steps: 93, steps per second: 64, episode reward: -87.374, mean reward: -0.940 [-100.000, 12.481], mean action: 0.806 [0.000, 3.000], mean observation: -0.022 [-1.378, 3.901], loss: 1.440138, mean_absolute_error: 29.300740, mean_q: 38.796575, mean_eps: 0.100000
 1471744/2000000: episode: 6897, duration: 1.472s, episode steps: 92, steps per second: 63, episode reward: -76.608, mean reward: -0.833 [-100.000, 16.266], mean action: 0.859 [0.000, 3.000], mean observation: 0.019 [-1.432, 1.000], loss: 0.804850, mean_absolute_error: 27.822539, mean_q: 36.565105, mean_eps: 0.100000
 1471881/2000000: episode: 6898, duration: 2.184s, episode steps: 137, steps per second: 63, episode reward: 10.978, mean reward: 0.080 [-100.000, 18.901], mean action: 1.613 [0.000, 3.000], mean observation: -0.048 [-0.903, 1.000], loss: 0.592943, mean_absolute_error: 28.019994, mean_q: 37.321364, mean_eps: 0.100000
 1471940/2000000: episode: 6899, duration: 0.948s, episode steps: 59, steps per second: 62, episode reward: -81.377, mean reward: -1.379 [-100.000, 17.349], mean action: 1.966 [0.000, 3.000], mean observation: -0.077 [-1.505, 1.000], loss: 0.684893, mean_absolute_error: 31.304960, mean_q: 41.230153, mean_eps: 0.100000
 1472012/2000000: episode: 6900, duration: 1.175s, episode steps: 72, steps per second: 61, episode reward: -114.726, mean reward: -1.593 [-100.000, 15.726], mean action: 1.972 [0.000, 3.000], mean observation: -0.011 [-1.459, 1.000], loss: 0.826427, mean_absolute_error: 27.883185, mean_q: 37.374058, mean_eps: 0.100000
 1472117/2000000: episode: 6901, duration: 1.660s, episode steps: 105, steps per second: 63, episode reward: -42.543, mean reward: -0.405 [-100.000, 19.726], mean action: 1.905 [0.000, 3.000], mean observation: 0.069 [-1.312, 2.398], loss: 0.663268, mean_absolute_error: 28.708608, mean_q: 38.374561, mean_eps: 0.100000
 1472198/2000000: episode: 6902, duration: 1.261s, episode steps: 81, steps per second: 64, episode reward: -50.868, mean reward: -0.628 [-100.000, 10.446], mean action: 1.469 [0.000, 3.000], mean observation: 0.118 [-1.058, 1.000], loss: 0.626482, mean_absolute_error: 27.802433, mean_q: 37.322766, mean_eps: 0.100000
 1472266/2000000: episode: 6903, duration: 1.046s, episode steps: 68, steps per second: 65, episode reward: -105.465, mean reward: -1.551 [-100.000, 12.042], mean action: 1.176 [0.000, 3.000], mean observation: 0.032 [-1.329, 1.000], loss: 0.632302, mean_absolute_error: 28.906036, mean_q: 38.691319, mean_eps: 0.100000
 1472690/2000000: episode: 6904, duration: 6.788s, episode steps: 424, steps per second: 62, episode reward: 195.999, mean reward: 0.462 [-22.070, 100.000], mean action: 2.257 [0.000, 3.000], mean observation: 0.254 [-0.997, 1.000], loss: 0.720529, mean_absolute_error: 29.750595, mean_q: 39.706493, mean_eps: 0.100000
 1472772/2000000: episode: 6905, duration: 1.282s, episode steps: 82, steps per second: 64, episode reward: -108.278, mean reward: -1.320 [-100.000, 16.608], mean action: 1.402 [0.000, 3.000], mean observation: -0.019 [-1.496, 1.000], loss: 0.807646, mean_absolute_error: 30.731067, mean_q: 40.892417, mean_eps: 0.100000
 1473045/2000000: episode: 6906, duration: 4.265s, episode steps: 273, steps per second: 64, episode reward: 197.877, mean reward: 0.725 [-17.593, 100.000], mean action: 0.927 [0.000, 3.000], mean observation: 0.180 [-1.171, 1.000], loss: 0.809187, mean_absolute_error: 28.864498, mean_q: 38.221482, mean_eps: 0.100000
 1473101/2000000: episode: 6907, duration: 0.898s, episode steps: 56, steps per second: 62, episode reward: -87.620, mean reward: -1.565 [-100.000, 27.608], mean action: 2.036 [1.000, 3.000], mean observation: -0.091 [-1.611, 1.000], loss: 0.786482, mean_absolute_error: 29.607689, mean_q: 39.584856, mean_eps: 0.100000
 1473170/2000000: episode: 6908, duration: 1.059s, episode steps: 69, steps per second: 65, episode reward: -130.627, mean reward: -1.893 [-100.000, 19.473], mean action: 1.014 [0.000, 3.000], mean observation: -0.037 [-1.547, 1.000], loss: 0.705489, mean_absolute_error: 27.344311, mean_q: 36.024554, mean_eps: 0.100000
 1473238/2000000: episode: 6909, duration: 1.060s, episode steps: 68, steps per second: 64, episode reward: -88.189, mean reward: -1.297 [-100.000, 18.397], mean action: 1.118 [0.000, 3.000], mean observation: -0.052 [-1.501, 1.000], loss: 1.604158, mean_absolute_error: 26.730833, mean_q: 35.548730, mean_eps: 0.100000
 1473303/2000000: episode: 6910, duration: 0.997s, episode steps: 65, steps per second: 65, episode reward: -86.398, mean reward: -1.329 [-100.000, 15.632], mean action: 1.108 [0.000, 3.000], mean observation: -0.140 [-1.367, 1.000], loss: 0.836071, mean_absolute_error: 30.869444, mean_q: 40.915807, mean_eps: 0.100000
 1473448/2000000: episode: 6911, duration: 2.275s, episode steps: 145, steps per second: 64, episode reward: -16.856, mean reward: -0.116 [-100.000, 20.345], mean action: 1.903 [0.000, 3.000], mean observation: -0.007 [-0.933, 1.000], loss: 1.097768, mean_absolute_error: 29.465900, mean_q: 39.319602, mean_eps: 0.100000
 1473524/2000000: episode: 6912, duration: 1.265s, episode steps: 76, steps per second: 60, episode reward: -76.636, mean reward: -1.008 [-100.000, 15.403], mean action: 1.882 [0.000, 3.000], mean observation: 0.032 [-2.978, 1.000], loss: 0.921424, mean_absolute_error: 29.714703, mean_q: 39.385575, mean_eps: 0.100000
 1473597/2000000: episode: 6913, duration: 1.178s, episode steps: 73, steps per second: 62, episode reward: 2.573, mean reward: 0.035 [-100.000, 18.846], mean action: 1.438 [0.000, 3.000], mean observation: 0.033 [-1.116, 1.032], loss: 0.748952, mean_absolute_error: 26.543185, mean_q: 34.497475, mean_eps: 0.100000
 1473680/2000000: episode: 6914, duration: 1.296s, episode steps: 83, steps per second: 64, episode reward: -89.192, mean reward: -1.075 [-100.000, 24.138], mean action: 0.855 [0.000, 3.000], mean observation: -0.020 [-1.466, 1.000], loss: 0.615218, mean_absolute_error: 29.246873, mean_q: 39.222044, mean_eps: 0.100000
 1473745/2000000: episode: 6915, duration: 1.052s, episode steps: 65, steps per second: 62, episode reward: -68.176, mean reward: -1.049 [-100.000, 24.831], mean action: 0.831 [0.000, 3.000], mean observation: -0.031 [-1.407, 1.000], loss: 0.916739, mean_absolute_error: 29.408522, mean_q: 38.819324, mean_eps: 0.100000
 1473972/2000000: episode: 6916, duration: 3.542s, episode steps: 227, steps per second: 64, episode reward: -15.162, mean reward: -0.067 [-100.000, 19.894], mean action: 1.621 [0.000, 3.000], mean observation: -0.002 [-0.765, 1.000], loss: 0.608095, mean_absolute_error: 29.079390, mean_q: 38.936970, mean_eps: 0.100000
 1474208/2000000: episode: 6917, duration: 3.709s, episode steps: 236, steps per second: 64, episode reward: -25.068, mean reward: -0.106 [-100.000, 7.818], mean action: 1.674 [0.000, 3.000], mean observation: 0.007 [-0.782, 1.178], loss: 1.168743, mean_absolute_error: 29.234607, mean_q: 38.670675, mean_eps: 0.100000
 1474700/2000000: episode: 6918, duration: 7.820s, episode steps: 492, steps per second: 63, episode reward: 209.622, mean reward: 0.426 [-17.814, 100.000], mean action: 0.707 [0.000, 3.000], mean observation: 0.226 [-0.784, 1.000], loss: 0.935676, mean_absolute_error: 29.418849, mean_q: 39.107004, mean_eps: 0.100000
 1474790/2000000: episode: 6919, duration: 1.464s, episode steps: 90, steps per second: 61, episode reward: -15.980, mean reward: -0.178 [-100.000, 12.729], mean action: 1.967 [0.000, 3.000], mean observation: -0.114 [-1.070, 1.613], loss: 1.378255, mean_absolute_error: 29.373427, mean_q: 38.692821, mean_eps: 0.100000
 1474873/2000000: episode: 6920, duration: 1.300s, episode steps: 83, steps per second: 64, episode reward: -91.841, mean reward: -1.107 [-100.000, 22.626], mean action: 1.928 [0.000, 3.000], mean observation: 0.033 [-1.430, 1.000], loss: 1.327994, mean_absolute_error: 30.038609, mean_q: 39.849174, mean_eps: 0.100000
 1474956/2000000: episode: 6921, duration: 1.300s, episode steps: 83, steps per second: 64, episode reward: -85.560, mean reward: -1.031 [-100.000, 13.687], mean action: 0.904 [0.000, 3.000], mean observation: 0.108 [-1.242, 2.766], loss: 0.876551, mean_absolute_error: 28.269261, mean_q: 37.840063, mean_eps: 0.100000
 1475086/2000000: episode: 6922, duration: 2.072s, episode steps: 130, steps per second: 63, episode reward: 1.295, mean reward: 0.010 [-100.000, 19.434], mean action: 1.754 [0.000, 3.000], mean observation: -0.046 [-1.113, 1.000], loss: 0.957758, mean_absolute_error: 27.242779, mean_q: 36.279783, mean_eps: 0.100000
 1475156/2000000: episode: 6923, duration: 1.139s, episode steps: 70, steps per second: 61, episode reward: -52.805, mean reward: -0.754 [-100.000, 10.987], mean action: 1.857 [0.000, 3.000], mean observation: 0.046 [-1.173, 1.000], loss: 0.913738, mean_absolute_error: 31.898032, mean_q: 42.534425, mean_eps: 0.100000
 1475301/2000000: episode: 6924, duration: 2.299s, episode steps: 145, steps per second: 63, episode reward: -31.219, mean reward: -0.215 [-100.000, 19.687], mean action: 1.917 [0.000, 3.000], mean observation: -0.003 [-0.954, 1.000], loss: 0.858756, mean_absolute_error: 31.073280, mean_q: 41.344886, mean_eps: 0.100000
 1475389/2000000: episode: 6925, duration: 1.375s, episode steps: 88, steps per second: 64, episode reward: -115.153, mean reward: -1.309 [-100.000, 22.318], mean action: 1.989 [0.000, 3.000], mean observation: -0.026 [-1.579, 4.730], loss: 0.749259, mean_absolute_error: 28.387897, mean_q: 38.028668, mean_eps: 0.100000
 1475520/2000000: episode: 6926, duration: 2.028s, episode steps: 131, steps per second: 65, episode reward: -69.945, mean reward: -0.534 [-100.000, 21.080], mean action: 1.405 [0.000, 3.000], mean observation: -0.012 [-1.253, 1.000], loss: 1.179606, mean_absolute_error: 30.461553, mean_q: 40.082364, mean_eps: 0.100000
 1475586/2000000: episode: 6927, duration: 1.075s, episode steps: 66, steps per second: 61, episode reward: -110.076, mean reward: -1.668 [-100.000, 20.520], mean action: 0.818 [0.000, 3.000], mean observation: -0.046 [-2.873, 1.000], loss: 0.911094, mean_absolute_error: 29.859512, mean_q: 38.002006, mean_eps: 0.100000
 1475666/2000000: episode: 6928, duration: 1.255s, episode steps: 80, steps per second: 64, episode reward: -34.866, mean reward: -0.436 [-100.000, 19.996], mean action: 1.525 [0.000, 3.000], mean observation: 0.054 [-2.046, 1.000], loss: 0.786496, mean_absolute_error: 29.160443, mean_q: 38.632876, mean_eps: 0.100000
 1475831/2000000: episode: 6929, duration: 2.545s, episode steps: 165, steps per second: 65, episode reward: -8.909, mean reward: -0.054 [-100.000, 17.561], mean action: 1.885 [0.000, 3.000], mean observation: 0.015 [-1.006, 1.000], loss: 0.894206, mean_absolute_error: 28.201006, mean_q: 37.371048, mean_eps: 0.100000
 1476054/2000000: episode: 6930, duration: 3.496s, episode steps: 223, steps per second: 64, episode reward: 27.468, mean reward: 0.123 [-100.000, 13.961], mean action: 1.812 [0.000, 3.000], mean observation: 0.004 [-0.738, 1.000], loss: 0.931323, mean_absolute_error: 29.682601, mean_q: 39.612590, mean_eps: 0.100000
 1476129/2000000: episode: 6931, duration: 1.196s, episode steps: 75, steps per second: 63, episode reward: -103.651, mean reward: -1.382 [-100.000, 17.126], mean action: 1.973 [0.000, 3.000], mean observation: 0.007 [-3.782, 1.000], loss: 0.500051, mean_absolute_error: 27.776833, mean_q: 37.114149, mean_eps: 0.100000
 1476202/2000000: episode: 6932, duration: 1.126s, episode steps: 73, steps per second: 65, episode reward: -103.621, mean reward: -1.419 [-100.000, 12.372], mean action: 1.904 [0.000, 3.000], mean observation: -0.013 [-3.640, 1.000], loss: 0.899059, mean_absolute_error: 29.357782, mean_q: 39.285436, mean_eps: 0.100000
 1476448/2000000: episode: 6933, duration: 3.888s, episode steps: 246, steps per second: 63, episode reward: 197.080, mean reward: 0.801 [-11.789, 100.000], mean action: 1.870 [0.000, 3.000], mean observation: 0.191 [-1.072, 1.202], loss: 0.802170, mean_absolute_error: 27.737046, mean_q: 36.786012, mean_eps: 0.100000
 1476514/2000000: episode: 6934, duration: 1.245s, episode steps: 66, steps per second: 53, episode reward: -34.410, mean reward: -0.521 [-100.000, 18.589], mean action: 1.500 [0.000, 3.000], mean observation: -0.097 [-1.299, 2.451], loss: 0.891191, mean_absolute_error: 27.261289, mean_q: 36.460927, mean_eps: 0.100000
 1476607/2000000: episode: 6935, duration: 1.507s, episode steps: 93, steps per second: 62, episode reward: -62.531, mean reward: -0.672 [-100.000, 18.891], mean action: 1.473 [0.000, 3.000], mean observation: 0.028 [-1.377, 1.000], loss: 0.628184, mean_absolute_error: 29.641947, mean_q: 39.740863, mean_eps: 0.100000
 1476685/2000000: episode: 6936, duration: 1.269s, episode steps: 78, steps per second: 61, episode reward: -97.857, mean reward: -1.255 [-100.000, 17.047], mean action: 1.615 [0.000, 3.000], mean observation: -0.016 [-1.413, 1.000], loss: 1.012810, mean_absolute_error: 30.582579, mean_q: 40.580583, mean_eps: 0.100000
 1476842/2000000: episode: 6937, duration: 2.432s, episode steps: 157, steps per second: 65, episode reward: 236.965, mean reward: 1.509 [-9.593, 100.000], mean action: 1.134 [0.000, 3.000], mean observation: 0.061 [-0.918, 1.000], loss: 0.906051, mean_absolute_error: 29.491411, mean_q: 39.259094, mean_eps: 0.100000
 1476925/2000000: episode: 6938, duration: 1.338s, episode steps: 83, steps per second: 62, episode reward: -97.770, mean reward: -1.178 [-100.000, 22.973], mean action: 1.434 [0.000, 3.000], mean observation: -0.021 [-1.460, 1.000], loss: 0.680925, mean_absolute_error: 30.297029, mean_q: 40.423994, mean_eps: 0.100000
 1476992/2000000: episode: 6939, duration: 1.058s, episode steps: 67, steps per second: 63, episode reward: -96.391, mean reward: -1.439 [-100.000, 17.484], mean action: 1.313 [0.000, 3.000], mean observation: 0.003 [-3.647, 1.000], loss: 0.724798, mean_absolute_error: 28.710132, mean_q: 38.415639, mean_eps: 0.100000
 1477090/2000000: episode: 6940, duration: 1.522s, episode steps: 98, steps per second: 64, episode reward: -44.844, mean reward: -0.458 [-100.000, 20.536], mean action: 0.765 [0.000, 3.000], mean observation: 0.049 [-1.374, 1.003], loss: 0.812721, mean_absolute_error: 28.936893, mean_q: 38.320469, mean_eps: 0.100000
 1477178/2000000: episode: 6941, duration: 1.362s, episode steps: 88, steps per second: 65, episode reward: -67.801, mean reward: -0.770 [-100.000, 16.553], mean action: 1.182 [0.000, 3.000], mean observation: 0.034 [-1.366, 1.000], loss: 1.049810, mean_absolute_error: 31.347552, mean_q: 41.734033, mean_eps: 0.100000
 1477275/2000000: episode: 6942, duration: 1.532s, episode steps: 97, steps per second: 63, episode reward: -101.438, mean reward: -1.046 [-100.000, 12.490], mean action: 0.660 [0.000, 3.000], mean observation: -0.035 [-1.432, 4.311], loss: 0.973850, mean_absolute_error: 28.239452, mean_q: 37.556589, mean_eps: 0.100000
 1477668/2000000: episode: 6943, duration: 6.309s, episode steps: 393, steps per second: 62, episode reward: 186.895, mean reward: 0.476 [-20.027, 100.000], mean action: 2.102 [0.000, 3.000], mean observation: 0.266 [-0.917, 1.000], loss: 0.980689, mean_absolute_error: 28.925184, mean_q: 38.524742, mean_eps: 0.100000
 1478668/2000000: episode: 6944, duration: 16.830s, episode steps: 1000, steps per second: 59, episode reward: -104.405, mean reward: -0.104 [-4.964, 4.711], mean action: 1.521 [0.000, 3.000], mean observation: 0.136 [-0.811, 0.937], loss: 0.826269, mean_absolute_error: 29.422402, mean_q: 39.188658, mean_eps: 0.100000
 1478737/2000000: episode: 6945, duration: 1.124s, episode steps: 69, steps per second: 61, episode reward: -58.871, mean reward: -0.853 [-100.000, 11.888], mean action: 1.942 [0.000, 3.000], mean observation: -0.001 [-1.332, 1.000], loss: 0.630546, mean_absolute_error: 28.948215, mean_q: 38.685360, mean_eps: 0.100000
 1478797/2000000: episode: 6946, duration: 0.933s, episode steps: 60, steps per second: 64, episode reward: -99.410, mean reward: -1.657 [-100.000, 14.023], mean action: 1.217 [0.000, 3.000], mean observation: -0.101 [-1.468, 4.245], loss: 0.585186, mean_absolute_error: 28.329728, mean_q: 37.970011, mean_eps: 0.100000
 1478861/2000000: episode: 6947, duration: 0.990s, episode steps: 64, steps per second: 65, episode reward: -89.961, mean reward: -1.406 [-100.000, 12.555], mean action: 1.266 [0.000, 3.000], mean observation: -0.104 [-1.437, 1.000], loss: 0.939480, mean_absolute_error: 29.401792, mean_q: 39.241128, mean_eps: 0.100000
 1478951/2000000: episode: 6948, duration: 1.368s, episode steps: 90, steps per second: 66, episode reward: -74.536, mean reward: -0.828 [-100.000, 20.273], mean action: 0.967 [0.000, 3.000], mean observation: -0.017 [-1.429, 1.000], loss: 0.832370, mean_absolute_error: 29.072156, mean_q: 38.778033, mean_eps: 0.100000
 1479043/2000000: episode: 6949, duration: 1.422s, episode steps: 92, steps per second: 65, episode reward: -76.702, mean reward: -0.834 [-100.000, 22.526], mean action: 1.641 [0.000, 3.000], mean observation: -0.010 [-1.509, 1.000], loss: 1.140651, mean_absolute_error: 30.931519, mean_q: 40.842212, mean_eps: 0.100000
 1479141/2000000: episode: 6950, duration: 1.537s, episode steps: 98, steps per second: 64, episode reward: -97.747, mean reward: -0.997 [-100.000, 29.277], mean action: 1.704 [0.000, 3.000], mean observation: -0.039 [-1.548, 1.025], loss: 0.644759, mean_absolute_error: 27.987231, mean_q: 36.962147, mean_eps: 0.100000
 1479243/2000000: episode: 6951, duration: 1.559s, episode steps: 102, steps per second: 65, episode reward: -42.049, mean reward: -0.412 [-100.000, 14.437], mean action: 1.363 [0.000, 3.000], mean observation: 0.083 [-1.394, 1.000], loss: 0.941149, mean_absolute_error: 28.428024, mean_q: 37.832553, mean_eps: 0.100000
 1479338/2000000: episode: 6952, duration: 1.502s, episode steps: 95, steps per second: 63, episode reward: -45.675, mean reward: -0.481 [-100.000, 21.805], mean action: 2.000 [0.000, 3.000], mean observation: 0.070 [-1.335, 2.143], loss: 0.734455, mean_absolute_error: 28.597725, mean_q: 37.799591, mean_eps: 0.100000
 1479406/2000000: episode: 6953, duration: 1.078s, episode steps: 68, steps per second: 63, episode reward: -103.948, mean reward: -1.529 [-100.000, 18.674], mean action: 1.397 [0.000, 3.000], mean observation: -0.080 [-1.481, 1.000], loss: 0.800701, mean_absolute_error: 29.112059, mean_q: 39.045047, mean_eps: 0.100000
 1479492/2000000: episode: 6954, duration: 1.355s, episode steps: 86, steps per second: 63, episode reward: -62.625, mean reward: -0.728 [-100.000, 12.180], mean action: 1.581 [0.000, 3.000], mean observation: 0.072 [-1.251, 2.317], loss: 0.714956, mean_absolute_error: 29.102568, mean_q: 38.742117, mean_eps: 0.100000
 1479581/2000000: episode: 6955, duration: 1.397s, episode steps: 89, steps per second: 64, episode reward: -79.910, mean reward: -0.898 [-100.000, 28.030], mean action: 1.292 [0.000, 3.000], mean observation: 0.000 [-1.439, 1.000], loss: 0.732605, mean_absolute_error: 29.526590, mean_q: 39.516453, mean_eps: 0.100000
 1479646/2000000: episode: 6956, duration: 1.005s, episode steps: 65, steps per second: 65, episode reward: -70.195, mean reward: -1.080 [-100.000, 14.307], mean action: 1.338 [0.000, 3.000], mean observation: -0.021 [-3.245, 1.000], loss: 0.873078, mean_absolute_error: 28.586597, mean_q: 37.737773, mean_eps: 0.100000
 1479725/2000000: episode: 6957, duration: 1.250s, episode steps: 79, steps per second: 63, episode reward: -79.494, mean reward: -1.006 [-100.000, 11.374], mean action: 1.152 [0.000, 3.000], mean observation: -0.073 [-1.145, 1.000], loss: 0.537877, mean_absolute_error: 27.988486, mean_q: 37.552966, mean_eps: 0.100000
 1479901/2000000: episode: 6958, duration: 2.766s, episode steps: 176, steps per second: 64, episode reward: -42.350, mean reward: -0.241 [-100.000, 6.703], mean action: 1.773 [0.000, 3.000], mean observation: 0.106 [-1.064, 1.000], loss: 0.801190, mean_absolute_error: 27.815253, mean_q: 37.087824, mean_eps: 0.100000
 1480047/2000000: episode: 6959, duration: 2.287s, episode steps: 146, steps per second: 64, episode reward: -47.662, mean reward: -0.326 [-100.000, 14.288], mean action: 1.438 [0.000, 3.000], mean observation: 0.006 [-1.100, 1.000], loss: 0.882821, mean_absolute_error: 30.831201, mean_q: 40.658264, mean_eps: 0.100000
 1480335/2000000: episode: 6960, duration: 4.515s, episode steps: 288, steps per second: 64, episode reward: 239.710, mean reward: 0.832 [-3.078, 100.000], mean action: 1.233 [0.000, 3.000], mean observation: 0.070 [-0.602, 1.000], loss: 0.701584, mean_absolute_error: 29.341677, mean_q: 39.109407, mean_eps: 0.100000
 1480405/2000000: episode: 6961, duration: 1.111s, episode steps: 70, steps per second: 63, episode reward: -54.659, mean reward: -0.781 [-100.000, 13.507], mean action: 1.200 [0.000, 3.000], mean observation: 0.060 [-1.149, 1.000], loss: 0.843582, mean_absolute_error: 29.987219, mean_q: 39.898835, mean_eps: 0.100000
 1480484/2000000: episode: 6962, duration: 1.232s, episode steps: 79, steps per second: 64, episode reward: -90.845, mean reward: -1.150 [-100.000, 7.033], mean action: 1.076 [0.000, 3.000], mean observation: 0.026 [-3.079, 1.000], loss: 0.460615, mean_absolute_error: 29.150040, mean_q: 39.164524, mean_eps: 0.100000
 1480552/2000000: episode: 6963, duration: 1.094s, episode steps: 68, steps per second: 62, episode reward: -95.411, mean reward: -1.403 [-100.000, 23.613], mean action: 1.088 [0.000, 3.000], mean observation: -0.067 [-1.317, 3.614], loss: 0.571623, mean_absolute_error: 29.783154, mean_q: 39.736369, mean_eps: 0.100000
 1480668/2000000: episode: 6964, duration: 1.876s, episode steps: 116, steps per second: 62, episode reward: -51.861, mean reward: -0.447 [-100.000, 8.873], mean action: 1.802 [0.000, 3.000], mean observation: -0.065 [-0.930, 2.914], loss: 0.874274, mean_absolute_error: 29.578009, mean_q: 38.884938, mean_eps: 0.100000
 1480881/2000000: episode: 6965, duration: 3.348s, episode steps: 213, steps per second: 64, episode reward: 215.713, mean reward: 1.013 [-17.668, 100.000], mean action: 1.277 [0.000, 3.000], mean observation: 0.142 [-0.902, 1.000], loss: 0.966097, mean_absolute_error: 29.479474, mean_q: 39.050113, mean_eps: 0.100000
 1480957/2000000: episode: 6966, duration: 1.175s, episode steps: 76, steps per second: 65, episode reward: -69.602, mean reward: -0.916 [-100.000, 14.776], mean action: 1.211 [0.000, 3.000], mean observation: -0.062 [-1.361, 2.487], loss: 0.643675, mean_absolute_error: 30.029376, mean_q: 39.904018, mean_eps: 0.100000
 1481055/2000000: episode: 6967, duration: 1.495s, episode steps: 98, steps per second: 66, episode reward: -51.838, mean reward: -0.529 [-100.000, 19.630], mean action: 0.857 [0.000, 3.000], mean observation: 0.064 [-1.325, 1.000], loss: 0.864438, mean_absolute_error: 29.128978, mean_q: 38.140095, mean_eps: 0.100000
 1481153/2000000: episode: 6968, duration: 1.548s, episode steps: 98, steps per second: 63, episode reward: -71.375, mean reward: -0.728 [-100.000, 17.163], mean action: 0.949 [0.000, 3.000], mean observation: -0.024 [-1.328, 1.000], loss: 0.698541, mean_absolute_error: 29.764070, mean_q: 39.939474, mean_eps: 0.100000
 1481242/2000000: episode: 6969, duration: 1.391s, episode steps: 89, steps per second: 64, episode reward: -50.502, mean reward: -0.567 [-100.000, 12.844], mean action: 1.146 [0.000, 3.000], mean observation: 0.051 [-1.659, 1.000], loss: 0.760718, mean_absolute_error: 29.675452, mean_q: 39.554362, mean_eps: 0.100000
 1481703/2000000: episode: 6970, duration: 7.484s, episode steps: 461, steps per second: 62, episode reward: 215.973, mean reward: 0.468 [-18.613, 100.000], mean action: 0.722 [0.000, 3.000], mean observation: 0.195 [-0.676, 1.000], loss: 0.870417, mean_absolute_error: 28.959886, mean_q: 38.362603, mean_eps: 0.100000
 1482081/2000000: episode: 6971, duration: 6.224s, episode steps: 378, steps per second: 61, episode reward: 196.708, mean reward: 0.520 [-8.653, 100.000], mean action: 0.839 [0.000, 3.000], mean observation: 0.170 [-0.680, 1.058], loss: 0.904859, mean_absolute_error: 29.204862, mean_q: 38.753777, mean_eps: 0.100000
 1482180/2000000: episode: 6972, duration: 1.557s, episode steps: 99, steps per second: 64, episode reward: -98.547, mean reward: -0.995 [-100.000, 13.964], mean action: 0.929 [0.000, 3.000], mean observation: -0.041 [-1.258, 3.681], loss: 0.805941, mean_absolute_error: 29.823004, mean_q: 39.760126, mean_eps: 0.100000
 1482254/2000000: episode: 6973, duration: 1.184s, episode steps: 74, steps per second: 63, episode reward: -67.733, mean reward: -0.915 [-100.000, 15.874], mean action: 1.851 [0.000, 3.000], mean observation: 0.009 [-1.289, 1.000], loss: 1.040190, mean_absolute_error: 29.561709, mean_q: 39.561710, mean_eps: 0.100000
 1482318/2000000: episode: 6974, duration: 0.985s, episode steps: 64, steps per second: 65, episode reward: -91.431, mean reward: -1.429 [-100.000, 18.153], mean action: 1.172 [0.000, 3.000], mean observation: -0.062 [-1.432, 1.000], loss: 0.906571, mean_absolute_error: 28.330884, mean_q: 37.349046, mean_eps: 0.100000
 1482381/2000000: episode: 6975, duration: 1.002s, episode steps: 63, steps per second: 63, episode reward: -32.917, mean reward: -0.522 [-100.000, 24.975], mean action: 1.333 [0.000, 3.000], mean observation: -0.106 [-1.353, 1.000], loss: 0.891399, mean_absolute_error: 28.707149, mean_q: 38.442017, mean_eps: 0.100000
 1482452/2000000: episode: 6976, duration: 1.276s, episode steps: 71, steps per second: 56, episode reward: -50.536, mean reward: -0.712 [-100.000, 17.567], mean action: 1.028 [0.000, 3.000], mean observation: 0.016 [-1.255, 1.000], loss: 0.978084, mean_absolute_error: 29.464356, mean_q: 37.700959, mean_eps: 0.100000
 1482536/2000000: episode: 6977, duration: 1.388s, episode steps: 84, steps per second: 61, episode reward: -108.570, mean reward: -1.292 [-100.000, 9.785], mean action: 0.833 [0.000, 3.000], mean observation: 0.034 [-3.474, 1.000], loss: 1.276426, mean_absolute_error: 29.914005, mean_q: 39.162203, mean_eps: 0.100000
 1482668/2000000: episode: 6978, duration: 2.099s, episode steps: 132, steps per second: 63, episode reward: -85.756, mean reward: -0.650 [-100.000, 19.163], mean action: 1.326 [0.000, 3.000], mean observation: 0.003 [-3.643, 1.000], loss: 0.965938, mean_absolute_error: 29.859399, mean_q: 39.476134, mean_eps: 0.100000
 1482735/2000000: episode: 6979, duration: 1.063s, episode steps: 67, steps per second: 63, episode reward: -62.878, mean reward: -0.938 [-100.000, 18.952], mean action: 1.090 [0.000, 3.000], mean observation: -0.082 [-1.427, 2.960], loss: 1.344577, mean_absolute_error: 28.839623, mean_q: 37.541174, mean_eps: 0.100000
 1482960/2000000: episode: 6980, duration: 3.534s, episode steps: 225, steps per second: 64, episode reward: -92.094, mean reward: -0.409 [-100.000, 8.875], mean action: 1.529 [0.000, 3.000], mean observation: -0.012 [-1.094, 3.072], loss: 0.845805, mean_absolute_error: 28.896868, mean_q: 38.486771, mean_eps: 0.100000
 1483015/2000000: episode: 6981, duration: 0.908s, episode steps: 55, steps per second: 61, episode reward: -92.399, mean reward: -1.680 [-100.000, 13.791], mean action: 1.182 [0.000, 3.000], mean observation: -0.170 [-4.215, 1.000], loss: 0.825042, mean_absolute_error: 29.009073, mean_q: 38.883888, mean_eps: 0.100000
 1483084/2000000: episode: 6982, duration: 1.142s, episode steps: 69, steps per second: 60, episode reward: -80.041, mean reward: -1.160 [-100.000, 10.996], mean action: 1.275 [0.000, 3.000], mean observation: 0.018 [-3.221, 1.000], loss: 1.299205, mean_absolute_error: 30.034854, mean_q: 39.537354, mean_eps: 0.100000
 1483217/2000000: episode: 6983, duration: 2.131s, episode steps: 133, steps per second: 62, episode reward: -2.368, mean reward: -0.018 [-100.000, 16.549], mean action: 1.820 [0.000, 3.000], mean observation: -0.047 [-0.834, 1.000], loss: 0.702665, mean_absolute_error: 29.489987, mean_q: 39.096131, mean_eps: 0.100000
 1483311/2000000: episode: 6984, duration: 1.449s, episode steps: 94, steps per second: 65, episode reward: -84.223, mean reward: -0.896 [-100.000, 9.149], mean action: 1.000 [0.000, 3.000], mean observation: 0.050 [-3.199, 1.000], loss: 0.707004, mean_absolute_error: 29.137698, mean_q: 37.969816, mean_eps: 0.100000
 1483420/2000000: episode: 6985, duration: 1.738s, episode steps: 109, steps per second: 63, episode reward: -17.857, mean reward: -0.164 [-100.000, 16.829], mean action: 1.569 [0.000, 3.000], mean observation: -0.098 [-2.302, 1.000], loss: 0.963195, mean_absolute_error: 27.227533, mean_q: 36.366248, mean_eps: 0.100000
 1483489/2000000: episode: 6986, duration: 1.133s, episode steps: 69, steps per second: 61, episode reward: -78.175, mean reward: -1.133 [-100.000, 17.474], mean action: 0.783 [0.000, 3.000], mean observation: -0.015 [-1.372, 1.000], loss: 1.040188, mean_absolute_error: 33.540015, mean_q: 44.795975, mean_eps: 0.100000
 1483563/2000000: episode: 6987, duration: 1.125s, episode steps: 74, steps per second: 66, episode reward: -24.254, mean reward: -0.328 [-100.000, 17.184], mean action: 1.378 [0.000, 3.000], mean observation: 0.044 [-1.146, 1.000], loss: 1.013853, mean_absolute_error: 30.130811, mean_q: 38.921967, mean_eps: 0.100000
 1483646/2000000: episode: 6988, duration: 1.306s, episode steps: 83, steps per second: 64, episode reward: -62.202, mean reward: -0.749 [-100.000, 11.822], mean action: 1.000 [0.000, 3.000], mean observation: 0.055 [-2.730, 1.000], loss: 1.290597, mean_absolute_error: 29.293922, mean_q: 38.312405, mean_eps: 0.100000
 1483741/2000000: episode: 6989, duration: 1.490s, episode steps: 95, steps per second: 64, episode reward: -76.524, mean reward: -0.806 [-100.000, 19.715], mean action: 1.800 [0.000, 3.000], mean observation: 0.056 [-2.675, 1.000], loss: 0.854726, mean_absolute_error: 28.432978, mean_q: 38.042535, mean_eps: 0.100000
 1483823/2000000: episode: 6990, duration: 1.313s, episode steps: 82, steps per second: 62, episode reward: -101.577, mean reward: -1.239 [-100.000, 10.367], mean action: 1.598 [0.000, 3.000], mean observation: 0.054 [-2.905, 1.000], loss: 0.744546, mean_absolute_error: 30.741142, mean_q: 40.126084, mean_eps: 0.100000
 1484344/2000000: episode: 6991, duration: 8.552s, episode steps: 521, steps per second: 61, episode reward: 180.550, mean reward: 0.347 [-23.613, 100.000], mean action: 1.549 [0.000, 3.000], mean observation: 0.269 [-0.799, 1.006], loss: 0.835701, mean_absolute_error: 29.992299, mean_q: 39.893423, mean_eps: 0.100000
 1484481/2000000: episode: 6992, duration: 2.172s, episode steps: 137, steps per second: 63, episode reward: -75.156, mean reward: -0.549 [-100.000, 7.133], mean action: 1.124 [0.000, 3.000], mean observation: 0.148 [-0.926, 1.116], loss: 1.086118, mean_absolute_error: 29.296345, mean_q: 38.617030, mean_eps: 0.100000
 1484555/2000000: episode: 6993, duration: 1.139s, episode steps: 74, steps per second: 65, episode reward: -69.533, mean reward: -0.940 [-100.000, 27.627], mean action: 1.405 [0.000, 3.000], mean observation: -0.035 [-1.379, 1.000], loss: 1.010176, mean_absolute_error: 28.976072, mean_q: 38.378239, mean_eps: 0.100000
 1484635/2000000: episode: 6994, duration: 1.234s, episode steps: 80, steps per second: 65, episode reward: -94.953, mean reward: -1.187 [-100.000, 18.815], mean action: 1.950 [0.000, 3.000], mean observation: -0.032 [-3.114, 1.000], loss: 0.705748, mean_absolute_error: 28.870598, mean_q: 38.571578, mean_eps: 0.100000
 1484777/2000000: episode: 6995, duration: 2.237s, episode steps: 142, steps per second: 63, episode reward: 14.296, mean reward: 0.101 [-100.000, 18.033], mean action: 1.873 [0.000, 3.000], mean observation: -0.030 [-0.828, 1.000], loss: 1.088699, mean_absolute_error: 29.497825, mean_q: 38.915866, mean_eps: 0.100000
 1484899/2000000: episode: 6996, duration: 1.875s, episode steps: 122, steps per second: 65, episode reward: -22.959, mean reward: -0.188 [-100.000, 16.583], mean action: 1.639 [0.000, 3.000], mean observation: 0.086 [-0.953, 1.000], loss: 1.000067, mean_absolute_error: 29.304994, mean_q: 38.726596, mean_eps: 0.100000
 1484989/2000000: episode: 6997, duration: 1.436s, episode steps: 90, steps per second: 63, episode reward: -50.259, mean reward: -0.558 [-100.000, 13.478], mean action: 1.811 [0.000, 3.000], mean observation: 0.041 [-2.449, 1.000], loss: 1.109426, mean_absolute_error: 29.939363, mean_q: 39.545983, mean_eps: 0.100000
 1485100/2000000: episode: 6998, duration: 1.719s, episode steps: 111, steps per second: 65, episode reward: -4.862, mean reward: -0.044 [-100.000, 15.561], mean action: 1.613 [0.000, 3.000], mean observation: -0.043 [-0.696, 1.000], loss: 0.817068, mean_absolute_error: 28.687442, mean_q: 38.400724, mean_eps: 0.100000
 1485272/2000000: episode: 6999, duration: 2.707s, episode steps: 172, steps per second: 64, episode reward: 25.291, mean reward: 0.147 [-100.000, 27.864], mean action: 1.512 [0.000, 3.000], mean observation: 0.008 [-1.057, 1.000], loss: 0.766338, mean_absolute_error: 29.290427, mean_q: 38.767743, mean_eps: 0.100000
 1485334/2000000: episode: 7000, duration: 0.992s, episode steps: 62, steps per second: 62, episode reward: -77.271, mean reward: -1.246 [-100.000, 12.352], mean action: 1.306 [0.000, 3.000], mean observation: -0.164 [-1.341, 3.451], loss: 0.808344, mean_absolute_error: 30.050897, mean_q: 40.029252, mean_eps: 0.100000
 1485489/2000000: episode: 7001, duration: 2.420s, episode steps: 155, steps per second: 64, episode reward: -206.286, mean reward: -1.331 [-100.000, 5.616], mean action: 1.800 [0.000, 3.000], mean observation: 0.196 [-0.927, 1.005], loss: 1.233000, mean_absolute_error: 30.086987, mean_q: 39.841322, mean_eps: 0.100000
 1485893/2000000: episode: 7002, duration: 6.400s, episode steps: 404, steps per second: 63, episode reward: -316.161, mean reward: -0.783 [-100.000, 3.995], mean action: 1.770 [0.000, 3.000], mean observation: 0.123 [-0.751, 1.100], loss: 0.910519, mean_absolute_error: 29.407940, mean_q: 38.762300, mean_eps: 0.100000
 1486153/2000000: episode: 7003, duration: 4.028s, episode steps: 260, steps per second: 65, episode reward: 238.222, mean reward: 0.916 [-11.308, 100.000], mean action: 1.396 [0.000, 3.000], mean observation: 0.031 [-0.840, 1.000], loss: 1.047552, mean_absolute_error: 29.651842, mean_q: 39.027544, mean_eps: 0.100000
 1486325/2000000: episode: 7004, duration: 2.669s, episode steps: 172, steps per second: 64, episode reward: -5.399, mean reward: -0.031 [-100.000, 16.721], mean action: 1.837 [0.000, 3.000], mean observation: 0.005 [-0.895, 1.000], loss: 0.852155, mean_absolute_error: 30.310523, mean_q: 40.446050, mean_eps: 0.100000
 1486395/2000000: episode: 7005, duration: 1.068s, episode steps: 70, steps per second: 66, episode reward: -61.420, mean reward: -0.877 [-100.000, 22.062], mean action: 1.943 [0.000, 3.000], mean observation: -0.000 [-1.295, 1.000], loss: 0.959466, mean_absolute_error: 30.865364, mean_q: 41.218145, mean_eps: 0.100000
 1486568/2000000: episode: 7006, duration: 2.703s, episode steps: 173, steps per second: 64, episode reward: -35.700, mean reward: -0.206 [-100.000, 13.039], mean action: 1.439 [0.000, 3.000], mean observation: -0.005 [-1.013, 1.000], loss: 1.132329, mean_absolute_error: 30.988406, mean_q: 40.652698, mean_eps: 0.100000
 1486666/2000000: episode: 7007, duration: 1.561s, episode steps: 98, steps per second: 63, episode reward: -51.691, mean reward: -0.527 [-100.000, 9.852], mean action: 0.888 [0.000, 3.000], mean observation: 0.048 [-2.581, 1.000], loss: 0.527783, mean_absolute_error: 31.381754, mean_q: 41.809794, mean_eps: 0.100000
 1486805/2000000: episode: 7008, duration: 2.185s, episode steps: 139, steps per second: 64, episode reward: 0.374, mean reward: 0.003 [-100.000, 22.784], mean action: 1.784 [0.000, 3.000], mean observation: -0.010 [-0.935, 1.000], loss: 0.710670, mean_absolute_error: 29.383995, mean_q: 39.341098, mean_eps: 0.100000
 1486928/2000000: episode: 7009, duration: 1.926s, episode steps: 123, steps per second: 64, episode reward: -55.379, mean reward: -0.450 [-100.000, 17.990], mean action: 1.350 [0.000, 3.000], mean observation: -0.027 [-1.101, 2.605], loss: 1.124216, mean_absolute_error: 30.664548, mean_q: 40.100758, mean_eps: 0.100000
 1487067/2000000: episode: 7010, duration: 2.140s, episode steps: 139, steps per second: 65, episode reward: -75.616, mean reward: -0.544 [-100.000, 16.937], mean action: 1.108 [0.000, 3.000], mean observation: 0.009 [-1.465, 1.023], loss: 0.999497, mean_absolute_error: 31.125839, mean_q: 41.517793, mean_eps: 0.100000
 1487160/2000000: episode: 7011, duration: 1.481s, episode steps: 93, steps per second: 63, episode reward: -36.244, mean reward: -0.390 [-100.000, 11.628], mean action: 1.430 [0.000, 3.000], mean observation: -0.127 [-1.045, 1.801], loss: 0.976564, mean_absolute_error: 30.165195, mean_q: 40.062374, mean_eps: 0.100000
 1487223/2000000: episode: 7012, duration: 0.995s, episode steps: 63, steps per second: 63, episode reward: -48.803, mean reward: -0.775 [-100.000, 19.570], mean action: 1.857 [0.000, 3.000], mean observation: -0.025 [-1.338, 1.000], loss: 0.836493, mean_absolute_error: 30.275053, mean_q: 40.596225, mean_eps: 0.100000
 1487298/2000000: episode: 7013, duration: 1.181s, episode steps: 75, steps per second: 63, episode reward: -102.267, mean reward: -1.364 [-100.000, 26.842], mean action: 1.920 [0.000, 3.000], mean observation: -0.058 [-1.417, 1.000], loss: 0.693008, mean_absolute_error: 30.584961, mean_q: 40.914620, mean_eps: 0.100000
 1487374/2000000: episode: 7014, duration: 1.189s, episode steps: 76, steps per second: 64, episode reward: -72.360, mean reward: -0.952 [-100.000, 19.894], mean action: 1.789 [0.000, 3.000], mean observation: 0.023 [-1.318, 1.000], loss: 0.555285, mean_absolute_error: 26.725719, mean_q: 35.772728, mean_eps: 0.100000
 1487471/2000000: episode: 7015, duration: 1.515s, episode steps: 97, steps per second: 64, episode reward: -47.338, mean reward: -0.488 [-100.000, 12.068], mean action: 1.938 [0.000, 3.000], mean observation: -0.022 [-1.323, 2.071], loss: 0.772616, mean_absolute_error: 32.103878, mean_q: 42.765126, mean_eps: 0.100000
 1487649/2000000: episode: 7016, duration: 2.806s, episode steps: 178, steps per second: 63, episode reward: 23.630, mean reward: 0.133 [-100.000, 15.796], mean action: 1.517 [0.000, 3.000], mean observation: -0.007 [-0.931, 1.000], loss: 1.005494, mean_absolute_error: 30.034825, mean_q: 39.847397, mean_eps: 0.100000
 1487775/2000000: episode: 7017, duration: 1.936s, episode steps: 126, steps per second: 65, episode reward: -65.076, mean reward: -0.516 [-100.000, 21.036], mean action: 1.913 [0.000, 3.000], mean observation: -0.018 [-1.108, 1.000], loss: 0.839293, mean_absolute_error: 30.502352, mean_q: 40.532770, mean_eps: 0.100000
 1487848/2000000: episode: 7018, duration: 1.173s, episode steps: 73, steps per second: 62, episode reward: -70.656, mean reward: -0.968 [-100.000, 12.551], mean action: 1.534 [0.000, 3.000], mean observation: 0.059 [-1.219, 1.000], loss: 0.780626, mean_absolute_error: 29.607364, mean_q: 39.647376, mean_eps: 0.100000
 1487913/2000000: episode: 7019, duration: 1.068s, episode steps: 65, steps per second: 61, episode reward: -47.040, mean reward: -0.724 [-100.000, 17.599], mean action: 1.154 [0.000, 3.000], mean observation: -0.068 [-1.292, 3.126], loss: 1.058491, mean_absolute_error: 30.218189, mean_q: 40.371486, mean_eps: 0.100000
 1487985/2000000: episode: 7020, duration: 1.108s, episode steps: 72, steps per second: 65, episode reward: -107.670, mean reward: -1.495 [-100.000, 15.441], mean action: 0.833 [0.000, 3.000], mean observation: -0.043 [-1.448, 3.699], loss: 1.402352, mean_absolute_error: 27.863842, mean_q: 36.730877, mean_eps: 0.100000
 1488057/2000000: episode: 7021, duration: 1.144s, episode steps: 72, steps per second: 63, episode reward: -67.482, mean reward: -0.937 [-100.000, 21.515], mean action: 1.361 [0.000, 3.000], mean observation: -0.136 [-1.192, 2.536], loss: 0.637522, mean_absolute_error: 30.524310, mean_q: 40.864499, mean_eps: 0.100000
 1488263/2000000: episode: 7022, duration: 3.190s, episode steps: 206, steps per second: 65, episode reward: -73.687, mean reward: -0.358 [-100.000, 11.039], mean action: 1.354 [0.000, 3.000], mean observation: 0.128 [-1.303, 3.898], loss: 0.908399, mean_absolute_error: 30.301907, mean_q: 40.191036, mean_eps: 0.100000
 1488361/2000000: episode: 7023, duration: 1.555s, episode steps: 98, steps per second: 63, episode reward: -73.026, mean reward: -0.745 [-100.000, 10.606], mean action: 0.969 [0.000, 3.000], mean observation: 0.058 [-2.956, 1.000], loss: 1.113649, mean_absolute_error: 29.581104, mean_q: 38.724141, mean_eps: 0.100000
 1488745/2000000: episode: 7024, duration: 6.097s, episode steps: 384, steps per second: 63, episode reward: 225.855, mean reward: 0.588 [-10.740, 100.000], mean action: 1.354 [0.000, 3.000], mean observation: 0.050 [-0.784, 1.000], loss: 0.865310, mean_absolute_error: 29.926744, mean_q: 39.478429, mean_eps: 0.100000
 1488876/2000000: episode: 7025, duration: 2.065s, episode steps: 131, steps per second: 63, episode reward: -73.476, mean reward: -0.561 [-100.000, 18.168], mean action: 1.443 [0.000, 3.000], mean observation: 0.006 [-1.383, 1.000], loss: 1.321839, mean_absolute_error: 30.994925, mean_q: 41.146820, mean_eps: 0.100000
 1488952/2000000: episode: 7026, duration: 1.243s, episode steps: 76, steps per second: 61, episode reward: -68.413, mean reward: -0.900 [-100.000, 13.650], mean action: 1.447 [0.000, 3.000], mean observation: 0.086 [-1.052, 2.595], loss: 0.921171, mean_absolute_error: 30.238689, mean_q: 40.401501, mean_eps: 0.100000
 1489019/2000000: episode: 7027, duration: 1.054s, episode steps: 67, steps per second: 64, episode reward: -63.595, mean reward: -0.949 [-100.000, 11.606], mean action: 1.537 [0.000, 3.000], mean observation: -0.054 [-1.384, 1.000], loss: 0.482416, mean_absolute_error: 28.930910, mean_q: 38.633243, mean_eps: 0.100000
 1489156/2000000: episode: 7028, duration: 2.162s, episode steps: 137, steps per second: 63, episode reward: -39.648, mean reward: -0.289 [-100.000, 17.161], mean action: 1.949 [0.000, 3.000], mean observation: -0.004 [-1.006, 1.000], loss: 0.938881, mean_absolute_error: 31.341039, mean_q: 41.639274, mean_eps: 0.100000
 1490035/2000000: episode: 7029, duration: 14.000s, episode steps: 879, steps per second: 63, episode reward: 227.097, mean reward: 0.258 [-18.749, 100.000], mean action: 1.190 [0.000, 3.000], mean observation: 0.225 [-0.904, 1.000], loss: 0.725109, mean_absolute_error: 29.436686, mean_q: 39.159259, mean_eps: 0.100000
 1490108/2000000: episode: 7030, duration: 1.187s, episode steps: 73, steps per second: 61, episode reward: -34.344, mean reward: -0.470 [-100.000, 10.907], mean action: 1.329 [0.000, 3.000], mean observation: -0.114 [-1.249, 2.580], loss: 1.362210, mean_absolute_error: 29.508428, mean_q: 39.197667, mean_eps: 0.100000
 1490296/2000000: episode: 7031, duration: 2.973s, episode steps: 188, steps per second: 63, episode reward: 187.335, mean reward: 0.996 [-6.891, 100.000], mean action: 2.016 [0.000, 3.000], mean observation: 0.182 [-1.086, 1.000], loss: 0.829777, mean_absolute_error: 30.264244, mean_q: 39.980740, mean_eps: 0.100000
 1490398/2000000: episode: 7032, duration: 1.637s, episode steps: 102, steps per second: 62, episode reward: -64.114, mean reward: -0.629 [-100.000, 12.065], mean action: 1.598 [0.000, 3.000], mean observation: 0.057 [-1.818, 1.000], loss: 0.723938, mean_absolute_error: 29.966690, mean_q: 39.794031, mean_eps: 0.100000
 1490461/2000000: episode: 7033, duration: 0.992s, episode steps: 63, steps per second: 63, episode reward: -40.879, mean reward: -0.649 [-100.000, 18.564], mean action: 1.619 [0.000, 3.000], mean observation: -0.180 [-1.344, 1.000], loss: 0.491712, mean_absolute_error: 31.506105, mean_q: 42.062392, mean_eps: 0.100000
 1490824/2000000: episode: 7034, duration: 5.793s, episode steps: 363, steps per second: 63, episode reward: 221.373, mean reward: 0.610 [-19.158, 100.000], mean action: 1.457 [0.000, 3.000], mean observation: 0.211 [-0.764, 1.000], loss: 0.786755, mean_absolute_error: 30.749441, mean_q: 40.931403, mean_eps: 0.100000
 1490902/2000000: episode: 7035, duration: 1.257s, episode steps: 78, steps per second: 62, episode reward: -69.049, mean reward: -0.885 [-100.000, 6.796], mean action: 1.987 [1.000, 3.000], mean observation: 0.056 [-1.089, 1.480], loss: 0.878647, mean_absolute_error: 29.404103, mean_q: 39.127098, mean_eps: 0.100000
 1491114/2000000: episode: 7036, duration: 3.309s, episode steps: 212, steps per second: 64, episode reward: 196.172, mean reward: 0.925 [-11.558, 100.000], mean action: 1.321 [0.000, 3.000], mean observation: 0.160 [-1.162, 1.000], loss: 0.817999, mean_absolute_error: 29.858123, mean_q: 39.790263, mean_eps: 0.100000
 1491266/2000000: episode: 7037, duration: 2.390s, episode steps: 152, steps per second: 64, episode reward: 5.226, mean reward: 0.034 [-100.000, 17.163], mean action: 1.822 [0.000, 3.000], mean observation: -0.024 [-0.995, 1.000], loss: 0.932638, mean_absolute_error: 29.584225, mean_q: 38.957330, mean_eps: 0.100000
 1491403/2000000: episode: 7038, duration: 2.113s, episode steps: 137, steps per second: 65, episode reward: -64.392, mean reward: -0.470 [-100.000, 16.336], mean action: 1.343 [0.000, 3.000], mean observation: 0.013 [-2.805, 1.000], loss: 0.768349, mean_absolute_error: 30.396845, mean_q: 40.534937, mean_eps: 0.100000
 1491700/2000000: episode: 7039, duration: 4.972s, episode steps: 297, steps per second: 60, episode reward: -395.054, mean reward: -1.330 [-100.000, 5.000], mean action: 1.832 [0.000, 3.000], mean observation: 0.183 [-0.743, 2.242], loss: 0.901325, mean_absolute_error: 29.803250, mean_q: 39.630866, mean_eps: 0.100000
 1492110/2000000: episode: 7040, duration: 6.510s, episode steps: 410, steps per second: 63, episode reward: 169.800, mean reward: 0.414 [-5.366, 100.000], mean action: 1.405 [0.000, 3.000], mean observation: 0.091 [-1.203, 1.010], loss: 0.842515, mean_absolute_error: 29.711110, mean_q: 39.356514, mean_eps: 0.100000
 1492185/2000000: episode: 7041, duration: 1.189s, episode steps: 75, steps per second: 63, episode reward: -104.234, mean reward: -1.390 [-100.000, 11.745], mean action: 1.880 [0.000, 3.000], mean observation: 0.010 [-3.544, 1.000], loss: 0.998596, mean_absolute_error: 30.891777, mean_q: 41.325887, mean_eps: 0.100000
 1492331/2000000: episode: 7042, duration: 2.256s, episode steps: 146, steps per second: 65, episode reward: -44.298, mean reward: -0.303 [-100.000, 20.560], mean action: 1.938 [0.000, 3.000], mean observation: -0.024 [-1.118, 1.000], loss: 1.063652, mean_absolute_error: 30.936448, mean_q: 40.591693, mean_eps: 0.100000
 1492463/2000000: episode: 7043, duration: 2.040s, episode steps: 132, steps per second: 65, episode reward: -66.393, mean reward: -0.503 [-100.000, 20.544], mean action: 1.568 [0.000, 3.000], mean observation: 0.005 [-1.310, 1.000], loss: 0.735821, mean_absolute_error: 29.522874, mean_q: 39.267871, mean_eps: 0.100000
 1492540/2000000: episode: 7044, duration: 1.280s, episode steps: 77, steps per second: 60, episode reward: -48.816, mean reward: -0.634 [-100.000, 17.989], mean action: 1.935 [0.000, 3.000], mean observation: 0.031 [-1.655, 1.000], loss: 1.251703, mean_absolute_error: 29.385643, mean_q: 38.362907, mean_eps: 0.100000
 1492671/2000000: episode: 7045, duration: 2.055s, episode steps: 131, steps per second: 64, episode reward: -54.704, mean reward: -0.418 [-100.000, 19.684], mean action: 1.824 [0.000, 3.000], mean observation: 0.008 [-2.072, 1.000], loss: 0.761354, mean_absolute_error: 31.226856, mean_q: 41.246969, mean_eps: 0.100000
 1492740/2000000: episode: 7046, duration: 1.128s, episode steps: 69, steps per second: 61, episode reward: -36.906, mean reward: -0.535 [-100.000, 16.152], mean action: 1.899 [1.000, 3.000], mean observation: 0.032 [-2.511, 1.000], loss: 0.686677, mean_absolute_error: 28.176037, mean_q: 37.193540, mean_eps: 0.100000
 1492827/2000000: episode: 7047, duration: 1.367s, episode steps: 87, steps per second: 64, episode reward: -87.226, mean reward: -1.003 [-100.000, 12.491], mean action: 1.586 [0.000, 3.000], mean observation: 0.060 [-3.331, 1.000], loss: 0.697674, mean_absolute_error: 30.850637, mean_q: 40.677996, mean_eps: 0.100000
 1493132/2000000: episode: 7048, duration: 4.860s, episode steps: 305, steps per second: 63, episode reward: 167.304, mean reward: 0.549 [-13.516, 100.000], mean action: 2.049 [0.000, 3.000], mean observation: 0.185 [-0.830, 1.000], loss: 0.683861, mean_absolute_error: 30.485300, mean_q: 40.588180, mean_eps: 0.100000
 1493207/2000000: episode: 7049, duration: 1.197s, episode steps: 75, steps per second: 63, episode reward: -56.873, mean reward: -0.758 [-100.000, 18.475], mean action: 1.547 [0.000, 3.000], mean observation: -0.050 [-1.372, 1.000], loss: 0.786996, mean_absolute_error: 29.619642, mean_q: 38.879874, mean_eps: 0.100000
 1493281/2000000: episode: 7050, duration: 1.170s, episode steps: 74, steps per second: 63, episode reward: -93.285, mean reward: -1.261 [-100.000, 14.888], mean action: 1.189 [0.000, 3.000], mean observation: -0.011 [-3.314, 1.000], loss: 1.043787, mean_absolute_error: 29.558502, mean_q: 39.067796, mean_eps: 0.100000
 1493359/2000000: episode: 7051, duration: 1.183s, episode steps: 78, steps per second: 66, episode reward: -96.113, mean reward: -1.232 [-100.000, 16.103], mean action: 1.397 [0.000, 3.000], mean observation: -0.023 [-1.365, 1.000], loss: 0.876348, mean_absolute_error: 28.872650, mean_q: 38.463563, mean_eps: 0.100000
 1493427/2000000: episode: 7052, duration: 1.062s, episode steps: 68, steps per second: 64, episode reward: -31.094, mean reward: -0.457 [-100.000, 18.331], mean action: 1.971 [1.000, 3.000], mean observation: 0.005 [-1.323, 1.000], loss: 0.713972, mean_absolute_error: 29.409565, mean_q: 39.027649, mean_eps: 0.100000
 1493503/2000000: episode: 7053, duration: 1.179s, episode steps: 76, steps per second: 64, episode reward: -23.491, mean reward: -0.309 [-100.000, 14.062], mean action: 1.500 [0.000, 3.000], mean observation: 0.022 [-1.269, 1.000], loss: 1.303736, mean_absolute_error: 30.406397, mean_q: 40.129895, mean_eps: 0.100000
 1493590/2000000: episode: 7054, duration: 1.359s, episode steps: 87, steps per second: 64, episode reward: -71.205, mean reward: -0.818 [-100.000, 19.775], mean action: 1.609 [0.000, 3.000], mean observation: -0.042 [-1.314, 1.000], loss: 0.640715, mean_absolute_error: 29.213857, mean_q: 39.127438, mean_eps: 0.100000
 1493722/2000000: episode: 7055, duration: 2.036s, episode steps: 132, steps per second: 65, episode reward: -5.578, mean reward: -0.042 [-100.000, 12.414], mean action: 1.106 [0.000, 3.000], mean observation: 0.130 [-1.367, 1.002], loss: 0.691359, mean_absolute_error: 30.557676, mean_q: 40.550880, mean_eps: 0.100000
 1493791/2000000: episode: 7056, duration: 1.067s, episode steps: 69, steps per second: 65, episode reward: -65.643, mean reward: -0.951 [-100.000, 19.846], mean action: 1.275 [0.000, 3.000], mean observation: -0.102 [-1.301, 1.000], loss: 0.717998, mean_absolute_error: 29.050628, mean_q: 38.794629, mean_eps: 0.100000
 1493882/2000000: episode: 7057, duration: 1.426s, episode steps: 91, steps per second: 64, episode reward: -72.743, mean reward: -0.799 [-100.000, 12.943], mean action: 1.308 [0.000, 3.000], mean observation: 0.080 [-1.156, 1.928], loss: 0.615190, mean_absolute_error: 29.530158, mean_q: 39.223446, mean_eps: 0.100000
 1494036/2000000: episode: 7058, duration: 2.412s, episode steps: 154, steps per second: 64, episode reward: -36.295, mean reward: -0.236 [-100.000, 20.847], mean action: 1.604 [0.000, 3.000], mean observation: -0.001 [-1.277, 1.000], loss: 1.029541, mean_absolute_error: 30.024763, mean_q: 39.074914, mean_eps: 0.100000
 1494149/2000000: episode: 7059, duration: 1.802s, episode steps: 113, steps per second: 63, episode reward: -8.724, mean reward: -0.077 [-100.000, 9.406], mean action: 1.894 [0.000, 3.000], mean observation: 0.120 [-0.841, 1.367], loss: 0.771339, mean_absolute_error: 30.146955, mean_q: 40.176587, mean_eps: 0.100000
 1494285/2000000: episode: 7060, duration: 2.096s, episode steps: 136, steps per second: 65, episode reward: -35.835, mean reward: -0.263 [-100.000, 15.442], mean action: 1.926 [0.000, 3.000], mean observation: 0.013 [-1.132, 1.000], loss: 0.877794, mean_absolute_error: 30.572955, mean_q: 40.520045, mean_eps: 0.100000
 1494357/2000000: episode: 7061, duration: 1.139s, episode steps: 72, steps per second: 63, episode reward: -8.738, mean reward: -0.121 [-100.000, 13.645], mean action: 1.583 [0.000, 3.000], mean observation: -0.124 [-1.204, 2.438], loss: 0.861065, mean_absolute_error: 30.891386, mean_q: 40.974173, mean_eps: 0.100000
 1494440/2000000: episode: 7062, duration: 1.300s, episode steps: 83, steps per second: 64, episode reward: -112.129, mean reward: -1.351 [-100.000, 12.480], mean action: 0.807 [0.000, 3.000], mean observation: 0.024 [-3.363, 1.000], loss: 1.066592, mean_absolute_error: 29.844895, mean_q: 39.345941, mean_eps: 0.100000
 1494503/2000000: episode: 7063, duration: 0.991s, episode steps: 63, steps per second: 64, episode reward: -33.760, mean reward: -0.536 [-100.000, 16.583], mean action: 1.635 [0.000, 3.000], mean observation: -0.010 [-1.304, 1.000], loss: 0.644282, mean_absolute_error: 30.712319, mean_q: 40.936992, mean_eps: 0.100000
 1494591/2000000: episode: 7064, duration: 1.339s, episode steps: 88, steps per second: 66, episode reward: -85.142, mean reward: -0.968 [-100.000, 12.412], mean action: 1.773 [0.000, 3.000], mean observation: -0.019 [-1.392, 1.000], loss: 1.013367, mean_absolute_error: 29.470280, mean_q: 38.363222, mean_eps: 0.100000
 1494721/2000000: episode: 7065, duration: 2.033s, episode steps: 130, steps per second: 64, episode reward: -28.336, mean reward: -0.218 [-100.000, 10.747], mean action: 1.646 [0.000, 3.000], mean observation: 0.125 [-0.903, 1.155], loss: 0.790699, mean_absolute_error: 30.698289, mean_q: 40.611919, mean_eps: 0.100000
 1494990/2000000: episode: 7066, duration: 4.150s, episode steps: 269, steps per second: 65, episode reward: 162.892, mean reward: 0.606 [-10.856, 100.000], mean action: 1.067 [0.000, 3.000], mean observation: 0.166 [-1.319, 1.000], loss: 1.161075, mean_absolute_error: 30.685158, mean_q: 40.591780, mean_eps: 0.100000
 1495075/2000000: episode: 7067, duration: 1.331s, episode steps: 85, steps per second: 64, episode reward: -65.443, mean reward: -0.770 [-100.000, 10.805], mean action: 2.012 [0.000, 3.000], mean observation: -0.107 [-1.084, 1.564], loss: 0.715670, mean_absolute_error: 29.983048, mean_q: 39.622951, mean_eps: 0.100000
 1495239/2000000: episode: 7068, duration: 2.554s, episode steps: 164, steps per second: 64, episode reward: -5.414, mean reward: -0.033 [-100.000, 22.641], mean action: 1.835 [0.000, 3.000], mean observation: 0.017 [-1.161, 1.000], loss: 0.611991, mean_absolute_error: 29.397168, mean_q: 39.225147, mean_eps: 0.100000
 1495302/2000000: episode: 7069, duration: 1.004s, episode steps: 63, steps per second: 63, episode reward: -41.023, mean reward: -0.651 [-100.000, 15.660], mean action: 1.984 [1.000, 3.000], mean observation: -0.050 [-1.402, 1.000], loss: 0.556138, mean_absolute_error: 30.079944, mean_q: 40.079985, mean_eps: 0.100000
 1495368/2000000: episode: 7070, duration: 1.059s, episode steps: 66, steps per second: 62, episode reward: -25.967, mean reward: -0.393 [-100.000, 18.765], mean action: 1.909 [0.000, 3.000], mean observation: -0.009 [-1.320, 1.000], loss: 1.134151, mean_absolute_error: 28.538533, mean_q: 38.176081, mean_eps: 0.100000
 1495531/2000000: episode: 7071, duration: 2.555s, episode steps: 163, steps per second: 64, episode reward: -24.338, mean reward: -0.149 [-100.000, 11.737], mean action: 1.871 [0.000, 3.000], mean observation: 0.011 [-1.028, 1.000], loss: 1.008009, mean_absolute_error: 29.134639, mean_q: 38.706649, mean_eps: 0.100000
 1495627/2000000: episode: 7072, duration: 1.493s, episode steps: 96, steps per second: 64, episode reward: -47.246, mean reward: -0.492 [-100.000, 17.411], mean action: 1.083 [0.000, 3.000], mean observation: 0.021 [-2.824, 1.000], loss: 0.684323, mean_absolute_error: 30.552135, mean_q: 40.917309, mean_eps: 0.100000
 1495719/2000000: episode: 7073, duration: 1.417s, episode steps: 92, steps per second: 65, episode reward: -48.665, mean reward: -0.529 [-100.000, 18.347], mean action: 0.837 [0.000, 3.000], mean observation: 0.046 [-2.236, 1.000], loss: 0.835349, mean_absolute_error: 27.999340, mean_q: 37.125182, mean_eps: 0.100000
 1495849/2000000: episode: 7074, duration: 2.153s, episode steps: 130, steps per second: 60, episode reward: -47.457, mean reward: -0.365 [-100.000, 15.699], mean action: 1.854 [0.000, 3.000], mean observation: -0.030 [-1.084, 2.551], loss: 0.738758, mean_absolute_error: 29.788528, mean_q: 39.807785, mean_eps: 0.100000
 1495915/2000000: episode: 7075, duration: 1.100s, episode steps: 66, steps per second: 60, episode reward: -54.178, mean reward: -0.821 [-100.000, 19.447], mean action: 1.712 [0.000, 3.000], mean observation: -0.036 [-1.375, 1.000], loss: 0.934263, mean_absolute_error: 29.915686, mean_q: 39.527692, mean_eps: 0.100000
 1496267/2000000: episode: 7076, duration: 5.484s, episode steps: 352, steps per second: 64, episode reward: 237.967, mean reward: 0.676 [-7.650, 100.000], mean action: 0.915 [0.000, 3.000], mean observation: 0.130 [-1.127, 1.000], loss: 0.926509, mean_absolute_error: 31.422851, mean_q: 41.658640, mean_eps: 0.100000
 1496421/2000000: episode: 7077, duration: 2.442s, episode steps: 154, steps per second: 63, episode reward: 11.216, mean reward: 0.073 [-100.000, 12.479], mean action: 1.610 [0.000, 3.000], mean observation: -0.013 [-1.148, 1.000], loss: 1.236517, mean_absolute_error: 29.341061, mean_q: 38.554433, mean_eps: 0.100000
 1496499/2000000: episode: 7078, duration: 1.181s, episode steps: 78, steps per second: 66, episode reward: -41.305, mean reward: -0.530 [-100.000, 13.999], mean action: 1.705 [0.000, 3.000], mean observation: -0.028 [-1.197, 1.000], loss: 0.836467, mean_absolute_error: 28.915793, mean_q: 38.742443, mean_eps: 0.100000
 1497259/2000000: episode: 7079, duration: 13.101s, episode steps: 760, steps per second: 58, episode reward: -103.382, mean reward: -0.136 [-100.000, 18.155], mean action: 1.770 [0.000, 3.000], mean observation: 0.100 [-0.873, 1.000], loss: 0.871542, mean_absolute_error: 30.382638, mean_q: 40.207518, mean_eps: 0.100000
 1497351/2000000: episode: 7080, duration: 1.448s, episode steps: 92, steps per second: 64, episode reward: -59.621, mean reward: -0.648 [-100.000, 13.074], mean action: 1.652 [0.000, 3.000], mean observation: -0.079 [-1.175, 1.000], loss: 0.834464, mean_absolute_error: 29.150232, mean_q: 38.784645, mean_eps: 0.100000
 1497452/2000000: episode: 7081, duration: 1.610s, episode steps: 101, steps per second: 63, episode reward: -46.666, mean reward: -0.462 [-100.000, 18.798], mean action: 1.248 [0.000, 3.000], mean observation: 0.016 [-2.859, 1.005], loss: 1.133317, mean_absolute_error: 28.845821, mean_q: 38.088491, mean_eps: 0.100000
 1497528/2000000: episode: 7082, duration: 1.246s, episode steps: 76, steps per second: 61, episode reward: -97.186, mean reward: -1.279 [-100.000, 21.371], mean action: 0.868 [0.000, 3.000], mean observation: -0.030 [-1.364, 1.000], loss: 0.727677, mean_absolute_error: 29.682970, mean_q: 39.711165, mean_eps: 0.100000
 1497601/2000000: episode: 7083, duration: 1.174s, episode steps: 73, steps per second: 62, episode reward: -17.146, mean reward: -0.235 [-100.000, 9.913], mean action: 1.822 [0.000, 3.000], mean observation: 0.003 [-1.287, 1.000], loss: 1.269351, mean_absolute_error: 29.586699, mean_q: 38.002379, mean_eps: 0.100000
 1497695/2000000: episode: 7084, duration: 1.428s, episode steps: 94, steps per second: 66, episode reward: -71.919, mean reward: -0.765 [-100.000, 12.726], mean action: 1.468 [0.000, 3.000], mean observation: 0.092 [-1.253, 1.000], loss: 0.875709, mean_absolute_error: 30.117033, mean_q: 40.006618, mean_eps: 0.100000
 1497788/2000000: episode: 7085, duration: 1.486s, episode steps: 93, steps per second: 63, episode reward: -65.486, mean reward: -0.704 [-100.000, 10.062], mean action: 1.022 [0.000, 3.000], mean observation: 0.064 [-2.596, 1.000], loss: 0.618775, mean_absolute_error: 28.938802, mean_q: 38.113864, mean_eps: 0.100000
 1497961/2000000: episode: 7086, duration: 2.743s, episode steps: 173, steps per second: 63, episode reward: 11.293, mean reward: 0.065 [-100.000, 29.478], mean action: 1.913 [0.000, 3.000], mean observation: -0.002 [-0.981, 1.000], loss: 1.158712, mean_absolute_error: 30.391777, mean_q: 40.288981, mean_eps: 0.100000
 1498036/2000000: episode: 7087, duration: 1.192s, episode steps: 75, steps per second: 63, episode reward: -114.496, mean reward: -1.527 [-100.000, 13.103], mean action: 1.920 [0.000, 3.000], mean observation: -0.042 [-3.673, 1.000], loss: 0.765055, mean_absolute_error: 30.414876, mean_q: 39.768487, mean_eps: 0.100000
 1498171/2000000: episode: 7088, duration: 2.124s, episode steps: 135, steps per second: 64, episode reward: -16.730, mean reward: -0.124 [-100.000, 15.639], mean action: 1.815 [0.000, 3.000], mean observation: -0.014 [-1.167, 1.000], loss: 0.694133, mean_absolute_error: 30.140274, mean_q: 39.379238, mean_eps: 0.100000
 1498474/2000000: episode: 7089, duration: 4.777s, episode steps: 303, steps per second: 63, episode reward: 225.073, mean reward: 0.743 [-4.641, 100.000], mean action: 0.944 [0.000, 3.000], mean observation: 0.222 [-1.403, 1.000], loss: 0.770256, mean_absolute_error: 29.790953, mean_q: 39.628718, mean_eps: 0.100000
 1498565/2000000: episode: 7090, duration: 1.446s, episode steps: 91, steps per second: 63, episode reward: -19.754, mean reward: -0.217 [-100.000, 16.582], mean action: 1.495 [0.000, 3.000], mean observation: 0.027 [-1.331, 1.000], loss: 1.212190, mean_absolute_error: 30.754142, mean_q: 40.640206, mean_eps: 0.100000
 1498667/2000000: episode: 7091, duration: 1.554s, episode steps: 102, steps per second: 66, episode reward: -36.566, mean reward: -0.358 [-100.000, 12.107], mean action: 1.490 [0.000, 3.000], mean observation: 0.052 [-1.229, 1.000], loss: 0.976832, mean_absolute_error: 30.243934, mean_q: 39.720265, mean_eps: 0.100000
 1498825/2000000: episode: 7092, duration: 2.483s, episode steps: 158, steps per second: 64, episode reward: -14.515, mean reward: -0.092 [-100.000, 20.541], mean action: 1.601 [0.000, 3.000], mean observation: 0.015 [-1.155, 1.000], loss: 0.957790, mean_absolute_error: 30.419384, mean_q: 40.310962, mean_eps: 0.100000
 1498968/2000000: episode: 7093, duration: 2.260s, episode steps: 143, steps per second: 63, episode reward: -58.242, mean reward: -0.407 [-100.000, 17.481], mean action: 1.909 [0.000, 3.000], mean observation: 0.006 [-3.201, 1.000], loss: 0.883402, mean_absolute_error: 30.865773, mean_q: 40.651191, mean_eps: 0.100000
 1499036/2000000: episode: 7094, duration: 1.124s, episode steps: 68, steps per second: 61, episode reward: -78.112, mean reward: -1.149 [-100.000, 21.192], mean action: 1.044 [0.000, 3.000], mean observation: -0.072 [-1.390, 1.000], loss: 1.010638, mean_absolute_error: 30.132718, mean_q: 40.001275, mean_eps: 0.100000
 1499145/2000000: episode: 7095, duration: 1.892s, episode steps: 109, steps per second: 58, episode reward: -12.145, mean reward: -0.111 [-100.000, 19.217], mean action: 0.963 [0.000, 3.000], mean observation: -0.002 [-1.296, 1.014], loss: 0.792288, mean_absolute_error: 31.217152, mean_q: 41.555978, mean_eps: 0.100000
 1499220/2000000: episode: 7096, duration: 1.170s, episode steps: 75, steps per second: 64, episode reward: 0.234, mean reward: 0.003 [-100.000, 17.873], mean action: 1.613 [0.000, 3.000], mean observation: -0.138 [-1.229, 1.602], loss: 0.811023, mean_absolute_error: 33.499977, mean_q: 44.701020, mean_eps: 0.100000
 1499294/2000000: episode: 7097, duration: 1.215s, episode steps: 74, steps per second: 61, episode reward: 1.798, mean reward: 0.024 [-100.000, 18.742], mean action: 1.595 [0.000, 3.000], mean observation: -0.096 [-1.236, 1.172], loss: 1.146172, mean_absolute_error: 31.338743, mean_q: 41.211392, mean_eps: 0.100000
 1499371/2000000: episode: 7098, duration: 1.192s, episode steps: 77, steps per second: 65, episode reward: -71.249, mean reward: -0.925 [-100.000, 25.141], mean action: 1.727 [0.000, 3.000], mean observation: -0.020 [-1.307, 1.000], loss: 0.654316, mean_absolute_error: 30.812000, mean_q: 41.037739, mean_eps: 0.100000
 1499505/2000000: episode: 7099, duration: 2.113s, episode steps: 134, steps per second: 63, episode reward: -18.393, mean reward: -0.137 [-100.000, 14.534], mean action: 1.373 [0.000, 3.000], mean observation: -0.010 [-1.120, 1.362], loss: 0.857666, mean_absolute_error: 29.240916, mean_q: 39.053626, mean_eps: 0.100000
 1499624/2000000: episode: 7100, duration: 1.886s, episode steps: 119, steps per second: 63, episode reward: -36.862, mean reward: -0.310 [-100.000, 13.079], mean action: 1.176 [0.000, 3.000], mean observation: -0.009 [-0.927, 1.000], loss: 1.184361, mean_absolute_error: 29.249738, mean_q: 38.063133, mean_eps: 0.100000
 1499756/2000000: episode: 7101, duration: 2.115s, episode steps: 132, steps per second: 62, episode reward: 39.989, mean reward: 0.303 [-100.000, 14.189], mean action: 1.644 [0.000, 3.000], mean observation: 0.091 [-0.970, 1.066], loss: 0.827437, mean_absolute_error: 29.361006, mean_q: 38.440201, mean_eps: 0.100000
 1499832/2000000: episode: 7102, duration: 1.234s, episode steps: 76, steps per second: 62, episode reward: -69.272, mean reward: -0.911 [-100.000, 20.257], mean action: 1.105 [0.000, 3.000], mean observation: -0.020 [-1.359, 1.000], loss: 0.694658, mean_absolute_error: 29.292927, mean_q: 38.948105, mean_eps: 0.100000
 1499904/2000000: episode: 7103, duration: 1.187s, episode steps: 72, steps per second: 61, episode reward: -92.614, mean reward: -1.286 [-100.000, 18.893], mean action: 1.125 [0.000, 3.000], mean observation: -0.024 [-1.394, 1.000], loss: 0.635830, mean_absolute_error: 30.985481, mean_q: 40.685856, mean_eps: 0.100000
 1499995/2000000: episode: 7104, duration: 1.469s, episode steps: 91, steps per second: 62, episode reward: -46.273, mean reward: -0.508 [-100.000, 18.799], mean action: 1.780 [0.000, 3.000], mean observation: -0.023 [-1.259, 1.000], loss: 0.552214, mean_absolute_error: 29.143538, mean_q: 38.966382, mean_eps: 0.100000
 1500081/2000000: episode: 7105, duration: 1.419s, episode steps: 86, steps per second: 61, episode reward: -30.090, mean reward: -0.350 [-100.000, 13.765], mean action: 1.872 [0.000, 3.000], mean observation: 0.069 [-1.142, 1.248], loss: 0.735188, mean_absolute_error: 30.127300, mean_q: 39.792060, mean_eps: 0.100000
 1500147/2000000: episode: 7106, duration: 1.023s, episode steps: 66, steps per second: 64, episode reward: -44.897, mean reward: -0.680 [-100.000, 19.933], mean action: 1.955 [0.000, 3.000], mean observation: -0.091 [-1.325, 1.000], loss: 0.628325, mean_absolute_error: 32.913610, mean_q: 43.641988, mean_eps: 0.100000
 1500216/2000000: episode: 7107, duration: 1.110s, episode steps: 69, steps per second: 62, episode reward: -39.128, mean reward: -0.567 [-100.000, 12.450], mean action: 1.652 [0.000, 3.000], mean observation: 0.001 [-2.526, 1.000], loss: 0.985484, mean_absolute_error: 30.288208, mean_q: 39.911072, mean_eps: 0.100000
 1500292/2000000: episode: 7108, duration: 1.268s, episode steps: 76, steps per second: 60, episode reward: -26.336, mean reward: -0.347 [-100.000, 11.288], mean action: 1.408 [0.000, 3.000], mean observation: -0.094 [-1.317, 2.098], loss: 1.131014, mean_absolute_error: 32.580730, mean_q: 42.524779, mean_eps: 0.100000
 1500379/2000000: episode: 7109, duration: 1.363s, episode steps: 87, steps per second: 64, episode reward: -60.110, mean reward: -0.691 [-100.000, 18.866], mean action: 1.609 [0.000, 3.000], mean observation: -0.015 [-2.359, 1.000], loss: 1.078615, mean_absolute_error: 30.905617, mean_q: 40.676195, mean_eps: 0.100000
 1500447/2000000: episode: 7110, duration: 1.063s, episode steps: 68, steps per second: 64, episode reward: -28.743, mean reward: -0.423 [-100.000, 16.929], mean action: 1.941 [0.000, 3.000], mean observation: -0.046 [-1.338, 1.000], loss: 0.504241, mean_absolute_error: 33.873557, mean_q: 45.249389, mean_eps: 0.100000
 1500560/2000000: episode: 7111, duration: 1.801s, episode steps: 113, steps per second: 63, episode reward: -21.886, mean reward: -0.194 [-100.000, 18.583], mean action: 1.398 [0.000, 3.000], mean observation: 0.105 [-1.010, 1.000], loss: 0.923425, mean_absolute_error: 28.866409, mean_q: 38.412052, mean_eps: 0.100000
 1500646/2000000: episode: 7112, duration: 1.376s, episode steps: 86, steps per second: 63, episode reward: -57.444, mean reward: -0.668 [-100.000, 16.510], mean action: 0.930 [0.000, 3.000], mean observation: 0.035 [-1.297, 1.000], loss: 1.089320, mean_absolute_error: 29.905874, mean_q: 39.391544, mean_eps: 0.100000
 1500981/2000000: episode: 7113, duration: 5.729s, episode steps: 335, steps per second: 58, episode reward: 193.543, mean reward: 0.578 [-17.113, 100.000], mean action: 1.788 [0.000, 3.000], mean observation: 0.215 [-1.117, 1.000], loss: 0.678972, mean_absolute_error: 31.126195, mean_q: 41.026007, mean_eps: 0.100000
 1501124/2000000: episode: 7114, duration: 2.241s, episode steps: 143, steps per second: 64, episode reward: -48.469, mean reward: -0.339 [-100.000, 21.484], mean action: 1.657 [0.000, 3.000], mean observation: -0.016 [-1.233, 2.154], loss: 0.904769, mean_absolute_error: 29.394352, mean_q: 39.041225, mean_eps: 0.100000
 1501210/2000000: episode: 7115, duration: 1.385s, episode steps: 86, steps per second: 62, episode reward: -1.851, mean reward: -0.022 [-100.000, 18.161], mean action: 1.453 [0.000, 3.000], mean observation: 0.043 [-1.373, 1.000], loss: 1.018514, mean_absolute_error: 30.493167, mean_q: 39.201223, mean_eps: 0.100000
 1501294/2000000: episode: 7116, duration: 1.283s, episode steps: 84, steps per second: 65, episode reward: -81.408, mean reward: -0.969 [-100.000, 20.257], mean action: 1.333 [0.000, 3.000], mean observation: -0.003 [-1.339, 1.000], loss: 0.865324, mean_absolute_error: 31.788636, mean_q: 41.964533, mean_eps: 0.100000
 1501379/2000000: episode: 7117, duration: 1.289s, episode steps: 85, steps per second: 66, episode reward: -76.033, mean reward: -0.895 [-100.000, 12.117], mean action: 1.024 [0.000, 3.000], mean observation: 0.065 [-1.280, 1.000], loss: 0.828023, mean_absolute_error: 30.874336, mean_q: 40.817274, mean_eps: 0.100000
 1501538/2000000: episode: 7118, duration: 2.509s, episode steps: 159, steps per second: 63, episode reward: -50.105, mean reward: -0.315 [-100.000, 8.517], mean action: 1.818 [0.000, 3.000], mean observation: -0.015 [-1.082, 2.343], loss: 0.810568, mean_absolute_error: 31.281930, mean_q: 41.677384, mean_eps: 0.100000
 1501660/2000000: episode: 7119, duration: 1.915s, episode steps: 122, steps per second: 64, episode reward: -74.863, mean reward: -0.614 [-100.000, 13.065], mean action: 1.426 [0.000, 3.000], mean observation: 0.081 [-1.147, 1.000], loss: 0.904622, mean_absolute_error: 32.030765, mean_q: 41.805445, mean_eps: 0.100000
 1501737/2000000: episode: 7120, duration: 1.241s, episode steps: 77, steps per second: 62, episode reward: -82.107, mean reward: -1.066 [-100.000, 20.021], mean action: 1.104 [0.000, 3.000], mean observation: -0.074 [-1.313, 1.000], loss: 2.186452, mean_absolute_error: 28.343449, mean_q: 36.920876, mean_eps: 0.100000
 1501830/2000000: episode: 7121, duration: 1.441s, episode steps: 93, steps per second: 65, episode reward: -53.846, mean reward: -0.579 [-100.000, 12.412], mean action: 1.000 [0.000, 3.000], mean observation: 0.045 [-2.318, 1.000], loss: 1.154872, mean_absolute_error: 30.703393, mean_q: 40.597365, mean_eps: 0.100000
 1501947/2000000: episode: 7122, duration: 1.822s, episode steps: 117, steps per second: 64, episode reward: -37.859, mean reward: -0.324 [-100.000, 10.177], mean action: 1.769 [0.000, 3.000], mean observation: -0.049 [-0.957, 1.591], loss: 0.662623, mean_absolute_error: 33.560056, mean_q: 44.801622, mean_eps: 0.100000
 1502028/2000000: episode: 7123, duration: 1.305s, episode steps: 81, steps per second: 62, episode reward: 10.044, mean reward: 0.124 [-100.000, 21.032], mean action: 1.543 [0.000, 3.000], mean observation: 0.038 [-1.454, 1.000], loss: 0.769954, mean_absolute_error: 31.321172, mean_q: 41.409291, mean_eps: 0.100000
 1502203/2000000: episode: 7124, duration: 2.772s, episode steps: 175, steps per second: 63, episode reward: 15.007, mean reward: 0.086 [-100.000, 18.405], mean action: 1.869 [0.000, 3.000], mean observation: -0.011 [-1.517, 1.000], loss: 0.887389, mean_absolute_error: 31.592140, mean_q: 42.226079, mean_eps: 0.100000
 1502273/2000000: episode: 7125, duration: 1.151s, episode steps: 70, steps per second: 61, episode reward: -39.450, mean reward: -0.564 [-100.000, 12.224], mean action: 1.714 [0.000, 3.000], mean observation: 0.003 [-1.230, 1.000], loss: 0.984398, mean_absolute_error: 32.119139, mean_q: 41.801747, mean_eps: 0.100000
 1502341/2000000: episode: 7126, duration: 1.078s, episode steps: 68, steps per second: 63, episode reward: -50.386, mean reward: -0.741 [-100.000, 18.920], mean action: 1.603 [0.000, 3.000], mean observation: -0.161 [-1.193, 1.000], loss: 1.535650, mean_absolute_error: 31.606944, mean_q: 41.876060, mean_eps: 0.100000
 1502418/2000000: episode: 7127, duration: 1.199s, episode steps: 77, steps per second: 64, episode reward: -82.850, mean reward: -1.076 [-100.000, 11.831], mean action: 1.052 [0.000, 3.000], mean observation: -0.013 [-1.335, 1.000], loss: 0.560769, mean_absolute_error: 32.038129, mean_q: 42.576876, mean_eps: 0.100000
 1502505/2000000: episode: 7128, duration: 1.367s, episode steps: 87, steps per second: 64, episode reward: -44.273, mean reward: -0.509 [-100.000, 17.905], mean action: 1.701 [0.000, 3.000], mean observation: -0.015 [-1.358, 1.000], loss: 1.052715, mean_absolute_error: 31.783521, mean_q: 42.464498, mean_eps: 0.100000
 1502639/2000000: episode: 7129, duration: 2.051s, episode steps: 134, steps per second: 65, episode reward: -52.651, mean reward: -0.393 [-100.000, 17.050], mean action: 1.552 [0.000, 3.000], mean observation: -0.016 [-1.047, 1.000], loss: 1.067910, mean_absolute_error: 31.066887, mean_q: 40.822965, mean_eps: 0.100000
 1502713/2000000: episode: 7130, duration: 1.177s, episode steps: 74, steps per second: 63, episode reward: -4.319, mean reward: -0.058 [-100.000, 19.425], mean action: 1.459 [0.000, 3.000], mean observation: 0.015 [-1.241, 1.000], loss: 0.906606, mean_absolute_error: 31.405678, mean_q: 41.475111, mean_eps: 0.100000
 1502844/2000000: episode: 7131, duration: 2.037s, episode steps: 131, steps per second: 64, episode reward: -63.259, mean reward: -0.483 [-100.000, 22.019], mean action: 1.237 [0.000, 3.000], mean observation: -0.001 [-1.252, 1.000], loss: 0.649914, mean_absolute_error: 30.595908, mean_q: 40.879686, mean_eps: 0.100000
 1502922/2000000: episode: 7132, duration: 1.250s, episode steps: 78, steps per second: 62, episode reward: -71.487, mean reward: -0.916 [-100.000, 21.575], mean action: 1.731 [0.000, 3.000], mean observation: -0.050 [-1.260, 1.000], loss: 0.854659, mean_absolute_error: 32.532529, mean_q: 43.045269, mean_eps: 0.100000
 1503017/2000000: episode: 7133, duration: 1.491s, episode steps: 95, steps per second: 64, episode reward: -34.892, mean reward: -0.367 [-100.000, 18.770], mean action: 1.863 [0.000, 3.000], mean observation: 0.061 [-1.205, 1.000], loss: 1.004320, mean_absolute_error: 30.246771, mean_q: 39.460142, mean_eps: 0.100000
 1503106/2000000: episode: 7134, duration: 1.364s, episode steps: 89, steps per second: 65, episode reward: -13.123, mean reward: -0.147 [-100.000, 17.729], mean action: 1.562 [0.000, 3.000], mean observation: -0.084 [-1.090, 1.054], loss: 1.006823, mean_absolute_error: 31.672005, mean_q: 41.715927, mean_eps: 0.100000
 1503350/2000000: episode: 7135, duration: 3.821s, episode steps: 244, steps per second: 64, episode reward: 210.880, mean reward: 0.864 [-10.995, 100.000], mean action: 1.455 [0.000, 3.000], mean observation: 0.151 [-0.780, 1.000], loss: 0.727803, mean_absolute_error: 30.236606, mean_q: 40.166734, mean_eps: 0.100000
 1503421/2000000: episode: 7136, duration: 1.115s, episode steps: 71, steps per second: 64, episode reward: -57.003, mean reward: -0.803 [-100.000, 10.718], mean action: 1.690 [0.000, 3.000], mean observation: 0.018 [-2.506, 1.000], loss: 0.791390, mean_absolute_error: 30.750862, mean_q: 40.805181, mean_eps: 0.100000
 1503493/2000000: episode: 7137, duration: 1.124s, episode steps: 72, steps per second: 64, episode reward: -10.537, mean reward: -0.146 [-100.000, 21.484], mean action: 1.861 [0.000, 3.000], mean observation: -0.029 [-1.288, 1.000], loss: 0.435427, mean_absolute_error: 31.480731, mean_q: 42.303199, mean_eps: 0.100000
 1503615/2000000: episode: 7138, duration: 1.843s, episode steps: 122, steps per second: 66, episode reward: -29.892, mean reward: -0.245 [-100.000, 6.630], mean action: 1.582 [0.000, 3.000], mean observation: 0.119 [-0.837, 1.000], loss: 1.334117, mean_absolute_error: 30.490525, mean_q: 40.055485, mean_eps: 0.100000
 1504129/2000000: episode: 7139, duration: 8.398s, episode steps: 514, steps per second: 61, episode reward: 198.387, mean reward: 0.386 [-11.378, 100.000], mean action: 1.451 [0.000, 3.000], mean observation: 0.132 [-0.513, 1.053], loss: 0.874658, mean_absolute_error: 29.721312, mean_q: 39.268340, mean_eps: 0.100000
 1504213/2000000: episode: 7140, duration: 1.296s, episode steps: 84, steps per second: 65, episode reward: -67.039, mean reward: -0.798 [-100.000, 11.666], mean action: 1.607 [0.000, 3.000], mean observation: 0.059 [-1.209, 2.408], loss: 0.731408, mean_absolute_error: 34.397817, mean_q: 45.697149, mean_eps: 0.100000
 1504288/2000000: episode: 7141, duration: 1.170s, episode steps: 75, steps per second: 64, episode reward: -52.771, mean reward: -0.704 [-100.000, 14.591], mean action: 1.293 [0.000, 3.000], mean observation: -0.132 [-1.263, 1.000], loss: 0.608003, mean_absolute_error: 31.931003, mean_q: 42.458736, mean_eps: 0.100000
 1504406/2000000: episode: 7142, duration: 1.853s, episode steps: 118, steps per second: 64, episode reward: -42.624, mean reward: -0.361 [-100.000, 11.946], mean action: 1.797 [0.000, 3.000], mean observation: 0.083 [-1.045, 2.360], loss: 0.728085, mean_absolute_error: 32.159037, mean_q: 42.421921, mean_eps: 0.100000
 1504493/2000000: episode: 7143, duration: 1.345s, episode steps: 87, steps per second: 65, episode reward: -101.163, mean reward: -1.163 [-100.000, 17.605], mean action: 0.655 [0.000, 3.000], mean observation: -0.003 [-1.412, 1.000], loss: 0.545640, mean_absolute_error: 32.451024, mean_q: 42.637068, mean_eps: 0.100000
 1504675/2000000: episode: 7144, duration: 2.808s, episode steps: 182, steps per second: 65, episode reward: 15.342, mean reward: 0.084 [-100.000, 17.340], mean action: 1.484 [0.000, 3.000], mean observation: 0.008 [-1.110, 1.000], loss: 1.150564, mean_absolute_error: 30.446171, mean_q: 40.359883, mean_eps: 0.100000
 1504791/2000000: episode: 7145, duration: 1.831s, episode steps: 116, steps per second: 63, episode reward: -60.269, mean reward: -0.520 [-100.000, 10.830], mean action: 1.759 [0.000, 3.000], mean observation: 0.049 [-1.119, 1.000], loss: 1.057078, mean_absolute_error: 31.844612, mean_q: 41.639362, mean_eps: 0.100000
 1504877/2000000: episode: 7146, duration: 1.345s, episode steps: 86, steps per second: 64, episode reward: -81.180, mean reward: -0.944 [-100.000, 12.195], mean action: 1.767 [0.000, 3.000], mean observation: -0.027 [-1.344, 2.905], loss: 0.814097, mean_absolute_error: 30.649337, mean_q: 40.826731, mean_eps: 0.100000
 1504974/2000000: episode: 7147, duration: 1.498s, episode steps: 97, steps per second: 65, episode reward: -91.655, mean reward: -0.945 [-100.000, 11.628], mean action: 0.979 [0.000, 3.000], mean observation: -0.013 [-3.214, 1.000], loss: 0.630995, mean_absolute_error: 30.552462, mean_q: 40.741471, mean_eps: 0.100000
 1505036/2000000: episode: 7148, duration: 0.996s, episode steps: 62, steps per second: 62, episode reward: -72.708, mean reward: -1.173 [-100.000, 6.841], mean action: 1.355 [0.000, 3.000], mean observation: -0.158 [-2.957, 1.000], loss: 0.509784, mean_absolute_error: 32.465657, mean_q: 43.091384, mean_eps: 0.100000
 1505112/2000000: episode: 7149, duration: 1.238s, episode steps: 76, steps per second: 61, episode reward: -64.339, mean reward: -0.847 [-100.000, 9.772], mean action: 1.513 [0.000, 3.000], mean observation: -0.131 [-1.060, 1.000], loss: 0.856472, mean_absolute_error: 29.737258, mean_q: 39.365790, mean_eps: 0.100000
 1505371/2000000: episode: 7150, duration: 4.076s, episode steps: 259, steps per second: 64, episode reward: 207.604, mean reward: 0.802 [-17.961, 100.000], mean action: 1.653 [0.000, 3.000], mean observation: 0.164 [-0.752, 1.070], loss: 0.849872, mean_absolute_error: 30.072800, mean_q: 39.936139, mean_eps: 0.100000
 1505434/2000000: episode: 7151, duration: 1.011s, episode steps: 63, steps per second: 62, episode reward: -72.195, mean reward: -1.146 [-100.000, 11.929], mean action: 1.889 [0.000, 3.000], mean observation: -0.075 [-1.337, 3.691], loss: 0.937790, mean_absolute_error: 32.287086, mean_q: 42.570154, mean_eps: 0.100000
 1505810/2000000: episode: 7152, duration: 5.972s, episode steps: 376, steps per second: 63, episode reward: 237.804, mean reward: 0.632 [-18.346, 100.000], mean action: 0.862 [0.000, 3.000], mean observation: 0.134 [-0.846, 1.000], loss: 0.740259, mean_absolute_error: 29.717285, mean_q: 39.403324, mean_eps: 0.100000
 1505915/2000000: episode: 7153, duration: 1.621s, episode steps: 105, steps per second: 65, episode reward: -25.836, mean reward: -0.246 [-100.000, 20.913], mean action: 1.552 [0.000, 3.000], mean observation: 0.026 [-1.351, 2.240], loss: 0.734523, mean_absolute_error: 30.641681, mean_q: 40.707847, mean_eps: 0.100000
 1506000/2000000: episode: 7154, duration: 1.350s, episode steps: 85, steps per second: 63, episode reward: -62.915, mean reward: -0.740 [-100.000, 14.150], mean action: 1.212 [0.000, 3.000], mean observation: 0.058 [-1.222, 2.131], loss: 0.724018, mean_absolute_error: 29.359255, mean_q: 38.459365, mean_eps: 0.100000
 1506133/2000000: episode: 7155, duration: 2.101s, episode steps: 133, steps per second: 63, episode reward: -8.778, mean reward: -0.066 [-100.000, 17.975], mean action: 1.489 [0.000, 3.000], mean observation: -0.003 [-0.872, 1.000], loss: 1.269394, mean_absolute_error: 30.485390, mean_q: 40.171512, mean_eps: 0.100000
 1506203/2000000: episode: 7156, duration: 1.058s, episode steps: 70, steps per second: 66, episode reward: -68.952, mean reward: -0.985 [-100.000, 10.568], mean action: 1.200 [0.000, 3.000], mean observation: -0.103 [-1.328, 2.472], loss: 0.507713, mean_absolute_error: 31.614514, mean_q: 42.023761, mean_eps: 0.100000
 1506268/2000000: episode: 7157, duration: 1.053s, episode steps: 65, steps per second: 62, episode reward: -58.818, mean reward: -0.905 [-100.000, 17.181], mean action: 1.554 [0.000, 3.000], mean observation: -0.105 [-1.364, 2.690], loss: 0.697552, mean_absolute_error: 33.003076, mean_q: 43.541263, mean_eps: 0.100000
 1506371/2000000: episode: 7158, duration: 1.627s, episode steps: 103, steps per second: 63, episode reward: -66.158, mean reward: -0.642 [-100.000, 12.065], mean action: 1.913 [0.000, 3.000], mean observation: 0.115 [-1.064, 1.560], loss: 1.100562, mean_absolute_error: 32.425292, mean_q: 43.162791, mean_eps: 0.100000
 1506746/2000000: episode: 7159, duration: 6.357s, episode steps: 375, steps per second: 59, episode reward: 215.849, mean reward: 0.576 [-18.505, 100.000], mean action: 2.371 [0.000, 3.000], mean observation: 0.244 [-0.953, 1.000], loss: 0.910938, mean_absolute_error: 30.790545, mean_q: 40.641286, mean_eps: 0.100000
 1506846/2000000: episode: 7160, duration: 1.591s, episode steps: 100, steps per second: 63, episode reward: -41.965, mean reward: -0.420 [-100.000, 20.224], mean action: 1.690 [0.000, 3.000], mean observation: 0.066 [-1.310, 1.000], loss: 0.771741, mean_absolute_error: 31.036295, mean_q: 41.306588, mean_eps: 0.100000
 1506914/2000000: episode: 7161, duration: 1.062s, episode steps: 68, steps per second: 64, episode reward: -80.524, mean reward: -1.184 [-100.000, 16.257], mean action: 1.250 [0.000, 3.000], mean observation: -0.002 [-3.335, 1.000], loss: 0.645252, mean_absolute_error: 29.125718, mean_q: 38.507526, mean_eps: 0.100000
 1506984/2000000: episode: 7162, duration: 1.121s, episode steps: 70, steps per second: 62, episode reward: -33.881, mean reward: -0.484 [-100.000, 14.535], mean action: 1.329 [0.000, 3.000], mean observation: -0.022 [-1.295, 1.000], loss: 0.914642, mean_absolute_error: 29.906014, mean_q: 40.073478, mean_eps: 0.100000
 1507127/2000000: episode: 7163, duration: 2.248s, episode steps: 143, steps per second: 64, episode reward: -42.309, mean reward: -0.296 [-100.000, 13.831], mean action: 1.776 [0.000, 3.000], mean observation: 0.006 [-1.106, 1.000], loss: 1.073489, mean_absolute_error: 31.528723, mean_q: 41.864076, mean_eps: 0.100000
 1507217/2000000: episode: 7164, duration: 1.449s, episode steps: 90, steps per second: 62, episode reward: -74.175, mean reward: -0.824 [-100.000, 19.761], mean action: 1.489 [0.000, 3.000], mean observation: -0.009 [-1.392, 1.000], loss: 0.801446, mean_absolute_error: 32.092832, mean_q: 42.451483, mean_eps: 0.100000
 1507288/2000000: episode: 7165, duration: 1.115s, episode steps: 71, steps per second: 64, episode reward: -53.566, mean reward: -0.754 [-100.000, 14.835], mean action: 1.014 [0.000, 3.000], mean observation: -0.041 [-1.370, 1.000], loss: 0.725886, mean_absolute_error: 30.171391, mean_q: 39.706548, mean_eps: 0.100000
 1507572/2000000: episode: 7166, duration: 4.503s, episode steps: 284, steps per second: 63, episode reward: 232.336, mean reward: 0.818 [-17.393, 100.000], mean action: 1.067 [0.000, 3.000], mean observation: 0.149 [-0.844, 1.000], loss: 0.810843, mean_absolute_error: 31.210361, mean_q: 41.315642, mean_eps: 0.100000
 1507686/2000000: episode: 7167, duration: 1.826s, episode steps: 114, steps per second: 62, episode reward: -38.237, mean reward: -0.335 [-100.000, 16.622], mean action: 1.456 [0.000, 3.000], mean observation: -0.029 [-1.030, 1.504], loss: 1.242055, mean_absolute_error: 30.066131, mean_q: 39.515852, mean_eps: 0.100000
 1507767/2000000: episode: 7168, duration: 1.234s, episode steps: 81, steps per second: 66, episode reward: -10.224, mean reward: -0.126 [-100.000, 12.355], mean action: 1.914 [0.000, 3.000], mean observation: 0.022 [-1.413, 1.000], loss: 0.703161, mean_absolute_error: 31.991550, mean_q: 42.321266, mean_eps: 0.100000
 1507835/2000000: episode: 7169, duration: 1.064s, episode steps: 68, steps per second: 64, episode reward: -48.809, mean reward: -0.718 [-100.000, 18.785], mean action: 1.838 [0.000, 3.000], mean observation: -0.066 [-1.259, 1.000], loss: 0.686177, mean_absolute_error: 31.067684, mean_q: 41.489217, mean_eps: 0.100000
 1508142/2000000: episode: 7170, duration: 4.860s, episode steps: 307, steps per second: 63, episode reward: 227.618, mean reward: 0.741 [-10.152, 100.000], mean action: 1.166 [0.000, 3.000], mean observation: 0.193 [-1.308, 1.000], loss: 0.839110, mean_absolute_error: 31.382170, mean_q: 41.479051, mean_eps: 0.100000
 1508228/2000000: episode: 7171, duration: 1.391s, episode steps: 86, steps per second: 62, episode reward: -79.745, mean reward: -0.927 [-100.000, 14.998], mean action: 0.860 [0.000, 3.000], mean observation: 0.014 [-3.215, 1.000], loss: 0.893224, mean_absolute_error: 30.181194, mean_q: 39.992682, mean_eps: 0.100000
 1508429/2000000: episode: 7172, duration: 3.172s, episode steps: 201, steps per second: 63, episode reward: 0.778, mean reward: 0.004 [-100.000, 17.913], mean action: 1.647 [0.000, 3.000], mean observation: -0.007 [-0.852, 1.098], loss: 1.198590, mean_absolute_error: 31.407273, mean_q: 41.626049, mean_eps: 0.100000
 1508492/2000000: episode: 7173, duration: 0.999s, episode steps: 63, steps per second: 63, episode reward: -41.677, mean reward: -0.662 [-100.000, 17.704], mean action: 1.238 [0.000, 3.000], mean observation: -0.059 [-1.339, 1.000], loss: 0.675812, mean_absolute_error: 31.849116, mean_q: 42.343344, mean_eps: 0.100000
 1508583/2000000: episode: 7174, duration: 1.410s, episode steps: 91, steps per second: 65, episode reward: -73.292, mean reward: -0.805 [-100.000, 16.910], mean action: 1.022 [0.000, 3.000], mean observation: 0.025 [-2.518, 1.000], loss: 0.634371, mean_absolute_error: 32.625977, mean_q: 43.382555, mean_eps: 0.100000
 1508684/2000000: episode: 7175, duration: 1.597s, episode steps: 101, steps per second: 63, episode reward: -44.388, mean reward: -0.439 [-100.000, 19.418], mean action: 1.248 [0.000, 3.000], mean observation: -0.028 [-1.162, 1.000], loss: 1.165170, mean_absolute_error: 32.274141, mean_q: 42.423987, mean_eps: 0.100000
 1508771/2000000: episode: 7176, duration: 1.355s, episode steps: 87, steps per second: 64, episode reward: -98.121, mean reward: -1.128 [-100.000, 11.259], mean action: 1.793 [0.000, 3.000], mean observation: 0.077 [-1.127, 1.000], loss: 0.710929, mean_absolute_error: 29.958858, mean_q: 39.424917, mean_eps: 0.100000
 1508935/2000000: episode: 7177, duration: 2.756s, episode steps: 164, steps per second: 60, episode reward: -21.361, mean reward: -0.130 [-100.000, 14.410], mean action: 1.604 [0.000, 3.000], mean observation: 0.013 [-0.886, 1.000], loss: 0.682601, mean_absolute_error: 31.051614, mean_q: 41.469811, mean_eps: 0.100000
 1509191/2000000: episode: 7178, duration: 3.960s, episode steps: 256, steps per second: 65, episode reward: 222.252, mean reward: 0.868 [-7.989, 100.000], mean action: 0.938 [0.000, 3.000], mean observation: 0.194 [-1.150, 1.292], loss: 0.760944, mean_absolute_error: 31.501107, mean_q: 41.873299, mean_eps: 0.100000
 1509403/2000000: episode: 7179, duration: 3.293s, episode steps: 212, steps per second: 64, episode reward: -45.598, mean reward: -0.215 [-100.000, 10.804], mean action: 1.608 [0.000, 3.000], mean observation: 0.003 [-1.104, 1.000], loss: 0.816559, mean_absolute_error: 31.988739, mean_q: 42.644066, mean_eps: 0.100000
 1509511/2000000: episode: 7180, duration: 1.708s, episode steps: 108, steps per second: 63, episode reward: -42.635, mean reward: -0.395 [-100.000, 20.118], mean action: 1.093 [0.000, 3.000], mean observation: -0.043 [-1.249, 1.031], loss: 0.719854, mean_absolute_error: 30.516554, mean_q: 39.914047, mean_eps: 0.100000
 1509609/2000000: episode: 7181, duration: 1.566s, episode steps: 98, steps per second: 63, episode reward: -61.829, mean reward: -0.631 [-100.000, 10.430], mean action: 1.439 [0.000, 3.000], mean observation: -0.062 [-1.095, 1.290], loss: 0.837546, mean_absolute_error: 31.362830, mean_q: 41.704455, mean_eps: 0.100000
 1509689/2000000: episode: 7182, duration: 1.248s, episode steps: 80, steps per second: 64, episode reward: -71.411, mean reward: -0.893 [-100.000, 18.002], mean action: 0.988 [0.000, 3.000], mean observation: 0.010 [-1.311, 1.000], loss: 0.861384, mean_absolute_error: 30.619913, mean_q: 40.716186, mean_eps: 0.100000
 1510634/2000000: episode: 7183, duration: 15.349s, episode steps: 945, steps per second: 62, episode reward: 142.700, mean reward: 0.151 [-25.751, 100.000], mean action: 1.090 [0.000, 3.000], mean observation: 0.200 [-0.931, 1.000], loss: 0.940717, mean_absolute_error: 31.479419, mean_q: 41.856768, mean_eps: 0.100000
 1511634/2000000: episode: 7184, duration: 16.796s, episode steps: 1000, steps per second: 60, episode reward: -80.074, mean reward: -0.080 [-5.002, 5.357], mean action: 1.629 [0.000, 3.000], mean observation: 0.129 [-0.573, 0.971], loss: 0.747236, mean_absolute_error: 31.437378, mean_q: 41.473907, mean_eps: 0.100000
 1511764/2000000: episode: 7185, duration: 2.032s, episode steps: 130, steps per second: 64, episode reward: -48.609, mean reward: -0.374 [-100.000, 14.956], mean action: 1.762 [0.000, 3.000], mean observation: 0.005 [-1.129, 1.000], loss: 0.971416, mean_absolute_error: 31.851734, mean_q: 42.145101, mean_eps: 0.100000
 1512008/2000000: episode: 7186, duration: 3.870s, episode steps: 244, steps per second: 63, episode reward: 181.222, mean reward: 0.743 [-15.995, 100.000], mean action: 1.426 [0.000, 3.000], mean observation: 0.153 [-1.018, 1.000], loss: 0.813890, mean_absolute_error: 31.395750, mean_q: 41.758420, mean_eps: 0.100000
 1512094/2000000: episode: 7187, duration: 1.352s, episode steps: 86, steps per second: 64, episode reward: -60.427, mean reward: -0.703 [-100.000, 17.138], mean action: 1.174 [0.000, 3.000], mean observation: -0.034 [-1.240, 1.000], loss: 1.016459, mean_absolute_error: 30.946923, mean_q: 40.710786, mean_eps: 0.100000
 1512207/2000000: episode: 7188, duration: 1.729s, episode steps: 113, steps per second: 65, episode reward: 4.843, mean reward: 0.043 [-100.000, 24.307], mean action: 1.673 [0.000, 3.000], mean observation: 0.116 [-1.060, 1.000], loss: 0.935267, mean_absolute_error: 31.808181, mean_q: 42.053697, mean_eps: 0.100000
 1512301/2000000: episode: 7189, duration: 1.493s, episode steps: 94, steps per second: 63, episode reward: -50.244, mean reward: -0.535 [-100.000, 12.457], mean action: 1.138 [0.000, 3.000], mean observation: 0.061 [-1.223, 1.000], loss: 0.775770, mean_absolute_error: 30.979217, mean_q: 41.441668, mean_eps: 0.100000
 1512408/2000000: episode: 7190, duration: 1.653s, episode steps: 107, steps per second: 65, episode reward: -58.511, mean reward: -0.547 [-100.000, 12.899], mean action: 1.579 [0.000, 3.000], mean observation: -0.101 [-1.408, 1.000], loss: 0.824416, mean_absolute_error: 33.047216, mean_q: 44.057202, mean_eps: 0.100000
 1512529/2000000: episode: 7191, duration: 2.026s, episode steps: 121, steps per second: 60, episode reward: 24.221, mean reward: 0.200 [-100.000, 17.152], mean action: 1.810 [0.000, 3.000], mean observation: 0.104 [-1.029, 1.000], loss: 0.890684, mean_absolute_error: 33.073827, mean_q: 43.728347, mean_eps: 0.100000
 1512633/2000000: episode: 7192, duration: 1.641s, episode steps: 104, steps per second: 63, episode reward: -92.747, mean reward: -0.892 [-100.000, 10.916], mean action: 1.644 [0.000, 3.000], mean observation: -0.031 [-1.344, 3.957], loss: 1.019605, mean_absolute_error: 31.469444, mean_q: 41.683546, mean_eps: 0.100000
 1512706/2000000: episode: 7193, duration: 1.140s, episode steps: 73, steps per second: 64, episode reward: -10.771, mean reward: -0.148 [-100.000, 19.234], mean action: 1.452 [0.000, 3.000], mean observation: -0.120 [-1.235, 1.000], loss: 1.015610, mean_absolute_error: 32.618790, mean_q: 42.439959, mean_eps: 0.100000
 1512796/2000000: episode: 7194, duration: 1.402s, episode steps: 90, steps per second: 64, episode reward: -85.510, mean reward: -0.950 [-100.000, 15.171], mean action: 1.856 [0.000, 3.000], mean observation: -0.039 [-1.263, 1.000], loss: 0.813029, mean_absolute_error: 32.817128, mean_q: 43.208647, mean_eps: 0.100000
 1512871/2000000: episode: 7195, duration: 1.181s, episode steps: 75, steps per second: 64, episode reward: -90.859, mean reward: -1.211 [-100.000, 16.591], mean action: 1.640 [0.000, 3.000], mean observation: 0.036 [-3.704, 1.000], loss: 0.840774, mean_absolute_error: 32.540041, mean_q: 43.517973, mean_eps: 0.100000
 1513451/2000000: episode: 7196, duration: 9.358s, episode steps: 580, steps per second: 62, episode reward: 170.780, mean reward: 0.294 [-18.677, 100.000], mean action: 2.407 [0.000, 3.000], mean observation: 0.266 [-1.104, 1.079], loss: 0.872199, mean_absolute_error: 30.582386, mean_q: 40.287835, mean_eps: 0.100000
 1513515/2000000: episode: 7197, duration: 1.015s, episode steps: 64, steps per second: 63, episode reward: -61.595, mean reward: -0.962 [-100.000, 21.565], mean action: 1.078 [0.000, 3.000], mean observation: -0.049 [-1.401, 1.000], loss: 0.737941, mean_absolute_error: 30.076296, mean_q: 39.900359, mean_eps: 0.100000
 1513661/2000000: episode: 7198, duration: 2.306s, episode steps: 146, steps per second: 63, episode reward: -0.918, mean reward: -0.006 [-100.000, 12.729], mean action: 1.479 [0.000, 3.000], mean observation: -0.016 [-0.953, 1.000], loss: 0.815758, mean_absolute_error: 30.790296, mean_q: 40.733077, mean_eps: 0.100000
 1513804/2000000: episode: 7199, duration: 2.249s, episode steps: 143, steps per second: 64, episode reward: -71.312, mean reward: -0.499 [-100.000, 9.122], mean action: 1.678 [0.000, 3.000], mean observation: 0.070 [-1.160, 1.000], loss: 0.832040, mean_absolute_error: 32.210563, mean_q: 42.463717, mean_eps: 0.100000
 1513937/2000000: episode: 7200, duration: 2.135s, episode steps: 133, steps per second: 62, episode reward: 22.028, mean reward: 0.166 [-100.000, 10.773], mean action: 1.744 [0.000, 3.000], mean observation: 0.106 [-0.872, 1.378], loss: 0.919784, mean_absolute_error: 28.460728, mean_q: 37.531817, mean_eps: 0.100000
 1514019/2000000: episode: 7201, duration: 1.250s, episode steps: 82, steps per second: 66, episode reward: -71.677, mean reward: -0.874 [-100.000, 17.683], mean action: 1.476 [0.000, 3.000], mean observation: 0.014 [-1.319, 1.000], loss: 1.208771, mean_absolute_error: 29.266520, mean_q: 38.375008, mean_eps: 0.100000
 1514106/2000000: episode: 7202, duration: 1.400s, episode steps: 87, steps per second: 62, episode reward: -83.986, mean reward: -0.965 [-100.000, 15.867], mean action: 1.885 [0.000, 3.000], mean observation: -0.001 [-3.012, 1.000], loss: 0.872313, mean_absolute_error: 31.241380, mean_q: 41.217858, mean_eps: 0.100000
 1515106/2000000: episode: 7203, duration: 17.493s, episode steps: 1000, steps per second: 57, episode reward: -106.978, mean reward: -0.107 [-5.682, 5.455], mean action: 1.873 [0.000, 3.000], mean observation: 0.158 [-0.695, 0.991], loss: 0.890506, mean_absolute_error: 31.974297, mean_q: 42.256935, mean_eps: 0.100000
 1515313/2000000: episode: 7204, duration: 3.266s, episode steps: 207, steps per second: 63, episode reward: -19.565, mean reward: -0.095 [-100.000, 14.051], mean action: 1.821 [0.000, 3.000], mean observation: -0.008 [-0.851, 1.027], loss: 0.745793, mean_absolute_error: 30.604064, mean_q: 40.601747, mean_eps: 0.100000
 1515399/2000000: episode: 7205, duration: 1.326s, episode steps: 86, steps per second: 65, episode reward: -15.778, mean reward: -0.183 [-100.000, 22.399], mean action: 1.919 [0.000, 3.000], mean observation: 0.072 [-1.062, 1.000], loss: 0.773879, mean_absolute_error: 31.732860, mean_q: 42.239144, mean_eps: 0.100000
 1515635/2000000: episode: 7206, duration: 3.737s, episode steps: 236, steps per second: 63, episode reward: 179.093, mean reward: 0.759 [-3.657, 100.000], mean action: 1.623 [0.000, 3.000], mean observation: 0.145 [-1.003, 1.000], loss: 1.191068, mean_absolute_error: 30.850060, mean_q: 40.073681, mean_eps: 0.100000
 1515709/2000000: episode: 7207, duration: 1.198s, episode steps: 74, steps per second: 62, episode reward: -73.006, mean reward: -0.987 [-100.000, 26.035], mean action: 1.824 [0.000, 3.000], mean observation: 0.063 [-1.181, 1.000], loss: 0.710368, mean_absolute_error: 32.018732, mean_q: 42.752171, mean_eps: 0.100000
 1515785/2000000: episode: 7208, duration: 1.193s, episode steps: 76, steps per second: 64, episode reward: -82.159, mean reward: -1.081 [-100.000, 21.397], mean action: 1.737 [0.000, 3.000], mean observation: -0.075 [-2.603, 1.000], loss: 0.879738, mean_absolute_error: 31.417607, mean_q: 41.452313, mean_eps: 0.100000
 1515873/2000000: episode: 7209, duration: 1.375s, episode steps: 88, steps per second: 64, episode reward: -61.443, mean reward: -0.698 [-100.000, 12.088], mean action: 1.920 [0.000, 3.000], mean observation: 0.105 [-1.082, 1.564], loss: 0.986369, mean_absolute_error: 30.187922, mean_q: 39.914447, mean_eps: 0.100000
 1516021/2000000: episode: 7210, duration: 2.323s, episode steps: 148, steps per second: 64, episode reward: -21.250, mean reward: -0.144 [-100.000, 18.514], mean action: 1.953 [0.000, 3.000], mean observation: -0.005 [-1.134, 1.000], loss: 0.680014, mean_absolute_error: 30.807377, mean_q: 41.113547, mean_eps: 0.100000
 1516372/2000000: episode: 7211, duration: 5.673s, episode steps: 351, steps per second: 62, episode reward: -200.339, mean reward: -0.571 [-100.000, 5.608], mean action: 1.835 [0.000, 3.000], mean observation: 0.145 [-0.588, 1.002], loss: 0.974741, mean_absolute_error: 32.543519, mean_q: 43.129092, mean_eps: 0.100000
 1516435/2000000: episode: 7212, duration: 1.003s, episode steps: 63, steps per second: 63, episode reward: -63.777, mean reward: -1.012 [-100.000, 7.644], mean action: 1.873 [0.000, 3.000], mean observation: -0.140 [-3.451, 1.000], loss: 0.729504, mean_absolute_error: 32.852918, mean_q: 43.830268, mean_eps: 0.100000
 1516525/2000000: episode: 7213, duration: 1.408s, episode steps: 90, steps per second: 64, episode reward: -53.004, mean reward: -0.589 [-100.000, 23.419], mean action: 0.967 [0.000, 3.000], mean observation: -0.030 [-1.340, 1.000], loss: 0.690672, mean_absolute_error: 31.558990, mean_q: 41.363306, mean_eps: 0.100000
 1516603/2000000: episode: 7214, duration: 1.172s, episode steps: 78, steps per second: 67, episode reward: -76.822, mean reward: -0.985 [-100.000, 17.486], mean action: 0.987 [0.000, 3.000], mean observation: 0.073 [-2.248, 1.000], loss: 0.744126, mean_absolute_error: 30.718745, mean_q: 40.938603, mean_eps: 0.100000
 1516835/2000000: episode: 7215, duration: 3.599s, episode steps: 232, steps per second: 64, episode reward: 224.136, mean reward: 0.966 [-6.648, 100.000], mean action: 1.159 [0.000, 3.000], mean observation: 0.142 [-0.732, 1.000], loss: 0.962676, mean_absolute_error: 32.382095, mean_q: 42.256710, mean_eps: 0.100000
 1517004/2000000: episode: 7216, duration: 2.696s, episode steps: 169, steps per second: 63, episode reward: -1.742, mean reward: -0.010 [-100.000, 15.977], mean action: 1.811 [0.000, 3.000], mean observation: 0.009 [-1.125, 1.000], loss: 0.961249, mean_absolute_error: 32.543932, mean_q: 42.955761, mean_eps: 0.100000
 1517078/2000000: episode: 7217, duration: 1.201s, episode steps: 74, steps per second: 62, episode reward: -85.410, mean reward: -1.154 [-100.000, 18.919], mean action: 1.878 [0.000, 3.000], mean observation: -0.028 [-1.330, 1.000], loss: 1.166022, mean_absolute_error: 30.606078, mean_q: 40.504712, mean_eps: 0.100000
 1517221/2000000: episode: 7218, duration: 2.232s, episode steps: 143, steps per second: 64, episode reward: -44.430, mean reward: -0.311 [-100.000, 17.506], mean action: 1.392 [0.000, 3.000], mean observation: -0.011 [-1.120, 1.000], loss: 0.863883, mean_absolute_error: 30.795884, mean_q: 40.859818, mean_eps: 0.100000
 1517294/2000000: episode: 7219, duration: 1.134s, episode steps: 73, steps per second: 64, episode reward: -54.413, mean reward: -0.745 [-100.000, 19.718], mean action: 1.507 [0.000, 3.000], mean observation: 0.001 [-2.314, 1.000], loss: 0.877029, mean_absolute_error: 31.109697, mean_q: 41.233524, mean_eps: 0.100000
 1517368/2000000: episode: 7220, duration: 1.188s, episode steps: 74, steps per second: 62, episode reward: -44.472, mean reward: -0.601 [-100.000, 18.680], mean action: 1.824 [0.000, 3.000], mean observation: -0.007 [-1.220, 1.677], loss: 0.700797, mean_absolute_error: 32.784571, mean_q: 43.059518, mean_eps: 0.100000
 1517436/2000000: episode: 7221, duration: 1.118s, episode steps: 68, steps per second: 61, episode reward: -31.971, mean reward: -0.470 [-100.000, 20.301], mean action: 1.162 [0.000, 3.000], mean observation: -0.088 [-1.274, 1.000], loss: 0.772886, mean_absolute_error: 30.807024, mean_q: 40.177036, mean_eps: 0.100000
 1517573/2000000: episode: 7222, duration: 2.180s, episode steps: 137, steps per second: 63, episode reward: -10.230, mean reward: -0.075 [-100.000, 21.790], mean action: 1.307 [0.000, 3.000], mean observation: -0.025 [-1.261, 1.000], loss: 0.786679, mean_absolute_error: 32.463900, mean_q: 42.565507, mean_eps: 0.100000
 1517646/2000000: episode: 7223, duration: 1.146s, episode steps: 73, steps per second: 64, episode reward: 5.170, mean reward: 0.071 [-100.000, 18.742], mean action: 1.767 [0.000, 3.000], mean observation: -0.130 [-1.213, 1.000], loss: 1.069643, mean_absolute_error: 32.729012, mean_q: 43.075240, mean_eps: 0.100000
 1517730/2000000: episode: 7224, duration: 1.296s, episode steps: 84, steps per second: 65, episode reward: -72.828, mean reward: -0.867 [-100.000, 18.057], mean action: 1.417 [0.000, 3.000], mean observation: -0.004 [-1.357, 1.000], loss: 0.926159, mean_absolute_error: 31.756963, mean_q: 42.233496, mean_eps: 0.100000
 1517817/2000000: episode: 7225, duration: 1.368s, episode steps: 87, steps per second: 64, episode reward: -46.021, mean reward: -0.529 [-100.000, 10.536], mean action: 1.793 [0.000, 3.000], mean observation: -0.065 [-1.237, 1.000], loss: 0.815627, mean_absolute_error: 31.033674, mean_q: 41.295557, mean_eps: 0.100000
 1517908/2000000: episode: 7226, duration: 1.442s, episode steps: 91, steps per second: 63, episode reward: -57.199, mean reward: -0.629 [-100.000, 16.018], mean action: 1.560 [0.000, 3.000], mean observation: 0.009 [-1.284, 1.000], loss: 0.660994, mean_absolute_error: 28.617724, mean_q: 38.040819, mean_eps: 0.100000
 1518056/2000000: episode: 7227, duration: 2.359s, episode steps: 148, steps per second: 63, episode reward: 4.620, mean reward: 0.031 [-100.000, 18.862], mean action: 1.547 [0.000, 3.000], mean observation: -0.001 [-0.936, 1.000], loss: 0.901751, mean_absolute_error: 30.899737, mean_q: 40.946360, mean_eps: 0.100000
 1518140/2000000: episode: 7228, duration: 1.393s, episode steps: 84, steps per second: 60, episode reward: -100.379, mean reward: -1.195 [-100.000, 9.877], mean action: 0.821 [0.000, 3.000], mean observation: -0.008 [-3.088, 1.000], loss: 0.859821, mean_absolute_error: 31.995767, mean_q: 42.457938, mean_eps: 0.100000
 1518236/2000000: episode: 7229, duration: 1.560s, episode steps: 96, steps per second: 62, episode reward: -16.380, mean reward: -0.171 [-100.000, 14.699], mean action: 1.365 [0.000, 3.000], mean observation: 0.030 [-1.319, 1.000], loss: 0.776145, mean_absolute_error: 32.315800, mean_q: 42.655470, mean_eps: 0.100000
 1518516/2000000: episode: 7230, duration: 4.465s, episode steps: 280, steps per second: 63, episode reward: 233.761, mean reward: 0.835 [-14.479, 100.000], mean action: 0.804 [0.000, 3.000], mean observation: 0.206 [-1.046, 1.000], loss: 0.891479, mean_absolute_error: 31.982966, mean_q: 42.248252, mean_eps: 0.100000
 1518717/2000000: episode: 7231, duration: 3.208s, episode steps: 201, steps per second: 63, episode reward: 187.103, mean reward: 0.931 [-8.347, 100.000], mean action: 2.075 [0.000, 3.000], mean observation: 0.193 [-0.968, 1.064], loss: 0.777480, mean_absolute_error: 30.887484, mean_q: 40.975756, mean_eps: 0.100000
 1519455/2000000: episode: 7232, duration: 12.223s, episode steps: 738, steps per second: 60, episode reward: 173.235, mean reward: 0.235 [-18.093, 100.000], mean action: 0.920 [0.000, 3.000], mean observation: 0.251 [-1.005, 1.013], loss: 0.877563, mean_absolute_error: 31.233210, mean_q: 41.143641, mean_eps: 0.100000
 1519600/2000000: episode: 7233, duration: 2.318s, episode steps: 145, steps per second: 63, episode reward: 20.725, mean reward: 0.143 [-100.000, 22.867], mean action: 2.021 [0.000, 3.000], mean observation: -0.030 [-1.668, 1.000], loss: 0.670093, mean_absolute_error: 31.316524, mean_q: 41.786086, mean_eps: 0.100000
 1519677/2000000: episode: 7234, duration: 1.251s, episode steps: 77, steps per second: 62, episode reward: -0.805, mean reward: -0.010 [-100.000, 21.116], mean action: 1.247 [0.000, 3.000], mean observation: -0.011 [-1.248, 1.000], loss: 0.779641, mean_absolute_error: 29.476149, mean_q: 39.293606, mean_eps: 0.100000
 1519735/2000000: episode: 7235, duration: 0.875s, episode steps: 58, steps per second: 66, episode reward: -89.157, mean reward: -1.537 [-100.000, 4.152], mean action: 1.517 [0.000, 3.000], mean observation: -0.183 [-3.595, 1.000], loss: 0.983739, mean_absolute_error: 32.569483, mean_q: 42.979829, mean_eps: 0.100000
 1519833/2000000: episode: 7236, duration: 1.548s, episode steps: 98, steps per second: 63, episode reward: -38.461, mean reward: -0.392 [-100.000, 20.250], mean action: 1.143 [0.000, 3.000], mean observation: 0.059 [-2.009, 1.000], loss: 0.921458, mean_absolute_error: 30.670707, mean_q: 41.024929, mean_eps: 0.100000
 1519902/2000000: episode: 7237, duration: 1.083s, episode steps: 69, steps per second: 64, episode reward: -53.678, mean reward: -0.778 [-100.000, 17.784], mean action: 1.478 [0.000, 3.000], mean observation: 0.024 [-2.357, 1.000], loss: 0.894359, mean_absolute_error: 32.263580, mean_q: 42.833859, mean_eps: 0.100000
 1519969/2000000: episode: 7238, duration: 1.070s, episode steps: 67, steps per second: 63, episode reward: -70.524, mean reward: -1.053 [-100.000, 13.147], mean action: 1.239 [0.000, 3.000], mean observation: -0.106 [-1.292, 2.898], loss: 0.785133, mean_absolute_error: 29.573858, mean_q: 39.459991, mean_eps: 0.100000
 1520049/2000000: episode: 7239, duration: 1.248s, episode steps: 80, steps per second: 64, episode reward: -21.930, mean reward: -0.274 [-100.000, 21.033], mean action: 1.375 [0.000, 3.000], mean observation: -0.034 [-1.311, 1.000], loss: 0.869919, mean_absolute_error: 32.624272, mean_q: 43.697953, mean_eps: 0.100000
 1520112/2000000: episode: 7240, duration: 1.178s, episode steps: 63, steps per second: 53, episode reward: -57.668, mean reward: -0.915 [-100.000, 10.222], mean action: 1.492 [0.000, 3.000], mean observation: -0.169 [-1.286, 1.000], loss: 0.672220, mean_absolute_error: 28.489145, mean_q: 37.739972, mean_eps: 0.100000
 1520218/2000000: episode: 7241, duration: 1.657s, episode steps: 106, steps per second: 64, episode reward: -3.688, mean reward: -0.035 [-100.000, 17.493], mean action: 1.132 [0.000, 3.000], mean observation: 0.013 [-1.202, 1.000], loss: 1.077256, mean_absolute_error: 31.780518, mean_q: 41.724492, mean_eps: 0.100000
 1520343/2000000: episode: 7242, duration: 1.910s, episode steps: 125, steps per second: 65, episode reward: 1.409, mean reward: 0.011 [-100.000, 18.622], mean action: 1.888 [0.000, 3.000], mean observation: 0.069 [-0.797, 1.355], loss: 0.768612, mean_absolute_error: 32.206698, mean_q: 42.299051, mean_eps: 0.100000
 1520471/2000000: episode: 7243, duration: 1.995s, episode steps: 128, steps per second: 64, episode reward: -100.586, mean reward: -0.786 [-100.000, 11.826], mean action: 1.852 [0.000, 3.000], mean observation: 0.073 [-1.009, 1.000], loss: 1.091553, mean_absolute_error: 31.421910, mean_q: 41.730134, mean_eps: 0.100000
 1520551/2000000: episode: 7244, duration: 1.249s, episode steps: 80, steps per second: 64, episode reward: -86.939, mean reward: -1.087 [-100.000, 10.816], mean action: 1.325 [0.000, 3.000], mean observation: -0.112 [-3.243, 1.000], loss: 0.557756, mean_absolute_error: 32.287902, mean_q: 42.669990, mean_eps: 0.100000
 1520627/2000000: episode: 7245, duration: 1.208s, episode steps: 76, steps per second: 63, episode reward: -97.557, mean reward: -1.284 [-100.000, 14.001], mean action: 1.908 [0.000, 3.000], mean observation: -0.032 [-1.288, 3.191], loss: 0.841666, mean_absolute_error: 29.706083, mean_q: 39.113840, mean_eps: 0.100000
 1520725/2000000: episode: 7246, duration: 1.571s, episode steps: 98, steps per second: 62, episode reward: -41.605, mean reward: -0.425 [-100.000, 18.559], mean action: 1.367 [0.000, 3.000], mean observation: 0.030 [-1.253, 1.000], loss: 0.758630, mean_absolute_error: 30.969028, mean_q: 41.027816, mean_eps: 0.100000
 1520802/2000000: episode: 7247, duration: 1.205s, episode steps: 77, steps per second: 64, episode reward: -38.727, mean reward: -0.503 [-100.000, 21.224], mean action: 1.104 [0.000, 3.000], mean observation: -0.064 [-1.250, 1.000], loss: 0.815003, mean_absolute_error: 30.281804, mean_q: 40.488001, mean_eps: 0.100000
 1520927/2000000: episode: 7248, duration: 1.915s, episode steps: 125, steps per second: 65, episode reward: -19.551, mean reward: -0.156 [-100.000, 16.921], mean action: 1.792 [0.000, 3.000], mean observation: 0.112 [-0.927, 1.000], loss: 1.080527, mean_absolute_error: 31.084106, mean_q: 41.576162, mean_eps: 0.100000
 1521022/2000000: episode: 7249, duration: 1.525s, episode steps: 95, steps per second: 62, episode reward: -42.350, mean reward: -0.446 [-100.000, 18.608], mean action: 0.958 [0.000, 3.000], mean observation: 0.037 [-1.272, 1.000], loss: 0.668713, mean_absolute_error: 30.981957, mean_q: 40.803940, mean_eps: 0.100000
 1521301/2000000: episode: 7250, duration: 4.354s, episode steps: 279, steps per second: 64, episode reward: 206.071, mean reward: 0.739 [-19.594, 100.000], mean action: 0.914 [0.000, 3.000], mean observation: 0.156 [-0.944, 1.015], loss: 1.012146, mean_absolute_error: 31.712369, mean_q: 41.734658, mean_eps: 0.100000
 1521541/2000000: episode: 7251, duration: 3.887s, episode steps: 240, steps per second: 62, episode reward: 2.762, mean reward: 0.012 [-100.000, 28.015], mean action: 1.683 [0.000, 3.000], mean observation: -0.001 [-0.669, 1.007], loss: 0.880961, mean_absolute_error: 32.406085, mean_q: 42.748863, mean_eps: 0.100000
 1521816/2000000: episode: 7252, duration: 4.555s, episode steps: 275, steps per second: 60, episode reward: 211.324, mean reward: 0.768 [-13.644, 100.000], mean action: 2.018 [0.000, 3.000], mean observation: 0.177 [-0.784, 1.000], loss: 0.967999, mean_absolute_error: 31.040354, mean_q: 40.465773, mean_eps: 0.100000
 1521907/2000000: episode: 7253, duration: 1.437s, episode steps: 91, steps per second: 63, episode reward: -54.300, mean reward: -0.597 [-100.000, 9.380], mean action: 1.220 [0.000, 3.000], mean observation: -0.075 [-1.269, 2.568], loss: 0.729826, mean_absolute_error: 31.328843, mean_q: 40.905528, mean_eps: 0.100000
 1521981/2000000: episode: 7254, duration: 1.220s, episode steps: 74, steps per second: 61, episode reward: -73.991, mean reward: -1.000 [-100.000, 16.651], mean action: 1.676 [0.000, 3.000], mean observation: -0.081 [-1.297, 1.000], loss: 0.806198, mean_absolute_error: 31.041052, mean_q: 41.518315, mean_eps: 0.100000
 1522070/2000000: episode: 7255, duration: 1.382s, episode steps: 89, steps per second: 64, episode reward: -26.779, mean reward: -0.301 [-100.000, 14.725], mean action: 1.348 [0.000, 3.000], mean observation: 0.054 [-1.131, 1.000], loss: 0.855554, mean_absolute_error: 31.884500, mean_q: 42.202308, mean_eps: 0.100000
 1522395/2000000: episode: 7256, duration: 5.202s, episode steps: 325, steps per second: 62, episode reward: 202.751, mean reward: 0.624 [-19.176, 100.000], mean action: 2.080 [0.000, 3.000], mean observation: 0.239 [-0.911, 1.212], loss: 0.890802, mean_absolute_error: 30.069581, mean_q: 39.946680, mean_eps: 0.100000
 1522546/2000000: episode: 7257, duration: 2.504s, episode steps: 151, steps per second: 60, episode reward: -9.074, mean reward: -0.060 [-100.000, 14.894], mean action: 1.305 [0.000, 3.000], mean observation: 0.007 [-1.147, 1.000], loss: 1.076949, mean_absolute_error: 32.059448, mean_q: 42.334532, mean_eps: 0.100000
 1522625/2000000: episode: 7258, duration: 1.259s, episode steps: 79, steps per second: 63, episode reward: -17.147, mean reward: -0.217 [-100.000, 20.573], mean action: 1.418 [0.000, 3.000], mean observation: 0.002 [-1.234, 1.000], loss: 0.666559, mean_absolute_error: 31.651228, mean_q: 41.564455, mean_eps: 0.100000
 1522770/2000000: episode: 7259, duration: 2.526s, episode steps: 145, steps per second: 57, episode reward: -17.896, mean reward: -0.123 [-100.000, 18.500], mean action: 1.655 [0.000, 3.000], mean observation: 0.005 [-1.141, 1.000], loss: 0.958000, mean_absolute_error: 30.872078, mean_q: 40.161608, mean_eps: 0.100000
 1522848/2000000: episode: 7260, duration: 1.669s, episode steps: 78, steps per second: 47, episode reward: -50.361, mean reward: -0.646 [-100.000, 12.526], mean action: 1.372 [0.000, 3.000], mean observation: -0.005 [-1.302, 1.000], loss: 1.048921, mean_absolute_error: 31.097107, mean_q: 41.215506, mean_eps: 0.100000
 1523848/2000000: episode: 7261, duration: 18.066s, episode steps: 1000, steps per second: 55, episode reward: 64.168, mean reward: 0.064 [-19.466, 20.815], mean action: 0.747 [0.000, 3.000], mean observation: 0.262 [-1.062, 1.000], loss: 0.780167, mean_absolute_error: 31.666817, mean_q: 41.642136, mean_eps: 0.100000
 1523934/2000000: episode: 7262, duration: 1.477s, episode steps: 86, steps per second: 58, episode reward: -5.239, mean reward: -0.061 [-100.000, 22.908], mean action: 1.221 [0.000, 3.000], mean observation: -0.055 [-1.204, 1.000], loss: 0.723518, mean_absolute_error: 33.643617, mean_q: 44.626970, mean_eps: 0.100000
 1524022/2000000: episode: 7263, duration: 1.618s, episode steps: 88, steps per second: 54, episode reward: -41.585, mean reward: -0.473 [-100.000, 16.227], mean action: 1.284 [0.000, 3.000], mean observation: -0.030 [-1.283, 1.000], loss: 0.969039, mean_absolute_error: 30.679417, mean_q: 40.222594, mean_eps: 0.100000
 1524101/2000000: episode: 7264, duration: 1.413s, episode steps: 79, steps per second: 56, episode reward: -36.454, mean reward: -0.461 [-100.000, 13.226], mean action: 1.658 [0.000, 3.000], mean observation: 0.028 [-1.568, 1.000], loss: 0.622645, mean_absolute_error: 30.811017, mean_q: 40.866148, mean_eps: 0.100000
 1524164/2000000: episode: 7265, duration: 1.091s, episode steps: 63, steps per second: 58, episode reward: -64.332, mean reward: -1.021 [-100.000, 12.244], mean action: 1.444 [0.000, 3.000], mean observation: -0.154 [-1.252, 1.000], loss: 0.548453, mean_absolute_error: 28.704354, mean_q: 38.253621, mean_eps: 0.100000
 1524277/2000000: episode: 7266, duration: 2.154s, episode steps: 113, steps per second: 52, episode reward: -24.385, mean reward: -0.216 [-100.000, 7.310], mean action: 1.593 [0.000, 3.000], mean observation: 0.045 [-1.520, 1.000], loss: 0.720445, mean_absolute_error: 30.678904, mean_q: 40.526856, mean_eps: 0.100000
 1524562/2000000: episode: 7267, duration: 4.936s, episode steps: 285, steps per second: 58, episode reward: 235.479, mean reward: 0.826 [-9.556, 100.000], mean action: 1.172 [0.000, 3.000], mean observation: 0.067 [-0.677, 1.000], loss: 1.147244, mean_absolute_error: 31.679673, mean_q: 40.861476, mean_eps: 0.100000
 1524651/2000000: episode: 7268, duration: 1.469s, episode steps: 89, steps per second: 61, episode reward: -22.908, mean reward: -0.257 [-100.000, 16.847], mean action: 1.438 [0.000, 3.000], mean observation: 0.020 [-1.200, 1.000], loss: 0.653292, mean_absolute_error: 30.592735, mean_q: 40.766624, mean_eps: 0.100000
 1524722/2000000: episode: 7269, duration: 1.143s, episode steps: 71, steps per second: 62, episode reward: -53.891, mean reward: -0.759 [-100.000, 16.431], mean action: 1.873 [0.000, 3.000], mean observation: -0.013 [-2.512, 1.000], loss: 1.043052, mean_absolute_error: 29.489139, mean_q: 37.899257, mean_eps: 0.100000
 1524805/2000000: episode: 7270, duration: 1.361s, episode steps: 83, steps per second: 61, episode reward: -72.096, mean reward: -0.869 [-100.000, 22.968], mean action: 0.916 [0.000, 3.000], mean observation: -0.015 [-1.347, 1.000], loss: 0.801394, mean_absolute_error: 29.657686, mean_q: 39.162910, mean_eps: 0.100000
 1524869/2000000: episode: 7271, duration: 1.029s, episode steps: 64, steps per second: 62, episode reward: -48.323, mean reward: -0.755 [-100.000, 11.384], mean action: 1.391 [0.000, 3.000], mean observation: -0.158 [-2.306, 1.000], loss: 0.848593, mean_absolute_error: 31.083540, mean_q: 41.212025, mean_eps: 0.100000
 1524971/2000000: episode: 7272, duration: 1.590s, episode steps: 102, steps per second: 64, episode reward: -42.455, mean reward: -0.416 [-100.000, 11.715], mean action: 1.422 [0.000, 3.000], mean observation: 0.124 [-0.908, 1.422], loss: 1.417572, mean_absolute_error: 29.348454, mean_q: 38.637611, mean_eps: 0.100000
 1525055/2000000: episode: 7273, duration: 1.322s, episode steps: 84, steps per second: 64, episode reward: -41.232, mean reward: -0.491 [-100.000, 12.158], mean action: 1.726 [0.000, 3.000], mean observation: 0.029 [-1.182, 1.000], loss: 0.723237, mean_absolute_error: 31.191178, mean_q: 40.756026, mean_eps: 0.100000
 1525501/2000000: episode: 7274, duration: 7.082s, episode steps: 446, steps per second: 63, episode reward: 17.275, mean reward: 0.039 [-100.000, 15.452], mean action: 1.220 [0.000, 3.000], mean observation: 0.044 [-1.435, 1.017], loss: 0.654478, mean_absolute_error: 30.837445, mean_q: 40.921534, mean_eps: 0.100000
 1525611/2000000: episode: 7275, duration: 1.705s, episode steps: 110, steps per second: 65, episode reward: -69.172, mean reward: -0.629 [-100.000, 10.244], mean action: 1.991 [0.000, 3.000], mean observation: 0.123 [-1.406, 1.000], loss: 1.054833, mean_absolute_error: 31.285901, mean_q: 40.866839, mean_eps: 0.100000
 1525693/2000000: episode: 7276, duration: 1.311s, episode steps: 82, steps per second: 63, episode reward: -53.385, mean reward: -0.651 [-100.000, 17.144], mean action: 1.927 [0.000, 3.000], mean observation: -0.058 [-1.213, 1.000], loss: 0.747864, mean_absolute_error: 30.742835, mean_q: 40.922828, mean_eps: 0.100000
 1526693/2000000: episode: 7277, duration: 17.460s, episode steps: 1000, steps per second: 57, episode reward: -104.130, mean reward: -0.104 [-4.925, 5.633], mean action: 1.875 [0.000, 3.000], mean observation: 0.116 [-0.685, 0.942], loss: 0.875100, mean_absolute_error: 31.680838, mean_q: 41.510533, mean_eps: 0.100000
 1526784/2000000: episode: 7278, duration: 1.443s, episode steps: 91, steps per second: 63, episode reward: -93.344, mean reward: -1.026 [-100.000, 16.592], mean action: 1.198 [0.000, 3.000], mean observation: -0.034 [-1.273, 1.000], loss: 0.982274, mean_absolute_error: 32.549733, mean_q: 42.230749, mean_eps: 0.100000
 1526883/2000000: episode: 7279, duration: 1.565s, episode steps: 99, steps per second: 63, episode reward: -61.918, mean reward: -0.625 [-100.000, 13.450], mean action: 1.848 [0.000, 3.000], mean observation: -0.068 [-1.109, 1.436], loss: 1.300168, mean_absolute_error: 32.344193, mean_q: 42.370917, mean_eps: 0.100000
 1526965/2000000: episode: 7280, duration: 1.301s, episode steps: 82, steps per second: 63, episode reward: -75.685, mean reward: -0.923 [-100.000, 13.781], mean action: 0.988 [0.000, 3.000], mean observation: -0.064 [-1.267, 1.000], loss: 1.422064, mean_absolute_error: 30.372261, mean_q: 38.479761, mean_eps: 0.100000
 1527081/2000000: episode: 7281, duration: 1.815s, episode steps: 116, steps per second: 64, episode reward: -36.101, mean reward: -0.311 [-100.000, 20.249], mean action: 1.336 [0.000, 3.000], mean observation: -0.110 [-1.025, 1.654], loss: 0.989573, mean_absolute_error: 31.014659, mean_q: 41.124898, mean_eps: 0.100000
 1527181/2000000: episode: 7282, duration: 1.566s, episode steps: 100, steps per second: 64, episode reward: -27.997, mean reward: -0.280 [-100.000, 12.423], mean action: 1.850 [0.000, 3.000], mean observation: 0.105 [-1.077, 1.337], loss: 1.396007, mean_absolute_error: 32.820404, mean_q: 42.868043, mean_eps: 0.100000
 1527249/2000000: episode: 7283, duration: 1.097s, episode steps: 68, steps per second: 62, episode reward: -21.359, mean reward: -0.314 [-100.000, 13.458], mean action: 1.515 [0.000, 3.000], mean observation: -0.148 [-1.271, 2.231], loss: 1.224269, mean_absolute_error: 33.949196, mean_q: 44.809965, mean_eps: 0.100000
 1527345/2000000: episode: 7284, duration: 1.502s, episode steps: 96, steps per second: 64, episode reward: -65.600, mean reward: -0.683 [-100.000, 10.489], mean action: 1.344 [0.000, 3.000], mean observation: 0.023 [-2.705, 1.000], loss: 0.768862, mean_absolute_error: 29.978484, mean_q: 39.401898, mean_eps: 0.100000
 1527419/2000000: episode: 7285, duration: 1.164s, episode steps: 74, steps per second: 64, episode reward: -49.767, mean reward: -0.673 [-100.000, 6.875], mean action: 1.432 [0.000, 3.000], mean observation: 0.081 [-1.068, 2.162], loss: 0.915612, mean_absolute_error: 31.055731, mean_q: 41.398283, mean_eps: 0.100000
 1528327/2000000: episode: 7286, duration: 14.969s, episode steps: 908, steps per second: 61, episode reward: 168.527, mean reward: 0.186 [-18.792, 100.000], mean action: 1.120 [0.000, 3.000], mean observation: 0.198 [-0.853, 1.000], loss: 1.044652, mean_absolute_error: 32.040830, mean_q: 41.987206, mean_eps: 0.100000
 1528399/2000000: episode: 7287, duration: 1.146s, episode steps: 72, steps per second: 63, episode reward: -48.656, mean reward: -0.676 [-100.000, 9.749], mean action: 1.569 [0.000, 3.000], mean observation: -0.092 [-1.220, 2.518], loss: 0.542189, mean_absolute_error: 32.233192, mean_q: 42.786373, mean_eps: 0.100000
 1528471/2000000: episode: 7288, duration: 1.130s, episode steps: 72, steps per second: 64, episode reward: -63.556, mean reward: -0.883 [-100.000, 22.469], mean action: 1.833 [0.000, 3.000], mean observation: -0.054 [-1.309, 1.000], loss: 0.851271, mean_absolute_error: 31.116454, mean_q: 40.149493, mean_eps: 0.100000
 1528569/2000000: episode: 7289, duration: 1.540s, episode steps: 98, steps per second: 64, episode reward: -50.575, mean reward: -0.516 [-100.000, 10.811], mean action: 1.806 [0.000, 3.000], mean observation: -0.051 [-1.183, 2.742], loss: 0.754752, mean_absolute_error: 31.313230, mean_q: 41.434094, mean_eps: 0.100000
 1528885/2000000: episode: 7290, duration: 5.190s, episode steps: 316, steps per second: 61, episode reward: 229.299, mean reward: 0.726 [-10.581, 100.000], mean action: 1.731 [0.000, 3.000], mean observation: 0.065 [-0.649, 1.000], loss: 0.667639, mean_absolute_error: 31.249562, mean_q: 41.032603, mean_eps: 0.100000
 1528965/2000000: episode: 7291, duration: 1.245s, episode steps: 80, steps per second: 64, episode reward: -66.139, mean reward: -0.827 [-100.000, 13.878], mean action: 1.137 [0.000, 3.000], mean observation: 0.046 [-1.169, 1.000], loss: 0.819525, mean_absolute_error: 32.999077, mean_q: 43.073161, mean_eps: 0.100000
 1529058/2000000: episode: 7292, duration: 1.428s, episode steps: 93, steps per second: 65, episode reward: -83.429, mean reward: -0.897 [-100.000, 12.973], mean action: 0.699 [0.000, 3.000], mean observation: -0.018 [-3.213, 1.000], loss: 0.821104, mean_absolute_error: 32.580873, mean_q: 43.437692, mean_eps: 0.100000
 1529151/2000000: episode: 7293, duration: 1.468s, episode steps: 93, steps per second: 63, episode reward: -25.450, mean reward: -0.274 [-100.000, 18.196], mean action: 1.710 [0.000, 3.000], mean observation: 0.101 [-1.683, 1.000], loss: 1.411613, mean_absolute_error: 31.898406, mean_q: 42.385656, mean_eps: 0.100000
 1529231/2000000: episode: 7294, duration: 1.239s, episode steps: 80, steps per second: 65, episode reward: -31.537, mean reward: -0.394 [-100.000, 27.890], mean action: 1.250 [0.000, 3.000], mean observation: -0.036 [-1.278, 1.000], loss: 0.708043, mean_absolute_error: 32.660214, mean_q: 43.188142, mean_eps: 0.100000
 1529324/2000000: episode: 7295, duration: 1.474s, episode steps: 93, steps per second: 63, episode reward: -45.003, mean reward: -0.484 [-100.000, 20.283], mean action: 1.290 [0.000, 3.000], mean observation: -0.055 [-1.237, 1.000], loss: 0.833297, mean_absolute_error: 29.976880, mean_q: 39.947591, mean_eps: 0.100000
 1529448/2000000: episode: 7296, duration: 1.957s, episode steps: 124, steps per second: 63, episode reward: -52.513, mean reward: -0.423 [-100.000, 13.940], mean action: 1.476 [0.000, 3.000], mean observation: -0.042 [-2.026, 1.000], loss: 1.321544, mean_absolute_error: 31.987127, mean_q: 41.942037, mean_eps: 0.100000
 1529516/2000000: episode: 7297, duration: 1.118s, episode steps: 68, steps per second: 61, episode reward: -47.055, mean reward: -0.692 [-100.000, 11.923], mean action: 2.000 [0.000, 3.000], mean observation: -0.109 [-1.255, 2.591], loss: 0.918019, mean_absolute_error: 31.276367, mean_q: 41.349569, mean_eps: 0.100000
 1529599/2000000: episode: 7298, duration: 1.306s, episode steps: 83, steps per second: 64, episode reward: -82.017, mean reward: -0.988 [-100.000, 18.308], mean action: 1.783 [0.000, 3.000], mean observation: -0.002 [-2.912, 1.000], loss: 0.818652, mean_absolute_error: 31.270528, mean_q: 41.458225, mean_eps: 0.100000
 1529719/2000000: episode: 7299, duration: 1.894s, episode steps: 120, steps per second: 63, episode reward: -42.304, mean reward: -0.353 [-100.000, 12.036], mean action: 1.608 [0.000, 3.000], mean observation: 0.103 [-0.896, 1.597], loss: 0.880020, mean_absolute_error: 32.343111, mean_q: 42.740017, mean_eps: 0.100000
 1529790/2000000: episode: 7300, duration: 1.129s, episode steps: 71, steps per second: 63, episode reward: -9.627, mean reward: -0.136 [-100.000, 25.909], mean action: 1.352 [0.000, 3.000], mean observation: -0.077 [-1.291, 1.000], loss: 0.525084, mean_absolute_error: 30.270180, mean_q: 40.267797, mean_eps: 0.100000
 1529902/2000000: episode: 7301, duration: 1.729s, episode steps: 112, steps per second: 65, episode reward: -218.748, mean reward: -1.953 [-100.000, 26.751], mean action: 1.384 [0.000, 3.000], mean observation: 0.050 [-1.644, 1.000], loss: 0.793549, mean_absolute_error: 32.787965, mean_q: 43.496602, mean_eps: 0.100000
 1530202/2000000: episode: 7302, duration: 4.732s, episode steps: 300, steps per second: 63, episode reward: 198.448, mean reward: 0.661 [-10.174, 100.000], mean action: 1.033 [0.000, 3.000], mean observation: 0.130 [-0.779, 1.000], loss: 0.971049, mean_absolute_error: 30.795899, mean_q: 40.371892, mean_eps: 0.100000
 1530269/2000000: episode: 7303, duration: 1.080s, episode steps: 67, steps per second: 62, episode reward: -70.487, mean reward: -1.052 [-100.000, 11.580], mean action: 1.299 [0.000, 3.000], mean observation: -0.141 [-1.230, 3.067], loss: 1.326637, mean_absolute_error: 32.535290, mean_q: 41.292993, mean_eps: 0.100000
 1531135/2000000: episode: 7304, duration: 14.379s, episode steps: 866, steps per second: 60, episode reward: -322.594, mean reward: -0.373 [-100.000, 5.341], mean action: 1.516 [0.000, 3.000], mean observation: 0.196 [-0.682, 1.607], loss: 0.865005, mean_absolute_error: 31.320607, mean_q: 41.293847, mean_eps: 0.100000
 1531229/2000000: episode: 7305, duration: 1.523s, episode steps: 94, steps per second: 62, episode reward: -59.926, mean reward: -0.638 [-100.000, 9.191], mean action: 1.957 [0.000, 3.000], mean observation: -0.032 [-1.285, 1.000], loss: 0.783142, mean_absolute_error: 31.001400, mean_q: 41.213880, mean_eps: 0.100000
 1531338/2000000: episode: 7306, duration: 1.683s, episode steps: 109, steps per second: 65, episode reward: -54.028, mean reward: -0.496 [-100.000, 16.870], mean action: 1.817 [0.000, 3.000], mean observation: -0.033 [-1.176, 1.722], loss: 0.741593, mean_absolute_error: 32.347967, mean_q: 42.530728, mean_eps: 0.100000
 1531436/2000000: episode: 7307, duration: 1.575s, episode steps: 98, steps per second: 62, episode reward: -70.844, mean reward: -0.723 [-100.000, 16.422], mean action: 1.949 [0.000, 3.000], mean observation: 0.050 [-1.691, 1.000], loss: 1.030818, mean_absolute_error: 31.533422, mean_q: 40.955597, mean_eps: 0.100000
 1531528/2000000: episode: 7308, duration: 1.489s, episode steps: 92, steps per second: 62, episode reward: -80.150, mean reward: -0.871 [-100.000, 11.987], mean action: 1.087 [0.000, 3.000], mean observation: 0.094 [-1.087, 1.688], loss: 0.982136, mean_absolute_error: 30.979170, mean_q: 40.067933, mean_eps: 0.100000
 1531645/2000000: episode: 7309, duration: 1.876s, episode steps: 117, steps per second: 62, episode reward: -39.828, mean reward: -0.340 [-100.000, 11.928], mean action: 1.675 [0.000, 3.000], mean observation: 0.073 [-0.881, 1.266], loss: 1.134910, mean_absolute_error: 32.334577, mean_q: 42.437695, mean_eps: 0.100000
 1531895/2000000: episode: 7310, duration: 3.937s, episode steps: 250, steps per second: 63, episode reward: -2.688, mean reward: -0.011 [-100.000, 16.033], mean action: 1.620 [0.000, 3.000], mean observation: -0.010 [-0.709, 1.000], loss: 0.779668, mean_absolute_error: 31.933764, mean_q: 41.935509, mean_eps: 0.100000
 1532095/2000000: episode: 7311, duration: 3.137s, episode steps: 200, steps per second: 64, episode reward: 211.104, mean reward: 1.056 [-7.114, 100.000], mean action: 1.365 [0.000, 3.000], mean observation: 0.163 [-1.099, 1.000], loss: 0.879754, mean_absolute_error: 32.898534, mean_q: 43.297994, mean_eps: 0.100000
 1532482/2000000: episode: 7312, duration: 6.160s, episode steps: 387, steps per second: 63, episode reward: 212.400, mean reward: 0.549 [-17.971, 100.000], mean action: 1.078 [0.000, 3.000], mean observation: 0.209 [-0.965, 1.284], loss: 0.855235, mean_absolute_error: 32.024716, mean_q: 42.454191, mean_eps: 0.100000
 1532584/2000000: episode: 7313, duration: 1.611s, episode steps: 102, steps per second: 63, episode reward: -26.627, mean reward: -0.261 [-100.000, 17.251], mean action: 1.657 [0.000, 3.000], mean observation: 0.147 [-1.085, 1.000], loss: 1.095561, mean_absolute_error: 33.138894, mean_q: 43.417173, mean_eps: 0.100000
 1532854/2000000: episode: 7314, duration: 4.314s, episode steps: 270, steps per second: 63, episode reward: 218.379, mean reward: 0.809 [-15.161, 100.000], mean action: 1.222 [0.000, 3.000], mean observation: 0.186 [-0.998, 1.051], loss: 0.691720, mean_absolute_error: 32.294980, mean_q: 42.338862, mean_eps: 0.100000
 1532948/2000000: episode: 7315, duration: 1.532s, episode steps: 94, steps per second: 61, episode reward: -75.008, mean reward: -0.798 [-100.000, 15.287], mean action: 0.968 [0.000, 3.000], mean observation: 0.036 [-1.384, 1.000], loss: 1.141278, mean_absolute_error: 31.805148, mean_q: 41.423173, mean_eps: 0.100000
 1533468/2000000: episode: 7316, duration: 8.494s, episode steps: 520, steps per second: 61, episode reward: 192.661, mean reward: 0.371 [-20.683, 100.000], mean action: 0.785 [0.000, 3.000], mean observation: 0.209 [-0.816, 1.000], loss: 0.909827, mean_absolute_error: 30.946336, mean_q: 40.912848, mean_eps: 0.100000
 1533627/2000000: episode: 7317, duration: 2.499s, episode steps: 159, steps per second: 64, episode reward: -19.907, mean reward: -0.125 [-100.000, 13.917], mean action: 1.497 [0.000, 3.000], mean observation: -0.006 [-1.058, 1.000], loss: 0.753558, mean_absolute_error: 32.740462, mean_q: 43.676742, mean_eps: 0.100000
 1533737/2000000: episode: 7318, duration: 1.742s, episode steps: 110, steps per second: 63, episode reward: -23.560, mean reward: -0.214 [-100.000, 19.789], mean action: 0.782 [0.000, 3.000], mean observation: 0.033 [-1.937, 1.017], loss: 0.861095, mean_absolute_error: 32.970755, mean_q: 43.962936, mean_eps: 0.100000
 1534413/2000000: episode: 7319, duration: 10.912s, episode steps: 676, steps per second: 62, episode reward: 203.626, mean reward: 0.301 [-22.020, 100.000], mean action: 0.612 [0.000, 3.000], mean observation: 0.156 [-0.812, 1.000], loss: 0.746449, mean_absolute_error: 31.698657, mean_q: 42.106629, mean_eps: 0.100000
 1534480/2000000: episode: 7320, duration: 1.074s, episode steps: 67, steps per second: 62, episode reward: -41.495, mean reward: -0.619 [-100.000, 19.047], mean action: 1.567 [0.000, 3.000], mean observation: -0.099 [-1.273, 2.612], loss: 0.597278, mean_absolute_error: 29.899842, mean_q: 39.819729, mean_eps: 0.100000
 1534552/2000000: episode: 7321, duration: 1.180s, episode steps: 72, steps per second: 61, episode reward: 9.623, mean reward: 0.134 [-100.000, 15.363], mean action: 1.792 [0.000, 3.000], mean observation: -0.035 [-1.290, 1.000], loss: 0.933075, mean_absolute_error: 32.834814, mean_q: 43.028824, mean_eps: 0.100000
 1534649/2000000: episode: 7322, duration: 1.595s, episode steps: 97, steps per second: 61, episode reward: -50.063, mean reward: -0.516 [-100.000, 12.838], mean action: 1.814 [0.000, 3.000], mean observation: -0.068 [-1.149, 2.611], loss: 0.898742, mean_absolute_error: 30.236595, mean_q: 39.666441, mean_eps: 0.100000
 1534723/2000000: episode: 7323, duration: 1.157s, episode steps: 74, steps per second: 64, episode reward: -52.380, mean reward: -0.708 [-100.000, 17.586], mean action: 1.554 [0.000, 3.000], mean observation: 0.018 [-1.221, 1.000], loss: 0.912774, mean_absolute_error: 31.510124, mean_q: 41.751371, mean_eps: 0.100000
 1534803/2000000: episode: 7324, duration: 1.249s, episode steps: 80, steps per second: 64, episode reward: -35.330, mean reward: -0.442 [-100.000, 14.335], mean action: 1.625 [0.000, 3.000], mean observation: -0.005 [-1.210, 1.000], loss: 0.610275, mean_absolute_error: 31.514629, mean_q: 41.218124, mean_eps: 0.100000
 1534929/2000000: episode: 7325, duration: 1.986s, episode steps: 126, steps per second: 63, episode reward: -28.149, mean reward: -0.223 [-100.000, 12.858], mean action: 1.857 [0.000, 3.000], mean observation: 0.094 [-0.955, 1.000], loss: 1.375849, mean_absolute_error: 32.403253, mean_q: 41.878615, mean_eps: 0.100000
 1535019/2000000: episode: 7326, duration: 1.387s, episode steps: 90, steps per second: 65, episode reward: -27.222, mean reward: -0.302 [-100.000, 20.405], mean action: 1.411 [0.000, 3.000], mean observation: -0.067 [-1.250, 1.103], loss: 1.103003, mean_absolute_error: 33.997657, mean_q: 45.144550, mean_eps: 0.100000
 1535095/2000000: episode: 7327, duration: 1.204s, episode steps: 76, steps per second: 63, episode reward: -4.392, mean reward: -0.058 [-100.000, 21.579], mean action: 1.474 [0.000, 3.000], mean observation: -0.097 [-1.217, 1.000], loss: 1.233489, mean_absolute_error: 31.748513, mean_q: 41.458694, mean_eps: 0.100000
 1536095/2000000: episode: 7328, duration: 16.803s, episode steps: 1000, steps per second: 60, episode reward: 77.102, mean reward: 0.077 [-20.535, 23.933], mean action: 1.057 [0.000, 3.000], mean observation: 0.228 [-0.900, 1.003], loss: 0.934955, mean_absolute_error: 32.205045, mean_q: 42.310395, mean_eps: 0.100000
 1536515/2000000: episode: 7329, duration: 7.040s, episode steps: 420, steps per second: 60, episode reward: 225.517, mean reward: 0.537 [-19.554, 100.000], mean action: 1.269 [0.000, 3.000], mean observation: 0.072 [-0.644, 1.000], loss: 1.069145, mean_absolute_error: 31.291025, mean_q: 41.218040, mean_eps: 0.100000
 1536592/2000000: episode: 7330, duration: 1.328s, episode steps: 77, steps per second: 58, episode reward: -40.948, mean reward: -0.532 [-100.000, 18.256], mean action: 1.338 [0.000, 3.000], mean observation: 0.032 [-1.544, 1.000], loss: 0.708052, mean_absolute_error: 30.915211, mean_q: 40.755845, mean_eps: 0.100000
 1536673/2000000: episode: 7331, duration: 1.443s, episode steps: 81, steps per second: 56, episode reward: -52.953, mean reward: -0.654 [-100.000, 16.386], mean action: 1.519 [0.000, 3.000], mean observation: -0.108 [-1.254, 1.653], loss: 1.241787, mean_absolute_error: 31.722188, mean_q: 40.011253, mean_eps: 0.100000
 1536766/2000000: episode: 7332, duration: 1.444s, episode steps: 93, steps per second: 64, episode reward: -38.580, mean reward: -0.415 [-100.000, 16.556], mean action: 1.032 [0.000, 3.000], mean observation: 0.015 [-1.263, 1.000], loss: 0.961618, mean_absolute_error: 31.877193, mean_q: 42.045582, mean_eps: 0.100000
 1536847/2000000: episode: 7333, duration: 1.259s, episode steps: 81, steps per second: 64, episode reward: 5.641, mean reward: 0.070 [-100.000, 20.659], mean action: 1.358 [0.000, 3.000], mean observation: -0.033 [-1.249, 1.000], loss: 0.828316, mean_absolute_error: 32.947762, mean_q: 43.519473, mean_eps: 0.100000
 1537120/2000000: episode: 7334, duration: 4.318s, episode steps: 273, steps per second: 63, episode reward: 207.337, mean reward: 0.759 [-10.243, 100.000], mean action: 1.033 [0.000, 3.000], mean observation: 0.156 [-1.127, 1.000], loss: 0.758132, mean_absolute_error: 32.817435, mean_q: 43.193284, mean_eps: 0.100000
 1537506/2000000: episode: 7335, duration: 6.136s, episode steps: 386, steps per second: 63, episode reward: 265.168, mean reward: 0.687 [-17.573, 100.000], mean action: 0.837 [0.000, 3.000], mean observation: 0.107 [-1.026, 1.156], loss: 0.877656, mean_absolute_error: 30.476416, mean_q: 39.906309, mean_eps: 0.100000
 1537594/2000000: episode: 7336, duration: 1.391s, episode steps: 88, steps per second: 63, episode reward: -40.492, mean reward: -0.460 [-100.000, 18.492], mean action: 1.205 [0.000, 3.000], mean observation: -0.130 [-1.187, 1.794], loss: 0.778472, mean_absolute_error: 33.876420, mean_q: 43.915118, mean_eps: 0.100000
 1537873/2000000: episode: 7337, duration: 4.440s, episode steps: 279, steps per second: 63, episode reward: 207.266, mean reward: 0.743 [-9.568, 100.000], mean action: 1.409 [0.000, 3.000], mean observation: 0.139 [-1.112, 1.000], loss: 0.898773, mean_absolute_error: 32.679631, mean_q: 43.464960, mean_eps: 0.100000
 1537951/2000000: episode: 7338, duration: 1.212s, episode steps: 78, steps per second: 64, episode reward: -85.074, mean reward: -1.091 [-100.000, 11.931], mean action: 2.000 [0.000, 3.000], mean observation: -0.135 [-1.280, 2.949], loss: 0.755875, mean_absolute_error: 32.937946, mean_q: 43.779400, mean_eps: 0.100000
 1538128/2000000: episode: 7339, duration: 2.772s, episode steps: 177, steps per second: 64, episode reward: 246.324, mean reward: 1.392 [-3.920, 100.000], mean action: 1.124 [0.000, 3.000], mean observation: 0.132 [-1.263, 1.000], loss: 1.002480, mean_absolute_error: 31.849679, mean_q: 40.953679, mean_eps: 0.100000
 1538317/2000000: episode: 7340, duration: 2.994s, episode steps: 189, steps per second: 63, episode reward: -3.397, mean reward: -0.018 [-100.000, 18.776], mean action: 1.693 [0.000, 3.000], mean observation: -0.006 [-0.986, 1.449], loss: 1.033418, mean_absolute_error: 32.013353, mean_q: 42.177568, mean_eps: 0.100000
 1538675/2000000: episode: 7341, duration: 6.034s, episode steps: 358, steps per second: 59, episode reward: 255.164, mean reward: 0.713 [-11.825, 100.000], mean action: 1.008 [0.000, 3.000], mean observation: 0.105 [-0.743, 1.000], loss: 0.866258, mean_absolute_error: 32.311512, mean_q: 42.643771, mean_eps: 0.100000
 1538779/2000000: episode: 7342, duration: 1.642s, episode steps: 104, steps per second: 63, episode reward: -30.614, mean reward: -0.294 [-100.000, 18.053], mean action: 0.962 [0.000, 3.000], mean observation: -0.010 [-1.267, 1.000], loss: 0.849652, mean_absolute_error: 32.132537, mean_q: 42.529595, mean_eps: 0.100000
 1539185/2000000: episode: 7343, duration: 6.615s, episode steps: 406, steps per second: 61, episode reward: 201.150, mean reward: 0.495 [-21.834, 100.000], mean action: 1.951 [0.000, 3.000], mean observation: 0.083 [-0.703, 1.014], loss: 0.926977, mean_absolute_error: 32.147402, mean_q: 42.007121, mean_eps: 0.100000
 1539277/2000000: episode: 7344, duration: 1.445s, episode steps: 92, steps per second: 64, episode reward: -103.983, mean reward: -1.130 [-100.000, 14.852], mean action: 0.891 [0.000, 3.000], mean observation: 0.086 [-2.812, 1.000], loss: 0.986420, mean_absolute_error: 32.028825, mean_q: 42.373242, mean_eps: 0.100000
 1540160/2000000: episode: 7345, duration: 14.371s, episode steps: 883, steps per second: 61, episode reward: 191.009, mean reward: 0.216 [-18.678, 100.000], mean action: 0.901 [0.000, 3.000], mean observation: 0.231 [-1.383, 1.000], loss: 0.972333, mean_absolute_error: 31.958393, mean_q: 41.653538, mean_eps: 0.100000
 1540229/2000000: episode: 7346, duration: 1.149s, episode steps: 69, steps per second: 60, episode reward: -15.837, mean reward: -0.230 [-100.000, 14.625], mean action: 1.435 [0.000, 3.000], mean observation: -0.036 [-2.007, 1.000], loss: 0.728137, mean_absolute_error: 32.862700, mean_q: 43.122196, mean_eps: 0.100000
 1540301/2000000: episode: 7347, duration: 1.119s, episode steps: 72, steps per second: 64, episode reward: -57.496, mean reward: -0.799 [-100.000, 20.090], mean action: 1.486 [0.000, 3.000], mean observation: -0.017 [-1.281, 1.000], loss: 1.287456, mean_absolute_error: 32.613843, mean_q: 42.273753, mean_eps: 0.100000
 1540473/2000000: episode: 7348, duration: 2.699s, episode steps: 172, steps per second: 64, episode reward: 5.327, mean reward: 0.031 [-100.000, 25.238], mean action: 1.674 [0.000, 3.000], mean observation: -0.008 [-0.925, 1.000], loss: 0.981705, mean_absolute_error: 31.558979, mean_q: 41.388456, mean_eps: 0.100000
 1540572/2000000: episode: 7349, duration: 1.592s, episode steps: 99, steps per second: 62, episode reward: -61.586, mean reward: -0.622 [-100.000, 10.912], mean action: 1.838 [0.000, 3.000], mean observation: 0.084 [-1.101, 1.350], loss: 0.690340, mean_absolute_error: 32.656024, mean_q: 43.544873, mean_eps: 0.100000
 1540675/2000000: episode: 7350, duration: 1.633s, episode steps: 103, steps per second: 63, episode reward: -53.192, mean reward: -0.516 [-100.000, 5.328], mean action: 1.505 [0.000, 3.000], mean observation: 0.073 [-1.056, 0.941], loss: 0.911842, mean_absolute_error: 31.570322, mean_q: 41.138434, mean_eps: 0.100000
 1540771/2000000: episode: 7351, duration: 1.531s, episode steps: 96, steps per second: 63, episode reward: -49.694, mean reward: -0.518 [-100.000, 16.981], mean action: 1.250 [0.000, 3.000], mean observation: -0.039 [-1.144, 1.000], loss: 1.328497, mean_absolute_error: 33.032469, mean_q: 42.798969, mean_eps: 0.100000
 1541046/2000000: episode: 7352, duration: 4.350s, episode steps: 275, steps per second: 63, episode reward: -241.585, mean reward: -0.878 [-100.000, 5.157], mean action: 1.745 [0.000, 3.000], mean observation: 0.145 [-0.987, 1.013], loss: 0.811910, mean_absolute_error: 32.055597, mean_q: 42.492166, mean_eps: 0.100000
 1541187/2000000: episode: 7353, duration: 2.199s, episode steps: 141, steps per second: 64, episode reward: -60.456, mean reward: -0.429 [-100.000, 13.416], mean action: 1.277 [0.000, 3.000], mean observation: 0.002 [-1.333, 1.000], loss: 1.312131, mean_absolute_error: 31.691444, mean_q: 41.592798, mean_eps: 0.100000
 1541281/2000000: episode: 7354, duration: 1.489s, episode steps: 94, steps per second: 63, episode reward: -50.645, mean reward: -0.539 [-100.000, 25.963], mean action: 0.947 [0.000, 3.000], mean observation: 0.022 [-1.276, 1.000], loss: 1.073663, mean_absolute_error: 32.665042, mean_q: 42.848412, mean_eps: 0.100000
 1541410/2000000: episode: 7355, duration: 2.009s, episode steps: 129, steps per second: 64, episode reward: -80.691, mean reward: -0.626 [-100.000, 10.776], mean action: 1.667 [0.000, 3.000], mean observation: -0.022 [-1.167, 2.780], loss: 1.007431, mean_absolute_error: 32.479811, mean_q: 43.083080, mean_eps: 0.100000
 1541478/2000000: episode: 7356, duration: 1.086s, episode steps: 68, steps per second: 63, episode reward: -24.418, mean reward: -0.359 [-100.000, 15.423], mean action: 1.603 [0.000, 3.000], mean observation: -0.128 [-1.247, 1.000], loss: 1.261701, mean_absolute_error: 34.507965, mean_q: 45.759939, mean_eps: 0.100000
 1541568/2000000: episode: 7357, duration: 1.431s, episode steps: 90, steps per second: 63, episode reward: -5.993, mean reward: -0.067 [-100.000, 12.024], mean action: 1.567 [0.000, 3.000], mean observation: 0.051 [-1.111, 1.000], loss: 1.378047, mean_absolute_error: 32.204181, mean_q: 42.289934, mean_eps: 0.100000
 1541647/2000000: episode: 7358, duration: 1.245s, episode steps: 79, steps per second: 63, episode reward: -76.028, mean reward: -0.962 [-100.000, 22.221], mean action: 1.924 [0.000, 3.000], mean observation: -0.063 [-1.330, 1.000], loss: 0.832519, mean_absolute_error: 32.428424, mean_q: 43.016830, mean_eps: 0.100000
 1541718/2000000: episode: 7359, duration: 1.129s, episode steps: 71, steps per second: 63, episode reward: -29.826, mean reward: -0.420 [-100.000, 18.644], mean action: 1.831 [0.000, 3.000], mean observation: -0.026 [-1.285, 1.000], loss: 0.753814, mean_absolute_error: 34.790817, mean_q: 45.948520, mean_eps: 0.100000
 1541782/2000000: episode: 7360, duration: 1.005s, episode steps: 64, steps per second: 64, episode reward: -59.638, mean reward: -0.932 [-100.000, 6.764], mean action: 1.531 [0.000, 3.000], mean observation: -0.147 [-3.088, 1.000], loss: 0.736714, mean_absolute_error: 30.589366, mean_q: 40.755487, mean_eps: 0.100000
 1541854/2000000: episode: 7361, duration: 1.142s, episode steps: 72, steps per second: 63, episode reward: -70.409, mean reward: -0.978 [-100.000, 12.831], mean action: 1.361 [0.000, 3.000], mean observation: -0.109 [-1.246, 2.826], loss: 1.259325, mean_absolute_error: 32.131537, mean_q: 42.340501, mean_eps: 0.100000
 1542043/2000000: episode: 7362, duration: 2.944s, episode steps: 189, steps per second: 64, episode reward: 219.388, mean reward: 1.161 [-8.661, 100.000], mean action: 1.079 [0.000, 3.000], mean observation: 0.164 [-1.140, 1.000], loss: 0.990406, mean_absolute_error: 33.396572, mean_q: 44.145351, mean_eps: 0.100000
 1542474/2000000: episode: 7363, duration: 6.900s, episode steps: 431, steps per second: 62, episode reward: 219.186, mean reward: 0.509 [-19.473, 100.000], mean action: 0.979 [0.000, 3.000], mean observation: 0.216 [-1.509, 1.000], loss: 0.907362, mean_absolute_error: 32.674212, mean_q: 42.907832, mean_eps: 0.100000
 1542588/2000000: episode: 7364, duration: 1.832s, episode steps: 114, steps per second: 62, episode reward: -27.681, mean reward: -0.243 [-100.000, 18.045], mean action: 1.886 [0.000, 3.000], mean observation: -0.091 [-0.908, 1.650], loss: 0.823148, mean_absolute_error: 32.290776, mean_q: 42.667439, mean_eps: 0.100000
 1542657/2000000: episode: 7365, duration: 1.141s, episode steps: 69, steps per second: 60, episode reward: -17.850, mean reward: -0.259 [-100.000, 18.800], mean action: 1.957 [0.000, 3.000], mean observation: -0.086 [-1.291, 1.000], loss: 0.663914, mean_absolute_error: 31.435536, mean_q: 42.187189, mean_eps: 0.100000
 1542751/2000000: episode: 7366, duration: 1.635s, episode steps: 94, steps per second: 58, episode reward: -5.264, mean reward: -0.056 [-100.000, 32.974], mean action: 1.670 [0.000, 3.000], mean observation: -0.102 [-1.152, 1.207], loss: 0.603432, mean_absolute_error: 34.046277, mean_q: 45.235751, mean_eps: 0.100000
 1542848/2000000: episode: 7367, duration: 1.570s, episode steps: 97, steps per second: 62, episode reward: -48.785, mean reward: -0.503 [-100.000, 18.668], mean action: 1.155 [0.000, 3.000], mean observation: 0.037 [-2.435, 1.000], loss: 1.120091, mean_absolute_error: 31.330992, mean_q: 40.409435, mean_eps: 0.100000
 1542923/2000000: episode: 7368, duration: 1.178s, episode steps: 75, steps per second: 64, episode reward: -51.500, mean reward: -0.687 [-100.000, 16.195], mean action: 1.147 [0.000, 3.000], mean observation: -0.068 [-1.268, 1.000], loss: 0.954887, mean_absolute_error: 31.718892, mean_q: 41.585813, mean_eps: 0.100000
 1543116/2000000: episode: 7369, duration: 3.045s, episode steps: 193, steps per second: 63, episode reward: -6.340, mean reward: -0.033 [-100.000, 14.926], mean action: 1.637 [0.000, 3.000], mean observation: -0.004 [-0.921, 1.000], loss: 0.688764, mean_absolute_error: 32.041785, mean_q: 42.596478, mean_eps: 0.100000
 1543225/2000000: episode: 7370, duration: 1.729s, episode steps: 109, steps per second: 63, episode reward: -2.019, mean reward: -0.019 [-100.000, 19.925], mean action: 1.248 [0.000, 3.000], mean observation: -0.012 [-1.261, 1.011], loss: 0.722330, mean_absolute_error: 32.181756, mean_q: 42.401956, mean_eps: 0.100000
 1543294/2000000: episode: 7371, duration: 1.069s, episode steps: 69, steps per second: 65, episode reward: -47.405, mean reward: -0.687 [-100.000, 9.817], mean action: 1.594 [0.000, 3.000], mean observation: -0.129 [-1.229, 2.227], loss: 1.330381, mean_absolute_error: 31.365475, mean_q: 40.070761, mean_eps: 0.100000
 1543540/2000000: episode: 7372, duration: 3.933s, episode steps: 246, steps per second: 63, episode reward: 196.304, mean reward: 0.798 [-19.000, 100.000], mean action: 2.049 [0.000, 3.000], mean observation: 0.197 [-1.134, 1.000], loss: 0.768139, mean_absolute_error: 32.877533, mean_q: 43.579045, mean_eps: 0.100000
 1543642/2000000: episode: 7373, duration: 1.860s, episode steps: 102, steps per second: 55, episode reward: -43.733, mean reward: -0.429 [-100.000, 13.376], mean action: 1.843 [0.000, 3.000], mean observation: 0.068 [-1.184, 2.399], loss: 0.667185, mean_absolute_error: 31.967178, mean_q: 42.177397, mean_eps: 0.100000
 1543735/2000000: episode: 7374, duration: 1.466s, episode steps: 93, steps per second: 63, episode reward: -41.436, mean reward: -0.446 [-100.000, 18.828], mean action: 1.957 [0.000, 3.000], mean observation: -0.011 [-1.527, 1.000], loss: 1.186854, mean_absolute_error: 31.065157, mean_q: 40.237586, mean_eps: 0.100000
 1543812/2000000: episode: 7375, duration: 1.255s, episode steps: 77, steps per second: 61, episode reward: -41.378, mean reward: -0.537 [-100.000, 11.685], mean action: 1.870 [0.000, 3.000], mean observation: -0.082 [-1.182, 2.030], loss: 1.036361, mean_absolute_error: 33.940596, mean_q: 44.656686, mean_eps: 0.100000
 1543874/2000000: episode: 7376, duration: 1.012s, episode steps: 62, steps per second: 61, episode reward: -63.735, mean reward: -1.028 [-100.000, 13.107], mean action: 1.952 [0.000, 3.000], mean observation: -0.182 [-3.442, 1.000], loss: 1.136239, mean_absolute_error: 32.775029, mean_q: 42.036481, mean_eps: 0.100000
 1543938/2000000: episode: 7377, duration: 1.000s, episode steps: 64, steps per second: 64, episode reward: -40.795, mean reward: -0.637 [-100.000, 15.582], mean action: 1.750 [0.000, 3.000], mean observation: -0.044 [-1.256, 1.000], loss: 1.234036, mean_absolute_error: 30.814337, mean_q: 40.011498, mean_eps: 0.100000
 1544189/2000000: episode: 7378, duration: 3.990s, episode steps: 251, steps per second: 63, episode reward: 218.928, mean reward: 0.872 [-14.223, 100.000], mean action: 0.984 [0.000, 3.000], mean observation: 0.172 [-0.993, 1.000], loss: 0.797418, mean_absolute_error: 31.784101, mean_q: 41.768488, mean_eps: 0.100000
 1544272/2000000: episode: 7379, duration: 1.318s, episode steps: 83, steps per second: 63, episode reward: -41.007, mean reward: -0.494 [-100.000, 16.244], mean action: 1.699 [0.000, 3.000], mean observation: 0.033 [-1.835, 1.000], loss: 1.062256, mean_absolute_error: 32.510389, mean_q: 42.999696, mean_eps: 0.100000
 1544349/2000000: episode: 7380, duration: 1.233s, episode steps: 77, steps per second: 62, episode reward: -61.531, mean reward: -0.799 [-100.000, 13.261], mean action: 1.143 [0.000, 3.000], mean observation: -0.108 [-1.588, 1.000], loss: 1.543360, mean_absolute_error: 32.323380, mean_q: 41.698880, mean_eps: 0.100000
 1544426/2000000: episode: 7381, duration: 1.196s, episode steps: 77, steps per second: 64, episode reward: 5.597, mean reward: 0.073 [-100.000, 22.756], mean action: 1.364 [0.000, 3.000], mean observation: 0.023 [-1.231, 1.000], loss: 0.953909, mean_absolute_error: 30.754264, mean_q: 40.339776, mean_eps: 0.100000
 1544492/2000000: episode: 7382, duration: 1.060s, episode steps: 66, steps per second: 62, episode reward: -76.114, mean reward: -1.153 [-100.000, 27.863], mean action: 1.303 [0.000, 3.000], mean observation: -0.139 [-1.218, 1.000], loss: 1.860966, mean_absolute_error: 31.754752, mean_q: 40.376077, mean_eps: 0.100000
 1544576/2000000: episode: 7383, duration: 1.515s, episode steps: 84, steps per second: 55, episode reward: -68.025, mean reward: -0.810 [-100.000, 22.779], mean action: 1.167 [0.000, 3.000], mean observation: -0.014 [-1.307, 1.000], loss: 1.053675, mean_absolute_error: 33.144697, mean_q: 43.170569, mean_eps: 0.100000
 1544656/2000000: episode: 7384, duration: 1.331s, episode steps: 80, steps per second: 60, episode reward: 3.782, mean reward: 0.047 [-100.000, 18.887], mean action: 1.262 [0.000, 3.000], mean observation: 0.005 [-1.228, 1.000], loss: 1.208364, mean_absolute_error: 32.913465, mean_q: 43.435201, mean_eps: 0.100000
 1544739/2000000: episode: 7385, duration: 1.305s, episode steps: 83, steps per second: 64, episode reward: -38.717, mean reward: -0.466 [-100.000, 14.309], mean action: 1.145 [0.000, 3.000], mean observation: -0.017 [-1.266, 1.000], loss: 1.087208, mean_absolute_error: 33.261625, mean_q: 44.067140, mean_eps: 0.100000
 1545194/2000000: episode: 7386, duration: 7.246s, episode steps: 455, steps per second: 63, episode reward: 191.767, mean reward: 0.421 [-18.101, 100.000], mean action: 1.048 [0.000, 3.000], mean observation: 0.203 [-1.282, 1.000], loss: 1.019459, mean_absolute_error: 32.462055, mean_q: 42.664260, mean_eps: 0.100000
 1545383/2000000: episode: 7387, duration: 2.941s, episode steps: 189, steps per second: 64, episode reward: 14.696, mean reward: 0.078 [-100.000, 18.501], mean action: 1.571 [0.000, 3.000], mean observation: 0.012 [-1.078, 1.000], loss: 0.816632, mean_absolute_error: 32.495710, mean_q: 42.145465, mean_eps: 0.100000
 1545472/2000000: episode: 7388, duration: 1.439s, episode steps: 89, steps per second: 62, episode reward: -3.762, mean reward: -0.042 [-100.000, 21.334], mean action: 1.258 [0.000, 3.000], mean observation: -0.029 [-1.219, 1.017], loss: 0.894065, mean_absolute_error: 32.612188, mean_q: 42.349163, mean_eps: 0.100000
 1545733/2000000: episode: 7389, duration: 4.131s, episode steps: 261, steps per second: 63, episode reward: 232.584, mean reward: 0.891 [-17.361, 100.000], mean action: 1.042 [0.000, 3.000], mean observation: 0.146 [-0.852, 1.000], loss: 0.964509, mean_absolute_error: 33.232343, mean_q: 43.472805, mean_eps: 0.100000
 1546016/2000000: episode: 7390, duration: 4.476s, episode steps: 283, steps per second: 63, episode reward: 235.303, mean reward: 0.831 [-8.515, 100.000], mean action: 1.074 [0.000, 3.000], mean observation: 0.093 [-0.760, 1.000], loss: 0.834842, mean_absolute_error: 32.710514, mean_q: 43.087315, mean_eps: 0.100000
 1546112/2000000: episode: 7391, duration: 1.560s, episode steps: 96, steps per second: 62, episode reward: -37.364, mean reward: -0.389 [-100.000, 16.000], mean action: 1.438 [0.000, 3.000], mean observation: 0.023 [-1.322, 1.000], loss: 0.785222, mean_absolute_error: 32.848609, mean_q: 43.189208, mean_eps: 0.100000
 1546187/2000000: episode: 7392, duration: 1.237s, episode steps: 75, steps per second: 61, episode reward: -61.435, mean reward: -0.819 [-100.000, 10.753], mean action: 1.960 [0.000, 3.000], mean observation: -0.161 [-2.802, 1.000], loss: 0.928115, mean_absolute_error: 31.750407, mean_q: 42.021925, mean_eps: 0.100000
 1546331/2000000: episode: 7393, duration: 2.233s, episode steps: 144, steps per second: 65, episode reward: -64.067, mean reward: -0.445 [-100.000, 12.454], mean action: 1.299 [0.000, 3.000], mean observation: -0.056 [-1.002, 1.672], loss: 0.831986, mean_absolute_error: 31.259349, mean_q: 41.337586, mean_eps: 0.100000
 1546426/2000000: episode: 7394, duration: 1.497s, episode steps: 95, steps per second: 63, episode reward: -42.694, mean reward: -0.449 [-100.000, 26.962], mean action: 0.937 [0.000, 3.000], mean observation: 0.010 [-1.309, 1.000], loss: 1.178537, mean_absolute_error: 32.672423, mean_q: 42.912227, mean_eps: 0.100000
 1546706/2000000: episode: 7395, duration: 4.434s, episode steps: 280, steps per second: 63, episode reward: 168.955, mean reward: 0.603 [-15.558, 100.000], mean action: 1.457 [0.000, 3.000], mean observation: 0.052 [-0.867, 1.000], loss: 0.808927, mean_absolute_error: 32.568206, mean_q: 42.643687, mean_eps: 0.100000
 1546781/2000000: episode: 7396, duration: 1.184s, episode steps: 75, steps per second: 63, episode reward: -0.913, mean reward: -0.012 [-100.000, 20.216], mean action: 1.933 [0.000, 3.000], mean observation: -0.011 [-1.200, 1.000], loss: 0.931026, mean_absolute_error: 32.744311, mean_q: 42.893690, mean_eps: 0.100000
 1546864/2000000: episode: 7397, duration: 1.311s, episode steps: 83, steps per second: 63, episode reward: -39.525, mean reward: -0.476 [-100.000, 16.070], mean action: 1.904 [0.000, 3.000], mean observation: -0.078 [-1.183, 1.151], loss: 0.971117, mean_absolute_error: 31.690712, mean_q: 41.022346, mean_eps: 0.100000
 1547060/2000000: episode: 7398, duration: 3.122s, episode steps: 196, steps per second: 63, episode reward: 5.014, mean reward: 0.026 [-100.000, 17.670], mean action: 1.801 [0.000, 3.000], mean observation: -0.001 [-1.007, 1.002], loss: 1.170993, mean_absolute_error: 33.809892, mean_q: 44.380169, mean_eps: 0.100000
 1547161/2000000: episode: 7399, duration: 1.627s, episode steps: 101, steps per second: 62, episode reward: -44.419, mean reward: -0.440 [-100.000, 27.626], mean action: 1.832 [0.000, 3.000], mean observation: 0.010 [-1.329, 1.009], loss: 1.003509, mean_absolute_error: 32.777706, mean_q: 42.215780, mean_eps: 0.100000
 1547247/2000000: episode: 7400, duration: 1.315s, episode steps: 86, steps per second: 65, episode reward: -71.109, mean reward: -0.827 [-100.000, 17.789], mean action: 1.430 [0.000, 3.000], mean observation: 0.057 [-1.162, 1.000], loss: 0.852274, mean_absolute_error: 31.794250, mean_q: 42.265628, mean_eps: 0.100000
 1547319/2000000: episode: 7401, duration: 1.136s, episode steps: 72, steps per second: 63, episode reward: -41.406, mean reward: -0.575 [-100.000, 16.456], mean action: 1.417 [0.000, 3.000], mean observation: -0.053 [-1.268, 1.000], loss: 1.023278, mean_absolute_error: 33.911420, mean_q: 44.508754, mean_eps: 0.100000
 1547644/2000000: episode: 7402, duration: 5.175s, episode steps: 325, steps per second: 63, episode reward: 234.541, mean reward: 0.722 [-17.481, 100.000], mean action: 1.178 [0.000, 3.000], mean observation: 0.082 [-0.596, 1.000], loss: 0.912891, mean_absolute_error: 32.915791, mean_q: 42.533311, mean_eps: 0.100000
 1547949/2000000: episode: 7403, duration: 4.910s, episode steps: 305, steps per second: 62, episode reward: 211.352, mean reward: 0.693 [-12.491, 100.000], mean action: 1.305 [0.000, 3.000], mean observation: 0.126 [-1.090, 1.000], loss: 0.798930, mean_absolute_error: 33.018830, mean_q: 43.655094, mean_eps: 0.100000
 1548068/2000000: episode: 7404, duration: 1.851s, episode steps: 119, steps per second: 64, episode reward: -18.213, mean reward: -0.153 [-100.000, 17.936], mean action: 1.387 [0.000, 3.000], mean observation: 0.017 [-1.180, 1.431], loss: 1.108336, mean_absolute_error: 33.194262, mean_q: 43.958080, mean_eps: 0.100000
 1548217/2000000: episode: 7405, duration: 2.398s, episode steps: 149, steps per second: 62, episode reward: -34.352, mean reward: -0.231 [-100.000, 12.272], mean action: 1.906 [0.000, 3.000], mean observation: 0.093 [-0.950, 1.105], loss: 0.695571, mean_absolute_error: 32.242960, mean_q: 42.071321, mean_eps: 0.100000
 1548408/2000000: episode: 7406, duration: 3.044s, episode steps: 191, steps per second: 63, episode reward: 184.092, mean reward: 0.964 [-17.944, 100.000], mean action: 2.068 [0.000, 3.000], mean observation: 0.218 [-1.051, 1.125], loss: 0.759110, mean_absolute_error: 33.038581, mean_q: 43.375514, mean_eps: 0.100000
 1548478/2000000: episode: 7407, duration: 1.119s, episode steps: 70, steps per second: 63, episode reward: -84.878, mean reward: -1.213 [-100.000, 11.265], mean action: 1.457 [0.000, 3.000], mean observation: -0.126 [-1.351, 1.780], loss: 1.037824, mean_absolute_error: 32.754130, mean_q: 42.823714, mean_eps: 0.100000
 1548550/2000000: episode: 7408, duration: 1.125s, episode steps: 72, steps per second: 64, episode reward: -72.453, mean reward: -1.006 [-100.000, 8.718], mean action: 1.431 [0.000, 3.000], mean observation: 0.023 [-2.575, 1.000], loss: 0.755584, mean_absolute_error: 34.042828, mean_q: 43.493394, mean_eps: 0.100000
 1548753/2000000: episode: 7409, duration: 3.208s, episode steps: 203, steps per second: 63, episode reward: 224.090, mean reward: 1.104 [-17.823, 100.000], mean action: 1.276 [0.000, 3.000], mean observation: 0.148 [-1.227, 1.000], loss: 1.114525, mean_absolute_error: 32.829230, mean_q: 42.956635, mean_eps: 0.100000
 1548832/2000000: episode: 7410, duration: 1.251s, episode steps: 79, steps per second: 63, episode reward: -27.160, mean reward: -0.344 [-100.000, 11.413], mean action: 1.519 [0.000, 3.000], mean observation: -0.133 [-1.238, 1.738], loss: 0.874248, mean_absolute_error: 32.463672, mean_q: 43.487679, mean_eps: 0.100000
 1549017/2000000: episode: 7411, duration: 2.966s, episode steps: 185, steps per second: 62, episode reward: -31.133, mean reward: -0.168 [-100.000, 12.227], mean action: 1.530 [0.000, 3.000], mean observation: -0.013 [-0.958, 1.136], loss: 1.043123, mean_absolute_error: 33.090555, mean_q: 43.028453, mean_eps: 0.100000
 1549090/2000000: episode: 7412, duration: 1.141s, episode steps: 73, steps per second: 64, episode reward: -39.761, mean reward: -0.545 [-100.000, 18.400], mean action: 1.274 [0.000, 3.000], mean observation: -0.025 [-1.244, 1.000], loss: 1.180025, mean_absolute_error: 32.437797, mean_q: 41.084547, mean_eps: 0.100000
 1549219/2000000: episode: 7413, duration: 2.023s, episode steps: 129, steps per second: 64, episode reward: -28.948, mean reward: -0.224 [-100.000, 10.252], mean action: 1.713 [0.000, 3.000], mean observation: 0.147 [-0.982, 1.334], loss: 0.688413, mean_absolute_error: 32.991266, mean_q: 42.946366, mean_eps: 0.100000
 1549304/2000000: episode: 7414, duration: 1.363s, episode steps: 85, steps per second: 62, episode reward: -51.101, mean reward: -0.601 [-100.000, 20.221], mean action: 1.153 [0.000, 3.000], mean observation: -0.049 [-1.265, 1.000], loss: 0.869480, mean_absolute_error: 33.246309, mean_q: 43.964254, mean_eps: 0.100000
 1549842/2000000: episode: 7415, duration: 8.962s, episode steps: 538, steps per second: 60, episode reward: 235.174, mean reward: 0.437 [-18.205, 100.000], mean action: 0.885 [0.000, 3.000], mean observation: 0.125 [-0.677, 1.000], loss: 0.921303, mean_absolute_error: 33.063431, mean_q: 43.683665, mean_eps: 0.100000
 1550211/2000000: episode: 7416, duration: 5.865s, episode steps: 369, steps per second: 63, episode reward: 173.971, mean reward: 0.471 [-19.881, 100.000], mean action: 1.005 [0.000, 3.000], mean observation: 0.049 [-1.284, 1.000], loss: 1.050492, mean_absolute_error: 32.867219, mean_q: 43.069444, mean_eps: 0.100000
 1550315/2000000: episode: 7417, duration: 1.643s, episode steps: 104, steps per second: 63, episode reward: -34.485, mean reward: -0.332 [-100.000, 12.948], mean action: 1.260 [0.000, 3.000], mean observation: -0.032 [-1.164, 1.000], loss: 0.760769, mean_absolute_error: 32.506059, mean_q: 43.064079, mean_eps: 0.100000
 1550564/2000000: episode: 7418, duration: 3.953s, episode steps: 249, steps per second: 63, episode reward: 239.016, mean reward: 0.960 [-8.561, 100.000], mean action: 1.450 [0.000, 3.000], mean observation: 0.127 [-1.302, 1.005], loss: 0.922770, mean_absolute_error: 32.175958, mean_q: 41.778462, mean_eps: 0.100000
 1550674/2000000: episode: 7419, duration: 1.738s, episode steps: 110, steps per second: 63, episode reward: -30.784, mean reward: -0.280 [-100.000, 7.158], mean action: 1.291 [0.000, 3.000], mean observation: 0.076 [-1.104, 1.243], loss: 1.293591, mean_absolute_error: 32.382851, mean_q: 42.212711, mean_eps: 0.100000
 1550764/2000000: episode: 7420, duration: 1.469s, episode steps: 90, steps per second: 61, episode reward: -23.572, mean reward: -0.262 [-100.000, 12.874], mean action: 1.944 [0.000, 3.000], mean observation: -0.145 [-1.156, 1.000], loss: 0.818581, mean_absolute_error: 31.325204, mean_q: 40.540474, mean_eps: 0.100000
 1550867/2000000: episode: 7421, duration: 1.630s, episode steps: 103, steps per second: 63, episode reward: -50.859, mean reward: -0.494 [-100.000, 13.304], mean action: 2.068 [0.000, 3.000], mean observation: -0.088 [-1.102, 1.288], loss: 0.890663, mean_absolute_error: 32.135911, mean_q: 42.127132, mean_eps: 0.100000
 1550982/2000000: episode: 7422, duration: 1.817s, episode steps: 115, steps per second: 63, episode reward: -4.071, mean reward: -0.035 [-100.000, 13.037], mean action: 1.904 [0.000, 3.000], mean observation: 0.088 [-1.055, 1.000], loss: 0.753280, mean_absolute_error: 33.679925, mean_q: 44.544587, mean_eps: 0.100000
 1551068/2000000: episode: 7423, duration: 1.551s, episode steps: 86, steps per second: 55, episode reward: -38.066, mean reward: -0.443 [-100.000, 17.019], mean action: 1.302 [0.000, 3.000], mean observation: -0.027 [-1.244, 1.000], loss: 0.868058, mean_absolute_error: 32.500284, mean_q: 43.211586, mean_eps: 0.100000
 1551288/2000000: episode: 7424, duration: 3.579s, episode steps: 220, steps per second: 61, episode reward: -265.538, mean reward: -1.207 [-100.000, 4.484], mean action: 1.527 [0.000, 3.000], mean observation: 0.231 [-0.744, 1.115], loss: 0.994490, mean_absolute_error: 33.486730, mean_q: 43.746463, mean_eps: 0.100000
 1551423/2000000: episode: 7425, duration: 2.131s, episode steps: 135, steps per second: 63, episode reward: -28.625, mean reward: -0.212 [-100.000, 10.527], mean action: 1.585 [0.000, 3.000], mean observation: 0.088 [-1.047, 1.098], loss: 0.923913, mean_absolute_error: 33.034379, mean_q: 43.163981, mean_eps: 0.100000
 1551772/2000000: episode: 7426, duration: 5.504s, episode steps: 349, steps per second: 63, episode reward: 230.914, mean reward: 0.662 [-11.670, 100.000], mean action: 0.736 [0.000, 3.000], mean observation: 0.201 [-1.480, 1.000], loss: 1.088898, mean_absolute_error: 32.718749, mean_q: 42.653064, mean_eps: 0.100000
 1552018/2000000: episode: 7427, duration: 3.917s, episode steps: 246, steps per second: 63, episode reward: 185.024, mean reward: 0.752 [-13.200, 100.000], mean action: 1.293 [0.000, 3.000], mean observation: 0.164 [-1.381, 1.000], loss: 0.892920, mean_absolute_error: 32.531336, mean_q: 42.532099, mean_eps: 0.100000
 1552112/2000000: episode: 7428, duration: 1.491s, episode steps: 94, steps per second: 63, episode reward: -34.191, mean reward: -0.364 [-100.000, 13.834], mean action: 1.500 [0.000, 3.000], mean observation: 0.025 [-1.197, 1.000], loss: 1.310976, mean_absolute_error: 34.308554, mean_q: 45.229442, mean_eps: 0.100000
 1552428/2000000: episode: 7429, duration: 5.095s, episode steps: 316, steps per second: 62, episode reward: 161.448, mean reward: 0.511 [-19.192, 100.000], mean action: 2.244 [0.000, 3.000], mean observation: 0.179 [-1.115, 1.000], loss: 1.123352, mean_absolute_error: 32.291879, mean_q: 42.101528, mean_eps: 0.100000
 1552691/2000000: episode: 7430, duration: 4.193s, episode steps: 263, steps per second: 63, episode reward: -340.233, mean reward: -1.294 [-100.000, 4.610], mean action: 1.791 [0.000, 3.000], mean observation: 0.067 [-0.738, 2.083], loss: 0.884020, mean_absolute_error: 33.654868, mean_q: 43.975383, mean_eps: 0.100000
 1552912/2000000: episode: 7431, duration: 3.497s, episode steps: 221, steps per second: 63, episode reward: 202.595, mean reward: 0.917 [-18.959, 100.000], mean action: 0.950 [0.000, 3.000], mean observation: 0.132 [-1.369, 1.000], loss: 1.001555, mean_absolute_error: 34.215141, mean_q: 44.568554, mean_eps: 0.100000
 1553009/2000000: episode: 7432, duration: 1.559s, episode steps: 97, steps per second: 62, episode reward: -57.231, mean reward: -0.590 [-100.000, 11.896], mean action: 1.093 [0.000, 3.000], mean observation: 0.027 [-2.271, 1.000], loss: 0.925778, mean_absolute_error: 33.017185, mean_q: 43.017608, mean_eps: 0.100000
 1553092/2000000: episode: 7433, duration: 1.292s, episode steps: 83, steps per second: 64, episode reward: -45.381, mean reward: -0.547 [-100.000, 14.012], mean action: 1.988 [0.000, 3.000], mean observation: -0.069 [-1.226, 1.231], loss: 1.303495, mean_absolute_error: 32.742161, mean_q: 42.717166, mean_eps: 0.100000
 1553190/2000000: episode: 7434, duration: 1.579s, episode steps: 98, steps per second: 62, episode reward: -34.095, mean reward: -0.348 [-100.000, 14.372], mean action: 1.929 [0.000, 3.000], mean observation: -0.042 [-1.168, 1.000], loss: 0.559153, mean_absolute_error: 32.970404, mean_q: 43.948101, mean_eps: 0.100000
 1553779/2000000: episode: 7435, duration: 9.502s, episode steps: 589, steps per second: 62, episode reward: 195.139, mean reward: 0.331 [-17.297, 100.000], mean action: 2.068 [0.000, 3.000], mean observation: 0.214 [-0.783, 1.000], loss: 1.018108, mean_absolute_error: 33.017861, mean_q: 43.141479, mean_eps: 0.100000
 1553904/2000000: episode: 7436, duration: 2.031s, episode steps: 125, steps per second: 62, episode reward: -51.764, mean reward: -0.414 [-100.000, 10.827], mean action: 1.456 [0.000, 3.000], mean observation: 0.095 [-0.976, 1.540], loss: 0.933706, mean_absolute_error: 33.402948, mean_q: 42.824304, mean_eps: 0.100000
 1554014/2000000: episode: 7437, duration: 1.743s, episode steps: 110, steps per second: 63, episode reward: -40.692, mean reward: -0.370 [-100.000, 13.242], mean action: 1.173 [0.000, 3.000], mean observation: 0.043 [-1.131, 1.000], loss: 0.876088, mean_absolute_error: 34.253519, mean_q: 44.727017, mean_eps: 0.100000
 1554108/2000000: episode: 7438, duration: 1.470s, episode steps: 94, steps per second: 64, episode reward: -10.629, mean reward: -0.113 [-100.000, 19.372], mean action: 1.213 [0.000, 3.000], mean observation: 0.046 [-1.512, 1.000], loss: 1.399643, mean_absolute_error: 33.731023, mean_q: 42.975717, mean_eps: 0.100000
 1554322/2000000: episode: 7439, duration: 3.412s, episode steps: 214, steps per second: 63, episode reward: 212.278, mean reward: 0.992 [-13.458, 100.000], mean action: 1.519 [0.000, 3.000], mean observation: 0.132 [-1.044, 1.000], loss: 0.847694, mean_absolute_error: 33.345920, mean_q: 43.565719, mean_eps: 0.100000
 1554586/2000000: episode: 7440, duration: 4.161s, episode steps: 264, steps per second: 63, episode reward: 212.639, mean reward: 0.805 [-18.441, 100.000], mean action: 0.985 [0.000, 3.000], mean observation: 0.105 [-1.204, 1.000], loss: 1.062264, mean_absolute_error: 33.283087, mean_q: 43.607674, mean_eps: 0.100000
 1555480/2000000: episode: 7441, duration: 14.769s, episode steps: 894, steps per second: 61, episode reward: 173.459, mean reward: 0.194 [-21.059, 100.000], mean action: 2.343 [0.000, 3.000], mean observation: 0.246 [-1.057, 1.000], loss: 1.045502, mean_absolute_error: 33.052278, mean_q: 43.216889, mean_eps: 0.100000
 1555552/2000000: episode: 7442, duration: 1.181s, episode steps: 72, steps per second: 61, episode reward: -36.476, mean reward: -0.507 [-100.000, 16.754], mean action: 1.958 [1.000, 3.000], mean observation: -0.046 [-1.253, 1.000], loss: 0.756583, mean_absolute_error: 33.360585, mean_q: 44.404245, mean_eps: 0.100000
 1555670/2000000: episode: 7443, duration: 1.865s, episode steps: 118, steps per second: 63, episode reward: -33.617, mean reward: -0.285 [-100.000, 9.145], mean action: 1.619 [0.000, 3.000], mean observation: 0.067 [-1.462, 1.008], loss: 0.989630, mean_absolute_error: 32.565180, mean_q: 42.228686, mean_eps: 0.100000
 1555751/2000000: episode: 7444, duration: 1.255s, episode steps: 81, steps per second: 65, episode reward: -76.004, mean reward: -0.938 [-100.000, 15.001], mean action: 1.160 [0.000, 3.000], mean observation: -0.026 [-1.277, 2.468], loss: 1.083264, mean_absolute_error: 33.588692, mean_q: 43.578312, mean_eps: 0.100000
 1556149/2000000: episode: 7445, duration: 6.351s, episode steps: 398, steps per second: 63, episode reward: 225.199, mean reward: 0.566 [-17.468, 100.000], mean action: 2.302 [0.000, 3.000], mean observation: 0.211 [-0.860, 1.000], loss: 0.980061, mean_absolute_error: 34.544746, mean_q: 45.441758, mean_eps: 0.100000
 1556218/2000000: episode: 7446, duration: 1.073s, episode steps: 69, steps per second: 64, episode reward: -56.321, mean reward: -0.816 [-100.000, 11.257], mean action: 1.623 [0.000, 3.000], mean observation: -0.167 [-1.222, 2.522], loss: 0.521777, mean_absolute_error: 32.969013, mean_q: 43.680353, mean_eps: 0.100000
 1556516/2000000: episode: 7447, duration: 5.049s, episode steps: 298, steps per second: 59, episode reward: -194.719, mean reward: -0.653 [-100.000, 5.886], mean action: 1.812 [0.000, 3.000], mean observation: 0.136 [-0.974, 1.001], loss: 0.835325, mean_absolute_error: 33.332096, mean_q: 43.611527, mean_eps: 0.100000
 1556707/2000000: episode: 7448, duration: 3.027s, episode steps: 191, steps per second: 63, episode reward: -220.998, mean reward: -1.157 [-100.000, 35.627], mean action: 1.806 [0.000, 3.000], mean observation: 0.083 [-0.687, 2.032], loss: 1.324396, mean_absolute_error: 31.996060, mean_q: 41.678523, mean_eps: 0.100000
 1557111/2000000: episode: 7449, duration: 6.513s, episode steps: 404, steps per second: 62, episode reward: -185.721, mean reward: -0.460 [-100.000, 5.770], mean action: 1.839 [0.000, 3.000], mean observation: 0.097 [-1.075, 1.002], loss: 0.842137, mean_absolute_error: 33.327289, mean_q: 43.129222, mean_eps: 0.100000
 1557399/2000000: episode: 7450, duration: 4.587s, episode steps: 288, steps per second: 63, episode reward: 213.365, mean reward: 0.741 [-12.215, 100.000], mean action: 1.382 [0.000, 3.000], mean observation: 0.019 [-0.736, 1.000], loss: 0.715290, mean_absolute_error: 32.885172, mean_q: 43.684280, mean_eps: 0.100000
 1557820/2000000: episode: 7451, duration: 6.838s, episode steps: 421, steps per second: 62, episode reward: 208.740, mean reward: 0.496 [-18.912, 100.000], mean action: 1.354 [0.000, 3.000], mean observation: 0.172 [-0.957, 1.000], loss: 1.047160, mean_absolute_error: 33.084796, mean_q: 43.030679, mean_eps: 0.100000
 1557925/2000000: episode: 7452, duration: 1.691s, episode steps: 105, steps per second: 62, episode reward: -33.182, mean reward: -0.316 [-100.000, 25.873], mean action: 1.048 [0.000, 3.000], mean observation: -0.015 [-2.414, 1.013], loss: 0.817846, mean_absolute_error: 33.093167, mean_q: 43.233004, mean_eps: 0.100000
 1558177/2000000: episode: 7453, duration: 3.999s, episode steps: 252, steps per second: 63, episode reward: 167.804, mean reward: 0.666 [-19.372, 100.000], mean action: 2.163 [0.000, 3.000], mean observation: 0.180 [-1.088, 1.000], loss: 0.832308, mean_absolute_error: 33.263794, mean_q: 43.690036, mean_eps: 0.100000
 1558273/2000000: episode: 7454, duration: 1.530s, episode steps: 96, steps per second: 63, episode reward: -45.685, mean reward: -0.476 [-100.000, 9.722], mean action: 1.781 [0.000, 3.000], mean observation: 0.015 [-1.148, 1.000], loss: 0.891255, mean_absolute_error: 33.421856, mean_q: 44.013701, mean_eps: 0.100000
 1558565/2000000: episode: 7455, duration: 4.819s, episode steps: 292, steps per second: 61, episode reward: 253.121, mean reward: 0.867 [-11.451, 100.000], mean action: 1.127 [0.000, 3.000], mean observation: 0.149 [-0.710, 1.000], loss: 0.892473, mean_absolute_error: 32.312435, mean_q: 42.629295, mean_eps: 0.100000
 1558652/2000000: episode: 7456, duration: 1.362s, episode steps: 87, steps per second: 64, episode reward: -6.744, mean reward: -0.078 [-100.000, 19.279], mean action: 1.506 [0.000, 3.000], mean observation: -0.060 [-1.196, 1.498], loss: 0.904111, mean_absolute_error: 32.560994, mean_q: 42.148101, mean_eps: 0.100000
 1558730/2000000: episode: 7457, duration: 1.283s, episode steps: 78, steps per second: 61, episode reward: -46.785, mean reward: -0.600 [-100.000, 15.163], mean action: 1.462 [0.000, 3.000], mean observation: 0.044 [-1.575, 1.000], loss: 0.964294, mean_absolute_error: 33.722694, mean_q: 43.524038, mean_eps: 0.100000
 1558801/2000000: episode: 7458, duration: 1.144s, episode steps: 71, steps per second: 62, episode reward: -61.046, mean reward: -0.860 [-100.000, 10.885], mean action: 1.831 [0.000, 3.000], mean observation: -0.024 [-2.554, 1.000], loss: 0.962886, mean_absolute_error: 32.568867, mean_q: 43.283650, mean_eps: 0.100000
 1559207/2000000: episode: 7459, duration: 6.418s, episode steps: 406, steps per second: 63, episode reward: 249.918, mean reward: 0.616 [-10.466, 100.000], mean action: 1.022 [0.000, 3.000], mean observation: 0.133 [-0.847, 1.000], loss: 1.113616, mean_absolute_error: 33.266901, mean_q: 43.439055, mean_eps: 0.100000
 1559274/2000000: episode: 7460, duration: 1.071s, episode steps: 67, steps per second: 63, episode reward: -17.647, mean reward: -0.263 [-100.000, 18.908], mean action: 1.896 [0.000, 3.000], mean observation: -0.069 [-1.326, 2.120], loss: 0.818822, mean_absolute_error: 33.852722, mean_q: 44.514546, mean_eps: 0.100000
 1559540/2000000: episode: 7461, duration: 4.227s, episode steps: 266, steps per second: 63, episode reward: -186.248, mean reward: -0.700 [-100.000, 9.513], mean action: 1.722 [0.000, 3.000], mean observation: 0.132 [-1.078, 1.055], loss: 1.164233, mean_absolute_error: 31.994018, mean_q: 41.820029, mean_eps: 0.100000
 1559687/2000000: episode: 7462, duration: 2.308s, episode steps: 147, steps per second: 64, episode reward: -48.781, mean reward: -0.332 [-100.000, 11.502], mean action: 1.721 [0.000, 3.000], mean observation: -0.042 [-1.309, 1.018], loss: 0.896875, mean_absolute_error: 32.432484, mean_q: 42.001316, mean_eps: 0.100000
 1560034/2000000: episode: 7463, duration: 5.539s, episode steps: 347, steps per second: 63, episode reward: -36.432, mean reward: -0.105 [-100.000, 13.237], mean action: 1.827 [0.000, 3.000], mean observation: -0.016 [-1.709, 1.000], loss: 0.954328, mean_absolute_error: 33.097357, mean_q: 43.184708, mean_eps: 0.100000
 1560490/2000000: episode: 7464, duration: 7.274s, episode steps: 456, steps per second: 63, episode reward: 218.168, mean reward: 0.478 [-18.319, 100.000], mean action: 0.993 [0.000, 3.000], mean observation: 0.116 [-0.644, 1.000], loss: 0.883060, mean_absolute_error: 33.627661, mean_q: 44.185005, mean_eps: 0.100000
 1560581/2000000: episode: 7465, duration: 1.437s, episode steps: 91, steps per second: 63, episode reward: -66.861, mean reward: -0.735 [-100.000, 13.704], mean action: 1.187 [0.000, 3.000], mean observation: -0.079 [-1.152, 1.000], loss: 0.941036, mean_absolute_error: 33.630809, mean_q: 43.127036, mean_eps: 0.100000
 1560662/2000000: episode: 7466, duration: 1.253s, episode steps: 81, steps per second: 65, episode reward: -43.621, mean reward: -0.539 [-100.000, 19.219], mean action: 1.568 [0.000, 3.000], mean observation: -0.013 [-1.166, 1.782], loss: 1.141205, mean_absolute_error: 31.074639, mean_q: 40.286947, mean_eps: 0.100000
 1560757/2000000: episode: 7467, duration: 1.537s, episode steps: 95, steps per second: 62, episode reward: -17.306, mean reward: -0.182 [-100.000, 19.462], mean action: 1.305 [0.000, 3.000], mean observation: -0.078 [-1.162, 1.000], loss: 0.903931, mean_absolute_error: 31.208613, mean_q: 39.875142, mean_eps: 0.100000
 1560918/2000000: episode: 7468, duration: 2.680s, episode steps: 161, steps per second: 60, episode reward: -25.640, mean reward: -0.159 [-100.000, 12.945], mean action: 1.870 [0.000, 3.000], mean observation: -0.002 [-0.989, 1.000], loss: 0.808560, mean_absolute_error: 31.725419, mean_q: 41.928028, mean_eps: 0.100000
 1561077/2000000: episode: 7469, duration: 2.504s, episode steps: 159, steps per second: 63, episode reward: -245.242, mean reward: -1.542 [-100.000, 6.952], mean action: 1.761 [0.000, 3.000], mean observation: 0.096 [-3.945, 1.487], loss: 0.839255, mean_absolute_error: 35.117040, mean_q: 45.817623, mean_eps: 0.100000
 1561500/2000000: episode: 7470, duration: 6.719s, episode steps: 423, steps per second: 63, episode reward: 231.334, mean reward: 0.547 [-18.640, 100.000], mean action: 0.960 [0.000, 3.000], mean observation: 0.210 [-0.950, 1.000], loss: 0.859658, mean_absolute_error: 32.833027, mean_q: 43.138413, mean_eps: 0.100000
 1561713/2000000: episode: 7471, duration: 3.392s, episode steps: 213, steps per second: 63, episode reward: -263.180, mean reward: -1.236 [-100.000, 5.453], mean action: 1.690 [0.000, 3.000], mean observation: 0.197 [-0.808, 1.072], loss: 0.988485, mean_absolute_error: 33.348191, mean_q: 43.155826, mean_eps: 0.100000
 1562713/2000000: episode: 7472, duration: 16.505s, episode steps: 1000, steps per second: 61, episode reward: -194.721, mean reward: -0.195 [-5.424, 5.002], mean action: 1.675 [0.000, 3.000], mean observation: 0.188 [-1.008, 1.427], loss: 0.946071, mean_absolute_error: 33.675885, mean_q: 43.727578, mean_eps: 0.100000
 1562789/2000000: episode: 7473, duration: 1.199s, episode steps: 76, steps per second: 63, episode reward: -52.651, mean reward: -0.693 [-100.000, 9.986], mean action: 1.882 [0.000, 3.000], mean observation: -0.121 [-1.164, 1.000], loss: 0.931830, mean_absolute_error: 34.884310, mean_q: 46.603138, mean_eps: 0.100000
 1562898/2000000: episode: 7474, duration: 1.664s, episode steps: 109, steps per second: 66, episode reward: -15.408, mean reward: -0.141 [-100.000, 19.981], mean action: 1.202 [0.000, 3.000], mean observation: 0.068 [-1.101, 1.000], loss: 1.179807, mean_absolute_error: 33.798299, mean_q: 44.128970, mean_eps: 0.100000
 1563000/2000000: episode: 7475, duration: 1.617s, episode steps: 102, steps per second: 63, episode reward: -19.264, mean reward: -0.189 [-100.000, 16.288], mean action: 1.118 [0.000, 3.000], mean observation: -0.021 [-1.234, 1.000], loss: 0.902570, mean_absolute_error: 34.706899, mean_q: 45.424513, mean_eps: 0.100000
 1563337/2000000: episode: 7476, duration: 5.496s, episode steps: 337, steps per second: 61, episode reward: 236.612, mean reward: 0.702 [-13.525, 100.000], mean action: 1.412 [0.000, 3.000], mean observation: 0.039 [-0.720, 1.000], loss: 1.127042, mean_absolute_error: 32.500237, mean_q: 42.729736, mean_eps: 0.100000
 1563460/2000000: episode: 7477, duration: 1.922s, episode steps: 123, steps per second: 64, episode reward: -145.876, mean reward: -1.186 [-100.000, 9.924], mean action: 1.301 [0.000, 3.000], mean observation: -0.113 [-1.214, 1.000], loss: 1.070105, mean_absolute_error: 33.919210, mean_q: 44.244580, mean_eps: 0.100000
 1563546/2000000: episode: 7478, duration: 1.360s, episode steps: 86, steps per second: 63, episode reward: -1.052, mean reward: -0.012 [-100.000, 15.707], mean action: 1.314 [0.000, 3.000], mean observation: -0.046 [-1.247, 1.000], loss: 1.052966, mean_absolute_error: 33.389530, mean_q: 43.237060, mean_eps: 0.100000
 1563624/2000000: episode: 7479, duration: 1.243s, episode steps: 78, steps per second: 63, episode reward: -65.201, mean reward: -0.836 [-100.000, 13.451], mean action: 1.872 [0.000, 3.000], mean observation: -0.016 [-1.247, 2.433], loss: 0.800957, mean_absolute_error: 34.408785, mean_q: 44.157551, mean_eps: 0.100000
 1563731/2000000: episode: 7480, duration: 1.731s, episode steps: 107, steps per second: 62, episode reward: -38.044, mean reward: -0.356 [-100.000, 26.137], mean action: 1.925 [0.000, 3.000], mean observation: 0.068 [-1.692, 1.000], loss: 0.901450, mean_absolute_error: 32.940336, mean_q: 43.154094, mean_eps: 0.100000
 1563830/2000000: episode: 7481, duration: 1.573s, episode steps: 99, steps per second: 63, episode reward: -21.839, mean reward: -0.221 [-100.000, 16.134], mean action: 1.242 [0.000, 3.000], mean observation: -0.027 [-1.198, 1.000], loss: 0.869771, mean_absolute_error: 32.823245, mean_q: 42.993554, mean_eps: 0.100000
 1563922/2000000: episode: 7482, duration: 1.492s, episode steps: 92, steps per second: 62, episode reward: -64.155, mean reward: -0.697 [-100.000, 16.623], mean action: 1.946 [0.000, 3.000], mean observation: -0.071 [-1.140, 1.720], loss: 0.833248, mean_absolute_error: 34.275785, mean_q: 44.631562, mean_eps: 0.100000
 1564184/2000000: episode: 7483, duration: 4.139s, episode steps: 262, steps per second: 63, episode reward: 258.832, mean reward: 0.988 [-18.457, 100.000], mean action: 1.252 [0.000, 3.000], mean observation: 0.114 [-1.382, 1.000], loss: 0.985973, mean_absolute_error: 32.091179, mean_q: 42.236469, mean_eps: 0.100000
 1565184/2000000: episode: 7484, duration: 16.861s, episode steps: 1000, steps per second: 59, episode reward: -84.970, mean reward: -0.085 [-5.007, 6.014], mean action: 1.525 [0.000, 3.000], mean observation: 0.112 [-0.567, 0.997], loss: 0.958298, mean_absolute_error: 33.279464, mean_q: 43.264829, mean_eps: 0.100000
 1565462/2000000: episode: 7485, duration: 4.445s, episode steps: 278, steps per second: 63, episode reward: 211.674, mean reward: 0.761 [-17.359, 100.000], mean action: 2.126 [0.000, 3.000], mean observation: 0.191 [-0.913, 1.202], loss: 0.870429, mean_absolute_error: 32.880663, mean_q: 42.837735, mean_eps: 0.100000
 1565556/2000000: episode: 7486, duration: 1.496s, episode steps: 94, steps per second: 63, episode reward: -35.902, mean reward: -0.382 [-100.000, 19.557], mean action: 1.670 [0.000, 3.000], mean observation: 0.028 [-1.164, 1.000], loss: 0.860706, mean_absolute_error: 33.870449, mean_q: 44.144397, mean_eps: 0.100000
 1565891/2000000: episode: 7487, duration: 5.298s, episode steps: 335, steps per second: 63, episode reward: 264.658, mean reward: 0.790 [-10.355, 100.000], mean action: 1.340 [0.000, 3.000], mean observation: 0.118 [-0.915, 1.000], loss: 1.196763, mean_absolute_error: 33.897217, mean_q: 44.267647, mean_eps: 0.100000
 1566232/2000000: episode: 7488, duration: 5.743s, episode steps: 341, steps per second: 59, episode reward: 227.370, mean reward: 0.667 [-6.125, 100.000], mean action: 1.302 [0.000, 3.000], mean observation: 0.072 [-0.900, 1.115], loss: 0.937287, mean_absolute_error: 33.992183, mean_q: 44.662120, mean_eps: 0.100000
 1566315/2000000: episode: 7489, duration: 1.340s, episode steps: 83, steps per second: 62, episode reward: -50.330, mean reward: -0.606 [-100.000, 10.493], mean action: 1.229 [0.000, 3.000], mean observation: -0.082 [-1.179, 1.748], loss: 0.681505, mean_absolute_error: 32.120521, mean_q: 42.470652, mean_eps: 0.100000
 1566489/2000000: episode: 7490, duration: 2.716s, episode steps: 174, steps per second: 64, episode reward: -2.407, mean reward: -0.014 [-100.000, 13.604], mean action: 1.466 [0.000, 3.000], mean observation: 0.064 [-0.893, 1.000], loss: 1.046817, mean_absolute_error: 34.548119, mean_q: 44.567070, mean_eps: 0.100000
 1566577/2000000: episode: 7491, duration: 1.386s, episode steps: 88, steps per second: 63, episode reward: -8.488, mean reward: -0.096 [-100.000, 18.987], mean action: 1.795 [0.000, 3.000], mean observation: -0.022 [-1.268, 1.000], loss: 0.724977, mean_absolute_error: 33.853826, mean_q: 43.889101, mean_eps: 0.100000
 1566677/2000000: episode: 7492, duration: 1.525s, episode steps: 100, steps per second: 66, episode reward: -134.307, mean reward: -1.343 [-100.000, 15.016], mean action: 1.320 [0.000, 3.000], mean observation: -0.128 [-1.445, 1.000], loss: 0.941827, mean_absolute_error: 33.909975, mean_q: 43.214387, mean_eps: 0.100000
 1566755/2000000: episode: 7493, duration: 1.178s, episode steps: 78, steps per second: 66, episode reward: -17.497, mean reward: -0.224 [-100.000, 16.120], mean action: 1.269 [0.000, 3.000], mean observation: -0.024 [-1.224, 1.000], loss: 1.215302, mean_absolute_error: 33.600172, mean_q: 43.255443, mean_eps: 0.100000
 1567030/2000000: episode: 7494, duration: 4.341s, episode steps: 275, steps per second: 63, episode reward: 253.671, mean reward: 0.922 [-19.972, 100.000], mean action: 1.276 [0.000, 3.000], mean observation: 0.169 [-0.778, 1.000], loss: 1.070087, mean_absolute_error: 33.659379, mean_q: 44.063687, mean_eps: 0.100000
 1567138/2000000: episode: 7495, duration: 1.700s, episode steps: 108, steps per second: 64, episode reward: -62.791, mean reward: -0.581 [-100.000, 15.412], mean action: 1.481 [0.000, 3.000], mean observation: -0.112 [-0.924, 1.488], loss: 1.144194, mean_absolute_error: 32.969485, mean_q: 43.674300, mean_eps: 0.100000
 1567216/2000000: episode: 7496, duration: 1.298s, episode steps: 78, steps per second: 60, episode reward: -21.013, mean reward: -0.269 [-100.000, 25.823], mean action: 1.923 [0.000, 3.000], mean observation: 0.013 [-1.132, 1.000], loss: 0.735851, mean_absolute_error: 30.201850, mean_q: 39.578613, mean_eps: 0.100000
 1567388/2000000: episode: 7497, duration: 2.763s, episode steps: 172, steps per second: 62, episode reward: -138.281, mean reward: -0.804 [-100.000, 10.341], mean action: 1.407 [0.000, 3.000], mean observation: 0.122 [-1.041, 1.000], loss: 0.982564, mean_absolute_error: 34.177258, mean_q: 45.290602, mean_eps: 0.100000
 1568388/2000000: episode: 7498, duration: 16.986s, episode steps: 1000, steps per second: 59, episode reward: -277.505, mean reward: -0.278 [-5.578, 5.070], mean action: 1.501 [0.000, 3.000], mean observation: 0.246 [-0.698, 2.346], loss: 1.024460, mean_absolute_error: 33.244587, mean_q: 43.409988, mean_eps: 0.100000
 1568476/2000000: episode: 7499, duration: 1.427s, episode steps: 88, steps per second: 62, episode reward: -63.963, mean reward: -0.727 [-100.000, 19.862], mean action: 1.591 [0.000, 3.000], mean observation: -0.062 [-1.226, 1.000], loss: 1.142438, mean_absolute_error: 35.319323, mean_q: 44.958922, mean_eps: 0.100000
 1568542/2000000: episode: 7500, duration: 1.052s, episode steps: 66, steps per second: 63, episode reward: -59.284, mean reward: -0.898 [-100.000, 14.354], mean action: 2.015 [0.000, 3.000], mean observation: -0.179 [-3.021, 1.000], loss: 1.030855, mean_absolute_error: 32.124734, mean_q: 42.129834, mean_eps: 0.100000
 1568626/2000000: episode: 7501, duration: 1.309s, episode steps: 84, steps per second: 64, episode reward: -38.942, mean reward: -0.464 [-100.000, 13.628], mean action: 1.476 [0.000, 3.000], mean observation: -0.052 [-1.198, 1.000], loss: 0.846334, mean_absolute_error: 31.453567, mean_q: 41.099389, mean_eps: 0.100000
 1568800/2000000: episode: 7502, duration: 2.748s, episode steps: 174, steps per second: 63, episode reward: -3.915, mean reward: -0.023 [-100.000, 9.629], mean action: 1.925 [0.000, 3.000], mean observation: 0.111 [-0.672, 1.000], loss: 1.213108, mean_absolute_error: 32.957920, mean_q: 42.689674, mean_eps: 0.100000
 1568889/2000000: episode: 7503, duration: 1.456s, episode steps: 89, steps per second: 61, episode reward: -35.638, mean reward: -0.400 [-100.000, 14.365], mean action: 1.978 [0.000, 3.000], mean observation: -0.060 [-1.620, 1.000], loss: 1.317535, mean_absolute_error: 32.512157, mean_q: 42.600373, mean_eps: 0.100000
 1569051/2000000: episode: 7504, duration: 2.503s, episode steps: 162, steps per second: 65, episode reward: -49.673, mean reward: -0.307 [-100.000, 25.477], mean action: 1.932 [0.000, 3.000], mean observation: 0.130 [-0.838, 1.117], loss: 1.061365, mean_absolute_error: 32.361255, mean_q: 42.393285, mean_eps: 0.100000
 1569121/2000000: episode: 7505, duration: 1.146s, episode steps: 70, steps per second: 61, episode reward: -60.201, mean reward: -0.860 [-100.000, 17.455], mean action: 2.043 [0.000, 3.000], mean observation: -0.159 [-1.231, 1.000], loss: 0.737867, mean_absolute_error: 35.074427, mean_q: 44.397881, mean_eps: 0.100000
 1569544/2000000: episode: 7506, duration: 6.791s, episode steps: 423, steps per second: 62, episode reward: 227.707, mean reward: 0.538 [-19.715, 100.000], mean action: 0.707 [0.000, 3.000], mean observation: 0.183 [-1.136, 1.000], loss: 0.850385, mean_absolute_error: 34.021908, mean_q: 44.560686, mean_eps: 0.100000
 1569816/2000000: episode: 7507, duration: 4.287s, episode steps: 272, steps per second: 63, episode reward: 223.912, mean reward: 0.823 [-19.072, 100.000], mean action: 0.886 [0.000, 3.000], mean observation: 0.099 [-1.195, 1.000], loss: 0.904580, mean_absolute_error: 33.876148, mean_q: 44.615773, mean_eps: 0.100000
 1569883/2000000: episode: 7508, duration: 1.077s, episode steps: 67, steps per second: 62, episode reward: -58.102, mean reward: -0.867 [-100.000, 10.196], mean action: 1.537 [0.000, 3.000], mean observation: -0.118 [-1.183, 3.030], loss: 1.935679, mean_absolute_error: 32.239122, mean_q: 41.181708, mean_eps: 0.100000
 1569973/2000000: episode: 7509, duration: 1.424s, episode steps: 90, steps per second: 63, episode reward: -27.697, mean reward: -0.308 [-100.000, 25.255], mean action: 1.311 [0.000, 3.000], mean observation: -0.076 [-1.182, 1.000], loss: 0.630744, mean_absolute_error: 32.701304, mean_q: 43.415927, mean_eps: 0.100000
 1570060/2000000: episode: 7510, duration: 1.372s, episode steps: 87, steps per second: 63, episode reward: -50.299, mean reward: -0.578 [-100.000, 20.778], mean action: 1.184 [0.000, 3.000], mean observation: -0.095 [-1.200, 2.181], loss: 0.964330, mean_absolute_error: 31.755211, mean_q: 41.988588, mean_eps: 0.100000
 1570173/2000000: episode: 7511, duration: 1.797s, episode steps: 113, steps per second: 63, episode reward: -35.074, mean reward: -0.310 [-100.000, 19.985], mean action: 1.204 [0.000, 3.000], mean observation: 0.078 [-1.069, 1.000], loss: 0.808743, mean_absolute_error: 33.803320, mean_q: 44.368110, mean_eps: 0.100000
 1570257/2000000: episode: 7512, duration: 1.385s, episode steps: 84, steps per second: 61, episode reward: -14.697, mean reward: -0.175 [-100.000, 17.102], mean action: 1.321 [0.000, 3.000], mean observation: -0.018 [-1.563, 1.000], loss: 0.750178, mean_absolute_error: 33.006707, mean_q: 44.372819, mean_eps: 0.100000
 1570339/2000000: episode: 7513, duration: 1.396s, episode steps: 82, steps per second: 59, episode reward: -30.049, mean reward: -0.366 [-100.000, 17.903], mean action: 1.232 [0.000, 3.000], mean observation: -0.040 [-1.220, 1.000], loss: 0.798825, mean_absolute_error: 31.802775, mean_q: 42.067326, mean_eps: 0.100000
 1570427/2000000: episode: 7514, duration: 1.361s, episode steps: 88, steps per second: 65, episode reward: -44.025, mean reward: -0.500 [-100.000, 18.793], mean action: 1.898 [0.000, 3.000], mean observation: -0.067 [-1.176, 1.078], loss: 0.905592, mean_absolute_error: 35.381669, mean_q: 46.889380, mean_eps: 0.100000
 1570831/2000000: episode: 7515, duration: 6.365s, episode steps: 404, steps per second: 63, episode reward: 230.844, mean reward: 0.571 [-24.348, 100.000], mean action: 1.136 [0.000, 3.000], mean observation: 0.087 [-1.170, 1.000], loss: 0.994445, mean_absolute_error: 32.828771, mean_q: 43.051148, mean_eps: 0.100000
 1570908/2000000: episode: 7516, duration: 1.257s, episode steps: 77, steps per second: 61, episode reward: -5.525, mean reward: -0.072 [-100.000, 16.321], mean action: 1.532 [0.000, 3.000], mean observation: 0.017 [-1.197, 1.000], loss: 0.948149, mean_absolute_error: 34.486090, mean_q: 46.172674, mean_eps: 0.100000
 1571198/2000000: episode: 7517, duration: 4.611s, episode steps: 290, steps per second: 63, episode reward: -200.131, mean reward: -0.690 [-100.000, 3.968], mean action: 1.417 [0.000, 3.000], mean observation: 0.138 [-0.983, 1.003], loss: 0.985016, mean_absolute_error: 33.724898, mean_q: 44.439331, mean_eps: 0.100000
 1571309/2000000: episode: 7518, duration: 1.746s, episode steps: 111, steps per second: 64, episode reward: 17.682, mean reward: 0.159 [-100.000, 16.210], mean action: 1.351 [0.000, 3.000], mean observation: -0.065 [-1.130, 1.277], loss: 1.293488, mean_absolute_error: 34.064919, mean_q: 44.562233, mean_eps: 0.100000
 1572097/2000000: episode: 7519, duration: 12.608s, episode steps: 788, steps per second: 62, episode reward: 222.051, mean reward: 0.282 [-17.997, 100.000], mean action: 1.098 [0.000, 3.000], mean observation: 0.231 [-1.008, 1.000], loss: 0.901238, mean_absolute_error: 34.052529, mean_q: 44.585908, mean_eps: 0.100000
 1572161/2000000: episode: 7520, duration: 1.012s, episode steps: 64, steps per second: 63, episode reward: -38.064, mean reward: -0.595 [-100.000, 12.994], mean action: 1.562 [0.000, 3.000], mean observation: -0.168 [-1.191, 1.000], loss: 0.853874, mean_absolute_error: 33.080053, mean_q: 43.064095, mean_eps: 0.100000
 1572263/2000000: episode: 7521, duration: 1.556s, episode steps: 102, steps per second: 66, episode reward: -33.481, mean reward: -0.328 [-100.000, 14.582], mean action: 1.931 [0.000, 3.000], mean observation: -0.090 [-1.164, 1.063], loss: 0.705117, mean_absolute_error: 33.058257, mean_q: 42.977117, mean_eps: 0.100000
 1572373/2000000: episode: 7522, duration: 1.747s, episode steps: 110, steps per second: 63, episode reward: -42.678, mean reward: -0.388 [-100.000, 15.696], mean action: 1.927 [0.000, 3.000], mean observation: 0.044 [-1.146, 1.091], loss: 0.863091, mean_absolute_error: 34.841502, mean_q: 44.782027, mean_eps: 0.100000
 1572454/2000000: episode: 7523, duration: 1.273s, episode steps: 81, steps per second: 64, episode reward: -70.046, mean reward: -0.865 [-100.000, 11.263], mean action: 1.988 [0.000, 3.000], mean observation: -0.116 [-1.243, 2.795], loss: 0.987479, mean_absolute_error: 32.437127, mean_q: 42.103287, mean_eps: 0.100000
 1572644/2000000: episode: 7524, duration: 2.992s, episode steps: 190, steps per second: 64, episode reward: 167.720, mean reward: 0.883 [-16.393, 100.000], mean action: 1.132 [0.000, 3.000], mean observation: 0.110 [-1.136, 1.000], loss: 1.004133, mean_absolute_error: 33.231953, mean_q: 43.309069, mean_eps: 0.100000
 1572760/2000000: episode: 7525, duration: 1.864s, episode steps: 116, steps per second: 62, episode reward: -16.279, mean reward: -0.140 [-100.000, 13.239], mean action: 1.879 [0.000, 3.000], mean observation: 0.083 [-0.973, 1.000], loss: 0.712200, mean_absolute_error: 33.548925, mean_q: 44.566375, mean_eps: 0.100000
 1572823/2000000: episode: 7526, duration: 0.993s, episode steps: 63, steps per second: 63, episode reward: -61.792, mean reward: -0.981 [-100.000, 12.229], mean action: 1.603 [0.000, 3.000], mean observation: -0.160 [-1.243, 1.692], loss: 1.074561, mean_absolute_error: 35.427397, mean_q: 45.858457, mean_eps: 0.100000
 1572903/2000000: episode: 7527, duration: 1.256s, episode steps: 80, steps per second: 64, episode reward: -49.070, mean reward: -0.613 [-100.000, 19.354], mean action: 1.462 [0.000, 3.000], mean observation: -0.006 [-1.163, 1.000], loss: 1.294880, mean_absolute_error: 34.934302, mean_q: 44.625060, mean_eps: 0.100000
 1572989/2000000: episode: 7528, duration: 1.374s, episode steps: 86, steps per second: 63, episode reward: 19.928, mean reward: 0.232 [-100.000, 26.026], mean action: 1.349 [0.000, 3.000], mean observation: -0.030 [-1.218, 1.000], loss: 0.893253, mean_absolute_error: 33.461030, mean_q: 43.739551, mean_eps: 0.100000
 1573583/2000000: episode: 7529, duration: 9.924s, episode steps: 594, steps per second: 60, episode reward: 197.645, mean reward: 0.333 [-17.233, 100.000], mean action: 1.436 [0.000, 3.000], mean observation: 0.247 [-0.864, 1.000], loss: 1.000571, mean_absolute_error: 33.980790, mean_q: 44.341432, mean_eps: 0.100000
 1573907/2000000: episode: 7530, duration: 5.133s, episode steps: 324, steps per second: 63, episode reward: 217.553, mean reward: 0.671 [-13.044, 100.000], mean action: 1.290 [0.000, 3.000], mean observation: 0.059 [-0.919, 1.000], loss: 1.103354, mean_absolute_error: 33.958966, mean_q: 44.451607, mean_eps: 0.100000
 1573994/2000000: episode: 7531, duration: 1.376s, episode steps: 87, steps per second: 63, episode reward: -12.005, mean reward: -0.138 [-100.000, 14.537], mean action: 1.874 [0.000, 3.000], mean observation: -0.044 [-1.217, 1.000], loss: 0.689644, mean_absolute_error: 33.355469, mean_q: 44.580750, mean_eps: 0.100000
 1574079/2000000: episode: 7532, duration: 1.293s, episode steps: 85, steps per second: 66, episode reward: -35.340, mean reward: -0.416 [-100.000, 20.961], mean action: 1.329 [0.000, 3.000], mean observation: -0.012 [-1.180, 1.115], loss: 0.829039, mean_absolute_error: 35.516599, mean_q: 47.026908, mean_eps: 0.100000
 1574173/2000000: episode: 7533, duration: 1.483s, episode steps: 94, steps per second: 63, episode reward: -40.380, mean reward: -0.430 [-100.000, 20.849], mean action: 1.149 [0.000, 3.000], mean observation: -0.031 [-1.203, 1.452], loss: 0.691846, mean_absolute_error: 34.584681, mean_q: 46.052127, mean_eps: 0.100000
 1574264/2000000: episode: 7534, duration: 1.419s, episode steps: 91, steps per second: 64, episode reward: -88.100, mean reward: -0.968 [-100.000, 9.901], mean action: 0.714 [0.000, 3.000], mean observation: 0.004 [-3.209, 1.000], loss: 0.999823, mean_absolute_error: 33.640372, mean_q: 43.396864, mean_eps: 0.100000
 1574430/2000000: episode: 7535, duration: 2.605s, episode steps: 166, steps per second: 64, episode reward: -90.380, mean reward: -0.544 [-100.000, 12.449], mean action: 1.705 [0.000, 3.000], mean observation: 0.072 [-1.103, 1.147], loss: 1.337764, mean_absolute_error: 34.655618, mean_q: 45.039266, mean_eps: 0.100000
 1575430/2000000: episode: 7536, duration: 16.160s, episode steps: 1000, steps per second: 62, episode reward: 11.545, mean reward: 0.012 [-19.441, 19.654], mean action: 0.955 [0.000, 3.000], mean observation: 0.216 [-0.968, 1.003], loss: 1.059235, mean_absolute_error: 33.589146, mean_q: 44.011044, mean_eps: 0.100000
 1575580/2000000: episode: 7537, duration: 2.390s, episode steps: 150, steps per second: 63, episode reward: -95.900, mean reward: -0.639 [-100.000, 18.372], mean action: 1.767 [0.000, 3.000], mean observation: 0.066 [-0.948, 1.000], loss: 0.615811, mean_absolute_error: 34.138719, mean_q: 45.276232, mean_eps: 0.100000
 1575664/2000000: episode: 7538, duration: 1.383s, episode steps: 84, steps per second: 61, episode reward: -51.281, mean reward: -0.610 [-100.000, 17.891], mean action: 1.071 [0.000, 3.000], mean observation: -0.047 [-1.232, 1.000], loss: 1.118248, mean_absolute_error: 33.384434, mean_q: 43.119246, mean_eps: 0.100000
 1575766/2000000: episode: 7539, duration: 1.627s, episode steps: 102, steps per second: 63, episode reward: -57.629, mean reward: -0.565 [-100.000, 7.014], mean action: 1.078 [0.000, 3.000], mean observation: 0.032 [-1.131, 1.000], loss: 0.945318, mean_absolute_error: 34.518022, mean_q: 45.977282, mean_eps: 0.100000
 1576049/2000000: episode: 7540, duration: 4.582s, episode steps: 283, steps per second: 62, episode reward: 172.939, mean reward: 0.611 [-17.345, 100.000], mean action: 2.244 [0.000, 3.000], mean observation: 0.209 [-1.047, 1.000], loss: 0.802723, mean_absolute_error: 33.123566, mean_q: 43.412504, mean_eps: 0.100000
 1576519/2000000: episode: 7541, duration: 7.331s, episode steps: 470, steps per second: 64, episode reward: 162.758, mean reward: 0.346 [-19.695, 100.000], mean action: 1.134 [0.000, 3.000], mean observation: 0.054 [-1.171, 1.051], loss: 0.845449, mean_absolute_error: 33.517450, mean_q: 44.001677, mean_eps: 0.100000
 1576600/2000000: episode: 7542, duration: 1.318s, episode steps: 81, steps per second: 61, episode reward: -44.619, mean reward: -0.551 [-100.000, 17.870], mean action: 1.272 [0.000, 3.000], mean observation: -0.114 [-1.195, 1.000], loss: 0.695906, mean_absolute_error: 33.361709, mean_q: 43.218386, mean_eps: 0.100000
 1576913/2000000: episode: 7543, duration: 4.943s, episode steps: 313, steps per second: 63, episode reward: 213.177, mean reward: 0.681 [-2.863, 100.000], mean action: 1.463 [0.000, 3.000], mean observation: 0.044 [-0.632, 1.000], loss: 0.939028, mean_absolute_error: 33.361019, mean_q: 43.890189, mean_eps: 0.100000
 1577023/2000000: episode: 7544, duration: 1.685s, episode steps: 110, steps per second: 65, episode reward: -7.617, mean reward: -0.069 [-100.000, 18.257], mean action: 1.282 [0.000, 3.000], mean observation: 0.019 [-1.192, 1.000], loss: 1.121475, mean_absolute_error: 33.866016, mean_q: 44.529680, mean_eps: 0.100000
 1577126/2000000: episode: 7545, duration: 1.626s, episode steps: 103, steps per second: 63, episode reward: -59.644, mean reward: -0.579 [-100.000, 22.622], mean action: 1.777 [0.000, 3.000], mean observation: -0.018 [-1.232, 3.004], loss: 1.019557, mean_absolute_error: 32.970452, mean_q: 43.244313, mean_eps: 0.100000
 1577338/2000000: episode: 7546, duration: 3.335s, episode steps: 212, steps per second: 64, episode reward: 204.831, mean reward: 0.966 [-10.254, 100.000], mean action: 2.009 [0.000, 3.000], mean observation: 0.126 [-1.022, 1.000], loss: 0.954068, mean_absolute_error: 32.919636, mean_q: 43.234325, mean_eps: 0.100000
 1577902/2000000: episode: 7547, duration: 9.146s, episode steps: 564, steps per second: 62, episode reward: 138.633, mean reward: 0.246 [-18.688, 100.000], mean action: 1.326 [0.000, 3.000], mean observation: 0.155 [-1.151, 1.000], loss: 0.999512, mean_absolute_error: 33.047511, mean_q: 43.217609, mean_eps: 0.100000
 1578339/2000000: episode: 7548, duration: 7.017s, episode steps: 437, steps per second: 62, episode reward: 174.352, mean reward: 0.399 [-19.068, 100.000], mean action: 2.483 [0.000, 3.000], mean observation: 0.263 [-1.071, 1.169], loss: 0.953260, mean_absolute_error: 34.686701, mean_q: 45.445402, mean_eps: 0.100000
 1578590/2000000: episode: 7549, duration: 3.926s, episode steps: 251, steps per second: 64, episode reward: 201.753, mean reward: 0.804 [-17.378, 100.000], mean action: 0.884 [0.000, 3.000], mean observation: 0.189 [-1.090, 1.000], loss: 1.294034, mean_absolute_error: 34.373545, mean_q: 45.594667, mean_eps: 0.100000
 1579422/2000000: episode: 7550, duration: 13.187s, episode steps: 832, steps per second: 63, episode reward: 132.836, mean reward: 0.160 [-23.733, 100.000], mean action: 0.863 [0.000, 3.000], mean observation: 0.236 [-1.079, 1.000], loss: 1.086115, mean_absolute_error: 33.160730, mean_q: 43.040931, mean_eps: 0.100000
 1579522/2000000: episode: 7551, duration: 1.564s, episode steps: 100, steps per second: 64, episode reward: -31.506, mean reward: -0.315 [-100.000, 23.493], mean action: 0.810 [0.000, 3.000], mean observation: -0.012 [-1.295, 1.000], loss: 1.226461, mean_absolute_error: 31.120659, mean_q: 40.520046, mean_eps: 0.100000
 1579954/2000000: episode: 7552, duration: 6.788s, episode steps: 432, steps per second: 64, episode reward: 136.560, mean reward: 0.316 [-14.505, 100.000], mean action: 1.326 [0.000, 3.000], mean observation: 0.145 [-0.993, 1.000], loss: 0.882144, mean_absolute_error: 33.011623, mean_q: 43.332759, mean_eps: 0.100000
 1580307/2000000: episode: 7553, duration: 5.615s, episode steps: 353, steps per second: 63, episode reward: 148.992, mean reward: 0.422 [-19.899, 100.000], mean action: 1.331 [0.000, 3.000], mean observation: 0.147 [-0.664, 1.000], loss: 1.043760, mean_absolute_error: 33.470442, mean_q: 43.374938, mean_eps: 0.100000
 1580634/2000000: episode: 7554, duration: 5.228s, episode steps: 327, steps per second: 63, episode reward: -374.513, mean reward: -1.145 [-100.000, 5.607], mean action: 1.853 [0.000, 3.000], mean observation: 0.110 [-0.712, 1.570], loss: 0.872204, mean_absolute_error: 34.195276, mean_q: 45.028814, mean_eps: 0.100000
 1580711/2000000: episode: 7555, duration: 1.198s, episode steps: 77, steps per second: 64, episode reward: -62.013, mean reward: -0.805 [-100.000, 8.889], mean action: 1.961 [0.000, 3.000], mean observation: -0.082 [-1.150, 2.651], loss: 0.947343, mean_absolute_error: 33.036525, mean_q: 42.673669, mean_eps: 0.100000
 1581433/2000000: episode: 7556, duration: 11.700s, episode steps: 722, steps per second: 62, episode reward: 208.001, mean reward: 0.288 [-23.196, 100.000], mean action: 1.608 [0.000, 3.000], mean observation: 0.103 [-1.170, 1.000], loss: 1.081576, mean_absolute_error: 33.722963, mean_q: 44.143659, mean_eps: 0.100000
 1581521/2000000: episode: 7557, duration: 1.357s, episode steps: 88, steps per second: 65, episode reward: -25.629, mean reward: -0.291 [-100.000, 20.476], mean action: 1.193 [0.000, 3.000], mean observation: -0.063 [-1.125, 1.368], loss: 0.974797, mean_absolute_error: 33.031988, mean_q: 43.548952, mean_eps: 0.100000
 1581708/2000000: episode: 7558, duration: 2.879s, episode steps: 187, steps per second: 65, episode reward: 243.373, mean reward: 1.301 [-10.120, 100.000], mean action: 1.005 [0.000, 3.000], mean observation: 0.115 [-1.419, 1.008], loss: 0.970401, mean_absolute_error: 34.200689, mean_q: 45.001187, mean_eps: 0.100000
 1581808/2000000: episode: 7559, duration: 1.647s, episode steps: 100, steps per second: 61, episode reward: -36.964, mean reward: -0.370 [-100.000, 13.104], mean action: 1.570 [0.000, 3.000], mean observation: 0.033 [-1.179, 1.000], loss: 1.344139, mean_absolute_error: 34.229739, mean_q: 44.113542, mean_eps: 0.100000
 1582808/2000000: episode: 7560, duration: 17.535s, episode steps: 1000, steps per second: 57, episode reward: -157.800, mean reward: -0.158 [-4.871, 4.036], mean action: 1.417 [0.000, 3.000], mean observation: 0.204 [-0.718, 1.197], loss: 0.947258, mean_absolute_error: 33.921102, mean_q: 44.284596, mean_eps: 0.100000
 1582892/2000000: episode: 7561, duration: 1.385s, episode steps: 84, steps per second: 61, episode reward: -14.720, mean reward: -0.175 [-100.000, 14.332], mean action: 1.905 [0.000, 3.000], mean observation: 0.011 [-1.121, 1.000], loss: 0.735873, mean_absolute_error: 32.348325, mean_q: 42.165080, mean_eps: 0.100000
 1582968/2000000: episode: 7562, duration: 1.271s, episode steps: 76, steps per second: 60, episode reward: -45.894, mean reward: -0.604 [-100.000, 13.116], mean action: 1.763 [0.000, 3.000], mean observation: -0.121 [-1.144, 1.458], loss: 1.410559, mean_absolute_error: 33.518560, mean_q: 43.799711, mean_eps: 0.100000
 1583458/2000000: episode: 7563, duration: 7.889s, episode steps: 490, steps per second: 62, episode reward: 212.530, mean reward: 0.434 [-18.405, 100.000], mean action: 2.388 [0.000, 3.000], mean observation: 0.232 [-1.071, 1.000], loss: 1.011105, mean_absolute_error: 33.082858, mean_q: 42.878992, mean_eps: 0.100000
 1583545/2000000: episode: 7564, duration: 1.370s, episode steps: 87, steps per second: 64, episode reward: 1.813, mean reward: 0.021 [-100.000, 18.321], mean action: 1.839 [0.000, 3.000], mean observation: -0.059 [-1.147, 1.382], loss: 0.971590, mean_absolute_error: 32.196815, mean_q: 42.429231, mean_eps: 0.100000
 1584095/2000000: episode: 7565, duration: 8.685s, episode steps: 550, steps per second: 63, episode reward: -389.874, mean reward: -0.709 [-100.000, 4.700], mean action: 1.680 [0.000, 3.000], mean observation: 0.158 [-0.617, 1.929], loss: 0.934082, mean_absolute_error: 33.695215, mean_q: 44.259732, mean_eps: 0.100000
 1584198/2000000: episode: 7566, duration: 1.597s, episode steps: 103, steps per second: 65, episode reward: -28.408, mean reward: -0.276 [-100.000, 21.568], mean action: 1.641 [0.000, 3.000], mean observation: -0.023 [-1.166, 1.000], loss: 0.801872, mean_absolute_error: 33.726899, mean_q: 43.856030, mean_eps: 0.100000
 1584283/2000000: episode: 7567, duration: 1.330s, episode steps: 85, steps per second: 64, episode reward: -44.313, mean reward: -0.521 [-100.000, 11.287], mean action: 1.941 [0.000, 3.000], mean observation: -0.138 [-1.117, 1.687], loss: 0.646681, mean_absolute_error: 33.826991, mean_q: 44.708362, mean_eps: 0.100000
 1584700/2000000: episode: 7568, duration: 6.619s, episode steps: 417, steps per second: 63, episode reward: 189.460, mean reward: 0.454 [-19.398, 100.000], mean action: 1.360 [0.000, 3.000], mean observation: 0.135 [-1.148, 1.000], loss: 1.031869, mean_absolute_error: 33.803989, mean_q: 44.245870, mean_eps: 0.100000
 1584799/2000000: episode: 7569, duration: 1.554s, episode steps: 99, steps per second: 64, episode reward: -18.173, mean reward: -0.184 [-100.000, 16.294], mean action: 1.232 [0.000, 3.000], mean observation: -0.013 [-1.194, 1.000], loss: 0.727331, mean_absolute_error: 32.532956, mean_q: 43.142235, mean_eps: 0.100000
 1584998/2000000: episode: 7570, duration: 3.091s, episode steps: 199, steps per second: 64, episode reward: 14.289, mean reward: 0.072 [-100.000, 17.381], mean action: 1.543 [0.000, 3.000], mean observation: 0.011 [-1.148, 1.040], loss: 0.915742, mean_absolute_error: 33.977701, mean_q: 44.832732, mean_eps: 0.100000
 1585432/2000000: episode: 7571, duration: 6.757s, episode steps: 434, steps per second: 64, episode reward: 217.300, mean reward: 0.501 [-23.459, 100.000], mean action: 0.882 [0.000, 3.000], mean observation: 0.212 [-0.802, 1.000], loss: 0.894074, mean_absolute_error: 34.255086, mean_q: 44.911859, mean_eps: 0.100000
 1585523/2000000: episode: 7572, duration: 1.440s, episode steps: 91, steps per second: 63, episode reward: -26.485, mean reward: -0.291 [-100.000, 15.112], mean action: 1.264 [0.000, 3.000], mean observation: -0.029 [-1.167, 1.000], loss: 0.989124, mean_absolute_error: 33.349267, mean_q: 44.090579, mean_eps: 0.100000
 1585621/2000000: episode: 7573, duration: 1.539s, episode steps: 98, steps per second: 64, episode reward: -8.597, mean reward: -0.088 [-100.000, 20.091], mean action: 1.582 [0.000, 3.000], mean observation: 0.001 [-1.176, 1.000], loss: 1.083382, mean_absolute_error: 31.690866, mean_q: 40.676169, mean_eps: 0.100000
 1585694/2000000: episode: 7574, duration: 1.121s, episode steps: 73, steps per second: 65, episode reward: -43.461, mean reward: -0.595 [-100.000, 9.762], mean action: 1.904 [0.000, 3.000], mean observation: -0.115 [-1.178, 2.961], loss: 0.856539, mean_absolute_error: 34.576961, mean_q: 44.298798, mean_eps: 0.100000
 1585801/2000000: episode: 7575, duration: 1.653s, episode steps: 107, steps per second: 65, episode reward: -39.725, mean reward: -0.371 [-100.000, 11.886], mean action: 1.570 [0.000, 3.000], mean observation: 0.042 [-1.363, 1.000], loss: 0.949107, mean_absolute_error: 34.692902, mean_q: 45.764621, mean_eps: 0.100000
 1585912/2000000: episode: 7576, duration: 1.717s, episode steps: 111, steps per second: 65, episode reward: -55.490, mean reward: -0.500 [-100.000, 21.579], mean action: 1.649 [0.000, 3.000], mean observation: -0.092 [-1.150, 1.840], loss: 0.909687, mean_absolute_error: 32.725497, mean_q: 43.259499, mean_eps: 0.100000
 1586190/2000000: episode: 7577, duration: 4.397s, episode steps: 278, steps per second: 63, episode reward: 263.533, mean reward: 0.948 [-18.552, 100.000], mean action: 1.356 [0.000, 3.000], mean observation: 0.126 [-1.162, 1.009], loss: 1.093116, mean_absolute_error: 34.101101, mean_q: 44.071499, mean_eps: 0.100000
 1586517/2000000: episode: 7578, duration: 5.146s, episode steps: 327, steps per second: 64, episode reward: -317.365, mean reward: -0.971 [-100.000, 4.604], mean action: 1.446 [0.000, 3.000], mean observation: 0.173 [-0.928, 1.090], loss: 0.990504, mean_absolute_error: 33.831984, mean_q: 44.170163, mean_eps: 0.100000
 1586954/2000000: episode: 7579, duration: 6.964s, episode steps: 437, steps per second: 63, episode reward: -211.996, mean reward: -0.485 [-100.000, 4.330], mean action: 1.794 [0.000, 3.000], mean observation: 0.148 [-1.022, 1.190], loss: 1.128437, mean_absolute_error: 34.510729, mean_q: 45.185154, mean_eps: 0.100000
 1587021/2000000: episode: 7580, duration: 1.074s, episode steps: 67, steps per second: 62, episode reward: -47.145, mean reward: -0.704 [-100.000, 23.153], mean action: 1.701 [0.000, 3.000], mean observation: -0.158 [-1.159, 1.000], loss: 0.755036, mean_absolute_error: 35.043312, mean_q: 46.188934, mean_eps: 0.100000
 1587109/2000000: episode: 7581, duration: 1.361s, episode steps: 88, steps per second: 65, episode reward: -33.408, mean reward: -0.380 [-100.000, 19.498], mean action: 2.034 [0.000, 3.000], mean observation: -0.108 [-1.197, 1.732], loss: 0.939023, mean_absolute_error: 33.888692, mean_q: 44.335298, mean_eps: 0.100000
 1587283/2000000: episode: 7582, duration: 2.677s, episode steps: 174, steps per second: 65, episode reward: 208.753, mean reward: 1.200 [-15.088, 100.000], mean action: 1.736 [0.000, 3.000], mean observation: 0.135 [-0.789, 1.108], loss: 0.955675, mean_absolute_error: 32.835854, mean_q: 42.399625, mean_eps: 0.100000
 1587530/2000000: episode: 7583, duration: 3.891s, episode steps: 247, steps per second: 63, episode reward: 208.498, mean reward: 0.844 [-10.251, 100.000], mean action: 1.421 [0.000, 3.000], mean observation: 0.111 [-1.192, 1.000], loss: 0.814903, mean_absolute_error: 33.303267, mean_q: 43.516057, mean_eps: 0.100000
 1587640/2000000: episode: 7584, duration: 1.731s, episode steps: 110, steps per second: 64, episode reward: -14.750, mean reward: -0.134 [-100.000, 20.146], mean action: 1.155 [0.000, 3.000], mean observation: 0.041 [-1.153, 1.000], loss: 0.809576, mean_absolute_error: 34.555311, mean_q: 45.884413, mean_eps: 0.100000
 1587919/2000000: episode: 7585, duration: 4.407s, episode steps: 279, steps per second: 63, episode reward: -223.165, mean reward: -0.800 [-100.000, 5.013], mean action: 1.645 [0.000, 3.000], mean observation: 0.127 [-1.044, 1.000], loss: 1.068658, mean_absolute_error: 33.808479, mean_q: 44.242903, mean_eps: 0.100000
 1588035/2000000: episode: 7586, duration: 1.797s, episode steps: 116, steps per second: 65, episode reward: -9.987, mean reward: -0.086 [-100.000, 16.192], mean action: 1.250 [0.000, 3.000], mean observation: 0.007 [-1.157, 1.000], loss: 1.095797, mean_absolute_error: 32.649113, mean_q: 42.506054, mean_eps: 0.100000
 1588124/2000000: episode: 7587, duration: 1.437s, episode steps: 89, steps per second: 62, episode reward: -37.456, mean reward: -0.421 [-100.000, 10.330], mean action: 1.674 [0.000, 3.000], mean observation: -0.090 [-1.097, 1.407], loss: 1.102929, mean_absolute_error: 34.856568, mean_q: 45.108872, mean_eps: 0.100000
 1588467/2000000: episode: 7588, duration: 5.676s, episode steps: 343, steps per second: 60, episode reward: 207.796, mean reward: 0.606 [-19.314, 100.000], mean action: 2.230 [0.000, 3.000], mean observation: 0.194 [-1.150, 1.000], loss: 0.971056, mean_absolute_error: 34.379553, mean_q: 45.295415, mean_eps: 0.100000
 1588568/2000000: episode: 7589, duration: 1.606s, episode steps: 101, steps per second: 63, episode reward: -11.708, mean reward: -0.116 [-100.000, 10.160], mean action: 1.584 [0.000, 3.000], mean observation: -0.049 [-1.115, 1.084], loss: 1.026331, mean_absolute_error: 34.258771, mean_q: 45.697015, mean_eps: 0.100000
 1588651/2000000: episode: 7590, duration: 1.304s, episode steps: 83, steps per second: 64, episode reward: -8.681, mean reward: -0.105 [-100.000, 18.545], mean action: 1.422 [0.000, 3.000], mean observation: -0.051 [-1.172, 1.000], loss: 0.866248, mean_absolute_error: 33.020504, mean_q: 42.449733, mean_eps: 0.100000
 1588840/2000000: episode: 7591, duration: 2.983s, episode steps: 189, steps per second: 63, episode reward: -53.338, mean reward: -0.282 [-100.000, 14.537], mean action: 1.370 [0.000, 3.000], mean observation: 0.062 [-1.032, 1.000], loss: 0.915167, mean_absolute_error: 35.171336, mean_q: 46.250021, mean_eps: 0.100000
 1588990/2000000: episode: 7592, duration: 2.338s, episode steps: 150, steps per second: 64, episode reward: -37.497, mean reward: -0.250 [-100.000, 19.891], mean action: 1.347 [0.000, 3.000], mean observation: -0.016 [-1.272, 1.007], loss: 0.893355, mean_absolute_error: 33.386979, mean_q: 43.160178, mean_eps: 0.100000
 1589251/2000000: episode: 7593, duration: 4.041s, episode steps: 261, steps per second: 65, episode reward: 252.190, mean reward: 0.966 [-9.744, 100.000], mean action: 1.264 [0.000, 3.000], mean observation: 0.109 [-0.959, 1.000], loss: 0.976009, mean_absolute_error: 33.607338, mean_q: 43.545607, mean_eps: 0.100000
 1589342/2000000: episode: 7594, duration: 1.452s, episode steps: 91, steps per second: 63, episode reward: -40.300, mean reward: -0.443 [-100.000, 19.068], mean action: 1.725 [0.000, 3.000], mean observation: -0.031 [-1.149, 1.000], loss: 0.901939, mean_absolute_error: 32.236072, mean_q: 42.472348, mean_eps: 0.100000
 1589568/2000000: episode: 7595, duration: 3.579s, episode steps: 226, steps per second: 63, episode reward: -102.645, mean reward: -0.454 [-100.000, 14.319], mean action: 1.704 [0.000, 3.000], mean observation: 0.146 [-0.841, 1.180], loss: 0.965195, mean_absolute_error: 34.143653, mean_q: 44.519256, mean_eps: 0.100000
 1589840/2000000: episode: 7596, duration: 4.268s, episode steps: 272, steps per second: 64, episode reward: 215.121, mean reward: 0.791 [-20.996, 100.000], mean action: 0.544 [0.000, 3.000], mean observation: 0.185 [-1.148, 1.000], loss: 1.314816, mean_absolute_error: 34.159688, mean_q: 44.315891, mean_eps: 0.100000
 1589932/2000000: episode: 7597, duration: 1.486s, episode steps: 92, steps per second: 62, episode reward: -42.070, mean reward: -0.457 [-100.000, 15.016], mean action: 1.293 [0.000, 3.000], mean observation: 0.014 [-1.171, 1.000], loss: 0.565570, mean_absolute_error: 34.278874, mean_q: 45.435227, mean_eps: 0.100000
 1590209/2000000: episode: 7598, duration: 4.715s, episode steps: 277, steps per second: 59, episode reward: 204.880, mean reward: 0.740 [-9.004, 100.000], mean action: 1.202 [0.000, 3.000], mean observation: 0.137 [-1.067, 1.000], loss: 0.867245, mean_absolute_error: 34.178100, mean_q: 44.498541, mean_eps: 0.100000
 1590490/2000000: episode: 7599, duration: 4.441s, episode steps: 281, steps per second: 63, episode reward: 168.402, mean reward: 0.599 [-8.448, 100.000], mean action: 1.587 [0.000, 3.000], mean observation: 0.156 [-1.035, 1.000], loss: 0.865715, mean_absolute_error: 33.027848, mean_q: 43.523155, mean_eps: 0.100000
 1590648/2000000: episode: 7600, duration: 2.491s, episode steps: 158, steps per second: 63, episode reward: -48.850, mean reward: -0.309 [-100.000, 16.058], mean action: 1.468 [0.000, 3.000], mean observation: 0.095 [-0.794, 1.446], loss: 0.748058, mean_absolute_error: 33.352355, mean_q: 43.490366, mean_eps: 0.100000
 1590742/2000000: episode: 7601, duration: 1.518s, episode steps: 94, steps per second: 62, episode reward: 7.950, mean reward: 0.085 [-100.000, 15.131], mean action: 1.479 [0.000, 3.000], mean observation: 0.016 [-1.147, 1.000], loss: 0.936516, mean_absolute_error: 33.322314, mean_q: 43.882380, mean_eps: 0.100000
 1591075/2000000: episode: 7602, duration: 5.235s, episode steps: 333, steps per second: 64, episode reward: 217.360, mean reward: 0.653 [-17.602, 100.000], mean action: 1.345 [0.000, 3.000], mean observation: 0.149 [-0.839, 1.000], loss: 1.109139, mean_absolute_error: 34.494900, mean_q: 44.947281, mean_eps: 0.100000
 1591226/2000000: episode: 7603, duration: 2.379s, episode steps: 151, steps per second: 63, episode reward: -31.965, mean reward: -0.212 [-100.000, 14.550], mean action: 1.675 [0.000, 3.000], mean observation: 0.067 [-1.075, 1.000], loss: 0.783581, mean_absolute_error: 32.441745, mean_q: 42.208898, mean_eps: 0.100000
 1591387/2000000: episode: 7604, duration: 2.497s, episode steps: 161, steps per second: 64, episode reward: -112.701, mean reward: -0.700 [-100.000, 19.689], mean action: 1.559 [0.000, 3.000], mean observation: 0.130 [-1.016, 1.000], loss: 0.812742, mean_absolute_error: 35.176207, mean_q: 46.275709, mean_eps: 0.100000
 1592387/2000000: episode: 7605, duration: 16.238s, episode steps: 1000, steps per second: 62, episode reward: 100.055, mean reward: 0.100 [-17.181, 12.070], mean action: 0.947 [0.000, 3.000], mean observation: 0.219 [-0.964, 1.000], loss: 0.956982, mean_absolute_error: 34.297726, mean_q: 44.954626, mean_eps: 0.100000
 1592471/2000000: episode: 7606, duration: 1.329s, episode steps: 84, steps per second: 63, episode reward: -32.065, mean reward: -0.382 [-100.000, 15.438], mean action: 1.857 [0.000, 3.000], mean observation: 0.007 [-1.140, 1.000], loss: 1.055273, mean_absolute_error: 34.206500, mean_q: 44.708740, mean_eps: 0.100000
 1592729/2000000: episode: 7607, duration: 4.065s, episode steps: 258, steps per second: 63, episode reward: 218.938, mean reward: 0.849 [-3.127, 100.000], mean action: 1.112 [0.000, 3.000], mean observation: 0.128 [-0.998, 1.000], loss: 0.880699, mean_absolute_error: 33.704402, mean_q: 44.082227, mean_eps: 0.100000
 1592837/2000000: episode: 7608, duration: 1.666s, episode steps: 108, steps per second: 65, episode reward: -35.184, mean reward: -0.326 [-100.000, 18.608], mean action: 1.685 [0.000, 3.000], mean observation: -0.026 [-1.103, 1.367], loss: 1.419964, mean_absolute_error: 33.332448, mean_q: 43.593373, mean_eps: 0.100000
 1592979/2000000: episode: 7609, duration: 2.149s, episode steps: 142, steps per second: 66, episode reward: -18.391, mean reward: -0.130 [-100.000, 17.187], mean action: 1.556 [0.000, 3.000], mean observation: 0.096 [-0.976, 1.000], loss: 0.742906, mean_absolute_error: 33.597704, mean_q: 43.534336, mean_eps: 0.100000
 1593064/2000000: episode: 7610, duration: 1.366s, episode steps: 85, steps per second: 62, episode reward: -41.981, mean reward: -0.494 [-100.000, 24.137], mean action: 1.494 [0.000, 3.000], mean observation: -0.071 [-1.100, 1.528], loss: 0.908989, mean_absolute_error: 35.454088, mean_q: 46.096246, mean_eps: 0.100000
 1593538/2000000: episode: 7611, duration: 7.482s, episode steps: 474, steps per second: 63, episode reward: 258.860, mean reward: 0.546 [-19.084, 100.000], mean action: 0.951 [0.000, 3.000], mean observation: 0.168 [-1.030, 1.000], loss: 0.864837, mean_absolute_error: 34.090114, mean_q: 44.443990, mean_eps: 0.100000
 1593702/2000000: episode: 7612, duration: 2.982s, episode steps: 164, steps per second: 55, episode reward: -163.430, mean reward: -0.997 [-100.000, 21.442], mean action: 1.640 [0.000, 3.000], mean observation: 0.118 [-1.005, 1.003], loss: 0.873871, mean_absolute_error: 33.361580, mean_q: 44.411479, mean_eps: 0.100000
 1593988/2000000: episode: 7613, duration: 4.559s, episode steps: 286, steps per second: 63, episode reward: -430.183, mean reward: -1.504 [-100.000, 4.682], mean action: 1.804 [0.000, 3.000], mean observation: 0.085 [-0.647, 1.981], loss: 1.259915, mean_absolute_error: 34.196075, mean_q: 44.807766, mean_eps: 0.100000
 1594281/2000000: episode: 7614, duration: 4.590s, episode steps: 293, steps per second: 64, episode reward: -11.524, mean reward: -0.039 [-100.000, 9.629], mean action: 1.519 [0.000, 3.000], mean observation: 0.100 [-0.927, 1.006], loss: 1.058916, mean_absolute_error: 34.172626, mean_q: 44.299875, mean_eps: 0.100000
 1594631/2000000: episode: 7615, duration: 5.493s, episode steps: 350, steps per second: 64, episode reward: 224.165, mean reward: 0.640 [-19.238, 100.000], mean action: 1.154 [0.000, 3.000], mean observation: 0.148 [-1.135, 1.000], loss: 1.188902, mean_absolute_error: 33.737342, mean_q: 44.101311, mean_eps: 0.100000
 1594738/2000000: episode: 7616, duration: 1.670s, episode steps: 107, steps per second: 64, episode reward: -12.776, mean reward: -0.119 [-100.000, 17.232], mean action: 1.000 [0.000, 3.000], mean observation: -0.006 [-1.252, 1.000], loss: 1.141851, mean_absolute_error: 33.515714, mean_q: 43.473353, mean_eps: 0.100000
 1595054/2000000: episode: 7617, duration: 5.050s, episode steps: 316, steps per second: 63, episode reward: 247.844, mean reward: 0.784 [-19.316, 100.000], mean action: 1.054 [0.000, 3.000], mean observation: 0.177 [-0.983, 1.000], loss: 0.876746, mean_absolute_error: 34.336886, mean_q: 45.196545, mean_eps: 0.100000
 1595148/2000000: episode: 7618, duration: 1.490s, episode steps: 94, steps per second: 63, episode reward: -58.541, mean reward: -0.623 [-100.000, 21.209], mean action: 1.330 [0.000, 3.000], mean observation: -0.016 [-2.552, 1.000], loss: 1.196224, mean_absolute_error: 34.013273, mean_q: 45.015648, mean_eps: 0.100000
 1595456/2000000: episode: 7619, duration: 4.874s, episode steps: 308, steps per second: 63, episode reward: 154.980, mean reward: 0.503 [-12.399, 100.000], mean action: 1.870 [0.000, 3.000], mean observation: 0.165 [-1.130, 1.046], loss: 0.855118, mean_absolute_error: 34.246427, mean_q: 45.139490, mean_eps: 0.100000
 1595560/2000000: episode: 7620, duration: 1.675s, episode steps: 104, steps per second: 62, episode reward: -223.392, mean reward: -2.148 [-100.000, 25.700], mean action: 1.846 [0.000, 3.000], mean observation: 0.142 [-0.928, 2.374], loss: 0.857866, mean_absolute_error: 34.352201, mean_q: 44.961330, mean_eps: 0.100000
 1595765/2000000: episode: 7621, duration: 3.205s, episode steps: 205, steps per second: 64, episode reward: 209.502, mean reward: 1.022 [-3.711, 100.000], mean action: 1.454 [0.000, 3.000], mean observation: 0.094 [-1.016, 1.000], loss: 1.034136, mean_absolute_error: 33.746698, mean_q: 43.555245, mean_eps: 0.100000
 1595868/2000000: episode: 7622, duration: 1.572s, episode steps: 103, steps per second: 66, episode reward: -27.195, mean reward: -0.264 [-100.000, 19.590], mean action: 1.330 [0.000, 3.000], mean observation: 0.011 [-1.348, 1.000], loss: 0.730625, mean_absolute_error: 33.246078, mean_q: 43.449145, mean_eps: 0.100000
 1596195/2000000: episode: 7623, duration: 5.441s, episode steps: 327, steps per second: 60, episode reward: -378.225, mean reward: -1.157 [-100.000, 6.270], mean action: 1.872 [0.000, 3.000], mean observation: 0.089 [-0.876, 1.636], loss: 0.894061, mean_absolute_error: 33.732643, mean_q: 44.201755, mean_eps: 0.100000
 1596389/2000000: episode: 7624, duration: 3.039s, episode steps: 194, steps per second: 64, episode reward: 249.162, mean reward: 1.284 [-7.845, 100.000], mean action: 1.727 [0.000, 3.000], mean observation: 0.145 [-1.055, 1.000], loss: 1.039126, mean_absolute_error: 34.584399, mean_q: 45.372217, mean_eps: 0.100000
 1596504/2000000: episode: 7625, duration: 1.795s, episode steps: 115, steps per second: 64, episode reward: -14.306, mean reward: -0.124 [-100.000, 23.014], mean action: 1.896 [0.000, 3.000], mean observation: 0.061 [-1.226, 1.010], loss: 0.792892, mean_absolute_error: 34.052923, mean_q: 45.354136, mean_eps: 0.100000
 1596590/2000000: episode: 7626, duration: 1.353s, episode steps: 86, steps per second: 64, episode reward: -18.258, mean reward: -0.212 [-100.000, 16.122], mean action: 1.349 [0.000, 3.000], mean observation: -0.015 [-1.189, 1.000], loss: 0.865701, mean_absolute_error: 34.252331, mean_q: 43.656719, mean_eps: 0.100000
 1596677/2000000: episode: 7627, duration: 1.353s, episode steps: 87, steps per second: 64, episode reward: -1.164, mean reward: -0.013 [-100.000, 16.121], mean action: 1.287 [0.000, 3.000], mean observation: -0.009 [-1.161, 1.000], loss: 0.738652, mean_absolute_error: 34.270642, mean_q: 44.047004, mean_eps: 0.100000
 1596762/2000000: episode: 7628, duration: 1.298s, episode steps: 85, steps per second: 65, episode reward: -38.013, mean reward: -0.447 [-100.000, 10.811], mean action: 1.824 [0.000, 3.000], mean observation: 0.074 [-1.144, 1.000], loss: 1.044799, mean_absolute_error: 33.981549, mean_q: 44.165910, mean_eps: 0.100000
 1597120/2000000: episode: 7629, duration: 5.680s, episode steps: 358, steps per second: 63, episode reward: -203.571, mean reward: -0.569 [-100.000, 5.798], mean action: 1.704 [0.000, 3.000], mean observation: 0.115 [-1.035, 1.007], loss: 0.828267, mean_absolute_error: 35.648479, mean_q: 46.524102, mean_eps: 0.100000
 1597205/2000000: episode: 7630, duration: 1.383s, episode steps: 85, steps per second: 61, episode reward: 13.516, mean reward: 0.159 [-100.000, 19.178], mean action: 2.106 [0.000, 3.000], mean observation: -0.121 [-1.111, 1.018], loss: 0.775179, mean_absolute_error: 34.511325, mean_q: 45.483304, mean_eps: 0.100000
 1597311/2000000: episode: 7631, duration: 1.616s, episode steps: 106, steps per second: 66, episode reward: -67.864, mean reward: -0.640 [-100.000, 9.371], mean action: 1.642 [0.000, 3.000], mean observation: 0.017 [-2.693, 1.004], loss: 0.971777, mean_absolute_error: 34.021352, mean_q: 44.688414, mean_eps: 0.100000
 1597645/2000000: episode: 7632, duration: 5.282s, episode steps: 334, steps per second: 63, episode reward: 205.340, mean reward: 0.615 [-19.332, 100.000], mean action: 0.856 [0.000, 3.000], mean observation: 0.110 [-1.175, 1.000], loss: 0.913962, mean_absolute_error: 34.202696, mean_q: 44.472423, mean_eps: 0.100000
 1597742/2000000: episode: 7633, duration: 1.501s, episode steps: 97, steps per second: 65, episode reward: -25.579, mean reward: -0.264 [-100.000, 19.400], mean action: 1.918 [0.000, 3.000], mean observation: 0.023 [-1.413, 1.000], loss: 0.945545, mean_absolute_error: 34.177865, mean_q: 45.122695, mean_eps: 0.100000
 1598127/2000000: episode: 7634, duration: 6.121s, episode steps: 385, steps per second: 63, episode reward: 196.443, mean reward: 0.510 [-17.973, 100.000], mean action: 1.694 [0.000, 3.000], mean observation: 0.219 [-1.104, 1.000], loss: 1.324632, mean_absolute_error: 33.883564, mean_q: 43.789731, mean_eps: 0.100000
 1598234/2000000: episode: 7635, duration: 1.692s, episode steps: 107, steps per second: 63, episode reward: -26.411, mean reward: -0.247 [-100.000, 26.049], mean action: 1.271 [0.000, 3.000], mean observation: 0.020 [-1.178, 1.000], loss: 0.854181, mean_absolute_error: 32.112590, mean_q: 42.052174, mean_eps: 0.100000
 1598378/2000000: episode: 7636, duration: 2.234s, episode steps: 144, steps per second: 64, episode reward: -59.908, mean reward: -0.416 [-100.000, 10.308], mean action: 1.625 [0.000, 3.000], mean observation: 0.067 [-0.986, 1.301], loss: 1.010548, mean_absolute_error: 33.141422, mean_q: 43.685809, mean_eps: 0.100000
 1598675/2000000: episode: 7637, duration: 4.658s, episode steps: 297, steps per second: 64, episode reward: -423.924, mean reward: -1.427 [-100.000, 4.553], mean action: 1.596 [0.000, 3.000], mean observation: 0.082 [-0.635, 2.160], loss: 0.874667, mean_absolute_error: 33.867719, mean_q: 44.303528, mean_eps: 0.100000
 1598920/2000000: episode: 7638, duration: 3.939s, episode steps: 245, steps per second: 62, episode reward: 240.668, mean reward: 0.982 [-12.532, 100.000], mean action: 1.347 [0.000, 3.000], mean observation: 0.182 [-0.965, 1.000], loss: 0.784899, mean_absolute_error: 34.375647, mean_q: 45.252636, mean_eps: 0.100000
 1599085/2000000: episode: 7639, duration: 2.563s, episode steps: 165, steps per second: 64, episode reward: 203.604, mean reward: 1.234 [-10.463, 100.000], mean action: 0.994 [0.000, 3.000], mean observation: 0.085 [-1.156, 1.000], loss: 1.062994, mean_absolute_error: 33.900810, mean_q: 44.871119, mean_eps: 0.100000
 1599368/2000000: episode: 7640, duration: 4.455s, episode steps: 283, steps per second: 64, episode reward: 208.301, mean reward: 0.736 [-17.731, 100.000], mean action: 1.035 [0.000, 3.000], mean observation: 0.203 [-1.102, 1.000], loss: 0.839951, mean_absolute_error: 34.130375, mean_q: 44.671600, mean_eps: 0.100000
 1599480/2000000: episode: 7641, duration: 1.809s, episode steps: 112, steps per second: 62, episode reward: -3.875, mean reward: -0.035 [-100.000, 11.251], mean action: 1.304 [0.000, 3.000], mean observation: -0.042 [-1.146, 1.000], loss: 0.990262, mean_absolute_error: 31.266426, mean_q: 40.765946, mean_eps: 0.100000
 1599635/2000000: episode: 7642, duration: 2.411s, episode steps: 155, steps per second: 64, episode reward: -32.642, mean reward: -0.211 [-100.000, 7.605], mean action: 1.703 [0.000, 3.000], mean observation: 0.073 [-1.026, 1.000], loss: 0.695316, mean_absolute_error: 33.141677, mean_q: 44.026416, mean_eps: 0.100000
 1599798/2000000: episode: 7643, duration: 2.526s, episode steps: 163, steps per second: 65, episode reward: -20.166, mean reward: -0.124 [-100.000, 18.956], mean action: 1.706 [0.000, 3.000], mean observation: 0.128 [-0.892, 1.251], loss: 1.270853, mean_absolute_error: 34.531331, mean_q: 45.243694, mean_eps: 0.100000
 1600082/2000000: episode: 7644, duration: 4.481s, episode steps: 284, steps per second: 63, episode reward: 265.637, mean reward: 0.935 [-7.470, 100.000], mean action: 1.239 [0.000, 3.000], mean observation: 0.134 [-1.323, 1.000], loss: 1.100769, mean_absolute_error: 34.447623, mean_q: 44.354170, mean_eps: 0.100000
 1600650/2000000: episode: 7645, duration: 9.350s, episode steps: 568, steps per second: 61, episode reward: 182.982, mean reward: 0.322 [-17.197, 100.000], mean action: 1.225 [0.000, 3.000], mean observation: 0.165 [-1.015, 1.000], loss: 1.010323, mean_absolute_error: 33.821328, mean_q: 43.869658, mean_eps: 0.100000
 1600730/2000000: episode: 7646, duration: 1.261s, episode steps: 80, steps per second: 63, episode reward: -1.260, mean reward: -0.016 [-100.000, 16.192], mean action: 1.488 [0.000, 3.000], mean observation: -0.034 [-1.152, 1.000], loss: 0.865864, mean_absolute_error: 33.808242, mean_q: 43.722974, mean_eps: 0.100000
 1601110/2000000: episode: 7647, duration: 5.948s, episode steps: 380, steps per second: 64, episode reward: 245.684, mean reward: 0.647 [-20.422, 100.000], mean action: 0.903 [0.000, 3.000], mean observation: 0.176 [-1.107, 1.560], loss: 0.985512, mean_absolute_error: 33.574961, mean_q: 43.917502, mean_eps: 0.100000
 1601209/2000000: episode: 7648, duration: 1.547s, episode steps: 99, steps per second: 64, episode reward: -49.126, mean reward: -0.496 [-100.000, 14.373], mean action: 1.212 [0.000, 3.000], mean observation: -0.001 [-1.185, 2.538], loss: 0.896484, mean_absolute_error: 34.945521, mean_q: 45.071714, mean_eps: 0.100000
 1601577/2000000: episode: 7649, duration: 5.849s, episode steps: 368, steps per second: 63, episode reward: -184.388, mean reward: -0.501 [-100.000, 5.212], mean action: 1.622 [0.000, 3.000], mean observation: 0.101 [-1.065, 1.005], loss: 0.945777, mean_absolute_error: 34.973075, mean_q: 45.620676, mean_eps: 0.100000
 1602577/2000000: episode: 7650, duration: 16.388s, episode steps: 1000, steps per second: 61, episode reward: 59.530, mean reward: 0.060 [-18.744, 22.211], mean action: 1.060 [0.000, 3.000], mean observation: 0.217 [-1.078, 1.000], loss: 1.043066, mean_absolute_error: 33.977216, mean_q: 44.626279, mean_eps: 0.100000
 1602856/2000000: episode: 7651, duration: 4.367s, episode steps: 279, steps per second: 64, episode reward: 165.041, mean reward: 0.592 [-19.304, 100.000], mean action: 1.118 [0.000, 3.000], mean observation: 0.150 [-1.083, 1.000], loss: 0.930611, mean_absolute_error: 34.264680, mean_q: 45.093130, mean_eps: 0.100000
 1603025/2000000: episode: 7652, duration: 2.654s, episode steps: 169, steps per second: 64, episode reward: -36.951, mean reward: -0.219 [-100.000, 10.078], mean action: 1.799 [0.000, 3.000], mean observation: -0.018 [-1.105, 1.000], loss: 0.842627, mean_absolute_error: 34.407527, mean_q: 45.370754, mean_eps: 0.100000
 1603149/2000000: episode: 7653, duration: 1.951s, episode steps: 124, steps per second: 64, episode reward: -56.212, mean reward: -0.453 [-100.000, 9.835], mean action: 1.403 [0.000, 3.000], mean observation: 0.017 [-1.074, 1.000], loss: 0.939815, mean_absolute_error: 34.731301, mean_q: 45.956989, mean_eps: 0.100000
 1603401/2000000: episode: 7654, duration: 3.921s, episode steps: 252, steps per second: 64, episode reward: 211.238, mean reward: 0.838 [-8.173, 100.000], mean action: 0.734 [0.000, 3.000], mean observation: 0.158 [-1.150, 1.000], loss: 1.230272, mean_absolute_error: 34.396927, mean_q: 44.915400, mean_eps: 0.100000
 1603658/2000000: episode: 7655, duration: 4.195s, episode steps: 257, steps per second: 61, episode reward: 177.854, mean reward: 0.692 [-13.775, 100.000], mean action: 1.525 [0.000, 3.000], mean observation: 0.097 [-1.020, 1.000], loss: 1.318502, mean_absolute_error: 34.329593, mean_q: 44.689630, mean_eps: 0.100000
 1603740/2000000: episode: 7656, duration: 1.302s, episode steps: 82, steps per second: 63, episode reward: -61.144, mean reward: -0.746 [-100.000, 17.386], mean action: 1.732 [0.000, 3.000], mean observation: -0.032 [-1.944, 1.000], loss: 1.019917, mean_absolute_error: 33.939998, mean_q: 43.990785, mean_eps: 0.100000
 1604740/2000000: episode: 7657, duration: 16.397s, episode steps: 1000, steps per second: 61, episode reward: 87.047, mean reward: 0.087 [-20.711, 22.434], mean action: 2.360 [0.000, 3.000], mean observation: 0.164 [-1.068, 1.000], loss: 0.999833, mean_absolute_error: 34.316121, mean_q: 44.888813, mean_eps: 0.100000
 1605740/2000000: episode: 7658, duration: 16.214s, episode steps: 1000, steps per second: 62, episode reward: 69.744, mean reward: 0.070 [-19.631, 22.699], mean action: 0.835 [0.000, 3.000], mean observation: 0.289 [-0.901, 1.235], loss: 1.025958, mean_absolute_error: 34.168234, mean_q: 44.707551, mean_eps: 0.100000
 1606364/2000000: episode: 7659, duration: 10.221s, episode steps: 624, steps per second: 61, episode reward: 130.573, mean reward: 0.209 [-19.369, 100.000], mean action: 1.144 [0.000, 3.000], mean observation: 0.149 [-1.049, 1.000], loss: 0.986945, mean_absolute_error: 34.424232, mean_q: 45.068667, mean_eps: 0.100000
 1606586/2000000: episode: 7660, duration: 3.498s, episode steps: 222, steps per second: 63, episode reward: 237.270, mean reward: 1.069 [-9.505, 100.000], mean action: 1.234 [0.000, 3.000], mean observation: 0.065 [-1.037, 1.000], loss: 1.048154, mean_absolute_error: 35.034789, mean_q: 46.016812, mean_eps: 0.100000
 1607188/2000000: episode: 7661, duration: 9.690s, episode steps: 602, steps per second: 62, episode reward: 158.849, mean reward: 0.264 [-20.642, 100.000], mean action: 2.040 [0.000, 3.000], mean observation: 0.244 [-1.047, 1.000], loss: 0.969753, mean_absolute_error: 35.033534, mean_q: 45.742120, mean_eps: 0.100000
 1607412/2000000: episode: 7662, duration: 3.528s, episode steps: 224, steps per second: 63, episode reward: -133.489, mean reward: -0.596 [-100.000, 16.464], mean action: 1.714 [0.000, 3.000], mean observation: 0.088 [-1.087, 1.000], loss: 0.809025, mean_absolute_error: 33.817910, mean_q: 44.497287, mean_eps: 0.100000
 1607608/2000000: episode: 7663, duration: 3.103s, episode steps: 196, steps per second: 63, episode reward: 201.169, mean reward: 1.026 [-9.965, 100.000], mean action: 0.852 [0.000, 3.000], mean observation: 0.054 [-1.210, 1.000], loss: 1.022010, mean_absolute_error: 34.642128, mean_q: 45.054299, mean_eps: 0.100000
 1607707/2000000: episode: 7664, duration: 1.562s, episode steps: 99, steps per second: 63, episode reward: 4.468, mean reward: 0.045 [-100.000, 15.575], mean action: 1.465 [0.000, 3.000], mean observation: -0.041 [-1.080, 1.243], loss: 1.020380, mean_absolute_error: 33.346290, mean_q: 43.416052, mean_eps: 0.100000
 1607964/2000000: episode: 7665, duration: 4.208s, episode steps: 257, steps per second: 61, episode reward: 204.649, mean reward: 0.796 [-11.138, 100.000], mean action: 0.802 [0.000, 3.000], mean observation: 0.140 [-1.175, 1.000], loss: 0.909303, mean_absolute_error: 35.823571, mean_q: 46.709934, mean_eps: 0.100000
 1608162/2000000: episode: 7666, duration: 3.110s, episode steps: 198, steps per second: 64, episode reward: 21.421, mean reward: 0.108 [-100.000, 20.667], mean action: 1.737 [0.000, 3.000], mean observation: -0.005 [-0.783, 1.083], loss: 0.816889, mean_absolute_error: 35.640825, mean_q: 46.735024, mean_eps: 0.100000
 1608238/2000000: episode: 7667, duration: 1.198s, episode steps: 76, steps per second: 63, episode reward: -92.885, mean reward: -1.222 [-100.000, 9.020], mean action: 1.487 [0.000, 3.000], mean observation: -0.097 [-1.211, 1.859], loss: 0.854564, mean_absolute_error: 32.546581, mean_q: 42.590440, mean_eps: 0.100000
 1608349/2000000: episode: 7668, duration: 1.735s, episode steps: 111, steps per second: 64, episode reward: -16.857, mean reward: -0.152 [-100.000, 20.622], mean action: 1.883 [0.000, 3.000], mean observation: 0.052 [-1.159, 1.000], loss: 0.809254, mean_absolute_error: 34.372300, mean_q: 44.815173, mean_eps: 0.100000
 1608763/2000000: episode: 7669, duration: 6.524s, episode steps: 414, steps per second: 63, episode reward: -195.199, mean reward: -0.471 [-100.000, 5.401], mean action: 1.819 [0.000, 3.000], mean observation: 0.106 [-0.756, 1.002], loss: 0.966476, mean_absolute_error: 33.685559, mean_q: 44.476080, mean_eps: 0.100000
 1608841/2000000: episode: 7670, duration: 1.249s, episode steps: 78, steps per second: 62, episode reward: -26.912, mean reward: -0.345 [-100.000, 17.861], mean action: 2.000 [0.000, 3.000], mean observation: -0.144 [-1.154, 1.000], loss: 0.880467, mean_absolute_error: 33.280017, mean_q: 43.585546, mean_eps: 0.100000
 1608940/2000000: episode: 7671, duration: 1.524s, episode steps: 99, steps per second: 65, episode reward: -31.085, mean reward: -0.314 [-100.000, 19.655], mean action: 1.323 [0.000, 3.000], mean observation: -0.037 [-1.165, 1.357], loss: 0.761153, mean_absolute_error: 34.315561, mean_q: 44.796650, mean_eps: 0.100000
 1609024/2000000: episode: 7672, duration: 1.380s, episode steps: 84, steps per second: 61, episode reward: -54.557, mean reward: -0.649 [-100.000, 14.870], mean action: 2.000 [0.000, 3.000], mean observation: -0.134 [-1.092, 1.361], loss: 1.124749, mean_absolute_error: 33.611340, mean_q: 44.197272, mean_eps: 0.100000
 1609119/2000000: episode: 7673, duration: 1.479s, episode steps: 95, steps per second: 64, episode reward: -21.726, mean reward: -0.229 [-100.000, 19.087], mean action: 1.379 [0.000, 3.000], mean observation: -0.055 [-1.137, 1.125], loss: 0.900191, mean_absolute_error: 32.455417, mean_q: 42.428706, mean_eps: 0.100000
 1609219/2000000: episode: 7674, duration: 1.557s, episode steps: 100, steps per second: 64, episode reward: -18.700, mean reward: -0.187 [-100.000, 14.886], mean action: 1.540 [0.000, 3.000], mean observation: -0.068 [-1.063, 1.469], loss: 0.671210, mean_absolute_error: 34.183750, mean_q: 45.224614, mean_eps: 0.100000
 1609617/2000000: episode: 7675, duration: 6.334s, episode steps: 398, steps per second: 63, episode reward: 247.278, mean reward: 0.621 [-21.037, 100.000], mean action: 0.781 [0.000, 3.000], mean observation: 0.219 [-0.789, 1.000], loss: 1.135914, mean_absolute_error: 34.404540, mean_q: 44.946147, mean_eps: 0.100000
 1609740/2000000: episode: 7676, duration: 1.926s, episode steps: 123, steps per second: 64, episode reward: 12.977, mean reward: 0.106 [-100.000, 33.288], mean action: 1.699 [0.000, 3.000], mean observation: -0.081 [-1.103, 1.200], loss: 1.016854, mean_absolute_error: 34.651908, mean_q: 44.751495, mean_eps: 0.100000
 1609962/2000000: episode: 7677, duration: 3.441s, episode steps: 222, steps per second: 65, episode reward: 200.917, mean reward: 0.905 [-3.456, 100.000], mean action: 0.865 [0.000, 3.000], mean observation: 0.187 [-1.086, 1.000], loss: 1.058022, mean_absolute_error: 33.426562, mean_q: 43.442514, mean_eps: 0.100000
 1610051/2000000: episode: 7678, duration: 1.389s, episode steps: 89, steps per second: 64, episode reward: -30.747, mean reward: -0.345 [-100.000, 17.803], mean action: 1.157 [0.000, 3.000], mean observation: 0.004 [-1.571, 1.000], loss: 0.952426, mean_absolute_error: 33.154587, mean_q: 43.143553, mean_eps: 0.100000
 1610230/2000000: episode: 7679, duration: 3.113s, episode steps: 179, steps per second: 58, episode reward: 220.010, mean reward: 1.229 [-4.208, 100.000], mean action: 1.184 [0.000, 3.000], mean observation: 0.089 [-1.123, 1.000], loss: 0.977703, mean_absolute_error: 34.774085, mean_q: 44.872858, mean_eps: 0.100000
 1610619/2000000: episode: 7680, duration: 6.271s, episode steps: 389, steps per second: 62, episode reward: -276.289, mean reward: -0.710 [-100.000, 5.436], mean action: 1.663 [0.000, 3.000], mean observation: 0.171 [-1.036, 1.339], loss: 0.821123, mean_absolute_error: 34.098935, mean_q: 44.700217, mean_eps: 0.100000
 1610917/2000000: episode: 7681, duration: 4.897s, episode steps: 298, steps per second: 61, episode reward: 201.283, mean reward: 0.675 [-10.535, 100.000], mean action: 0.990 [0.000, 3.000], mean observation: 0.109 [-1.165, 1.054], loss: 0.931175, mean_absolute_error: 34.305895, mean_q: 44.929111, mean_eps: 0.100000
 1611235/2000000: episode: 7682, duration: 5.015s, episode steps: 318, steps per second: 63, episode reward: 194.769, mean reward: 0.612 [-19.360, 100.000], mean action: 1.371 [0.000, 3.000], mean observation: 0.138 [-0.909, 1.000], loss: 0.950075, mean_absolute_error: 34.515860, mean_q: 45.123133, mean_eps: 0.100000
 1611338/2000000: episode: 7683, duration: 1.625s, episode steps: 103, steps per second: 63, episode reward: -46.036, mean reward: -0.447 [-100.000, 17.592], mean action: 1.320 [0.000, 3.000], mean observation: -0.050 [-1.105, 1.273], loss: 0.985816, mean_absolute_error: 34.561700, mean_q: 45.154105, mean_eps: 0.100000
 1611792/2000000: episode: 7684, duration: 7.241s, episode steps: 454, steps per second: 63, episode reward: 205.847, mean reward: 0.453 [-17.337, 100.000], mean action: 0.518 [0.000, 3.000], mean observation: 0.205 [-1.109, 1.000], loss: 1.057753, mean_absolute_error: 33.735359, mean_q: 44.055389, mean_eps: 0.100000
 1611871/2000000: episode: 7685, duration: 1.263s, episode steps: 79, steps per second: 63, episode reward: -43.614, mean reward: -0.552 [-100.000, 12.506], mean action: 1.494 [0.000, 3.000], mean observation: 0.078 [-1.080, 1.000], loss: 1.091414, mean_absolute_error: 34.365487, mean_q: 44.548542, mean_eps: 0.100000
 1611954/2000000: episode: 7686, duration: 1.311s, episode steps: 83, steps per second: 63, episode reward: 5.595, mean reward: 0.067 [-100.000, 18.076], mean action: 1.422 [0.000, 3.000], mean observation: -0.018 [-1.110, 1.050], loss: 0.714596, mean_absolute_error: 34.814943, mean_q: 45.956698, mean_eps: 0.100000
 1612039/2000000: episode: 7687, duration: 1.318s, episode steps: 85, steps per second: 65, episode reward: -37.679, mean reward: -0.443 [-100.000, 16.913], mean action: 1.765 [0.000, 3.000], mean observation: -0.075 [-1.101, 1.000], loss: 1.549147, mean_absolute_error: 35.137307, mean_q: 44.718717, mean_eps: 0.100000
 1612317/2000000: episode: 7688, duration: 4.364s, episode steps: 278, steps per second: 64, episode reward: -195.760, mean reward: -0.704 [-100.000, 9.439], mean action: 1.856 [0.000, 3.000], mean observation: 0.141 [-0.974, 1.408], loss: 0.892841, mean_absolute_error: 35.561888, mean_q: 46.320554, mean_eps: 0.100000
 1612415/2000000: episode: 7689, duration: 1.509s, episode steps: 98, steps per second: 65, episode reward: -12.182, mean reward: -0.124 [-100.000, 12.474], mean action: 1.571 [0.000, 3.000], mean observation: -0.075 [-1.113, 1.274], loss: 1.232706, mean_absolute_error: 34.839761, mean_q: 45.333711, mean_eps: 0.100000
 1612798/2000000: episode: 7690, duration: 6.068s, episode steps: 383, steps per second: 63, episode reward: 260.883, mean reward: 0.681 [-15.051, 100.000], mean action: 1.217 [0.000, 3.000], mean observation: 0.167 [-0.739, 1.298], loss: 0.989992, mean_absolute_error: 34.251982, mean_q: 44.866382, mean_eps: 0.100000
 1612950/2000000: episode: 7691, duration: 2.381s, episode steps: 152, steps per second: 64, episode reward: -42.189, mean reward: -0.278 [-100.000, 16.145], mean action: 1.862 [0.000, 3.000], mean observation: 0.110 [-0.814, 1.571], loss: 1.185012, mean_absolute_error: 32.340357, mean_q: 41.859418, mean_eps: 0.100000
 1613250/2000000: episode: 7692, duration: 4.764s, episode steps: 300, steps per second: 63, episode reward: 221.391, mean reward: 0.738 [-11.095, 100.000], mean action: 1.287 [0.000, 3.000], mean observation: 0.177 [-1.044, 1.000], loss: 1.090248, mean_absolute_error: 34.163289, mean_q: 44.699381, mean_eps: 0.100000
 1613485/2000000: episode: 7693, duration: 3.673s, episode steps: 235, steps per second: 64, episode reward: -136.057, mean reward: -0.579 [-100.000, 5.076], mean action: 1.528 [0.000, 3.000], mean observation: 0.212 [-0.725, 1.009], loss: 1.119742, mean_absolute_error: 33.336528, mean_q: 43.426797, mean_eps: 0.100000
 1613592/2000000: episode: 7694, duration: 1.666s, episode steps: 107, steps per second: 64, episode reward: -1.728, mean reward: -0.016 [-100.000, 15.214], mean action: 1.374 [0.000, 3.000], mean observation: -0.069 [-1.133, 1.420], loss: 0.874958, mean_absolute_error: 35.627651, mean_q: 46.553192, mean_eps: 0.100000
 1613679/2000000: episode: 7695, duration: 1.367s, episode steps: 87, steps per second: 64, episode reward: -58.960, mean reward: -0.678 [-100.000, 15.851], mean action: 1.103 [0.000, 3.000], mean observation: 0.025 [-1.128, 2.059], loss: 0.785047, mean_absolute_error: 33.528945, mean_q: 43.524442, mean_eps: 0.100000
 1613768/2000000: episode: 7696, duration: 1.437s, episode steps: 89, steps per second: 62, episode reward: -46.286, mean reward: -0.520 [-100.000, 8.846], mean action: 1.640 [0.000, 3.000], mean observation: -0.065 [-1.095, 1.000], loss: 0.937459, mean_absolute_error: 34.689963, mean_q: 43.457524, mean_eps: 0.100000
 1614059/2000000: episode: 7697, duration: 4.578s, episode steps: 291, steps per second: 64, episode reward: 170.028, mean reward: 0.584 [-18.152, 100.000], mean action: 2.309 [0.000, 3.000], mean observation: 0.226 [-1.103, 1.339], loss: 0.875028, mean_absolute_error: 34.894813, mean_q: 45.421144, mean_eps: 0.100000
 1614146/2000000: episode: 7698, duration: 1.360s, episode steps: 87, steps per second: 64, episode reward: -26.692, mean reward: -0.307 [-100.000, 15.449], mean action: 1.598 [0.000, 3.000], mean observation: -0.068 [-1.060, 1.390], loss: 0.980459, mean_absolute_error: 33.430054, mean_q: 43.886618, mean_eps: 0.100000
 1614362/2000000: episode: 7699, duration: 3.358s, episode steps: 216, steps per second: 64, episode reward: -171.533, mean reward: -0.794 [-100.000, 4.866], mean action: 1.787 [0.000, 3.000], mean observation: 0.199 [-0.923, 1.002], loss: 0.946283, mean_absolute_error: 34.632261, mean_q: 45.315946, mean_eps: 0.100000
 1614465/2000000: episode: 7700, duration: 1.627s, episode steps: 103, steps per second: 63, episode reward: -38.156, mean reward: -0.370 [-100.000, 8.982], mean action: 1.641 [0.000, 3.000], mean observation: 0.047 [-1.399, 1.001], loss: 1.089862, mean_absolute_error: 32.384699, mean_q: 42.865438, mean_eps: 0.100000
 1614579/2000000: episode: 7701, duration: 1.753s, episode steps: 114, steps per second: 65, episode reward: -2.056, mean reward: -0.018 [-100.000, 18.097], mean action: 1.360 [0.000, 3.000], mean observation: -0.028 [-1.144, 1.000], loss: 1.002188, mean_absolute_error: 34.731475, mean_q: 45.214741, mean_eps: 0.100000
 1614681/2000000: episode: 7702, duration: 1.605s, episode steps: 102, steps per second: 64, episode reward: -44.180, mean reward: -0.433 [-100.000, 21.572], mean action: 1.637 [0.000, 3.000], mean observation: -0.085 [-1.740, 1.000], loss: 0.938696, mean_absolute_error: 34.029023, mean_q: 44.595753, mean_eps: 0.100000
 1614919/2000000: episode: 7703, duration: 3.696s, episode steps: 238, steps per second: 64, episode reward: 261.354, mean reward: 1.098 [-11.670, 100.000], mean action: 1.454 [0.000, 3.000], mean observation: 0.070 [-1.067, 1.000], loss: 0.862943, mean_absolute_error: 33.903134, mean_q: 44.392276, mean_eps: 0.100000
 1615015/2000000: episode: 7704, duration: 1.490s, episode steps: 96, steps per second: 64, episode reward: -71.092, mean reward: -0.741 [-100.000, 15.093], mean action: 1.406 [0.000, 3.000], mean observation: -0.108 [-1.111, 1.447], loss: 1.152836, mean_absolute_error: 34.576689, mean_q: 44.769457, mean_eps: 0.100000
 1615320/2000000: episode: 7705, duration: 4.831s, episode steps: 305, steps per second: 63, episode reward: -70.594, mean reward: -0.231 [-100.000, 12.960], mean action: 1.787 [0.000, 3.000], mean observation: 0.189 [-1.253, 1.360], loss: 1.060669, mean_absolute_error: 34.137294, mean_q: 44.502410, mean_eps: 0.100000
 1615575/2000000: episode: 7706, duration: 4.054s, episode steps: 255, steps per second: 63, episode reward: 226.902, mean reward: 0.890 [-8.908, 100.000], mean action: 1.090 [0.000, 3.000], mean observation: 0.088 [-1.322, 1.000], loss: 1.108487, mean_absolute_error: 35.343026, mean_q: 45.855636, mean_eps: 0.100000
 1615677/2000000: episode: 7707, duration: 1.631s, episode steps: 102, steps per second: 63, episode reward: -18.777, mean reward: -0.184 [-100.000, 16.908], mean action: 1.206 [0.000, 3.000], mean observation: -0.035 [-1.250, 1.000], loss: 1.434422, mean_absolute_error: 34.168357, mean_q: 43.213071, mean_eps: 0.100000
 1615848/2000000: episode: 7708, duration: 2.661s, episode steps: 171, steps per second: 64, episode reward: -38.056, mean reward: -0.223 [-100.000, 10.467], mean action: 1.842 [0.000, 3.000], mean observation: 0.033 [-1.131, 1.000], loss: 0.970358, mean_absolute_error: 34.697321, mean_q: 44.806048, mean_eps: 0.100000
 1616125/2000000: episode: 7709, duration: 4.348s, episode steps: 277, steps per second: 64, episode reward: 243.961, mean reward: 0.881 [-9.457, 100.000], mean action: 1.646 [0.000, 3.000], mean observation: 0.113 [-0.797, 1.000], loss: 0.822071, mean_absolute_error: 34.369241, mean_q: 44.681667, mean_eps: 0.100000
 1616215/2000000: episode: 7710, duration: 1.365s, episode steps: 90, steps per second: 66, episode reward: -61.752, mean reward: -0.686 [-100.000, 10.961], mean action: 1.411 [0.000, 3.000], mean observation: -0.131 [-1.031, 2.108], loss: 0.990075, mean_absolute_error: 33.624305, mean_q: 43.446876, mean_eps: 0.100000
 1616942/2000000: episode: 7711, duration: 11.941s, episode steps: 727, steps per second: 61, episode reward: -252.946, mean reward: -0.348 [-100.000, 5.459], mean action: 1.799 [0.000, 3.000], mean observation: -0.064 [-1.016, 0.938], loss: 0.849106, mean_absolute_error: 35.123720, mean_q: 45.824093, mean_eps: 0.100000
 1617247/2000000: episode: 7712, duration: 4.731s, episode steps: 305, steps per second: 64, episode reward: 207.166, mean reward: 0.679 [-18.640, 100.000], mean action: 0.744 [0.000, 3.000], mean observation: 0.137 [-1.125, 1.000], loss: 1.034212, mean_absolute_error: 35.196455, mean_q: 46.124429, mean_eps: 0.100000
 1617518/2000000: episode: 7713, duration: 4.252s, episode steps: 271, steps per second: 64, episode reward: 248.887, mean reward: 0.918 [-18.830, 100.000], mean action: 1.328 [0.000, 3.000], mean observation: 0.155 [-0.916, 1.000], loss: 1.116058, mean_absolute_error: 33.834360, mean_q: 43.940878, mean_eps: 0.100000
 1617820/2000000: episode: 7714, duration: 4.841s, episode steps: 302, steps per second: 62, episode reward: 160.253, mean reward: 0.531 [-13.960, 100.000], mean action: 1.507 [0.000, 3.000], mean observation: 0.099 [-1.011, 1.000], loss: 1.163098, mean_absolute_error: 34.868630, mean_q: 45.039661, mean_eps: 0.100000
 1617932/2000000: episode: 7715, duration: 1.805s, episode steps: 112, steps per second: 62, episode reward: -10.838, mean reward: -0.097 [-100.000, 18.534], mean action: 1.875 [0.000, 3.000], mean observation: -0.057 [-1.070, 1.161], loss: 1.197372, mean_absolute_error: 35.070087, mean_q: 45.885703, mean_eps: 0.100000
 1618110/2000000: episode: 7716, duration: 2.739s, episode steps: 178, steps per second: 65, episode reward: 196.750, mean reward: 1.105 [-9.560, 100.000], mean action: 0.809 [0.000, 3.000], mean observation: 0.099 [-1.086, 1.000], loss: 0.830055, mean_absolute_error: 34.455505, mean_q: 45.566999, mean_eps: 0.100000
 1618195/2000000: episode: 7717, duration: 1.315s, episode steps: 85, steps per second: 65, episode reward: -1.620, mean reward: -0.019 [-100.000, 20.852], mean action: 1.424 [0.000, 3.000], mean observation: -0.001 [-1.276, 1.000], loss: 0.906984, mean_absolute_error: 34.743279, mean_q: 45.190047, mean_eps: 0.100000
 1618300/2000000: episode: 7718, duration: 1.650s, episode steps: 105, steps per second: 64, episode reward: -12.090, mean reward: -0.115 [-100.000, 20.467], mean action: 1.086 [0.000, 3.000], mean observation: 0.022 [-1.170, 1.000], loss: 0.694205, mean_absolute_error: 32.252449, mean_q: 42.622448, mean_eps: 0.100000
 1618690/2000000: episode: 7719, duration: 6.550s, episode steps: 390, steps per second: 60, episode reward: 152.803, mean reward: 0.392 [-15.464, 100.000], mean action: 1.962 [0.000, 3.000], mean observation: 0.145 [-1.001, 1.000], loss: 0.966208, mean_absolute_error: 35.145772, mean_q: 46.002560, mean_eps: 0.100000
 1618935/2000000: episode: 7720, duration: 3.790s, episode steps: 245, steps per second: 65, episode reward: 175.338, mean reward: 0.716 [-7.218, 100.000], mean action: 2.171 [0.000, 3.000], mean observation: 0.201 [-1.069, 1.142], loss: 1.116112, mean_absolute_error: 35.975266, mean_q: 46.821226, mean_eps: 0.100000
 1619083/2000000: episode: 7721, duration: 2.274s, episode steps: 148, steps per second: 65, episode reward: -26.589, mean reward: -0.180 [-100.000, 15.051], mean action: 1.514 [0.000, 3.000], mean observation: -0.016 [-1.119, 1.080], loss: 0.960544, mean_absolute_error: 35.815421, mean_q: 46.771805, mean_eps: 0.100000
 1619192/2000000: episode: 7722, duration: 1.754s, episode steps: 109, steps per second: 62, episode reward: -34.037, mean reward: -0.312 [-100.000, 18.796], mean action: 1.725 [0.000, 3.000], mean observation: -0.081 [-0.969, 1.564], loss: 1.380146, mean_absolute_error: 33.375917, mean_q: 42.810087, mean_eps: 0.100000
 1619275/2000000: episode: 7723, duration: 1.307s, episode steps: 83, steps per second: 63, episode reward: -133.918, mean reward: -1.613 [-100.000, 18.258], mean action: 1.518 [0.000, 3.000], mean observation: -0.170 [-1.036, 3.373], loss: 1.414970, mean_absolute_error: 34.884745, mean_q: 44.736895, mean_eps: 0.100000
 1619495/2000000: episode: 7724, duration: 3.465s, episode steps: 220, steps per second: 63, episode reward: 218.494, mean reward: 0.993 [-17.474, 100.000], mean action: 1.082 [0.000, 3.000], mean observation: 0.062 [-1.083, 1.000], loss: 0.983062, mean_absolute_error: 34.483912, mean_q: 44.957561, mean_eps: 0.100000
 1619635/2000000: episode: 7725, duration: 2.169s, episode steps: 140, steps per second: 65, episode reward: -29.451, mean reward: -0.210 [-100.000, 14.000], mean action: 1.486 [0.000, 3.000], mean observation: 0.036 [-1.098, 1.000], loss: 0.779689, mean_absolute_error: 34.860120, mean_q: 45.934551, mean_eps: 0.100000
 1620106/2000000: episode: 7726, duration: 7.767s, episode steps: 471, steps per second: 61, episode reward: 135.148, mean reward: 0.287 [-15.629, 100.000], mean action: 2.301 [0.000, 3.000], mean observation: 0.231 [-1.029, 1.000], loss: 0.940286, mean_absolute_error: 34.887790, mean_q: 45.664547, mean_eps: 0.100000
 1620183/2000000: episode: 7727, duration: 1.189s, episode steps: 77, steps per second: 65, episode reward: -21.535, mean reward: -0.280 [-100.000, 16.351], mean action: 1.442 [0.000, 3.000], mean observation: -0.096 [-1.598, 1.000], loss: 1.142265, mean_absolute_error: 36.578560, mean_q: 48.442322, mean_eps: 0.100000
 1620279/2000000: episode: 7728, duration: 1.503s, episode steps: 96, steps per second: 64, episode reward: -41.724, mean reward: -0.435 [-100.000, 17.657], mean action: 1.521 [0.000, 3.000], mean observation: 0.026 [-1.609, 1.000], loss: 0.615344, mean_absolute_error: 32.597837, mean_q: 42.155018, mean_eps: 0.100000
 1620487/2000000: episode: 7729, duration: 3.222s, episode steps: 208, steps per second: 65, episode reward: 215.157, mean reward: 1.034 [-3.608, 100.000], mean action: 0.837 [0.000, 3.000], mean observation: 0.131 [-1.416, 1.000], loss: 1.211939, mean_absolute_error: 35.616311, mean_q: 46.391889, mean_eps: 0.100000
 1621120/2000000: episode: 7730, duration: 10.067s, episode steps: 633, steps per second: 63, episode reward: 171.174, mean reward: 0.270 [-18.760, 100.000], mean action: 1.523 [0.000, 3.000], mean observation: 0.165 [-1.122, 1.000], loss: 1.246568, mean_absolute_error: 34.689624, mean_q: 44.955229, mean_eps: 0.100000
 1621374/2000000: episode: 7731, duration: 3.981s, episode steps: 254, steps per second: 64, episode reward: 192.046, mean reward: 0.756 [-17.602, 100.000], mean action: 0.799 [0.000, 3.000], mean observation: 0.098 [-1.129, 1.000], loss: 1.029501, mean_absolute_error: 35.207356, mean_q: 46.091458, mean_eps: 0.100000
 1622041/2000000: episode: 7732, duration: 10.786s, episode steps: 667, steps per second: 62, episode reward: 183.149, mean reward: 0.275 [-17.921, 100.000], mean action: 1.129 [0.000, 3.000], mean observation: 0.192 [-0.955, 1.000], loss: 1.124955, mean_absolute_error: 34.234166, mean_q: 44.594988, mean_eps: 0.100000
 1622128/2000000: episode: 7733, duration: 1.403s, episode steps: 87, steps per second: 62, episode reward: -13.396, mean reward: -0.154 [-100.000, 15.580], mean action: 1.241 [0.000, 3.000], mean observation: -0.030 [-1.093, 1.000], loss: 0.863916, mean_absolute_error: 36.183053, mean_q: 47.272442, mean_eps: 0.100000
 1622233/2000000: episode: 7734, duration: 1.633s, episode steps: 105, steps per second: 64, episode reward: -37.300, mean reward: -0.355 [-100.000, 15.517], mean action: 1.429 [0.000, 3.000], mean observation: -0.106 [-1.106, 1.540], loss: 0.982007, mean_absolute_error: 36.426380, mean_q: 48.125389, mean_eps: 0.100000
 1622325/2000000: episode: 7735, duration: 1.438s, episode steps: 92, steps per second: 64, episode reward: -0.603, mean reward: -0.007 [-100.000, 19.131], mean action: 1.467 [0.000, 3.000], mean observation: -0.041 [-1.093, 1.000], loss: 1.077279, mean_absolute_error: 35.284453, mean_q: 45.696879, mean_eps: 0.100000
 1622494/2000000: episode: 7736, duration: 2.593s, episode steps: 169, steps per second: 65, episode reward: 181.896, mean reward: 1.076 [-13.666, 100.000], mean action: 1.420 [0.000, 3.000], mean observation: 0.043 [-1.236, 1.000], loss: 0.884438, mean_absolute_error: 35.369260, mean_q: 46.546031, mean_eps: 0.100000
 1622680/2000000: episode: 7737, duration: 2.906s, episode steps: 186, steps per second: 64, episode reward: -233.957, mean reward: -1.258 [-100.000, 13.168], mean action: 1.672 [0.000, 3.000], mean observation: 0.021 [-1.167, 1.724], loss: 1.114535, mean_absolute_error: 35.719490, mean_q: 46.549501, mean_eps: 0.100000
 1622806/2000000: episode: 7738, duration: 1.969s, episode steps: 126, steps per second: 64, episode reward: -34.799, mean reward: -0.276 [-100.000, 12.993], mean action: 1.373 [0.000, 3.000], mean observation: -0.018 [-1.120, 1.390], loss: 1.131838, mean_absolute_error: 36.772135, mean_q: 48.504568, mean_eps: 0.100000
 1622956/2000000: episode: 7739, duration: 2.350s, episode steps: 150, steps per second: 64, episode reward: 14.571, mean reward: 0.097 [-100.000, 19.901], mean action: 1.500 [0.000, 3.000], mean observation: 0.003 [-1.023, 1.000], loss: 1.034044, mean_absolute_error: 35.350700, mean_q: 44.897465, mean_eps: 0.100000
 1623205/2000000: episode: 7740, duration: 3.913s, episode steps: 249, steps per second: 64, episode reward: 201.561, mean reward: 0.809 [-13.438, 100.000], mean action: 0.976 [0.000, 3.000], mean observation: 0.111 [-1.254, 1.000], loss: 1.190493, mean_absolute_error: 35.176567, mean_q: 45.703679, mean_eps: 0.100000
 1623595/2000000: episode: 7741, duration: 6.259s, episode steps: 390, steps per second: 62, episode reward: -368.481, mean reward: -0.945 [-100.000, 4.496], mean action: 1.626 [0.000, 3.000], mean observation: 0.108 [-1.096, 1.225], loss: 1.138232, mean_absolute_error: 34.328908, mean_q: 44.735168, mean_eps: 0.100000
 1624187/2000000: episode: 7742, duration: 9.624s, episode steps: 592, steps per second: 62, episode reward: 155.989, mean reward: 0.263 [-18.491, 100.000], mean action: 0.998 [0.000, 3.000], mean observation: 0.127 [-1.087, 1.000], loss: 1.159182, mean_absolute_error: 34.527670, mean_q: 45.043761, mean_eps: 0.100000
 1624437/2000000: episode: 7743, duration: 4.027s, episode steps: 250, steps per second: 62, episode reward: 273.219, mean reward: 1.093 [-7.697, 100.000], mean action: 1.260 [0.000, 3.000], mean observation: 0.066 [-0.891, 1.007], loss: 1.073104, mean_absolute_error: 34.846410, mean_q: 45.751085, mean_eps: 0.100000
 1624613/2000000: episode: 7744, duration: 2.774s, episode steps: 176, steps per second: 63, episode reward: 191.524, mean reward: 1.088 [-10.463, 100.000], mean action: 2.102 [0.000, 3.000], mean observation: 0.109 [-1.074, 1.226], loss: 1.037362, mean_absolute_error: 35.329033, mean_q: 46.256340, mean_eps: 0.100000
 1624981/2000000: episode: 7745, duration: 5.911s, episode steps: 368, steps per second: 62, episode reward: 175.183, mean reward: 0.476 [-14.193, 100.000], mean action: 2.242 [0.000, 3.000], mean observation: 0.185 [-1.105, 1.000], loss: 1.353198, mean_absolute_error: 35.107004, mean_q: 45.476097, mean_eps: 0.100000
 1625420/2000000: episode: 7746, duration: 7.046s, episode steps: 439, steps per second: 62, episode reward: 217.557, mean reward: 0.496 [-11.085, 100.000], mean action: 1.068 [0.000, 3.000], mean observation: 0.063 [-0.750, 1.000], loss: 1.142039, mean_absolute_error: 34.739547, mean_q: 45.286880, mean_eps: 0.100000
 1625540/2000000: episode: 7747, duration: 1.919s, episode steps: 120, steps per second: 63, episode reward: -34.972, mean reward: -0.291 [-100.000, 14.329], mean action: 1.400 [0.000, 3.000], mean observation: 0.045 [-1.265, 1.000], loss: 1.010496, mean_absolute_error: 35.272376, mean_q: 45.376828, mean_eps: 0.100000
 1626540/2000000: episode: 7748, duration: 16.906s, episode steps: 1000, steps per second: 59, episode reward: -5.143, mean reward: -0.005 [-17.766, 21.469], mean action: 1.500 [0.000, 3.000], mean observation: 0.116 [-1.054, 1.000], loss: 1.088577, mean_absolute_error: 35.174987, mean_q: 45.532129, mean_eps: 0.100000
 1626649/2000000: episode: 7749, duration: 1.737s, episode steps: 109, steps per second: 63, episode reward: -77.231, mean reward: -0.709 [-100.000, 13.450], mean action: 1.321 [0.000, 3.000], mean observation: 0.102 [-1.060, 1.301], loss: 1.122249, mean_absolute_error: 34.029873, mean_q: 43.638828, mean_eps: 0.100000
 1626983/2000000: episode: 7750, duration: 5.244s, episode steps: 334, steps per second: 64, episode reward: -273.403, mean reward: -0.819 [-100.000, 4.984], mean action: 1.880 [0.000, 3.000], mean observation: 0.161 [-0.963, 1.115], loss: 1.296238, mean_absolute_error: 34.063632, mean_q: 44.374009, mean_eps: 0.100000
 1627506/2000000: episode: 7751, duration: 8.380s, episode steps: 523, steps per second: 62, episode reward: -360.323, mean reward: -0.689 [-100.000, 4.849], mean action: 1.767 [0.000, 3.000], mean observation: 0.158 [-0.992, 1.410], loss: 1.017794, mean_absolute_error: 34.014854, mean_q: 44.313427, mean_eps: 0.100000
 1627649/2000000: episode: 7752, duration: 2.214s, episode steps: 143, steps per second: 65, episode reward: -24.558, mean reward: -0.172 [-100.000, 18.964], mean action: 1.175 [0.000, 3.000], mean observation: 0.021 [-2.307, 1.000], loss: 0.759842, mean_absolute_error: 35.205494, mean_q: 46.257909, mean_eps: 0.100000
 1627730/2000000: episode: 7753, duration: 1.234s, episode steps: 81, steps per second: 66, episode reward: -62.296, mean reward: -0.769 [-100.000, 18.838], mean action: 1.988 [0.000, 3.000], mean observation: -0.138 [-1.657, 1.000], loss: 0.816300, mean_absolute_error: 34.068754, mean_q: 45.015023, mean_eps: 0.100000
 1628730/2000000: episode: 7754, duration: 16.107s, episode steps: 1000, steps per second: 62, episode reward: 77.412, mean reward: 0.077 [-19.782, 13.202], mean action: 2.395 [0.000, 3.000], mean observation: 0.202 [-1.101, 1.000], loss: 1.096546, mean_absolute_error: 35.178048, mean_q: 45.784378, mean_eps: 0.100000
 1629055/2000000: episode: 7755, duration: 5.125s, episode steps: 325, steps per second: 63, episode reward: 245.770, mean reward: 0.756 [-18.433, 100.000], mean action: 1.455 [0.000, 3.000], mean observation: 0.065 [-0.726, 1.000], loss: 1.071370, mean_absolute_error: 35.546286, mean_q: 45.994365, mean_eps: 0.100000
 1629148/2000000: episode: 7756, duration: 1.488s, episode steps: 93, steps per second: 63, episode reward: -44.305, mean reward: -0.476 [-100.000, 22.829], mean action: 1.613 [0.000, 3.000], mean observation: -0.132 [-1.145, 1.627], loss: 0.744007, mean_absolute_error: 35.544313, mean_q: 46.397377, mean_eps: 0.100000
 1629256/2000000: episode: 7757, duration: 1.683s, episode steps: 108, steps per second: 64, episode reward: -34.960, mean reward: -0.324 [-100.000, 9.473], mean action: 1.324 [0.000, 3.000], mean observation: -0.050 [-1.045, 1.461], loss: 0.830576, mean_absolute_error: 36.205362, mean_q: 47.737837, mean_eps: 0.100000
 1629364/2000000: episode: 7758, duration: 1.709s, episode steps: 108, steps per second: 63, episode reward: -0.875, mean reward: -0.008 [-100.000, 19.217], mean action: 1.324 [0.000, 3.000], mean observation: -0.062 [-1.131, 1.236], loss: 1.154116, mean_absolute_error: 35.846796, mean_q: 47.016026, mean_eps: 0.100000
 1629480/2000000: episode: 7759, duration: 1.824s, episode steps: 116, steps per second: 64, episode reward: -13.849, mean reward: -0.119 [-100.000, 19.764], mean action: 1.164 [0.000, 3.000], mean observation: 0.049 [-1.108, 1.000], loss: 0.918193, mean_absolute_error: 34.493754, mean_q: 45.502495, mean_eps: 0.100000
 1629833/2000000: episode: 7760, duration: 5.485s, episode steps: 353, steps per second: 64, episode reward: 221.109, mean reward: 0.626 [-18.990, 100.000], mean action: 0.737 [0.000, 3.000], mean observation: 0.187 [-1.163, 1.030], loss: 1.086835, mean_absolute_error: 35.233556, mean_q: 46.051676, mean_eps: 0.100000
 1630337/2000000: episode: 7761, duration: 7.996s, episode steps: 504, steps per second: 63, episode reward: -237.635, mean reward: -0.471 [-100.000, 5.502], mean action: 1.829 [0.000, 3.000], mean observation: 0.128 [-1.066, 1.001], loss: 0.996876, mean_absolute_error: 35.553273, mean_q: 46.232822, mean_eps: 0.100000
 1630905/2000000: episode: 7762, duration: 9.114s, episode steps: 568, steps per second: 62, episode reward: -262.367, mean reward: -0.462 [-100.000, 4.896], mean action: 1.604 [0.000, 3.000], mean observation: 0.184 [-0.915, 1.484], loss: 1.065551, mean_absolute_error: 36.108591, mean_q: 47.096218, mean_eps: 0.100000
 1631173/2000000: episode: 7763, duration: 4.103s, episode steps: 268, steps per second: 65, episode reward: 242.895, mean reward: 0.906 [-2.733, 100.000], mean action: 0.743 [0.000, 3.000], mean observation: 0.156 [-1.190, 1.000], loss: 1.135599, mean_absolute_error: 35.948130, mean_q: 46.159226, mean_eps: 0.100000
 1631269/2000000: episode: 7764, duration: 1.633s, episode steps: 96, steps per second: 59, episode reward: -37.843, mean reward: -0.394 [-100.000, 19.180], mean action: 1.250 [0.000, 3.000], mean observation: 0.001 [-1.029, 1.000], loss: 0.915568, mean_absolute_error: 35.977570, mean_q: 45.756701, mean_eps: 0.100000
 1631573/2000000: episode: 7765, duration: 5.089s, episode steps: 304, steps per second: 60, episode reward: 178.570, mean reward: 0.587 [-11.935, 100.000], mean action: 2.082 [0.000, 3.000], mean observation: 0.159 [-1.012, 1.000], loss: 0.905743, mean_absolute_error: 35.851761, mean_q: 46.189871, mean_eps: 0.100000
 1631968/2000000: episode: 7766, duration: 6.248s, episode steps: 395, steps per second: 63, episode reward: 177.492, mean reward: 0.449 [-17.965, 100.000], mean action: 1.096 [0.000, 3.000], mean observation: 0.179 [-1.039, 1.000], loss: 1.084318, mean_absolute_error: 35.336928, mean_q: 46.387793, mean_eps: 0.100000
 1632654/2000000: episode: 7767, duration: 11.059s, episode steps: 686, steps per second: 62, episode reward: 178.993, mean reward: 0.261 [-20.159, 100.000], mean action: 0.755 [0.000, 3.000], mean observation: 0.121 [-1.121, 1.340], loss: 1.094112, mean_absolute_error: 34.854193, mean_q: 45.156965, mean_eps: 0.100000
 1632735/2000000: episode: 7768, duration: 1.265s, episode steps: 81, steps per second: 64, episode reward: -9.461, mean reward: -0.117 [-100.000, 8.392], mean action: 1.444 [0.000, 3.000], mean observation: -0.078 [-1.078, 1.191], loss: 0.941583, mean_absolute_error: 36.172369, mean_q: 45.847727, mean_eps: 0.100000
 1633081/2000000: episode: 7769, duration: 5.511s, episode steps: 346, steps per second: 63, episode reward: 195.767, mean reward: 0.566 [-9.084, 100.000], mean action: 1.066 [0.000, 3.000], mean observation: 0.146 [-1.070, 1.000], loss: 1.457588, mean_absolute_error: 35.613066, mean_q: 46.088440, mean_eps: 0.100000
 1633177/2000000: episode: 7770, duration: 1.497s, episode steps: 96, steps per second: 64, episode reward: -41.601, mean reward: -0.433 [-100.000, 17.859], mean action: 1.719 [0.000, 3.000], mean observation: 0.004 [-1.151, 1.000], loss: 1.030796, mean_absolute_error: 34.744034, mean_q: 45.150056, mean_eps: 0.100000
 1634177/2000000: episode: 7771, duration: 17.031s, episode steps: 1000, steps per second: 59, episode reward: -153.160, mean reward: -0.153 [-5.504, 5.684], mean action: 1.470 [0.000, 3.000], mean observation: 0.203 [-0.770, 1.361], loss: 1.070199, mean_absolute_error: 35.135357, mean_q: 45.438541, mean_eps: 0.100000
 1634469/2000000: episode: 7772, duration: 4.548s, episode steps: 292, steps per second: 64, episode reward: -279.709, mean reward: -0.958 [-100.000, 5.027], mean action: 1.760 [0.000, 3.000], mean observation: 0.133 [-0.851, 1.171], loss: 1.177275, mean_absolute_error: 34.917661, mean_q: 44.449422, mean_eps: 0.100000
 1634672/2000000: episode: 7773, duration: 3.142s, episode steps: 203, steps per second: 65, episode reward: 195.510, mean reward: 0.963 [-17.783, 100.000], mean action: 1.039 [0.000, 3.000], mean observation: 0.105 [-1.095, 1.000], loss: 0.899031, mean_absolute_error: 33.883108, mean_q: 44.225573, mean_eps: 0.100000
 1634939/2000000: episode: 7774, duration: 4.276s, episode steps: 267, steps per second: 62, episode reward: 173.057, mean reward: 0.648 [-17.886, 100.000], mean action: 1.262 [0.000, 3.000], mean observation: 0.060 [-1.068, 1.000], loss: 1.309458, mean_absolute_error: 35.109429, mean_q: 44.911031, mean_eps: 0.100000
 1635057/2000000: episode: 7775, duration: 1.920s, episode steps: 118, steps per second: 61, episode reward: -28.978, mean reward: -0.246 [-100.000, 44.912], mean action: 1.517 [0.000, 3.000], mean observation: -0.090 [-0.953, 1.553], loss: 1.051824, mean_absolute_error: 36.745837, mean_q: 47.894222, mean_eps: 0.100000
 1635222/2000000: episode: 7776, duration: 2.535s, episode steps: 165, steps per second: 65, episode reward: -25.982, mean reward: -0.157 [-100.000, 13.339], mean action: 1.679 [0.000, 3.000], mean observation: -0.018 [-1.067, 1.111], loss: 1.241229, mean_absolute_error: 33.736289, mean_q: 44.422941, mean_eps: 0.100000
 1635335/2000000: episode: 7777, duration: 1.734s, episode steps: 113, steps per second: 65, episode reward: -280.228, mean reward: -2.480 [-100.000, 9.888], mean action: 1.628 [0.000, 3.000], mean observation: -0.075 [-1.375, 2.818], loss: 0.870768, mean_absolute_error: 35.382526, mean_q: 46.298655, mean_eps: 0.100000
 1635705/2000000: episode: 7778, duration: 5.845s, episode steps: 370, steps per second: 63, episode reward: 166.644, mean reward: 0.450 [-8.021, 100.000], mean action: 1.578 [0.000, 3.000], mean observation: 0.120 [-0.650, 1.000], loss: 1.086445, mean_absolute_error: 36.248323, mean_q: 46.747032, mean_eps: 0.100000
 1635806/2000000: episode: 7779, duration: 1.547s, episode steps: 101, steps per second: 65, episode reward: -66.768, mean reward: -0.661 [-100.000, 16.124], mean action: 1.257 [0.000, 3.000], mean observation: -0.098 [-1.045, 1.525], loss: 0.881155, mean_absolute_error: 37.420151, mean_q: 48.409839, mean_eps: 0.100000
 1635979/2000000: episode: 7780, duration: 2.765s, episode steps: 173, steps per second: 63, episode reward: 198.330, mean reward: 1.146 [-8.020, 100.000], mean action: 1.075 [0.000, 3.000], mean observation: 0.090 [-1.086, 1.000], loss: 0.723816, mean_absolute_error: 35.139043, mean_q: 45.561854, mean_eps: 0.100000
 1636979/2000000: episode: 7781, duration: 16.399s, episode steps: 1000, steps per second: 61, episode reward: -114.207, mean reward: -0.114 [-5.257, 4.979], mean action: 1.427 [0.000, 3.000], mean observation: 0.165 [-1.008, 1.009], loss: 0.957920, mean_absolute_error: 35.668292, mean_q: 46.486909, mean_eps: 0.100000
 1637065/2000000: episode: 7782, duration: 1.357s, episode steps: 86, steps per second: 63, episode reward: -19.606, mean reward: -0.228 [-100.000, 16.934], mean action: 1.767 [0.000, 3.000], mean observation: -0.042 [-1.056, 1.027], loss: 0.954428, mean_absolute_error: 33.330386, mean_q: 43.527609, mean_eps: 0.100000
 1637262/2000000: episode: 7783, duration: 3.044s, episode steps: 197, steps per second: 65, episode reward: -315.768, mean reward: -1.603 [-100.000, 4.558], mean action: 1.660 [0.000, 3.000], mean observation: 0.123 [-1.021, 1.336], loss: 1.029191, mean_absolute_error: 33.828637, mean_q: 44.286682, mean_eps: 0.100000
 1637450/2000000: episode: 7784, duration: 2.902s, episode steps: 188, steps per second: 65, episode reward: 198.344, mean reward: 1.055 [-17.545, 100.000], mean action: 0.920 [0.000, 3.000], mean observation: 0.142 [-1.072, 1.000], loss: 0.985710, mean_absolute_error: 35.805776, mean_q: 45.814592, mean_eps: 0.100000
 1637543/2000000: episode: 7785, duration: 1.424s, episode steps: 93, steps per second: 65, episode reward: -37.041, mean reward: -0.398 [-100.000, 18.170], mean action: 1.022 [0.000, 3.000], mean observation: -0.007 [-1.206, 1.000], loss: 0.931875, mean_absolute_error: 34.324522, mean_q: 44.053673, mean_eps: 0.100000
 1637674/2000000: episode: 7786, duration: 2.023s, episode steps: 131, steps per second: 65, episode reward: -44.191, mean reward: -0.337 [-100.000, 13.096], mean action: 1.198 [0.000, 3.000], mean observation: 0.034 [-1.094, 1.000], loss: 1.114382, mean_absolute_error: 35.421476, mean_q: 45.866576, mean_eps: 0.100000
 1637835/2000000: episode: 7787, duration: 2.438s, episode steps: 161, steps per second: 66, episode reward: 172.858, mean reward: 1.074 [-15.763, 100.000], mean action: 1.627 [0.000, 3.000], mean observation: -0.001 [-1.347, 1.000], loss: 1.051256, mean_absolute_error: 35.589613, mean_q: 46.365065, mean_eps: 0.100000
 1637980/2000000: episode: 7788, duration: 2.277s, episode steps: 145, steps per second: 64, episode reward: 21.942, mean reward: 0.151 [-100.000, 11.903], mean action: 1.690 [0.000, 3.000], mean observation: 0.041 [-1.001, 1.000], loss: 1.229896, mean_absolute_error: 34.905805, mean_q: 44.989732, mean_eps: 0.100000
 1638469/2000000: episode: 7789, duration: 7.760s, episode steps: 489, steps per second: 63, episode reward: 201.476, mean reward: 0.412 [-17.392, 100.000], mean action: 1.517 [0.000, 3.000], mean observation: 0.187 [-0.979, 1.000], loss: 0.968758, mean_absolute_error: 35.550969, mean_q: 46.383105, mean_eps: 0.100000
 1638859/2000000: episode: 7790, duration: 6.077s, episode steps: 390, steps per second: 64, episode reward: 197.075, mean reward: 0.505 [-19.023, 100.000], mean action: 0.787 [0.000, 3.000], mean observation: 0.132 [-1.093, 1.000], loss: 1.217675, mean_absolute_error: 36.108890, mean_q: 46.676840, mean_eps: 0.100000
 1639331/2000000: episode: 7791, duration: 7.638s, episode steps: 472, steps per second: 62, episode reward: -336.571, mean reward: -0.713 [-100.000, 5.423], mean action: 1.875 [0.000, 3.000], mean observation: 0.131 [-1.072, 2.303], loss: 0.889253, mean_absolute_error: 35.480827, mean_q: 46.127380, mean_eps: 0.100000
 1639464/2000000: episode: 7792, duration: 2.086s, episode steps: 133, steps per second: 64, episode reward: -58.831, mean reward: -0.442 [-100.000, 13.234], mean action: 1.579 [0.000, 3.000], mean observation: 0.084 [-1.022, 1.000], loss: 0.773216, mean_absolute_error: 34.794774, mean_q: 45.473012, mean_eps: 0.100000
 1639757/2000000: episode: 7793, duration: 4.669s, episode steps: 293, steps per second: 63, episode reward: 156.183, mean reward: 0.533 [-16.633, 100.000], mean action: 1.932 [0.000, 3.000], mean observation: 0.132 [-1.034, 1.000], loss: 1.031235, mean_absolute_error: 35.863974, mean_q: 45.705257, mean_eps: 0.100000
 1639983/2000000: episode: 7794, duration: 3.590s, episode steps: 226, steps per second: 63, episode reward: 5.542, mean reward: 0.025 [-100.000, 13.041], mean action: 1.752 [0.000, 3.000], mean observation: -0.021 [-0.798, 1.097], loss: 1.165626, mean_absolute_error: 36.088475, mean_q: 46.900211, mean_eps: 0.100000
 1640251/2000000: episode: 7795, duration: 4.161s, episode steps: 268, steps per second: 64, episode reward: 198.301, mean reward: 0.740 [-3.398, 100.000], mean action: 1.362 [0.000, 3.000], mean observation: 0.161 [-0.839, 1.000], loss: 1.107454, mean_absolute_error: 35.582721, mean_q: 45.963812, mean_eps: 0.100000
 1640360/2000000: episode: 7796, duration: 1.696s, episode steps: 109, steps per second: 64, episode reward: -12.867, mean reward: -0.118 [-100.000, 13.084], mean action: 1.257 [0.000, 3.000], mean observation: 0.003 [-1.053, 1.000], loss: 1.293192, mean_absolute_error: 34.093058, mean_q: 44.187393, mean_eps: 0.100000
 1640767/2000000: episode: 7797, duration: 6.386s, episode steps: 407, steps per second: 64, episode reward: 222.481, mean reward: 0.547 [-19.324, 100.000], mean action: 1.528 [0.000, 3.000], mean observation: 0.181 [-1.010, 1.000], loss: 1.108514, mean_absolute_error: 34.628935, mean_q: 44.521834, mean_eps: 0.100000
 1640920/2000000: episode: 7798, duration: 2.392s, episode steps: 153, steps per second: 64, episode reward: 215.276, mean reward: 1.407 [-11.862, 100.000], mean action: 1.431 [0.000, 3.000], mean observation: 0.072 [-1.102, 1.000], loss: 0.941122, mean_absolute_error: 34.071011, mean_q: 43.924644, mean_eps: 0.100000
 1641043/2000000: episode: 7799, duration: 2.185s, episode steps: 123, steps per second: 56, episode reward: -9.949, mean reward: -0.081 [-100.000, 10.516], mean action: 1.285 [0.000, 3.000], mean observation: 0.009 [-1.085, 1.136], loss: 1.054399, mean_absolute_error: 34.861566, mean_q: 44.937334, mean_eps: 0.100000
 1641137/2000000: episode: 7800, duration: 1.480s, episode steps: 94, steps per second: 64, episode reward: -44.904, mean reward: -0.478 [-100.000, 20.315], mean action: 1.915 [0.000, 3.000], mean observation: -0.083 [-1.079, 1.901], loss: 1.077824, mean_absolute_error: 35.461917, mean_q: 45.108483, mean_eps: 0.100000
 1641236/2000000: episode: 7801, duration: 1.536s, episode steps: 99, steps per second: 64, episode reward: -26.167, mean reward: -0.264 [-100.000, 15.361], mean action: 1.687 [0.000, 3.000], mean observation: -0.009 [-1.505, 1.000], loss: 1.040059, mean_absolute_error: 35.039670, mean_q: 45.544555, mean_eps: 0.100000
 1641345/2000000: episode: 7802, duration: 1.731s, episode steps: 109, steps per second: 63, episode reward: -48.635, mean reward: -0.446 [-100.000, 20.019], mean action: 1.872 [0.000, 3.000], mean observation: -0.071 [-1.045, 1.202], loss: 0.831862, mean_absolute_error: 32.762640, mean_q: 42.940952, mean_eps: 0.100000
 1641550/2000000: episode: 7803, duration: 3.355s, episode steps: 205, steps per second: 61, episode reward: 222.504, mean reward: 1.085 [-19.106, 100.000], mean action: 1.059 [0.000, 3.000], mean observation: 0.096 [-1.058, 1.000], loss: 1.192865, mean_absolute_error: 35.417083, mean_q: 45.831035, mean_eps: 0.100000
 1641753/2000000: episode: 7804, duration: 3.123s, episode steps: 203, steps per second: 65, episode reward: 168.489, mean reward: 0.830 [-10.019, 100.000], mean action: 1.128 [0.000, 3.000], mean observation: 0.132 [-1.008, 1.000], loss: 0.842674, mean_absolute_error: 35.662125, mean_q: 46.850250, mean_eps: 0.100000
 1642061/2000000: episode: 7805, duration: 4.828s, episode steps: 308, steps per second: 64, episode reward: 170.925, mean reward: 0.555 [-11.749, 100.000], mean action: 1.948 [0.000, 3.000], mean observation: 0.139 [-1.080, 1.004], loss: 1.028398, mean_absolute_error: 34.319754, mean_q: 44.345475, mean_eps: 0.100000
 1642252/2000000: episode: 7806, duration: 2.951s, episode steps: 191, steps per second: 65, episode reward: 227.077, mean reward: 1.189 [-2.373, 100.000], mean action: 0.911 [0.000, 3.000], mean observation: 0.128 [-1.161, 1.000], loss: 1.051330, mean_absolute_error: 35.036849, mean_q: 44.026001, mean_eps: 0.100000
 1642494/2000000: episode: 7807, duration: 3.773s, episode steps: 242, steps per second: 64, episode reward: 196.708, mean reward: 0.813 [-13.738, 100.000], mean action: 2.405 [0.000, 3.000], mean observation: 0.169 [-1.018, 1.000], loss: 1.003429, mean_absolute_error: 34.372262, mean_q: 44.469919, mean_eps: 0.100000
 1642624/2000000: episode: 7808, duration: 2.020s, episode steps: 130, steps per second: 64, episode reward: -51.426, mean reward: -0.396 [-100.000, 13.774], mean action: 1.323 [0.000, 3.000], mean observation: 0.058 [-1.062, 1.000], loss: 1.232385, mean_absolute_error: 36.216497, mean_q: 46.168341, mean_eps: 0.100000
 1642872/2000000: episode: 7809, duration: 3.927s, episode steps: 248, steps per second: 63, episode reward: 179.737, mean reward: 0.725 [-3.375, 100.000], mean action: 1.657 [0.000, 3.000], mean observation: 0.104 [-1.001, 1.000], loss: 0.885748, mean_absolute_error: 35.537179, mean_q: 45.689065, mean_eps: 0.100000
 1643098/2000000: episode: 7810, duration: 3.558s, episode steps: 226, steps per second: 64, episode reward: -100.401, mean reward: -0.444 [-100.000, 6.637], mean action: 1.947 [0.000, 3.000], mean observation: 0.101 [-1.421, 1.023], loss: 1.132525, mean_absolute_error: 34.310031, mean_q: 44.473649, mean_eps: 0.100000
 1643215/2000000: episode: 7811, duration: 1.795s, episode steps: 117, steps per second: 65, episode reward: -40.815, mean reward: -0.349 [-100.000, 19.509], mean action: 1.470 [0.000, 3.000], mean observation: -0.034 [-0.974, 1.405], loss: 1.066346, mean_absolute_error: 35.111315, mean_q: 44.811824, mean_eps: 0.100000
 1643432/2000000: episode: 7812, duration: 3.387s, episode steps: 217, steps per second: 64, episode reward: 222.038, mean reward: 1.023 [-17.495, 100.000], mean action: 0.866 [0.000, 3.000], mean observation: 0.065 [-1.058, 1.000], loss: 1.109729, mean_absolute_error: 34.365609, mean_q: 44.637375, mean_eps: 0.100000
 1643725/2000000: episode: 7813, duration: 4.627s, episode steps: 293, steps per second: 63, episode reward: 192.830, mean reward: 0.658 [-9.640, 100.000], mean action: 1.072 [0.000, 3.000], mean observation: 0.135 [-1.027, 1.000], loss: 0.967144, mean_absolute_error: 34.873984, mean_q: 45.128270, mean_eps: 0.100000
 1643818/2000000: episode: 7814, duration: 1.442s, episode steps: 93, steps per second: 64, episode reward: -32.436, mean reward: -0.349 [-100.000, 13.072], mean action: 1.978 [0.000, 3.000], mean observation: -0.116 [-1.031, 1.602], loss: 1.260435, mean_absolute_error: 35.795067, mean_q: 46.863956, mean_eps: 0.100000
 1644084/2000000: episode: 7815, duration: 4.129s, episode steps: 266, steps per second: 64, episode reward: 239.832, mean reward: 0.902 [-8.726, 100.000], mean action: 0.985 [0.000, 3.000], mean observation: 0.094 [-1.090, 1.011], loss: 0.989426, mean_absolute_error: 35.051998, mean_q: 45.192450, mean_eps: 0.100000
 1644236/2000000: episode: 7816, duration: 2.401s, episode steps: 152, steps per second: 63, episode reward: -24.825, mean reward: -0.163 [-100.000, 11.833], mean action: 1.362 [0.000, 3.000], mean observation: 0.004 [-1.084, 1.000], loss: 1.073976, mean_absolute_error: 34.597295, mean_q: 44.494152, mean_eps: 0.100000
 1644473/2000000: episode: 7817, duration: 3.706s, episode steps: 237, steps per second: 64, episode reward: 208.937, mean reward: 0.882 [-17.670, 100.000], mean action: 1.468 [0.000, 3.000], mean observation: 0.105 [-1.027, 1.000], loss: 1.402447, mean_absolute_error: 35.919885, mean_q: 46.099069, mean_eps: 0.100000
 1644579/2000000: episode: 7818, duration: 1.627s, episode steps: 106, steps per second: 65, episode reward: -43.907, mean reward: -0.414 [-100.000, 10.399], mean action: 1.906 [0.000, 3.000], mean observation: -0.046 [-1.098, 1.218], loss: 1.068719, mean_absolute_error: 34.602125, mean_q: 45.169331, mean_eps: 0.100000
 1644773/2000000: episode: 7819, duration: 2.998s, episode steps: 194, steps per second: 65, episode reward: 213.214, mean reward: 1.099 [-11.323, 100.000], mean action: 0.959 [0.000, 3.000], mean observation: 0.073 [-1.080, 1.000], loss: 1.324324, mean_absolute_error: 33.126367, mean_q: 43.094296, mean_eps: 0.100000
 1645043/2000000: episode: 7820, duration: 4.141s, episode steps: 270, steps per second: 65, episode reward: 191.668, mean reward: 0.710 [-14.463, 100.000], mean action: 1.289 [0.000, 3.000], mean observation: 0.037 [-1.033, 1.000], loss: 1.223704, mean_absolute_error: 34.933586, mean_q: 44.876335, mean_eps: 0.100000
 1645172/2000000: episode: 7821, duration: 1.998s, episode steps: 129, steps per second: 65, episode reward: -41.471, mean reward: -0.321 [-100.000, 17.756], mean action: 1.209 [0.000, 3.000], mean observation: 0.017 [-1.203, 1.020], loss: 1.130657, mean_absolute_error: 35.722308, mean_q: 45.289060, mean_eps: 0.100000
 1645285/2000000: episode: 7822, duration: 1.783s, episode steps: 113, steps per second: 63, episode reward: -26.734, mean reward: -0.237 [-100.000, 19.839], mean action: 1.097 [0.000, 3.000], mean observation: 0.034 [-1.412, 1.000], loss: 0.855432, mean_absolute_error: 35.600925, mean_q: 46.392185, mean_eps: 0.100000
 1645541/2000000: episode: 7823, duration: 3.969s, episode steps: 256, steps per second: 65, episode reward: 241.852, mean reward: 0.945 [-17.531, 100.000], mean action: 0.957 [0.000, 3.000], mean observation: 0.153 [-0.996, 1.000], loss: 0.862846, mean_absolute_error: 35.134391, mean_q: 45.413389, mean_eps: 0.100000
 1645817/2000000: episode: 7824, duration: 4.550s, episode steps: 276, steps per second: 61, episode reward: -216.677, mean reward: -0.785 [-100.000, 4.658], mean action: 1.688 [0.000, 3.000], mean observation: 0.141 [-1.012, 1.000], loss: 1.278495, mean_absolute_error: 35.845611, mean_q: 45.793677, mean_eps: 0.100000
 1645937/2000000: episode: 7825, duration: 1.875s, episode steps: 120, steps per second: 64, episode reward: -62.239, mean reward: -0.519 [-100.000, 57.389], mean action: 1.783 [0.000, 3.000], mean observation: -0.079 [-0.996, 1.357], loss: 0.924927, mean_absolute_error: 34.922781, mean_q: 45.113966, mean_eps: 0.100000
 1646203/2000000: episode: 7826, duration: 4.073s, episode steps: 266, steps per second: 65, episode reward: 219.316, mean reward: 0.824 [-6.357, 100.000], mean action: 1.102 [0.000, 3.000], mean observation: 0.090 [-1.006, 1.000], loss: 0.852094, mean_absolute_error: 36.665475, mean_q: 47.064200, mean_eps: 0.100000
 1646424/2000000: episode: 7827, duration: 3.438s, episode steps: 221, steps per second: 64, episode reward: 215.966, mean reward: 0.977 [-9.302, 100.000], mean action: 0.928 [0.000, 3.000], mean observation: 0.077 [-1.013, 1.000], loss: 0.959544, mean_absolute_error: 35.051365, mean_q: 44.901706, mean_eps: 0.100000
 1646730/2000000: episode: 7828, duration: 4.885s, episode steps: 306, steps per second: 63, episode reward: -181.289, mean reward: -0.592 [-100.000, 5.107], mean action: 1.863 [0.000, 3.000], mean observation: 0.145 [-0.892, 1.002], loss: 1.153926, mean_absolute_error: 34.568631, mean_q: 44.896965, mean_eps: 0.100000
 1646882/2000000: episode: 7829, duration: 2.353s, episode steps: 152, steps per second: 65, episode reward: -33.531, mean reward: -0.221 [-100.000, 10.584], mean action: 1.914 [0.000, 3.000], mean observation: 0.073 [-0.966, 1.000], loss: 0.944636, mean_absolute_error: 34.936713, mean_q: 45.212461, mean_eps: 0.100000
 1647235/2000000: episode: 7830, duration: 5.569s, episode steps: 353, steps per second: 63, episode reward: 160.767, mean reward: 0.455 [-14.618, 100.000], mean action: 1.632 [0.000, 3.000], mean observation: 0.109 [-1.000, 1.000], loss: 1.164052, mean_absolute_error: 34.235897, mean_q: 43.597512, mean_eps: 0.100000
 1647349/2000000: episode: 7831, duration: 1.800s, episode steps: 114, steps per second: 63, episode reward: 6.712, mean reward: 0.059 [-100.000, 14.290], mean action: 0.974 [0.000, 3.000], mean observation: 0.022 [-1.156, 1.004], loss: 1.520601, mean_absolute_error: 36.047075, mean_q: 46.456462, mean_eps: 0.100000
 1647614/2000000: episode: 7832, duration: 4.051s, episode steps: 265, steps per second: 65, episode reward: 199.011, mean reward: 0.751 [-10.485, 100.000], mean action: 0.615 [0.000, 3.000], mean observation: 0.152 [-1.164, 1.000], loss: 0.906794, mean_absolute_error: 34.663329, mean_q: 44.257056, mean_eps: 0.100000
 1647852/2000000: episode: 7833, duration: 4.161s, episode steps: 238, steps per second: 57, episode reward: -208.482, mean reward: -0.876 [-100.000, 25.589], mean action: 1.693 [0.000, 3.000], mean observation: -0.127 [-1.559, 1.000], loss: 0.746962, mean_absolute_error: 34.872259, mean_q: 45.320442, mean_eps: 0.100000
 1647956/2000000: episode: 7834, duration: 1.710s, episode steps: 104, steps per second: 61, episode reward: -7.754, mean reward: -0.075 [-100.000, 17.024], mean action: 1.875 [0.000, 3.000], mean observation: -0.076 [-1.065, 1.105], loss: 1.251410, mean_absolute_error: 33.594237, mean_q: 43.270155, mean_eps: 0.100000
 1648199/2000000: episode: 7835, duration: 3.799s, episode steps: 243, steps per second: 64, episode reward: 208.042, mean reward: 0.856 [-17.400, 100.000], mean action: 0.868 [0.000, 3.000], mean observation: 0.136 [-1.026, 1.000], loss: 1.333751, mean_absolute_error: 36.390596, mean_q: 46.998891, mean_eps: 0.100000
 1649199/2000000: episode: 7836, duration: 16.448s, episode steps: 1000, steps per second: 61, episode reward: 80.493, mean reward: 0.080 [-19.180, 20.981], mean action: 0.804 [0.000, 3.000], mean observation: 0.143 [-1.222, 1.000], loss: 1.135256, mean_absolute_error: 35.530270, mean_q: 45.615413, mean_eps: 0.100000
 1649487/2000000: episode: 7837, duration: 4.525s, episode steps: 288, steps per second: 64, episode reward: 196.942, mean reward: 0.684 [-6.702, 100.000], mean action: 1.167 [0.000, 3.000], mean observation: 0.074 [-1.026, 1.000], loss: 0.861221, mean_absolute_error: 35.984855, mean_q: 45.992372, mean_eps: 0.100000
 1649946/2000000: episode: 7838, duration: 7.261s, episode steps: 459, steps per second: 63, episode reward: -555.807, mean reward: -1.211 [-100.000, 5.665], mean action: 1.671 [0.000, 3.000], mean observation: 0.026 [-0.985, 2.823], loss: 1.140921, mean_absolute_error: 35.124323, mean_q: 44.682812, mean_eps: 0.100000
 1650041/2000000: episode: 7839, duration: 1.495s, episode steps: 95, steps per second: 64, episode reward: 12.451, mean reward: 0.131 [-100.000, 11.406], mean action: 1.600 [0.000, 3.000], mean observation: -0.050 [-1.074, 1.151], loss: 1.299272, mean_absolute_error: 32.346567, mean_q: 41.072361, mean_eps: 0.100000
 1650394/2000000: episode: 7840, duration: 5.562s, episode steps: 353, steps per second: 63, episode reward: 209.246, mean reward: 0.593 [-18.965, 100.000], mean action: 1.133 [0.000, 3.000], mean observation: 0.127 [-1.008, 1.000], loss: 0.910066, mean_absolute_error: 35.159803, mean_q: 45.004479, mean_eps: 0.100000
 1650724/2000000: episode: 7841, duration: 5.149s, episode steps: 330, steps per second: 64, episode reward: 181.601, mean reward: 0.550 [-18.157, 100.000], mean action: 0.821 [0.000, 3.000], mean observation: 0.212 [-1.025, 1.000], loss: 1.024911, mean_absolute_error: 35.073986, mean_q: 44.686775, mean_eps: 0.100000
 1651028/2000000: episode: 7842, duration: 4.857s, episode steps: 304, steps per second: 63, episode reward: 170.402, mean reward: 0.561 [-12.572, 100.000], mean action: 1.477 [0.000, 3.000], mean observation: 0.110 [-0.961, 1.000], loss: 1.221193, mean_absolute_error: 35.553342, mean_q: 45.955604, mean_eps: 0.100000
 1651206/2000000: episode: 7843, duration: 2.730s, episode steps: 178, steps per second: 65, episode reward: -6.108, mean reward: -0.034 [-100.000, 17.432], mean action: 1.528 [0.000, 3.000], mean observation: 0.013 [-1.078, 1.337], loss: 1.033609, mean_absolute_error: 34.327186, mean_q: 44.443284, mean_eps: 0.100000
 1651396/2000000: episode: 7844, duration: 2.955s, episode steps: 190, steps per second: 64, episode reward: -31.336, mean reward: -0.165 [-100.000, 20.689], mean action: 1.863 [0.000, 3.000], mean observation: -0.010 [-1.031, 1.000], loss: 0.913478, mean_absolute_error: 35.013733, mean_q: 45.489468, mean_eps: 0.100000
 1651495/2000000: episode: 7845, duration: 1.544s, episode steps: 99, steps per second: 64, episode reward: -57.562, mean reward: -0.581 [-100.000, 13.182], mean action: 1.293 [0.000, 3.000], mean observation: -0.088 [-1.322, 1.000], loss: 1.023819, mean_absolute_error: 35.343334, mean_q: 46.462806, mean_eps: 0.100000
 1651790/2000000: episode: 7846, duration: 4.613s, episode steps: 295, steps per second: 64, episode reward: 209.244, mean reward: 0.709 [-19.502, 100.000], mean action: 1.105 [0.000, 3.000], mean observation: 0.092 [-1.060, 1.000], loss: 1.258326, mean_absolute_error: 35.265758, mean_q: 45.854818, mean_eps: 0.100000
 1652081/2000000: episode: 7847, duration: 4.521s, episode steps: 291, steps per second: 64, episode reward: 252.034, mean reward: 0.866 [-13.102, 100.000], mean action: 1.467 [0.000, 3.000], mean observation: 0.139 [-0.978, 1.000], loss: 1.327168, mean_absolute_error: 35.863913, mean_q: 45.872018, mean_eps: 0.100000
 1652339/2000000: episode: 7848, duration: 3.999s, episode steps: 258, steps per second: 65, episode reward: 181.823, mean reward: 0.705 [-7.528, 100.000], mean action: 1.380 [0.000, 3.000], mean observation: 0.037 [-1.135, 1.000], loss: 1.146331, mean_absolute_error: 35.760143, mean_q: 45.299927, mean_eps: 0.100000
 1652782/2000000: episode: 7849, duration: 7.027s, episode steps: 443, steps per second: 63, episode reward: 261.196, mean reward: 0.590 [-17.762, 100.000], mean action: 1.233 [0.000, 3.000], mean observation: 0.158 [-1.240, 1.000], loss: 1.276386, mean_absolute_error: 34.146043, mean_q: 43.860672, mean_eps: 0.100000
 1653024/2000000: episode: 7850, duration: 3.785s, episode steps: 242, steps per second: 64, episode reward: 203.585, mean reward: 0.841 [-9.745, 100.000], mean action: 1.579 [0.000, 3.000], mean observation: 0.034 [-1.048, 1.000], loss: 0.835370, mean_absolute_error: 35.313827, mean_q: 45.464509, mean_eps: 0.100000
 1653324/2000000: episode: 7851, duration: 4.734s, episode steps: 300, steps per second: 63, episode reward: 190.445, mean reward: 0.635 [-16.173, 100.000], mean action: 1.407 [0.000, 3.000], mean observation: 0.051 [-1.014, 1.000], loss: 0.941884, mean_absolute_error: 35.599746, mean_q: 45.943007, mean_eps: 0.100000
 1653595/2000000: episode: 7852, duration: 4.259s, episode steps: 271, steps per second: 64, episode reward: 215.645, mean reward: 0.796 [-19.683, 100.000], mean action: 1.325 [0.000, 3.000], mean observation: 0.141 [-0.943, 1.000], loss: 1.149104, mean_absolute_error: 35.329694, mean_q: 45.609471, mean_eps: 0.100000
 1653726/2000000: episode: 7853, duration: 2.046s, episode steps: 131, steps per second: 64, episode reward: -69.150, mean reward: -0.528 [-100.000, 12.029], mean action: 1.748 [0.000, 3.000], mean observation: -0.080 [-1.005, 1.104], loss: 1.214215, mean_absolute_error: 34.055548, mean_q: 44.800300, mean_eps: 0.100000
 1653841/2000000: episode: 7854, duration: 1.789s, episode steps: 115, steps per second: 64, episode reward: -6.338, mean reward: -0.055 [-100.000, 17.761], mean action: 1.643 [0.000, 3.000], mean observation: 0.011 [-1.122, 1.042], loss: 1.169858, mean_absolute_error: 33.710368, mean_q: 43.600788, mean_eps: 0.100000
 1654491/2000000: episode: 7855, duration: 10.689s, episode steps: 650, steps per second: 61, episode reward: 174.645, mean reward: 0.269 [-19.013, 100.000], mean action: 1.672 [0.000, 3.000], mean observation: 0.109 [-1.024, 1.000], loss: 1.180735, mean_absolute_error: 35.877690, mean_q: 46.220789, mean_eps: 0.100000
 1654741/2000000: episode: 7856, duration: 3.980s, episode steps: 250, steps per second: 63, episode reward: 167.331, mean reward: 0.669 [-13.542, 100.000], mean action: 1.656 [0.000, 3.000], mean observation: 0.068 [-1.022, 1.000], loss: 1.067882, mean_absolute_error: 35.258159, mean_q: 45.593107, mean_eps: 0.100000
 1654827/2000000: episode: 7857, duration: 1.311s, episode steps: 86, steps per second: 66, episode reward: -5.294, mean reward: -0.062 [-100.000, 17.620], mean action: 1.605 [0.000, 3.000], mean observation: -0.100 [-1.068, 1.000], loss: 1.018413, mean_absolute_error: 33.891480, mean_q: 44.262908, mean_eps: 0.100000
 1655161/2000000: episode: 7858, duration: 5.248s, episode steps: 334, steps per second: 64, episode reward: 190.125, mean reward: 0.569 [-17.830, 100.000], mean action: 1.269 [0.000, 3.000], mean observation: 0.139 [-1.005, 1.000], loss: 1.050763, mean_absolute_error: 34.787123, mean_q: 44.342898, mean_eps: 0.100000
 1655269/2000000: episode: 7859, duration: 1.701s, episode steps: 108, steps per second: 64, episode reward: -63.734, mean reward: -0.590 [-100.000, 15.641], mean action: 1.435 [0.000, 3.000], mean observation: -0.039 [-1.441, 1.000], loss: 1.042710, mean_absolute_error: 34.276678, mean_q: 43.881658, mean_eps: 0.100000
 1655352/2000000: episode: 7860, duration: 1.301s, episode steps: 83, steps per second: 64, episode reward: -63.250, mean reward: -0.762 [-100.000, 18.452], mean action: 1.627 [0.000, 3.000], mean observation: -0.027 [-2.229, 1.000], loss: 1.326254, mean_absolute_error: 35.284482, mean_q: 45.517124, mean_eps: 0.100000
 1655724/2000000: episode: 7861, duration: 5.914s, episode steps: 372, steps per second: 63, episode reward: 195.353, mean reward: 0.525 [-10.256, 100.000], mean action: 1.121 [0.000, 3.000], mean observation: 0.086 [-1.017, 1.044], loss: 1.244083, mean_absolute_error: 34.999170, mean_q: 45.094056, mean_eps: 0.100000
 1655921/2000000: episode: 7862, duration: 3.116s, episode steps: 197, steps per second: 63, episode reward: -226.705, mean reward: -1.151 [-100.000, 9.726], mean action: 1.305 [0.000, 3.000], mean observation: 0.118 [-1.008, 1.004], loss: 1.021234, mean_absolute_error: 33.848399, mean_q: 44.368238, mean_eps: 0.100000
 1656302/2000000: episode: 7863, duration: 6.363s, episode steps: 381, steps per second: 60, episode reward: 215.056, mean reward: 0.564 [-11.378, 100.000], mean action: 1.711 [0.000, 3.000], mean observation: 0.067 [-0.917, 1.000], loss: 1.108909, mean_absolute_error: 35.010431, mean_q: 45.179123, mean_eps: 0.100000
 1656604/2000000: episode: 7864, duration: 4.733s, episode steps: 302, steps per second: 64, episode reward: 175.528, mean reward: 0.581 [-9.860, 100.000], mean action: 1.185 [0.000, 3.000], mean observation: 0.112 [-0.964, 1.000], loss: 1.231532, mean_absolute_error: 35.817690, mean_q: 45.294638, mean_eps: 0.100000
 1657227/2000000: episode: 7865, duration: 9.914s, episode steps: 623, steps per second: 63, episode reward: 221.893, mean reward: 0.356 [-19.055, 100.000], mean action: 0.591 [0.000, 3.000], mean observation: 0.104 [-1.429, 1.000], loss: 1.216931, mean_absolute_error: 35.513223, mean_q: 45.390487, mean_eps: 0.100000
 1657543/2000000: episode: 7866, duration: 4.908s, episode steps: 316, steps per second: 64, episode reward: 247.743, mean reward: 0.784 [-6.313, 100.000], mean action: 1.348 [0.000, 3.000], mean observation: 0.051 [-0.799, 1.004], loss: 1.098103, mean_absolute_error: 35.937374, mean_q: 46.170126, mean_eps: 0.100000
 1658175/2000000: episode: 7867, duration: 10.274s, episode steps: 632, steps per second: 62, episode reward: 192.594, mean reward: 0.305 [-23.656, 100.000], mean action: 0.647 [0.000, 3.000], mean observation: 0.145 [-1.249, 1.000], loss: 1.279404, mean_absolute_error: 35.188050, mean_q: 45.323210, mean_eps: 0.100000
 1658527/2000000: episode: 7868, duration: 5.558s, episode steps: 352, steps per second: 63, episode reward: 196.030, mean reward: 0.557 [-17.604, 100.000], mean action: 1.060 [0.000, 3.000], mean observation: 0.123 [-1.056, 1.000], loss: 1.100304, mean_absolute_error: 35.510866, mean_q: 45.844829, mean_eps: 0.100000
 1658646/2000000: episode: 7869, duration: 1.846s, episode steps: 119, steps per second: 64, episode reward: -10.536, mean reward: -0.089 [-100.000, 15.369], mean action: 1.403 [0.000, 3.000], mean observation: -0.062 [-1.067, 1.000], loss: 1.173270, mean_absolute_error: 35.922913, mean_q: 46.354478, mean_eps: 0.100000
 1658750/2000000: episode: 7870, duration: 1.625s, episode steps: 104, steps per second: 64, episode reward: -50.241, mean reward: -0.483 [-100.000, 12.149], mean action: 1.894 [0.000, 3.000], mean observation: -0.091 [-1.085, 1.000], loss: 1.824205, mean_absolute_error: 34.943545, mean_q: 43.588391, mean_eps: 0.100000
 1658947/2000000: episode: 7871, duration: 3.015s, episode steps: 197, steps per second: 65, episode reward: 224.512, mean reward: 1.140 [-4.080, 100.000], mean action: 0.761 [0.000, 3.000], mean observation: 0.162 [-1.017, 1.000], loss: 1.095899, mean_absolute_error: 35.786869, mean_q: 46.349009, mean_eps: 0.100000
 1659251/2000000: episode: 7872, duration: 4.788s, episode steps: 304, steps per second: 63, episode reward: 161.615, mean reward: 0.532 [-17.344, 100.000], mean action: 2.220 [0.000, 3.000], mean observation: 0.245 [-0.953, 1.000], loss: 1.148778, mean_absolute_error: 35.379996, mean_q: 45.775546, mean_eps: 0.100000
 1659363/2000000: episode: 7873, duration: 1.721s, episode steps: 112, steps per second: 65, episode reward: -5.339, mean reward: -0.048 [-100.000, 18.333], mean action: 1.759 [0.000, 3.000], mean observation: -0.004 [-1.106, 1.000], loss: 1.420303, mean_absolute_error: 35.347804, mean_q: 46.487826, mean_eps: 0.100000
 1659656/2000000: episode: 7874, duration: 4.537s, episode steps: 293, steps per second: 65, episode reward: 215.691, mean reward: 0.736 [-18.471, 100.000], mean action: 0.590 [0.000, 3.000], mean observation: 0.152 [-1.086, 1.000], loss: 1.199072, mean_absolute_error: 34.958305, mean_q: 44.757591, mean_eps: 0.100000
 1659840/2000000: episode: 7875, duration: 2.904s, episode steps: 184, steps per second: 63, episode reward: -28.463, mean reward: -0.155 [-100.000, 15.836], mean action: 1.554 [0.000, 3.000], mean observation: 0.025 [-1.072, 1.000], loss: 1.016496, mean_absolute_error: 36.544382, mean_q: 47.047151, mean_eps: 0.100000
 1660106/2000000: episode: 7876, duration: 4.214s, episode steps: 266, steps per second: 63, episode reward: 169.023, mean reward: 0.635 [-16.120, 100.000], mean action: 1.444 [0.000, 3.000], mean observation: 0.149 [-0.995, 1.000], loss: 1.223265, mean_absolute_error: 34.811311, mean_q: 44.667872, mean_eps: 0.100000
 1660391/2000000: episode: 7877, duration: 4.475s, episode steps: 285, steps per second: 64, episode reward: 180.380, mean reward: 0.633 [-16.425, 100.000], mean action: 2.411 [0.000, 3.000], mean observation: 0.189 [-1.014, 1.033], loss: 1.210790, mean_absolute_error: 35.654302, mean_q: 45.278224, mean_eps: 0.100000
 1660574/2000000: episode: 7878, duration: 2.826s, episode steps: 183, steps per second: 65, episode reward: -395.132, mean reward: -2.159 [-100.000, 5.775], mean action: 1.568 [0.000, 3.000], mean observation: -0.056 [-1.733, 1.358], loss: 0.920249, mean_absolute_error: 35.147822, mean_q: 45.295822, mean_eps: 0.100000
 1661092/2000000: episode: 7879, duration: 8.402s, episode steps: 518, steps per second: 62, episode reward: 175.569, mean reward: 0.339 [-17.456, 100.000], mean action: 1.272 [0.000, 3.000], mean observation: 0.159 [-0.930, 1.000], loss: 0.978481, mean_absolute_error: 35.872977, mean_q: 46.251735, mean_eps: 0.100000
 1661222/2000000: episode: 7880, duration: 2.030s, episode steps: 130, steps per second: 64, episode reward: -85.105, mean reward: -0.655 [-100.000, 16.362], mean action: 1.369 [0.000, 3.000], mean observation: -0.033 [-1.434, 1.000], loss: 1.182938, mean_absolute_error: 35.378203, mean_q: 45.069073, mean_eps: 0.100000
 1661508/2000000: episode: 7881, duration: 4.441s, episode steps: 286, steps per second: 64, episode reward: 204.283, mean reward: 0.714 [-18.944, 100.000], mean action: 0.727 [0.000, 3.000], mean observation: 0.099 [-1.040, 1.000], loss: 1.139488, mean_absolute_error: 35.940529, mean_q: 46.315987, mean_eps: 0.100000
 1661697/2000000: episode: 7882, duration: 3.015s, episode steps: 189, steps per second: 63, episode reward: 200.729, mean reward: 1.062 [-10.353, 100.000], mean action: 1.148 [0.000, 3.000], mean observation: 0.059 [-1.023, 1.000], loss: 0.865764, mean_absolute_error: 33.770310, mean_q: 44.126303, mean_eps: 0.100000
 1661890/2000000: episode: 7883, duration: 3.072s, episode steps: 193, steps per second: 63, episode reward: -2.272, mean reward: -0.012 [-100.000, 12.469], mean action: 1.839 [0.000, 3.000], mean observation: 0.094 [-0.848, 1.000], loss: 1.576601, mean_absolute_error: 35.220526, mean_q: 45.669446, mean_eps: 0.100000
 1662292/2000000: episode: 7884, duration: 6.524s, episode steps: 402, steps per second: 62, episode reward: -647.669, mean reward: -1.611 [-100.000, 5.253], mean action: 1.965 [0.000, 3.000], mean observation: 0.027 [-0.902, 3.351], loss: 1.088047, mean_absolute_error: 36.160780, mean_q: 46.071768, mean_eps: 0.100000
 1662660/2000000: episode: 7885, duration: 5.839s, episode steps: 368, steps per second: 63, episode reward: 172.693, mean reward: 0.469 [-17.207, 100.000], mean action: 0.965 [0.000, 3.000], mean observation: 0.073 [-1.623, 1.000], loss: 0.924877, mean_absolute_error: 36.749488, mean_q: 47.011184, mean_eps: 0.100000
 1662934/2000000: episode: 7886, duration: 4.325s, episode steps: 274, steps per second: 63, episode reward: 194.090, mean reward: 0.708 [-23.538, 100.000], mean action: 2.255 [0.000, 3.000], mean observation: 0.144 [-1.070, 1.295], loss: 1.047245, mean_absolute_error: 35.645264, mean_q: 46.223191, mean_eps: 0.100000
 1663255/2000000: episode: 7887, duration: 4.967s, episode steps: 321, steps per second: 65, episode reward: 138.990, mean reward: 0.433 [-17.990, 100.000], mean action: 1.280 [0.000, 3.000], mean observation: -0.003 [-1.710, 1.000], loss: 1.315087, mean_absolute_error: 35.459829, mean_q: 46.063842, mean_eps: 0.100000
 1663385/2000000: episode: 7888, duration: 1.991s, episode steps: 130, steps per second: 65, episode reward: -50.164, mean reward: -0.386 [-100.000, 15.675], mean action: 1.862 [0.000, 3.000], mean observation: -0.067 [-1.034, 1.000], loss: 0.888686, mean_absolute_error: 35.299164, mean_q: 44.793494, mean_eps: 0.100000
 1663821/2000000: episode: 7889, duration: 7.008s, episode steps: 436, steps per second: 62, episode reward: 240.500, mean reward: 0.552 [-19.606, 100.000], mean action: 0.537 [0.000, 3.000], mean observation: 0.150 [-1.262, 1.000], loss: 1.195286, mean_absolute_error: 35.516538, mean_q: 45.417545, mean_eps: 0.100000
 1664095/2000000: episode: 7890, duration: 4.277s, episode steps: 274, steps per second: 64, episode reward: 239.144, mean reward: 0.873 [-10.747, 100.000], mean action: 0.741 [0.000, 3.000], mean observation: 0.105 [-1.076, 1.231], loss: 1.132799, mean_absolute_error: 36.313406, mean_q: 47.037548, mean_eps: 0.100000
 1664210/2000000: episode: 7891, duration: 1.792s, episode steps: 115, steps per second: 64, episode reward: -116.050, mean reward: -1.009 [-100.000, 10.386], mean action: 1.739 [0.000, 3.000], mean observation: -0.029 [-1.374, 1.000], loss: 1.250063, mean_absolute_error: 35.075733, mean_q: 44.790959, mean_eps: 0.100000
 1664501/2000000: episode: 7892, duration: 4.493s, episode steps: 291, steps per second: 65, episode reward: 231.577, mean reward: 0.796 [-10.890, 100.000], mean action: 0.821 [0.000, 3.000], mean observation: 0.146 [-1.056, 1.007], loss: 0.956515, mean_absolute_error: 34.853404, mean_q: 44.751740, mean_eps: 0.100000
 1664876/2000000: episode: 7893, duration: 5.974s, episode steps: 375, steps per second: 63, episode reward: 186.625, mean reward: 0.498 [-18.738, 100.000], mean action: 1.392 [0.000, 3.000], mean observation: 0.182 [-1.000, 1.000], loss: 1.156344, mean_absolute_error: 35.078817, mean_q: 44.993180, mean_eps: 0.100000
 1665021/2000000: episode: 7894, duration: 2.348s, episode steps: 145, steps per second: 62, episode reward: -20.288, mean reward: -0.140 [-100.000, 35.956], mean action: 1.683 [0.000, 3.000], mean observation: 0.020 [-1.013, 1.000], loss: 1.040791, mean_absolute_error: 35.967193, mean_q: 46.778820, mean_eps: 0.100000
 1665316/2000000: episode: 7895, duration: 4.620s, episode steps: 295, steps per second: 64, episode reward: 182.338, mean reward: 0.618 [-17.486, 100.000], mean action: 1.247 [0.000, 3.000], mean observation: 0.091 [-0.979, 1.000], loss: 1.407216, mean_absolute_error: 35.795792, mean_q: 45.404288, mean_eps: 0.100000
 1665443/2000000: episode: 7896, duration: 2.003s, episode steps: 127, steps per second: 63, episode reward: -6.153, mean reward: -0.048 [-100.000, 10.082], mean action: 1.441 [0.000, 3.000], mean observation: 0.027 [-1.064, 1.000], loss: 1.113077, mean_absolute_error: 35.155430, mean_q: 44.805271, mean_eps: 0.100000
 1665554/2000000: episode: 7897, duration: 1.755s, episode steps: 111, steps per second: 63, episode reward: -27.977, mean reward: -0.252 [-100.000, 11.111], mean action: 1.532 [0.000, 3.000], mean observation: -0.101 [-1.053, 1.304], loss: 2.042664, mean_absolute_error: 37.607922, mean_q: 46.161403, mean_eps: 0.100000
 1665678/2000000: episode: 7898, duration: 1.937s, episode steps: 124, steps per second: 64, episode reward: -21.283, mean reward: -0.172 [-100.000, 11.106], mean action: 1.605 [0.000, 3.000], mean observation: -0.041 [-1.042, 1.509], loss: 1.143232, mean_absolute_error: 34.473519, mean_q: 43.311366, mean_eps: 0.100000
 1666152/2000000: episode: 7899, duration: 7.530s, episode steps: 474, steps per second: 63, episode reward: 194.446, mean reward: 0.410 [-19.819, 100.000], mean action: 2.388 [0.000, 3.000], mean observation: 0.088 [-1.019, 1.153], loss: 1.134062, mean_absolute_error: 34.954529, mean_q: 44.717052, mean_eps: 0.100000
 1666441/2000000: episode: 7900, duration: 4.671s, episode steps: 289, steps per second: 62, episode reward: 177.999, mean reward: 0.616 [-14.975, 100.000], mean action: 1.900 [0.000, 3.000], mean observation: 0.135 [-1.028, 1.000], loss: 1.007225, mean_absolute_error: 33.991556, mean_q: 43.944378, mean_eps: 0.100000
 1666781/2000000: episode: 7901, duration: 5.424s, episode steps: 340, steps per second: 63, episode reward: 189.517, mean reward: 0.557 [-18.748, 100.000], mean action: 1.385 [0.000, 3.000], mean observation: 0.175 [-0.991, 1.000], loss: 1.246983, mean_absolute_error: 34.808104, mean_q: 44.907184, mean_eps: 0.100000
 1667002/2000000: episode: 7902, duration: 3.458s, episode steps: 221, steps per second: 64, episode reward: 182.092, mean reward: 0.824 [-17.565, 100.000], mean action: 2.204 [0.000, 3.000], mean observation: 0.166 [-1.105, 1.000], loss: 1.267052, mean_absolute_error: 34.756560, mean_q: 44.322071, mean_eps: 0.100000
 1667129/2000000: episode: 7903, duration: 2.013s, episode steps: 127, steps per second: 63, episode reward: -35.661, mean reward: -0.281 [-100.000, 11.625], mean action: 1.843 [0.000, 3.000], mean observation: 0.142 [-0.865, 1.000], loss: 1.674183, mean_absolute_error: 36.696248, mean_q: 47.465651, mean_eps: 0.100000
 1667247/2000000: episode: 7904, duration: 1.803s, episode steps: 118, steps per second: 65, episode reward: -45.840, mean reward: -0.388 [-100.000, 52.809], mean action: 1.636 [0.000, 3.000], mean observation: -0.083 [-1.077, 1.357], loss: 1.160117, mean_absolute_error: 35.924753, mean_q: 45.737245, mean_eps: 0.100000
 1668247/2000000: episode: 7905, duration: 16.192s, episode steps: 1000, steps per second: 62, episode reward: -185.253, mean reward: -0.185 [-5.187, 5.897], mean action: 1.534 [0.000, 3.000], mean observation: 0.192 [-0.920, 1.380], loss: 1.179070, mean_absolute_error: 35.692317, mean_q: 46.014529, mean_eps: 0.100000
 1668956/2000000: episode: 7906, duration: 11.291s, episode steps: 709, steps per second: 63, episode reward: 184.856, mean reward: 0.261 [-19.388, 100.000], mean action: 0.925 [0.000, 3.000], mean observation: 0.208 [-1.030, 1.088], loss: 1.023260, mean_absolute_error: 35.528255, mean_q: 45.797671, mean_eps: 0.100000
 1669152/2000000: episode: 7907, duration: 3.652s, episode steps: 196, steps per second: 54, episode reward: 217.890, mean reward: 1.112 [-14.392, 100.000], mean action: 0.954 [0.000, 3.000], mean observation: 0.110 [-1.058, 1.230], loss: 1.129324, mean_absolute_error: 35.649281, mean_q: 46.289566, mean_eps: 0.100000
 1669255/2000000: episode: 7908, duration: 1.660s, episode steps: 103, steps per second: 62, episode reward: -52.493, mean reward: -0.510 [-100.000, 12.536], mean action: 1.456 [0.000, 3.000], mean observation: 0.088 [-1.040, 1.295], loss: 1.008593, mean_absolute_error: 35.572624, mean_q: 45.829209, mean_eps: 0.100000
 1669337/2000000: episode: 7909, duration: 1.303s, episode steps: 82, steps per second: 63, episode reward: -16.363, mean reward: -0.200 [-100.000, 11.337], mean action: 1.659 [0.000, 3.000], mean observation: -0.121 [-1.067, 1.406], loss: 0.912072, mean_absolute_error: 33.715744, mean_q: 42.291992, mean_eps: 0.100000
 1669439/2000000: episode: 7910, duration: 1.565s, episode steps: 102, steps per second: 65, episode reward: -43.029, mean reward: -0.422 [-100.000, 14.408], mean action: 1.912 [0.000, 3.000], mean observation: -0.077 [-1.269, 1.000], loss: 1.059415, mean_absolute_error: 35.246031, mean_q: 45.668680, mean_eps: 0.100000
 1669613/2000000: episode: 7911, duration: 2.721s, episode steps: 174, steps per second: 64, episode reward: 198.405, mean reward: 1.140 [-9.081, 100.000], mean action: 2.362 [0.000, 3.000], mean observation: 0.164 [-1.045, 1.065], loss: 1.190010, mean_absolute_error: 36.232027, mean_q: 45.850564, mean_eps: 0.100000
 1669750/2000000: episode: 7912, duration: 2.093s, episode steps: 137, steps per second: 65, episode reward: -69.977, mean reward: -0.511 [-100.000, 14.664], mean action: 1.679 [0.000, 3.000], mean observation: -0.042 [-1.495, 1.000], loss: 1.339358, mean_absolute_error: 35.198468, mean_q: 45.508337, mean_eps: 0.100000
 1670452/2000000: episode: 7913, duration: 11.524s, episode steps: 702, steps per second: 61, episode reward: 211.971, mean reward: 0.302 [-19.233, 100.000], mean action: 0.793 [0.000, 3.000], mean observation: 0.111 [-1.023, 1.000], loss: 1.198268, mean_absolute_error: 35.008758, mean_q: 45.162027, mean_eps: 0.100000
 1670579/2000000: episode: 7914, duration: 1.976s, episode steps: 127, steps per second: 64, episode reward: 19.525, mean reward: 0.154 [-100.000, 15.299], mean action: 1.598 [0.000, 3.000], mean observation: -0.031 [-1.146, 1.014], loss: 1.614268, mean_absolute_error: 34.426337, mean_q: 44.160124, mean_eps: 0.100000
 1670844/2000000: episode: 7915, duration: 4.192s, episode steps: 265, steps per second: 63, episode reward: 180.326, mean reward: 0.680 [-15.265, 100.000], mean action: 2.426 [0.000, 3.000], mean observation: 0.094 [-1.649, 1.000], loss: 1.238642, mean_absolute_error: 34.825357, mean_q: 44.115838, mean_eps: 0.100000
 1671099/2000000: episode: 7916, duration: 3.975s, episode steps: 255, steps per second: 64, episode reward: 212.354, mean reward: 0.833 [-11.386, 100.000], mean action: 1.345 [0.000, 3.000], mean observation: 0.077 [-1.089, 1.013], loss: 0.944760, mean_absolute_error: 35.439341, mean_q: 45.908771, mean_eps: 0.100000
 1671289/2000000: episode: 7917, duration: 3.250s, episode steps: 190, steps per second: 58, episode reward: -168.261, mean reward: -0.886 [-100.000, 11.103], mean action: 1.974 [0.000, 3.000], mean observation: -0.146 [-1.025, 1.000], loss: 1.181584, mean_absolute_error: 34.254195, mean_q: 44.408265, mean_eps: 0.100000
 1671401/2000000: episode: 7918, duration: 1.730s, episode steps: 112, steps per second: 65, episode reward: -4.955, mean reward: -0.044 [-100.000, 16.221], mean action: 1.536 [0.000, 3.000], mean observation: 0.027 [-1.163, 1.000], loss: 1.517398, mean_absolute_error: 36.653520, mean_q: 47.165799, mean_eps: 0.100000
 1671732/2000000: episode: 7919, duration: 5.231s, episode steps: 331, steps per second: 63, episode reward: 153.195, mean reward: 0.463 [-18.233, 100.000], mean action: 1.275 [0.000, 3.000], mean observation: 0.041 [-1.049, 1.000], loss: 1.260029, mean_absolute_error: 34.153996, mean_q: 43.333077, mean_eps: 0.100000
 1671817/2000000: episode: 7920, duration: 1.370s, episode steps: 85, steps per second: 62, episode reward: -63.078, mean reward: -0.742 [-100.000, 11.680], mean action: 1.953 [0.000, 3.000], mean observation: -0.105 [-1.062, 1.000], loss: 0.999440, mean_absolute_error: 36.471746, mean_q: 46.038848, mean_eps: 0.100000
 1671973/2000000: episode: 7921, duration: 2.382s, episode steps: 156, steps per second: 65, episode reward: -9.079, mean reward: -0.058 [-100.000, 13.278], mean action: 1.500 [0.000, 3.000], mean observation: -0.044 [-0.974, 1.000], loss: 0.799305, mean_absolute_error: 35.430122, mean_q: 45.718263, mean_eps: 0.100000
 1672434/2000000: episode: 7922, duration: 7.241s, episode steps: 461, steps per second: 64, episode reward: 125.373, mean reward: 0.272 [-19.426, 100.000], mean action: 2.334 [0.000, 3.000], mean observation: 0.112 [-1.070, 1.000], loss: 1.131208, mean_absolute_error: 35.048915, mean_q: 45.259743, mean_eps: 0.100000
 1672512/2000000: episode: 7923, duration: 1.233s, episode steps: 78, steps per second: 63, episode reward: -12.599, mean reward: -0.162 [-100.000, 9.075], mean action: 2.077 [0.000, 3.000], mean observation: -0.137 [-1.059, 1.540], loss: 1.647087, mean_absolute_error: 35.597536, mean_q: 46.016399, mean_eps: 0.100000
 1672668/2000000: episode: 7924, duration: 2.462s, episode steps: 156, steps per second: 63, episode reward: -0.644, mean reward: -0.004 [-100.000, 20.349], mean action: 1.744 [0.000, 3.000], mean observation: 0.010 [-1.029, 1.000], loss: 1.306504, mean_absolute_error: 35.425397, mean_q: 45.885328, mean_eps: 0.100000
 1673123/2000000: episode: 7925, duration: 7.232s, episode steps: 455, steps per second: 63, episode reward: -206.843, mean reward: -0.455 [-100.000, 4.938], mean action: 1.844 [0.000, 3.000], mean observation: 0.163 [-0.869, 1.359], loss: 1.154036, mean_absolute_error: 35.531730, mean_q: 45.653635, mean_eps: 0.100000
 1673498/2000000: episode: 7926, duration: 5.882s, episode steps: 375, steps per second: 64, episode reward: 211.617, mean reward: 0.564 [-17.313, 100.000], mean action: 1.179 [0.000, 3.000], mean observation: 0.117 [-1.040, 1.000], loss: 1.030024, mean_absolute_error: 35.086189, mean_q: 45.157483, mean_eps: 0.100000
 1673665/2000000: episode: 7927, duration: 2.592s, episode steps: 167, steps per second: 64, episode reward: -51.818, mean reward: -0.310 [-100.000, 44.536], mean action: 1.826 [0.000, 3.000], mean observation: -0.049 [-1.107, 1.047], loss: 0.890554, mean_absolute_error: 35.048612, mean_q: 44.794542, mean_eps: 0.100000
 1673834/2000000: episode: 7928, duration: 2.763s, episode steps: 169, steps per second: 61, episode reward: 197.391, mean reward: 1.168 [-12.722, 100.000], mean action: 1.373 [0.000, 3.000], mean observation: 0.078 [-1.190, 1.000], loss: 1.036231, mean_absolute_error: 36.220923, mean_q: 45.867849, mean_eps: 0.100000
 1674305/2000000: episode: 7929, duration: 7.587s, episode steps: 471, steps per second: 62, episode reward: 179.765, mean reward: 0.382 [-19.139, 100.000], mean action: 1.176 [0.000, 3.000], mean observation: 0.133 [-1.024, 1.000], loss: 1.221711, mean_absolute_error: 36.322011, mean_q: 46.571021, mean_eps: 0.100000
 1674429/2000000: episode: 7930, duration: 1.917s, episode steps: 124, steps per second: 65, episode reward: -27.735, mean reward: -0.224 [-100.000, 18.142], mean action: 1.185 [0.000, 3.000], mean observation: -0.024 [-1.057, 1.246], loss: 1.050929, mean_absolute_error: 35.198990, mean_q: 45.435676, mean_eps: 0.100000
 1674529/2000000: episode: 7931, duration: 1.535s, episode steps: 100, steps per second: 65, episode reward: -22.552, mean reward: -0.226 [-100.000, 17.113], mean action: 1.780 [0.000, 3.000], mean observation: -0.026 [-1.065, 1.000], loss: 1.284697, mean_absolute_error: 37.487867, mean_q: 47.709657, mean_eps: 0.100000
 1674903/2000000: episode: 7932, duration: 5.879s, episode steps: 374, steps per second: 64, episode reward: 205.401, mean reward: 0.549 [-14.415, 100.000], mean action: 1.602 [0.000, 3.000], mean observation: 0.126 [-0.825, 1.017], loss: 1.219304, mean_absolute_error: 35.771446, mean_q: 45.847090, mean_eps: 0.100000
 1675232/2000000: episode: 7933, duration: 5.254s, episode steps: 329, steps per second: 63, episode reward: -253.976, mean reward: -0.772 [-100.000, 5.170], mean action: 1.751 [0.000, 3.000], mean observation: 0.149 [-0.855, 1.003], loss: 1.058341, mean_absolute_error: 36.129622, mean_q: 46.932098, mean_eps: 0.100000
 1675516/2000000: episode: 7934, duration: 4.457s, episode steps: 284, steps per second: 64, episode reward: 227.284, mean reward: 0.800 [-17.966, 100.000], mean action: 0.634 [0.000, 3.000], mean observation: 0.158 [-1.035, 1.126], loss: 1.376402, mean_absolute_error: 33.921688, mean_q: 43.198902, mean_eps: 0.100000
 1675613/2000000: episode: 7935, duration: 1.537s, episode steps: 97, steps per second: 63, episode reward: -89.461, mean reward: -0.922 [-100.000, 10.798], mean action: 1.825 [0.000, 3.000], mean observation: -0.045 [-1.014, 2.903], loss: 1.080543, mean_absolute_error: 36.145660, mean_q: 46.303766, mean_eps: 0.100000
 1675861/2000000: episode: 7936, duration: 3.815s, episode steps: 248, steps per second: 65, episode reward: 222.472, mean reward: 0.897 [-17.709, 100.000], mean action: 0.722 [0.000, 3.000], mean observation: 0.157 [-1.008, 1.000], loss: 1.137600, mean_absolute_error: 34.833596, mean_q: 44.908130, mean_eps: 0.100000
 1675969/2000000: episode: 7937, duration: 1.690s, episode steps: 108, steps per second: 64, episode reward: -44.880, mean reward: -0.416 [-100.000, 17.767], mean action: 1.481 [0.000, 3.000], mean observation: -0.008 [-0.979, 1.147], loss: 1.102773, mean_absolute_error: 35.355760, mean_q: 45.478474, mean_eps: 0.100000
 1676092/2000000: episode: 7938, duration: 1.906s, episode steps: 123, steps per second: 65, episode reward: -34.984, mean reward: -0.284 [-100.000, 16.136], mean action: 1.244 [0.000, 3.000], mean observation: -0.037 [-1.004, 1.368], loss: 1.308423, mean_absolute_error: 35.590144, mean_q: 46.356103, mean_eps: 0.100000
 1676730/2000000: episode: 7939, duration: 10.025s, episode steps: 638, steps per second: 64, episode reward: 203.155, mean reward: 0.318 [-9.136, 100.000], mean action: 0.937 [0.000, 3.000], mean observation: 0.204 [-0.881, 1.000], loss: 1.082565, mean_absolute_error: 35.231839, mean_q: 45.406979, mean_eps: 0.100000
 1677022/2000000: episode: 7940, duration: 4.611s, episode steps: 292, steps per second: 63, episode reward: 197.355, mean reward: 0.676 [-5.896, 100.000], mean action: 1.322 [0.000, 3.000], mean observation: 0.079 [-1.061, 1.000], loss: 1.240859, mean_absolute_error: 35.550522, mean_q: 46.143469, mean_eps: 0.100000
 1677740/2000000: episode: 7941, duration: 12.398s, episode steps: 718, steps per second: 58, episode reward: 196.108, mean reward: 0.273 [-20.369, 100.000], mean action: 1.776 [0.000, 3.000], mean observation: 0.189 [-1.075, 1.052], loss: 1.210737, mean_absolute_error: 35.820660, mean_q: 45.725984, mean_eps: 0.100000
 1677833/2000000: episode: 7942, duration: 1.498s, episode steps: 93, steps per second: 62, episode reward: -16.088, mean reward: -0.173 [-100.000, 23.485], mean action: 1.505 [0.000, 3.000], mean observation: -0.112 [-1.071, 1.887], loss: 1.283103, mean_absolute_error: 37.084357, mean_q: 47.638126, mean_eps: 0.100000
 1678235/2000000: episode: 7943, duration: 6.293s, episode steps: 402, steps per second: 64, episode reward: 193.600, mean reward: 0.482 [-19.034, 100.000], mean action: 0.704 [0.000, 3.000], mean observation: 0.127 [-1.045, 1.000], loss: 0.878401, mean_absolute_error: 34.407971, mean_q: 44.750700, mean_eps: 0.100000
 1678523/2000000: episode: 7944, duration: 4.417s, episode steps: 288, steps per second: 65, episode reward: 198.119, mean reward: 0.688 [-17.707, 100.000], mean action: 0.760 [0.000, 3.000], mean observation: 0.122 [-1.070, 1.000], loss: 1.295633, mean_absolute_error: 36.682096, mean_q: 47.671432, mean_eps: 0.100000
 1678710/2000000: episode: 7945, duration: 2.931s, episode steps: 187, steps per second: 64, episode reward: -75.897, mean reward: -0.406 [-100.000, 10.174], mean action: 1.668 [0.000, 3.000], mean observation: 0.027 [-1.036, 1.000], loss: 1.158872, mean_absolute_error: 36.693730, mean_q: 47.049967, mean_eps: 0.100000
 1678914/2000000: episode: 7946, duration: 3.356s, episode steps: 204, steps per second: 61, episode reward: 243.520, mean reward: 1.194 [-9.558, 100.000], mean action: 1.490 [0.000, 3.000], mean observation: 0.115 [-1.014, 1.000], loss: 1.109460, mean_absolute_error: 35.767613, mean_q: 45.368782, mean_eps: 0.100000
 1679914/2000000: episode: 7947, duration: 16.058s, episode steps: 1000, steps per second: 62, episode reward: 58.546, mean reward: 0.059 [-18.226, 22.441], mean action: 0.673 [0.000, 3.000], mean observation: 0.249 [-1.065, 1.000], loss: 1.285386, mean_absolute_error: 35.647716, mean_q: 45.883640, mean_eps: 0.100000
 1680254/2000000: episode: 7948, duration: 5.480s, episode steps: 340, steps per second: 62, episode reward: -247.766, mean reward: -0.729 [-100.000, 5.603], mean action: 1.818 [0.000, 3.000], mean observation: 0.199 [-0.999, 1.397], loss: 1.033509, mean_absolute_error: 34.725327, mean_q: 44.383920, mean_eps: 0.100000
 1680394/2000000: episode: 7949, duration: 2.137s, episode steps: 140, steps per second: 66, episode reward: -55.467, mean reward: -0.396 [-100.000, 16.949], mean action: 1.864 [0.000, 3.000], mean observation: 0.117 [-1.005, 1.197], loss: 1.094983, mean_absolute_error: 36.727957, mean_q: 46.046212, mean_eps: 0.100000
 1680502/2000000: episode: 7950, duration: 1.666s, episode steps: 108, steps per second: 65, episode reward: -80.045, mean reward: -0.741 [-100.000, 16.468], mean action: 1.546 [0.000, 3.000], mean observation: -0.111 [-0.945, 1.403], loss: 0.961812, mean_absolute_error: 35.424783, mean_q: 46.338118, mean_eps: 0.100000
 1680879/2000000: episode: 7951, duration: 5.945s, episode steps: 377, steps per second: 63, episode reward: 195.843, mean reward: 0.519 [-4.534, 100.000], mean action: 1.013 [0.000, 3.000], mean observation: 0.119 [-1.136, 1.000], loss: 0.968521, mean_absolute_error: 34.896547, mean_q: 44.601215, mean_eps: 0.100000
 1680995/2000000: episode: 7952, duration: 1.870s, episode steps: 116, steps per second: 62, episode reward: -19.383, mean reward: -0.167 [-100.000, 14.149], mean action: 1.310 [0.000, 3.000], mean observation: -0.076 [-1.056, 1.142], loss: 1.400543, mean_absolute_error: 36.416542, mean_q: 46.709161, mean_eps: 0.100000
 1681127/2000000: episode: 7953, duration: 2.055s, episode steps: 132, steps per second: 64, episode reward: -17.855, mean reward: -0.135 [-100.000, 18.768], mean action: 1.197 [0.000, 3.000], mean observation: -0.053 [-0.996, 1.000], loss: 0.829335, mean_absolute_error: 34.430873, mean_q: 44.137601, mean_eps: 0.100000
 1681300/2000000: episode: 7954, duration: 2.723s, episode steps: 173, steps per second: 64, episode reward: -187.699, mean reward: -1.085 [-100.000, 5.509], mean action: 1.821 [0.000, 3.000], mean observation: 0.151 [-1.024, 1.001], loss: 1.072801, mean_absolute_error: 36.354496, mean_q: 46.086169, mean_eps: 0.100000
 1681538/2000000: episode: 7955, duration: 3.761s, episode steps: 238, steps per second: 63, episode reward: 204.513, mean reward: 0.859 [-11.595, 100.000], mean action: 2.261 [0.000, 3.000], mean observation: 0.069 [-1.070, 1.278], loss: 1.046078, mean_absolute_error: 35.026491, mean_q: 44.721839, mean_eps: 0.100000
 1681926/2000000: episode: 7956, duration: 6.118s, episode steps: 388, steps per second: 63, episode reward: 235.910, mean reward: 0.608 [-18.088, 100.000], mean action: 0.637 [0.000, 3.000], mean observation: 0.143 [-1.337, 1.000], loss: 1.159926, mean_absolute_error: 35.574967, mean_q: 45.651248, mean_eps: 0.100000
 1682039/2000000: episode: 7957, duration: 1.754s, episode steps: 113, steps per second: 64, episode reward: -60.531, mean reward: -0.536 [-100.000, 12.777], mean action: 1.442 [0.000, 3.000], mean observation: -0.041 [-1.012, 1.247], loss: 1.718362, mean_absolute_error: 35.779835, mean_q: 45.475361, mean_eps: 0.100000
 1682132/2000000: episode: 7958, duration: 1.455s, episode steps: 93, steps per second: 64, episode reward: -21.291, mean reward: -0.229 [-100.000, 16.396], mean action: 1.344 [0.000, 3.000], mean observation: -0.056 [-1.049, 1.000], loss: 1.004545, mean_absolute_error: 35.619596, mean_q: 46.174115, mean_eps: 0.100000
 1682248/2000000: episode: 7959, duration: 1.857s, episode steps: 116, steps per second: 62, episode reward: -15.281, mean reward: -0.132 [-100.000, 22.201], mean action: 1.940 [0.000, 3.000], mean observation: 0.133 [-1.510, 1.000], loss: 1.078054, mean_absolute_error: 36.479372, mean_q: 46.625559, mean_eps: 0.100000
 1682410/2000000: episode: 7960, duration: 2.747s, episode steps: 162, steps per second: 59, episode reward: -207.665, mean reward: -1.282 [-100.000, 9.002], mean action: 1.506 [0.000, 3.000], mean observation: 0.151 [-0.996, 1.155], loss: 1.087508, mean_absolute_error: 36.610588, mean_q: 46.589539, mean_eps: 0.100000
 1682494/2000000: episode: 7961, duration: 1.315s, episode steps: 84, steps per second: 64, episode reward: -41.671, mean reward: -0.496 [-100.000, 6.954], mean action: 1.536 [0.000, 3.000], mean observation: -0.137 [-1.147, 1.000], loss: 0.961677, mean_absolute_error: 34.674215, mean_q: 43.303806, mean_eps: 0.100000
 1682626/2000000: episode: 7962, duration: 2.061s, episode steps: 132, steps per second: 64, episode reward: -155.053, mean reward: -1.175 [-100.000, 22.173], mean action: 1.788 [0.000, 3.000], mean observation: -0.103 [-1.391, 1.000], loss: 1.280809, mean_absolute_error: 35.341092, mean_q: 45.115839, mean_eps: 0.100000
 1682753/2000000: episode: 7963, duration: 1.982s, episode steps: 127, steps per second: 64, episode reward: -275.476, mean reward: -2.169 [-100.000, 29.857], mean action: 1.929 [0.000, 3.000], mean observation: 0.019 [-0.976, 1.815], loss: 1.262894, mean_absolute_error: 35.270668, mean_q: 44.810286, mean_eps: 0.100000
 1682861/2000000: episode: 7964, duration: 1.675s, episode steps: 108, steps per second: 64, episode reward: -37.783, mean reward: -0.350 [-100.000, 16.495], mean action: 1.648 [0.000, 3.000], mean observation: -0.071 [-0.997, 1.601], loss: 0.908436, mean_absolute_error: 35.187682, mean_q: 46.280094, mean_eps: 0.100000
 1683126/2000000: episode: 7965, duration: 4.146s, episode steps: 265, steps per second: 64, episode reward: 248.588, mean reward: 0.938 [-17.428, 100.000], mean action: 1.449 [0.000, 3.000], mean observation: 0.134 [-0.962, 1.000], loss: 1.201823, mean_absolute_error: 35.231946, mean_q: 44.853616, mean_eps: 0.100000
 1683223/2000000: episode: 7966, duration: 1.506s, episode steps: 97, steps per second: 64, episode reward: -63.357, mean reward: -0.653 [-100.000, 12.265], mean action: 1.216 [0.000, 3.000], mean observation: -0.082 [-1.188, 1.000], loss: 1.140114, mean_absolute_error: 34.522264, mean_q: 43.564734, mean_eps: 0.100000
 1683328/2000000: episode: 7967, duration: 1.636s, episode steps: 105, steps per second: 64, episode reward: -48.198, mean reward: -0.459 [-100.000, 13.144], mean action: 1.562 [0.000, 3.000], mean observation: -0.110 [-1.399, 1.000], loss: 1.714409, mean_absolute_error: 35.497785, mean_q: 45.325913, mean_eps: 0.100000
 1683432/2000000: episode: 7968, duration: 1.865s, episode steps: 104, steps per second: 56, episode reward: -2.219, mean reward: -0.021 [-100.000, 17.439], mean action: 1.423 [0.000, 3.000], mean observation: -0.037 [-1.024, 1.000], loss: 1.154523, mean_absolute_error: 36.215204, mean_q: 46.214296, mean_eps: 0.100000
 1683742/2000000: episode: 7969, duration: 4.944s, episode steps: 310, steps per second: 63, episode reward: 195.732, mean reward: 0.631 [-18.852, 100.000], mean action: 1.284 [0.000, 3.000], mean observation: 0.067 [-1.103, 1.010], loss: 1.036005, mean_absolute_error: 35.750111, mean_q: 45.912361, mean_eps: 0.100000
 1683985/2000000: episode: 7970, duration: 3.791s, episode steps: 243, steps per second: 64, episode reward: -96.879, mean reward: -0.399 [-100.000, 14.352], mean action: 1.621 [0.000, 3.000], mean observation: -0.031 [-1.008, 1.084], loss: 1.118690, mean_absolute_error: 36.990631, mean_q: 46.585243, mean_eps: 0.100000
 1684064/2000000: episode: 7971, duration: 1.247s, episode steps: 79, steps per second: 63, episode reward: -66.626, mean reward: -0.843 [-100.000, 12.747], mean action: 1.392 [0.000, 3.000], mean observation: -0.137 [-1.465, 1.000], loss: 0.864504, mean_absolute_error: 33.491159, mean_q: 42.899067, mean_eps: 0.100000
 1684399/2000000: episode: 7972, duration: 5.294s, episode steps: 335, steps per second: 63, episode reward: 106.881, mean reward: 0.319 [-10.585, 100.000], mean action: 2.018 [0.000, 3.000], mean observation: -0.000 [-1.013, 1.000], loss: 1.369150, mean_absolute_error: 35.482074, mean_q: 45.200934, mean_eps: 0.100000
 1684992/2000000: episode: 7973, duration: 9.782s, episode steps: 593, steps per second: 61, episode reward: 176.519, mean reward: 0.298 [-17.553, 100.000], mean action: 1.067 [0.000, 3.000], mean observation: 0.143 [-1.076, 1.000], loss: 1.107239, mean_absolute_error: 35.955932, mean_q: 46.295827, mean_eps: 0.100000
 1685094/2000000: episode: 7974, duration: 1.619s, episode steps: 102, steps per second: 63, episode reward: -50.536, mean reward: -0.495 [-100.000, 18.683], mean action: 1.951 [0.000, 3.000], mean observation: -0.013 [-1.017, 1.566], loss: 1.375695, mean_absolute_error: 36.711537, mean_q: 45.962216, mean_eps: 0.100000
 1685658/2000000: episode: 7975, duration: 9.072s, episode steps: 564, steps per second: 62, episode reward: -315.573, mean reward: -0.560 [-100.000, 5.075], mean action: 1.411 [0.000, 3.000], mean observation: 0.205 [-1.017, 1.904], loss: 1.097558, mean_absolute_error: 36.180436, mean_q: 46.747221, mean_eps: 0.100000
 1685824/2000000: episode: 7976, duration: 2.636s, episode steps: 166, steps per second: 63, episode reward: -259.201, mean reward: -1.561 [-100.000, 13.587], mean action: 1.699 [0.000, 3.000], mean observation: 0.010 [-1.743, 1.000], loss: 1.492102, mean_absolute_error: 35.328418, mean_q: 46.444322, mean_eps: 0.100000
 1685993/2000000: episode: 7977, duration: 2.665s, episode steps: 169, steps per second: 63, episode reward: -67.686, mean reward: -0.401 [-100.000, 14.948], mean action: 1.976 [0.000, 3.000], mean observation: -0.120 [-1.066, 1.367], loss: 0.921443, mean_absolute_error: 35.419010, mean_q: 45.769813, mean_eps: 0.100000
 1686262/2000000: episode: 7978, duration: 4.371s, episode steps: 269, steps per second: 62, episode reward: -178.892, mean reward: -0.665 [-100.000, 5.270], mean action: 1.866 [0.000, 3.000], mean observation: 0.154 [-0.853, 1.006], loss: 1.325659, mean_absolute_error: 35.646945, mean_q: 44.885638, mean_eps: 0.100000
 1686370/2000000: episode: 7979, duration: 1.750s, episode steps: 108, steps per second: 62, episode reward: -45.688, mean reward: -0.423 [-100.000, 17.419], mean action: 1.463 [0.000, 3.000], mean observation: 0.029 [-1.021, 1.097], loss: 1.347708, mean_absolute_error: 35.477681, mean_q: 45.629520, mean_eps: 0.100000
 1686563/2000000: episode: 7980, duration: 3.007s, episode steps: 193, steps per second: 64, episode reward: -47.292, mean reward: -0.245 [-100.000, 15.162], mean action: 1.762 [0.000, 3.000], mean observation: -0.060 [-1.021, 1.074], loss: 0.905085, mean_absolute_error: 34.424071, mean_q: 44.700646, mean_eps: 0.100000
 1686841/2000000: episode: 7981, duration: 4.370s, episode steps: 278, steps per second: 64, episode reward: 184.219, mean reward: 0.663 [-17.584, 100.000], mean action: 0.860 [0.000, 3.000], mean observation: 0.191 [-1.017, 1.051], loss: 1.101834, mean_absolute_error: 35.989551, mean_q: 45.991039, mean_eps: 0.100000
 1686953/2000000: episode: 7982, duration: 1.699s, episode steps: 112, steps per second: 66, episode reward: -27.533, mean reward: -0.246 [-100.000, 19.824], mean action: 1.295 [0.000, 3.000], mean observation: -0.023 [-1.054, 1.631], loss: 1.198103, mean_absolute_error: 36.297265, mean_q: 47.014474, mean_eps: 0.100000
 1687082/2000000: episode: 7983, duration: 1.976s, episode steps: 129, steps per second: 65, episode reward: -56.883, mean reward: -0.441 [-100.000, 18.778], mean action: 1.605 [0.000, 3.000], mean observation: 0.066 [-1.582, 1.000], loss: 0.896719, mean_absolute_error: 36.086879, mean_q: 46.225676, mean_eps: 0.100000
 1687308/2000000: episode: 7984, duration: 3.567s, episode steps: 226, steps per second: 63, episode reward: -141.754, mean reward: -0.627 [-100.000, 5.245], mean action: 1.863 [0.000, 3.000], mean observation: 0.219 [-0.902, 1.027], loss: 1.065881, mean_absolute_error: 35.197227, mean_q: 45.159640, mean_eps: 0.100000
 1687509/2000000: episode: 7985, duration: 3.145s, episode steps: 201, steps per second: 64, episode reward: 188.670, mean reward: 0.939 [-2.961, 100.000], mean action: 0.881 [0.000, 3.000], mean observation: 0.111 [-1.021, 1.000], loss: 1.617113, mean_absolute_error: 35.844358, mean_q: 45.619861, mean_eps: 0.100000
 1687687/2000000: episode: 7986, duration: 2.707s, episode steps: 178, steps per second: 66, episode reward: -106.476, mean reward: -0.598 [-100.000, 16.492], mean action: 1.612 [0.000, 3.000], mean observation: 0.011 [-1.388, 1.000], loss: 1.250227, mean_absolute_error: 36.510758, mean_q: 46.920675, mean_eps: 0.100000
 1687808/2000000: episode: 7987, duration: 1.898s, episode steps: 121, steps per second: 64, episode reward: -5.481, mean reward: -0.045 [-100.000, 17.113], mean action: 1.860 [0.000, 3.000], mean observation: -0.005 [-1.059, 1.000], loss: 1.044354, mean_absolute_error: 37.325575, mean_q: 47.214949, mean_eps: 0.100000
 1688062/2000000: episode: 7988, duration: 4.055s, episode steps: 254, steps per second: 63, episode reward: 180.844, mean reward: 0.712 [-10.763, 100.000], mean action: 1.335 [0.000, 3.000], mean observation: 0.044 [-1.019, 1.113], loss: 1.189475, mean_absolute_error: 35.762882, mean_q: 45.788028, mean_eps: 0.100000
 1688187/2000000: episode: 7989, duration: 1.916s, episode steps: 125, steps per second: 65, episode reward: -65.809, mean reward: -0.526 [-100.000, 12.446], mean action: 1.472 [0.000, 3.000], mean observation: 0.060 [-0.979, 1.000], loss: 1.650478, mean_absolute_error: 36.180363, mean_q: 46.364213, mean_eps: 0.100000
 1688401/2000000: episode: 7990, duration: 3.310s, episode steps: 214, steps per second: 65, episode reward: 216.675, mean reward: 1.012 [-2.839, 100.000], mean action: 1.070 [0.000, 3.000], mean observation: 0.085 [-1.016, 1.000], loss: 0.929953, mean_absolute_error: 34.745964, mean_q: 45.385643, mean_eps: 0.100000
 1688804/2000000: episode: 7991, duration: 6.275s, episode steps: 403, steps per second: 64, episode reward: -183.206, mean reward: -0.455 [-100.000, 5.212], mean action: 1.476 [0.000, 3.000], mean observation: 0.191 [-0.990, 1.106], loss: 0.983084, mean_absolute_error: 36.181670, mean_q: 46.050268, mean_eps: 0.100000
 1689452/2000000: episode: 7992, duration: 10.455s, episode steps: 648, steps per second: 62, episode reward: 197.324, mean reward: 0.305 [-19.639, 100.000], mean action: 1.350 [0.000, 3.000], mean observation: 0.271 [-0.932, 1.000], loss: 1.284180, mean_absolute_error: 35.002622, mean_q: 44.906496, mean_eps: 0.100000
 1689537/2000000: episode: 7993, duration: 1.361s, episode steps: 85, steps per second: 62, episode reward: 2.308, mean reward: 0.027 [-100.000, 20.448], mean action: 1.753 [0.000, 3.000], mean observation: -0.014 [-1.089, 1.065], loss: 1.470615, mean_absolute_error: 36.577975, mean_q: 46.783332, mean_eps: 0.100000
 1689823/2000000: episode: 7994, duration: 4.392s, episode steps: 286, steps per second: 65, episode reward: -263.836, mean reward: -0.923 [-100.000, 4.221], mean action: 1.797 [0.000, 3.000], mean observation: 0.021 [-1.090, 1.836], loss: 1.141306, mean_absolute_error: 35.869585, mean_q: 46.125374, mean_eps: 0.100000
 1689933/2000000: episode: 7995, duration: 1.726s, episode steps: 110, steps per second: 64, episode reward: -26.920, mean reward: -0.245 [-100.000, 17.768], mean action: 1.545 [0.000, 3.000], mean observation: -0.064 [-0.953, 1.288], loss: 1.145974, mean_absolute_error: 38.579924, mean_q: 48.804400, mean_eps: 0.100000
 1690441/2000000: episode: 7996, duration: 8.023s, episode steps: 508, steps per second: 63, episode reward: 219.269, mean reward: 0.432 [-19.567, 100.000], mean action: 1.327 [0.000, 3.000], mean observation: 0.158 [-0.749, 1.013], loss: 1.071583, mean_absolute_error: 35.905472, mean_q: 45.765746, mean_eps: 0.100000
 1690982/2000000: episode: 7997, duration: 8.718s, episode steps: 541, steps per second: 62, episode reward: 159.942, mean reward: 0.296 [-21.387, 100.000], mean action: 1.183 [0.000, 3.000], mean observation: 0.127 [-0.984, 1.000], loss: 1.230565, mean_absolute_error: 35.832306, mean_q: 45.272955, mean_eps: 0.100000
 1691235/2000000: episode: 7998, duration: 3.971s, episode steps: 253, steps per second: 64, episode reward: 173.492, mean reward: 0.686 [-17.199, 100.000], mean action: 2.040 [0.000, 3.000], mean observation: 0.121 [-0.978, 1.000], loss: 1.027767, mean_absolute_error: 35.295009, mean_q: 45.048811, mean_eps: 0.100000
 1692042/2000000: episode: 7999, duration: 13.273s, episode steps: 807, steps per second: 61, episode reward: 171.055, mean reward: 0.212 [-17.468, 100.000], mean action: 1.492 [0.000, 3.000], mean observation: 0.109 [-0.905, 1.000], loss: 1.186445, mean_absolute_error: 36.778105, mean_q: 47.400263, mean_eps: 0.100000
 1692446/2000000: episode: 8000, duration: 6.469s, episode steps: 404, steps per second: 62, episode reward: -215.616, mean reward: -0.534 [-100.000, 4.829], mean action: 1.851 [0.000, 3.000], mean observation: 0.151 [-0.926, 1.001], loss: 1.155717, mean_absolute_error: 36.353962, mean_q: 45.862964, mean_eps: 0.100000
 1692554/2000000: episode: 8001, duration: 1.679s, episode steps: 108, steps per second: 64, episode reward: -77.444, mean reward: -0.717 [-100.000, 17.348], mean action: 1.741 [0.000, 3.000], mean observation: 0.038 [-1.861, 1.000], loss: 1.274185, mean_absolute_error: 37.778605, mean_q: 48.633347, mean_eps: 0.100000
 1692745/2000000: episode: 8002, duration: 2.921s, episode steps: 191, steps per second: 65, episode reward: 234.028, mean reward: 1.225 [-12.358, 100.000], mean action: 1.079 [0.000, 3.000], mean observation: 0.121 [-1.223, 1.000], loss: 1.183184, mean_absolute_error: 36.552699, mean_q: 46.481020, mean_eps: 0.100000
 1692900/2000000: episode: 8003, duration: 2.426s, episode steps: 155, steps per second: 64, episode reward: -36.857, mean reward: -0.238 [-100.000, 12.282], mean action: 1.613 [0.000, 3.000], mean observation: -0.028 [-1.039, 1.000], loss: 1.324855, mean_absolute_error: 35.815729, mean_q: 46.346100, mean_eps: 0.100000
 1693131/2000000: episode: 8004, duration: 3.582s, episode steps: 231, steps per second: 64, episode reward: 197.108, mean reward: 0.853 [-18.723, 100.000], mean action: 1.753 [0.000, 3.000], mean observation: -0.002 [-1.028, 1.000], loss: 1.275962, mean_absolute_error: 36.334765, mean_q: 47.215610, mean_eps: 0.100000
 1693241/2000000: episode: 8005, duration: 1.728s, episode steps: 110, steps per second: 64, episode reward: -41.690, mean reward: -0.379 [-100.000, 19.603], mean action: 1.873 [0.000, 3.000], mean observation: -0.081 [-0.921, 1.569], loss: 0.976954, mean_absolute_error: 36.810946, mean_q: 46.203665, mean_eps: 0.100000
 1693357/2000000: episode: 8006, duration: 1.821s, episode steps: 116, steps per second: 64, episode reward: -47.091, mean reward: -0.406 [-100.000, 17.293], mean action: 2.017 [0.000, 3.000], mean observation: 0.071 [-1.433, 1.000], loss: 1.253299, mean_absolute_error: 35.232497, mean_q: 44.670805, mean_eps: 0.100000
 1694357/2000000: episode: 8007, duration: 16.352s, episode steps: 1000, steps per second: 61, episode reward: 50.429, mean reward: 0.050 [-18.093, 22.815], mean action: 0.856 [0.000, 3.000], mean observation: 0.244 [-1.049, 1.000], loss: 1.266017, mean_absolute_error: 36.162114, mean_q: 45.930617, mean_eps: 0.100000
 1695357/2000000: episode: 8008, duration: 16.349s, episode steps: 1000, steps per second: 61, episode reward: 74.325, mean reward: 0.074 [-18.222, 22.241], mean action: 0.855 [0.000, 3.000], mean observation: 0.217 [-1.007, 1.000], loss: 1.191038, mean_absolute_error: 36.158930, mean_q: 46.125073, mean_eps: 0.100000
 1695610/2000000: episode: 8009, duration: 3.923s, episode steps: 253, steps per second: 64, episode reward: -31.413, mean reward: -0.124 [-100.000, 8.074], mean action: 1.806 [0.000, 3.000], mean observation: -0.028 [-0.863, 1.375], loss: 1.197074, mean_absolute_error: 35.668506, mean_q: 46.112321, mean_eps: 0.100000
 1695929/2000000: episode: 8010, duration: 5.012s, episode steps: 319, steps per second: 64, episode reward: 208.551, mean reward: 0.654 [-17.586, 100.000], mean action: 0.875 [0.000, 3.000], mean observation: 0.131 [-0.995, 1.000], loss: 1.067932, mean_absolute_error: 36.094601, mean_q: 45.884157, mean_eps: 0.100000
 1696031/2000000: episode: 8011, duration: 1.566s, episode steps: 102, steps per second: 65, episode reward: -71.533, mean reward: -0.701 [-100.000, 16.800], mean action: 1.892 [0.000, 3.000], mean observation: 0.022 [-0.977, 1.459], loss: 1.386138, mean_absolute_error: 36.298501, mean_q: 45.759847, mean_eps: 0.100000
 1696131/2000000: episode: 8012, duration: 1.570s, episode steps: 100, steps per second: 64, episode reward: -65.118, mean reward: -0.651 [-100.000, 9.670], mean action: 2.000 [0.000, 3.000], mean observation: -0.114 [-1.000, 1.118], loss: 1.081778, mean_absolute_error: 35.732339, mean_q: 46.317974, mean_eps: 0.100000
 1696261/2000000: episode: 8013, duration: 2.046s, episode steps: 130, steps per second: 64, episode reward: -54.403, mean reward: -0.418 [-100.000, 8.595], mean action: 1.246 [0.000, 3.000], mean observation: 0.048 [-1.120, 1.327], loss: 1.003883, mean_absolute_error: 35.600293, mean_q: 46.108124, mean_eps: 0.100000
 1696476/2000000: episode: 8014, duration: 3.330s, episode steps: 215, steps per second: 65, episode reward: 234.909, mean reward: 1.093 [-10.337, 100.000], mean action: 1.028 [0.000, 3.000], mean observation: 0.026 [-1.000, 1.000], loss: 0.876855, mean_absolute_error: 35.593493, mean_q: 45.591028, mean_eps: 0.100000
 1696605/2000000: episode: 8015, duration: 2.030s, episode steps: 129, steps per second: 64, episode reward: -108.878, mean reward: -0.844 [-100.000, 18.651], mean action: 1.519 [0.000, 3.000], mean observation: -0.031 [-1.984, 1.000], loss: 1.135675, mean_absolute_error: 34.671008, mean_q: 44.424244, mean_eps: 0.100000
 1696755/2000000: episode: 8016, duration: 2.314s, episode steps: 150, steps per second: 65, episode reward: -275.736, mean reward: -1.838 [-100.000, 13.190], mean action: 1.880 [0.000, 3.000], mean observation: 0.029 [-1.002, 1.621], loss: 0.966172, mean_absolute_error: 36.911196, mean_q: 46.753039, mean_eps: 0.100000
 1697001/2000000: episode: 8017, duration: 3.858s, episode steps: 246, steps per second: 64, episode reward: -132.888, mean reward: -0.540 [-100.000, 15.567], mean action: 2.280 [0.000, 3.000], mean observation: -0.012 [-1.285, 1.000], loss: 1.263480, mean_absolute_error: 37.102690, mean_q: 46.853729, mean_eps: 0.100000
 1697139/2000000: episode: 8018, duration: 2.094s, episode steps: 138, steps per second: 66, episode reward: -393.776, mean reward: -2.853 [-100.000, 14.415], mean action: 1.529 [0.000, 3.000], mean observation: 0.110 [-1.874, 3.256], loss: 1.305631, mean_absolute_error: 35.508779, mean_q: 46.039829, mean_eps: 0.100000
 1697328/2000000: episode: 8019, duration: 2.937s, episode steps: 189, steps per second: 64, episode reward: 221.527, mean reward: 1.172 [-10.273, 100.000], mean action: 1.312 [0.000, 3.000], mean observation: 0.044 [-0.984, 1.000], loss: 1.193024, mean_absolute_error: 37.588985, mean_q: 47.518007, mean_eps: 0.100000
 1697603/2000000: episode: 8020, duration: 4.266s, episode steps: 275, steps per second: 64, episode reward: 235.071, mean reward: 0.855 [-17.528, 100.000], mean action: 0.720 [0.000, 3.000], mean observation: 0.096 [-1.057, 1.014], loss: 1.053806, mean_absolute_error: 34.926962, mean_q: 44.730214, mean_eps: 0.100000
 1697801/2000000: episode: 8021, duration: 3.075s, episode steps: 198, steps per second: 64, episode reward: -8.478, mean reward: -0.043 [-100.000, 13.695], mean action: 1.813 [0.000, 3.000], mean observation: 0.064 [-1.058, 1.277], loss: 0.980100, mean_absolute_error: 36.399980, mean_q: 47.466091, mean_eps: 0.100000
 1698157/2000000: episode: 8022, duration: 5.602s, episode steps: 356, steps per second: 64, episode reward: 198.610, mean reward: 0.558 [-17.981, 100.000], mean action: 0.792 [0.000, 3.000], mean observation: 0.118 [-0.989, 1.000], loss: 1.080579, mean_absolute_error: 36.459957, mean_q: 46.629317, mean_eps: 0.100000
 1698343/2000000: episode: 8023, duration: 2.901s, episode steps: 186, steps per second: 64, episode reward: 172.688, mean reward: 0.928 [-17.375, 100.000], mean action: 1.489 [0.000, 3.000], mean observation: 0.037 [-1.339, 1.000], loss: 1.103449, mean_absolute_error: 35.687969, mean_q: 44.464732, mean_eps: 0.100000
 1699343/2000000: episode: 8024, duration: 17.775s, episode steps: 1000, steps per second: 56, episode reward: -161.807, mean reward: -0.162 [-5.091, 5.452], mean action: 1.529 [0.000, 3.000], mean observation: 0.128 [-1.014, 0.942], loss: 1.268451, mean_absolute_error: 36.045047, mean_q: 46.241341, mean_eps: 0.100000
 1699497/2000000: episode: 8025, duration: 2.432s, episode steps: 154, steps per second: 63, episode reward: -57.886, mean reward: -0.376 [-100.000, 24.136], mean action: 1.649 [0.000, 3.000], mean observation: -0.078 [-0.992, 1.000], loss: 1.153535, mean_absolute_error: 35.998669, mean_q: 45.509827, mean_eps: 0.100000
 1699816/2000000: episode: 8026, duration: 5.038s, episode steps: 319, steps per second: 63, episode reward: -172.502, mean reward: -0.541 [-100.000, 4.327], mean action: 1.618 [0.000, 3.000], mean observation: 0.205 [-0.984, 1.111], loss: 1.307233, mean_absolute_error: 35.977935, mean_q: 46.137031, mean_eps: 0.100000
 1700077/2000000: episode: 8027, duration: 4.156s, episode steps: 261, steps per second: 63, episode reward: 173.856, mean reward: 0.666 [-8.153, 100.000], mean action: 1.375 [0.000, 3.000], mean observation: 0.044 [-1.004, 1.000], loss: 1.274622, mean_absolute_error: 37.056881, mean_q: 47.755880, mean_eps: 0.100000
 1700353/2000000: episode: 8028, duration: 4.356s, episode steps: 276, steps per second: 63, episode reward: 150.910, mean reward: 0.547 [-11.878, 100.000], mean action: 1.616 [0.000, 3.000], mean observation: 0.086 [-0.960, 1.000], loss: 0.985201, mean_absolute_error: 35.915739, mean_q: 45.933197, mean_eps: 0.100000
 1700498/2000000: episode: 8029, duration: 2.218s, episode steps: 145, steps per second: 65, episode reward: -68.922, mean reward: -0.475 [-100.000, 13.934], mean action: 1.448 [0.000, 3.000], mean observation: -0.036 [-1.463, 1.000], loss: 1.037242, mean_absolute_error: 34.905815, mean_q: 45.003063, mean_eps: 0.100000
 1700761/2000000: episode: 8030, duration: 4.148s, episode steps: 263, steps per second: 63, episode reward: 117.263, mean reward: 0.446 [-18.855, 100.000], mean action: 2.144 [0.000, 3.000], mean observation: 0.018 [-0.958, 1.000], loss: 1.184536, mean_absolute_error: 36.326743, mean_q: 46.235316, mean_eps: 0.100000
 1700887/2000000: episode: 8031, duration: 1.915s, episode steps: 126, steps per second: 66, episode reward: -72.185, mean reward: -0.573 [-100.000, 11.629], mean action: 1.302 [0.000, 3.000], mean observation: 0.115 [-1.021, 1.518], loss: 1.021127, mean_absolute_error: 38.081472, mean_q: 48.783389, mean_eps: 0.100000
 1701178/2000000: episode: 8032, duration: 4.660s, episode steps: 291, steps per second: 62, episode reward: 219.931, mean reward: 0.756 [-10.449, 100.000], mean action: 1.560 [0.000, 3.000], mean observation: 0.177 [-0.963, 1.000], loss: 1.384815, mean_absolute_error: 35.294991, mean_q: 44.712717, mean_eps: 0.100000
 1701687/2000000: episode: 8033, duration: 8.430s, episode steps: 509, steps per second: 60, episode reward: 197.589, mean reward: 0.388 [-7.394, 100.000], mean action: 0.534 [0.000, 3.000], mean observation: 0.075 [-0.962, 1.191], loss: 1.302808, mean_absolute_error: 35.146420, mean_q: 44.688575, mean_eps: 0.100000
 1701860/2000000: episode: 8034, duration: 2.966s, episode steps: 173, steps per second: 58, episode reward: -126.209, mean reward: -0.730 [-100.000, 5.673], mean action: 1.723 [0.000, 3.000], mean observation: 0.160 [-0.829, 1.000], loss: 1.366911, mean_absolute_error: 35.054655, mean_q: 44.689634, mean_eps: 0.100000
 1702050/2000000: episode: 8035, duration: 2.987s, episode steps: 190, steps per second: 64, episode reward: 250.935, mean reward: 1.321 [-8.458, 100.000], mean action: 1.479 [0.000, 3.000], mean observation: 0.086 [-0.942, 1.000], loss: 1.074289, mean_absolute_error: 35.656478, mean_q: 46.168119, mean_eps: 0.100000
 1702358/2000000: episode: 8036, duration: 4.844s, episode steps: 308, steps per second: 64, episode reward: 186.980, mean reward: 0.607 [-9.486, 100.000], mean action: 1.211 [0.000, 3.000], mean observation: 0.097 [-0.986, 1.000], loss: 1.056866, mean_absolute_error: 36.065899, mean_q: 45.923615, mean_eps: 0.100000
 1703358/2000000: episode: 8037, duration: 16.584s, episode steps: 1000, steps per second: 60, episode reward: 32.508, mean reward: 0.033 [-17.696, 12.182], mean action: 1.025 [0.000, 3.000], mean observation: 0.184 [-0.964, 1.000], loss: 1.071331, mean_absolute_error: 35.764721, mean_q: 45.552078, mean_eps: 0.100000
 1703647/2000000: episode: 8038, duration: 4.512s, episode steps: 289, steps per second: 64, episode reward: 214.221, mean reward: 0.741 [-4.049, 100.000], mean action: 1.201 [0.000, 3.000], mean observation: 0.090 [-1.018, 1.000], loss: 1.126013, mean_absolute_error: 35.773161, mean_q: 45.220462, mean_eps: 0.100000
 1704320/2000000: episode: 8039, duration: 10.850s, episode steps: 673, steps per second: 62, episode reward: 246.359, mean reward: 0.366 [-18.723, 100.000], mean action: 1.952 [0.000, 3.000], mean observation: 0.256 [-0.985, 1.000], loss: 1.254539, mean_absolute_error: 36.521630, mean_q: 46.202334, mean_eps: 0.100000
 1705320/2000000: episode: 8040, duration: 16.416s, episode steps: 1000, steps per second: 61, episode reward: 109.793, mean reward: 0.110 [-21.081, 36.271], mean action: 0.686 [0.000, 3.000], mean observation: 0.185 [-1.084, 1.000], loss: 1.262479, mean_absolute_error: 36.104604, mean_q: 46.040315, mean_eps: 0.100000
 1705596/2000000: episode: 8041, duration: 4.371s, episode steps: 276, steps per second: 63, episode reward: 191.922, mean reward: 0.695 [-17.629, 100.000], mean action: 1.094 [0.000, 3.000], mean observation: 0.109 [-1.318, 1.000], loss: 1.062339, mean_absolute_error: 35.245394, mean_q: 44.942733, mean_eps: 0.100000
 1705935/2000000: episode: 8042, duration: 5.381s, episode steps: 339, steps per second: 63, episode reward: 225.952, mean reward: 0.667 [-17.088, 100.000], mean action: 1.153 [0.000, 3.000], mean observation: 0.058 [-0.918, 1.000], loss: 1.110603, mean_absolute_error: 35.769081, mean_q: 46.064939, mean_eps: 0.100000
 1706416/2000000: episode: 8043, duration: 8.208s, episode steps: 481, steps per second: 59, episode reward: 136.605, mean reward: 0.284 [-18.249, 100.000], mean action: 2.214 [0.000, 3.000], mean observation: 0.210 [-1.000, 1.000], loss: 1.151386, mean_absolute_error: 36.053873, mean_q: 46.173587, mean_eps: 0.100000
 1706587/2000000: episode: 8044, duration: 2.667s, episode steps: 171, steps per second: 64, episode reward: -107.614, mean reward: -0.629 [-100.000, 19.875], mean action: 1.661 [0.000, 3.000], mean observation: -0.035 [-1.788, 1.056], loss: 1.448771, mean_absolute_error: 34.953796, mean_q: 44.463566, mean_eps: 0.100000
 1707106/2000000: episode: 8045, duration: 8.476s, episode steps: 519, steps per second: 61, episode reward: -152.430, mean reward: -0.294 [-100.000, 19.708], mean action: 1.938 [0.000, 3.000], mean observation: -0.057 [-0.937, 1.354], loss: 1.261526, mean_absolute_error: 35.891981, mean_q: 45.542342, mean_eps: 0.100000
 1707519/2000000: episode: 8046, duration: 6.609s, episode steps: 413, steps per second: 62, episode reward: 136.930, mean reward: 0.332 [-12.533, 100.000], mean action: 2.111 [0.000, 3.000], mean observation: 0.108 [-1.013, 1.000], loss: 1.310961, mean_absolute_error: 35.950616, mean_q: 45.879843, mean_eps: 0.100000
 1707907/2000000: episode: 8047, duration: 6.217s, episode steps: 388, steps per second: 62, episode reward: 139.429, mean reward: 0.359 [-20.673, 100.000], mean action: 1.474 [0.000, 3.000], mean observation: 0.001 [-1.334, 1.000], loss: 1.236056, mean_absolute_error: 36.032497, mean_q: 45.671348, mean_eps: 0.100000
 1708888/2000000: episode: 8048, duration: 16.716s, episode steps: 981, steps per second: 59, episode reward: -144.437, mean reward: -0.147 [-100.000, 13.959], mean action: 1.770 [0.000, 3.000], mean observation: 0.089 [-0.983, 1.000], loss: 1.137904, mean_absolute_error: 35.988926, mean_q: 45.957556, mean_eps: 0.100000
 1709004/2000000: episode: 8049, duration: 1.886s, episode steps: 116, steps per second: 62, episode reward: -255.087, mean reward: -2.199 [-100.000, 4.135], mean action: 1.690 [0.000, 3.000], mean observation: 0.135 [-0.901, 1.681], loss: 1.519838, mean_absolute_error: 36.095565, mean_q: 46.007040, mean_eps: 0.100000
 1709910/2000000: episode: 8050, duration: 14.722s, episode steps: 906, steps per second: 62, episode reward: 198.443, mean reward: 0.219 [-20.224, 100.000], mean action: 0.909 [0.000, 3.000], mean observation: 0.144 [-1.455, 1.000], loss: 1.116021, mean_absolute_error: 35.939985, mean_q: 45.847524, mean_eps: 0.100000
 1710174/2000000: episode: 8051, duration: 4.152s, episode steps: 264, steps per second: 64, episode reward: 171.397, mean reward: 0.649 [-11.937, 100.000], mean action: 1.178 [0.000, 3.000], mean observation: 0.081 [-1.290, 1.000], loss: 1.035934, mean_absolute_error: 35.600836, mean_q: 45.221974, mean_eps: 0.100000
 1710467/2000000: episode: 8052, duration: 4.623s, episode steps: 293, steps per second: 63, episode reward: -80.405, mean reward: -0.274 [-100.000, 14.536], mean action: 1.683 [0.000, 3.000], mean observation: -0.030 [-0.988, 1.000], loss: 1.378919, mean_absolute_error: 35.853463, mean_q: 44.886749, mean_eps: 0.100000
 1710752/2000000: episode: 8053, duration: 4.747s, episode steps: 285, steps per second: 60, episode reward: -134.393, mean reward: -0.472 [-100.000, 4.615], mean action: 1.888 [0.000, 3.000], mean observation: 0.140 [-0.902, 1.002], loss: 1.036408, mean_absolute_error: 36.202164, mean_q: 46.001305, mean_eps: 0.100000
 1711027/2000000: episode: 8054, duration: 4.616s, episode steps: 275, steps per second: 60, episode reward: 218.165, mean reward: 0.793 [-10.446, 100.000], mean action: 1.571 [0.000, 3.000], mean observation: 0.077 [-0.993, 1.000], loss: 1.333586, mean_absolute_error: 35.062386, mean_q: 44.995933, mean_eps: 0.100000
 1711329/2000000: episode: 8055, duration: 4.905s, episode steps: 302, steps per second: 62, episode reward: 237.945, mean reward: 0.788 [-11.021, 100.000], mean action: 1.424 [0.000, 3.000], mean observation: 0.065 [-1.672, 1.000], loss: 1.096330, mean_absolute_error: 35.730300, mean_q: 44.805346, mean_eps: 0.100000
 1711618/2000000: episode: 8056, duration: 4.728s, episode steps: 289, steps per second: 61, episode reward: 229.710, mean reward: 0.795 [-17.362, 100.000], mean action: 1.173 [0.000, 3.000], mean observation: 0.132 [-1.014, 1.000], loss: 1.272475, mean_absolute_error: 35.971189, mean_q: 45.659888, mean_eps: 0.100000
 1711989/2000000: episode: 8057, duration: 5.981s, episode steps: 371, steps per second: 62, episode reward: 179.965, mean reward: 0.485 [-4.183, 100.000], mean action: 1.391 [0.000, 3.000], mean observation: 0.075 [-0.983, 1.000], loss: 1.176556, mean_absolute_error: 35.260724, mean_q: 44.941486, mean_eps: 0.100000
 1712430/2000000: episode: 8058, duration: 7.073s, episode steps: 441, steps per second: 62, episode reward: 185.098, mean reward: 0.420 [-23.991, 100.000], mean action: 1.084 [0.000, 3.000], mean observation: 0.114 [-1.102, 1.028], loss: 0.990660, mean_absolute_error: 36.167895, mean_q: 46.386958, mean_eps: 0.100000
 1712534/2000000: episode: 8059, duration: 1.626s, episode steps: 104, steps per second: 64, episode reward: -70.112, mean reward: -0.674 [-100.000, 10.189], mean action: 1.923 [0.000, 3.000], mean observation: -0.121 [-0.893, 1.000], loss: 1.282881, mean_absolute_error: 35.580795, mean_q: 44.660418, mean_eps: 0.100000
 1712919/2000000: episode: 8060, duration: 6.134s, episode steps: 385, steps per second: 63, episode reward: 222.265, mean reward: 0.577 [-19.164, 100.000], mean action: 1.244 [0.000, 3.000], mean observation: 0.063 [-1.448, 1.000], loss: 0.979125, mean_absolute_error: 36.253840, mean_q: 46.390337, mean_eps: 0.100000
 1713029/2000000: episode: 8061, duration: 1.743s, episode steps: 110, steps per second: 63, episode reward: -44.657, mean reward: -0.406 [-100.000, 16.931], mean action: 1.500 [0.000, 3.000], mean observation: 0.013 [-1.704, 1.124], loss: 1.049560, mean_absolute_error: 34.944830, mean_q: 44.350079, mean_eps: 0.100000
 1713137/2000000: episode: 8062, duration: 1.657s, episode steps: 108, steps per second: 65, episode reward: -50.082, mean reward: -0.464 [-100.000, 10.605], mean action: 1.389 [0.000, 3.000], mean observation: -0.044 [-1.156, 1.000], loss: 0.853284, mean_absolute_error: 36.122315, mean_q: 45.967231, mean_eps: 0.100000
 1713383/2000000: episode: 8063, duration: 3.856s, episode steps: 246, steps per second: 64, episode reward: 231.119, mean reward: 0.940 [-3.883, 100.000], mean action: 1.467 [0.000, 3.000], mean observation: 0.101 [-1.209, 1.000], loss: 1.123133, mean_absolute_error: 35.722881, mean_q: 45.777873, mean_eps: 0.100000
 1713568/2000000: episode: 8064, duration: 2.917s, episode steps: 185, steps per second: 63, episode reward: 250.023, mean reward: 1.351 [-8.596, 100.000], mean action: 1.438 [0.000, 3.000], mean observation: 0.079 [-0.951, 1.000], loss: 1.099625, mean_absolute_error: 35.449688, mean_q: 45.641909, mean_eps: 0.100000
 1713698/2000000: episode: 8065, duration: 2.048s, episode steps: 130, steps per second: 63, episode reward: -102.394, mean reward: -0.788 [-100.000, 13.844], mean action: 1.631 [0.000, 3.000], mean observation: -0.011 [-1.093, 1.000], loss: 1.166399, mean_absolute_error: 35.496125, mean_q: 45.237812, mean_eps: 0.100000
 1714175/2000000: episode: 8066, duration: 7.554s, episode steps: 477, steps per second: 63, episode reward: 251.522, mean reward: 0.527 [-19.059, 100.000], mean action: 0.866 [0.000, 3.000], mean observation: 0.128 [-1.617, 1.000], loss: 1.232722, mean_absolute_error: 35.442410, mean_q: 44.722979, mean_eps: 0.100000
 1714286/2000000: episode: 8067, duration: 1.743s, episode steps: 111, steps per second: 64, episode reward: -392.615, mean reward: -3.537 [-100.000, 5.496], mean action: 1.541 [0.000, 3.000], mean observation: 0.036 [-1.440, 2.203], loss: 0.998650, mean_absolute_error: 36.950230, mean_q: 46.913342, mean_eps: 0.100000
 1714669/2000000: episode: 8068, duration: 6.073s, episode steps: 383, steps per second: 63, episode reward: -250.000, mean reward: -0.653 [-100.000, 5.526], mean action: 1.679 [0.000, 3.000], mean observation: 0.115 [-1.013, 1.002], loss: 1.122020, mean_absolute_error: 36.187669, mean_q: 46.092862, mean_eps: 0.100000
 1714791/2000000: episode: 8069, duration: 1.875s, episode steps: 122, steps per second: 65, episode reward: -70.941, mean reward: -0.581 [-100.000, 18.504], mean action: 1.426 [0.000, 3.000], mean observation: 0.061 [-1.443, 1.027], loss: 1.603776, mean_absolute_error: 35.832112, mean_q: 45.545766, mean_eps: 0.100000
 1714883/2000000: episode: 8070, duration: 1.426s, episode steps: 92, steps per second: 65, episode reward: -66.304, mean reward: -0.721 [-100.000, 11.369], mean action: 1.304 [0.000, 3.000], mean observation: -0.081 [-1.057, 1.000], loss: 1.583406, mean_absolute_error: 35.770324, mean_q: 46.057557, mean_eps: 0.100000
 1714985/2000000: episode: 8071, duration: 1.614s, episode steps: 102, steps per second: 63, episode reward: -61.753, mean reward: -0.605 [-100.000, 22.277], mean action: 1.431 [0.000, 3.000], mean observation: -0.075 [-1.009, 2.748], loss: 1.031864, mean_absolute_error: 34.475397, mean_q: 44.512642, mean_eps: 0.100000
 1715402/2000000: episode: 8072, duration: 6.599s, episode steps: 417, steps per second: 63, episode reward: -380.626, mean reward: -0.913 [-100.000, 5.183], mean action: 1.559 [0.000, 3.000], mean observation: 0.121 [-1.019, 1.231], loss: 1.170075, mean_absolute_error: 36.377116, mean_q: 45.072919, mean_eps: 0.100000
 1715512/2000000: episode: 8073, duration: 1.752s, episode steps: 110, steps per second: 63, episode reward: -44.967, mean reward: -0.409 [-100.000, 4.761], mean action: 1.709 [0.000, 3.000], mean observation: -0.136 [-1.011, 0.928], loss: 1.112135, mean_absolute_error: 35.788892, mean_q: 45.274194, mean_eps: 0.100000
 1715659/2000000: episode: 8074, duration: 2.309s, episode steps: 147, steps per second: 64, episode reward: 5.561, mean reward: 0.038 [-100.000, 16.677], mean action: 1.932 [0.000, 3.000], mean observation: -0.025 [-0.968, 1.000], loss: 1.117485, mean_absolute_error: 36.601412, mean_q: 46.785802, mean_eps: 0.100000
 1715817/2000000: episode: 8075, duration: 2.483s, episode steps: 158, steps per second: 64, episode reward: -354.199, mean reward: -2.242 [-100.000, 8.381], mean action: 1.589 [0.000, 3.000], mean observation: 0.036 [-0.962, 2.356], loss: 1.086565, mean_absolute_error: 35.501781, mean_q: 45.556356, mean_eps: 0.100000
 1716145/2000000: episode: 8076, duration: 5.442s, episode steps: 328, steps per second: 60, episode reward: 229.430, mean reward: 0.699 [-17.830, 100.000], mean action: 1.067 [0.000, 3.000], mean observation: 0.125 [-1.553, 1.002], loss: 1.221388, mean_absolute_error: 34.747376, mean_q: 44.326676, mean_eps: 0.100000
 1716264/2000000: episode: 8077, duration: 1.898s, episode steps: 119, steps per second: 63, episode reward: -30.610, mean reward: -0.257 [-100.000, 16.516], mean action: 1.277 [0.000, 3.000], mean observation: -0.092 [-0.919, 1.103], loss: 1.538936, mean_absolute_error: 35.961112, mean_q: 45.469422, mean_eps: 0.100000
 1716379/2000000: episode: 8078, duration: 1.800s, episode steps: 115, steps per second: 64, episode reward: -23.382, mean reward: -0.203 [-100.000, 10.445], mean action: 1.704 [0.000, 3.000], mean observation: 0.108 [-0.931, 1.000], loss: 1.705311, mean_absolute_error: 35.828229, mean_q: 45.415861, mean_eps: 0.100000
 1716826/2000000: episode: 8079, duration: 7.168s, episode steps: 447, steps per second: 62, episode reward: 213.687, mean reward: 0.478 [-18.943, 100.000], mean action: 1.416 [0.000, 3.000], mean observation: 0.194 [-1.000, 1.000], loss: 1.194606, mean_absolute_error: 35.952040, mean_q: 45.674357, mean_eps: 0.100000
 1717086/2000000: episode: 8080, duration: 4.086s, episode steps: 260, steps per second: 64, episode reward: 245.241, mean reward: 0.943 [-19.648, 100.000], mean action: 1.254 [0.000, 3.000], mean observation: 0.049 [-0.947, 1.000], loss: 1.233694, mean_absolute_error: 35.226598, mean_q: 44.668343, mean_eps: 0.100000
 1717421/2000000: episode: 8081, duration: 5.382s, episode steps: 335, steps per second: 62, episode reward: 227.763, mean reward: 0.680 [-12.301, 100.000], mean action: 1.621 [0.000, 3.000], mean observation: 0.194 [-0.916, 1.000], loss: 1.083853, mean_absolute_error: 34.855007, mean_q: 44.616324, mean_eps: 0.100000
 1717907/2000000: episode: 8082, duration: 7.674s, episode steps: 486, steps per second: 63, episode reward: 199.260, mean reward: 0.410 [-17.930, 100.000], mean action: 1.070 [0.000, 3.000], mean observation: 0.149 [-1.055, 1.000], loss: 1.252906, mean_absolute_error: 35.376252, mean_q: 45.042778, mean_eps: 0.100000
 1718013/2000000: episode: 8083, duration: 1.683s, episode steps: 106, steps per second: 63, episode reward: -44.833, mean reward: -0.423 [-100.000, 13.884], mean action: 1.736 [0.000, 3.000], mean observation: -0.012 [-1.520, 1.000], loss: 1.036194, mean_absolute_error: 37.073899, mean_q: 47.323957, mean_eps: 0.100000
 1718283/2000000: episode: 8084, duration: 4.270s, episode steps: 270, steps per second: 63, episode reward: 254.190, mean reward: 0.941 [-18.331, 100.000], mean action: 1.744 [0.000, 3.000], mean observation: 0.147 [-0.924, 1.000], loss: 1.263828, mean_absolute_error: 36.514794, mean_q: 46.037230, mean_eps: 0.100000
 1718651/2000000: episode: 8085, duration: 5.893s, episode steps: 368, steps per second: 62, episode reward: 122.581, mean reward: 0.333 [-17.995, 100.000], mean action: 2.353 [0.000, 3.000], mean observation: 0.029 [-0.997, 1.000], loss: 1.123334, mean_absolute_error: 35.724171, mean_q: 45.400734, mean_eps: 0.100000
 1719096/2000000: episode: 8086, duration: 7.111s, episode steps: 445, steps per second: 63, episode reward: 217.080, mean reward: 0.488 [-17.823, 100.000], mean action: 0.984 [0.000, 3.000], mean observation: 0.096 [-0.925, 1.000], loss: 1.388275, mean_absolute_error: 36.184618, mean_q: 45.735412, mean_eps: 0.100000
 1719412/2000000: episode: 8087, duration: 5.052s, episode steps: 316, steps per second: 63, episode reward: 184.114, mean reward: 0.583 [-17.496, 100.000], mean action: 1.066 [0.000, 3.000], mean observation: 0.089 [-0.985, 1.000], loss: 1.045568, mean_absolute_error: 36.292959, mean_q: 46.548983, mean_eps: 0.100000
 1719664/2000000: episode: 8088, duration: 3.973s, episode steps: 252, steps per second: 63, episode reward: 219.469, mean reward: 0.871 [-3.987, 100.000], mean action: 1.520 [0.000, 3.000], mean observation: 0.054 [-1.003, 1.002], loss: 1.131582, mean_absolute_error: 35.922168, mean_q: 46.659534, mean_eps: 0.100000
 1720035/2000000: episode: 8089, duration: 5.948s, episode steps: 371, steps per second: 62, episode reward: 205.127, mean reward: 0.553 [-8.901, 100.000], mean action: 1.978 [0.000, 3.000], mean observation: -0.001 [-1.057, 1.000], loss: 1.105218, mean_absolute_error: 36.623030, mean_q: 46.164554, mean_eps: 0.100000
 1720146/2000000: episode: 8090, duration: 1.757s, episode steps: 111, steps per second: 63, episode reward: -44.392, mean reward: -0.400 [-100.000, 12.131], mean action: 1.937 [0.000, 3.000], mean observation: -0.093 [-0.932, 2.534], loss: 0.857786, mean_absolute_error: 35.584461, mean_q: 45.361196, mean_eps: 0.100000
 1720499/2000000: episode: 8091, duration: 5.671s, episode steps: 353, steps per second: 62, episode reward: 233.174, mean reward: 0.661 [-12.569, 100.000], mean action: 1.295 [0.000, 3.000], mean observation: 0.003 [-0.996, 1.144], loss: 0.950203, mean_absolute_error: 35.227647, mean_q: 45.455788, mean_eps: 0.100000
 1720632/2000000: episode: 8092, duration: 2.107s, episode steps: 133, steps per second: 63, episode reward: -23.958, mean reward: -0.180 [-100.000, 14.625], mean action: 1.692 [0.000, 3.000], mean observation: -0.040 [-0.984, 1.000], loss: 1.280164, mean_absolute_error: 36.197554, mean_q: 45.332660, mean_eps: 0.100000
 1721075/2000000: episode: 8093, duration: 7.325s, episode steps: 443, steps per second: 60, episode reward: -219.620, mean reward: -0.496 [-100.000, 4.590], mean action: 1.609 [0.000, 3.000], mean observation: 0.155 [-0.957, 1.221], loss: 0.996121, mean_absolute_error: 36.301138, mean_q: 46.309867, mean_eps: 0.100000
 1721414/2000000: episode: 8094, duration: 5.430s, episode steps: 339, steps per second: 62, episode reward: 169.697, mean reward: 0.501 [-18.528, 100.000], mean action: 2.289 [0.000, 3.000], mean observation: 0.087 [-1.192, 1.000], loss: 0.991279, mean_absolute_error: 35.283158, mean_q: 45.534214, mean_eps: 0.100000
 1721609/2000000: episode: 8095, duration: 3.137s, episode steps: 195, steps per second: 62, episode reward: 234.052, mean reward: 1.200 [-11.044, 100.000], mean action: 1.092 [0.000, 3.000], mean observation: 0.057 [-1.043, 1.000], loss: 0.983014, mean_absolute_error: 34.874616, mean_q: 44.243441, mean_eps: 0.100000
 1721733/2000000: episode: 8096, duration: 2.022s, episode steps: 124, steps per second: 61, episode reward: -426.608, mean reward: -3.440 [-100.000, 6.314], mean action: 1.976 [0.000, 3.000], mean observation: 0.064 [-1.303, 2.758], loss: 1.679469, mean_absolute_error: 37.436790, mean_q: 46.288175, mean_eps: 0.100000
 1722305/2000000: episode: 8097, duration: 9.942s, episode steps: 572, steps per second: 58, episode reward: -253.064, mean reward: -0.442 [-100.000, 5.641], mean action: 1.923 [0.000, 3.000], mean observation: 0.129 [-0.994, 1.000], loss: 1.256973, mean_absolute_error: 35.651606, mean_q: 45.321255, mean_eps: 0.100000
 1722423/2000000: episode: 8098, duration: 1.889s, episode steps: 118, steps per second: 62, episode reward: -43.433, mean reward: -0.368 [-100.000, 14.900], mean action: 1.347 [0.000, 3.000], mean observation: 0.008 [-0.945, 1.000], loss: 1.101307, mean_absolute_error: 34.071895, mean_q: 42.999056, mean_eps: 0.100000
 1722748/2000000: episode: 8099, duration: 5.203s, episode steps: 325, steps per second: 62, episode reward: 116.550, mean reward: 0.359 [-11.164, 100.000], mean action: 2.172 [0.000, 3.000], mean observation: 0.039 [-0.971, 1.000], loss: 1.519956, mean_absolute_error: 35.953531, mean_q: 46.072130, mean_eps: 0.100000
 1723000/2000000: episode: 8100, duration: 4.027s, episode steps: 252, steps per second: 63, episode reward: 220.109, mean reward: 0.873 [-12.412, 100.000], mean action: 1.056 [0.000, 3.000], mean observation: 0.109 [-0.968, 1.000], loss: 1.174058, mean_absolute_error: 36.524140, mean_q: 47.126631, mean_eps: 0.100000
 1723365/2000000: episode: 8101, duration: 6.165s, episode steps: 365, steps per second: 59, episode reward: 241.086, mean reward: 0.661 [-18.330, 100.000], mean action: 1.052 [0.000, 3.000], mean observation: 0.162 [-0.619, 1.000], loss: 1.172444, mean_absolute_error: 35.723828, mean_q: 44.193567, mean_eps: 0.100000
 1723666/2000000: episode: 8102, duration: 4.982s, episode steps: 301, steps per second: 60, episode reward: 219.730, mean reward: 0.730 [-11.793, 100.000], mean action: 1.136 [0.000, 3.000], mean observation: 0.118 [-0.957, 1.000], loss: 1.341833, mean_absolute_error: 35.875409, mean_q: 46.023245, mean_eps: 0.100000
 1723789/2000000: episode: 8103, duration: 1.909s, episode steps: 123, steps per second: 64, episode reward: -29.328, mean reward: -0.238 [-100.000, 24.043], mean action: 1.211 [0.000, 3.000], mean observation: 0.050 [-1.028, 1.455], loss: 1.438422, mean_absolute_error: 35.593090, mean_q: 45.595928, mean_eps: 0.100000
 1724081/2000000: episode: 8104, duration: 4.620s, episode steps: 292, steps per second: 63, episode reward: 236.797, mean reward: 0.811 [-7.898, 100.000], mean action: 1.558 [0.000, 3.000], mean observation: 0.068 [-0.823, 1.000], loss: 1.405386, mean_absolute_error: 36.479341, mean_q: 47.023424, mean_eps: 0.100000
 1724708/2000000: episode: 8105, duration: 10.481s, episode steps: 627, steps per second: 60, episode reward: 222.250, mean reward: 0.354 [-9.178, 100.000], mean action: 1.475 [0.000, 3.000], mean observation: 0.015 [-0.959, 1.000], loss: 1.171697, mean_absolute_error: 36.182575, mean_q: 46.471608, mean_eps: 0.100000
 1724838/2000000: episode: 8106, duration: 2.084s, episode steps: 130, steps per second: 62, episode reward: -30.207, mean reward: -0.232 [-100.000, 11.279], mean action: 1.708 [0.000, 3.000], mean observation: -0.053 [-1.038, 1.000], loss: 1.214195, mean_absolute_error: 34.904661, mean_q: 44.634386, mean_eps: 0.100000
 1725383/2000000: episode: 8107, duration: 8.742s, episode steps: 545, steps per second: 62, episode reward: 227.602, mean reward: 0.418 [-19.928, 100.000], mean action: 1.391 [0.000, 3.000], mean observation: 0.135 [-1.360, 1.000], loss: 1.209539, mean_absolute_error: 35.962820, mean_q: 45.683140, mean_eps: 0.100000
 1725866/2000000: episode: 8108, duration: 7.931s, episode steps: 483, steps per second: 61, episode reward: 151.218, mean reward: 0.313 [-18.962, 100.000], mean action: 1.814 [0.000, 3.000], mean observation: 0.157 [-0.998, 1.000], loss: 1.156045, mean_absolute_error: 35.433518, mean_q: 45.425034, mean_eps: 0.100000
 1726242/2000000: episode: 8109, duration: 6.067s, episode steps: 376, steps per second: 62, episode reward: 199.203, mean reward: 0.530 [-10.376, 100.000], mean action: 1.199 [0.000, 3.000], mean observation: 0.061 [-0.981, 1.000], loss: 1.085615, mean_absolute_error: 35.768122, mean_q: 45.146504, mean_eps: 0.100000
 1726961/2000000: episode: 8110, duration: 11.999s, episode steps: 719, steps per second: 60, episode reward: 165.095, mean reward: 0.230 [-17.123, 100.000], mean action: 1.465 [0.000, 3.000], mean observation: 0.138 [-0.765, 1.000], loss: 1.336433, mean_absolute_error: 36.348904, mean_q: 46.068698, mean_eps: 0.100000
 1727103/2000000: episode: 8111, duration: 2.239s, episode steps: 142, steps per second: 63, episode reward: -81.768, mean reward: -0.576 [-100.000, 11.716], mean action: 1.465 [0.000, 3.000], mean observation: -0.030 [-1.461, 1.000], loss: 1.528770, mean_absolute_error: 35.247817, mean_q: 44.992487, mean_eps: 0.100000
 1727210/2000000: episode: 8112, duration: 1.706s, episode steps: 107, steps per second: 63, episode reward: -32.889, mean reward: -0.307 [-100.000, 17.386], mean action: 1.411 [0.000, 3.000], mean observation: 0.035 [-1.028, 1.227], loss: 1.059117, mean_absolute_error: 35.582721, mean_q: 45.611528, mean_eps: 0.100000
 1727318/2000000: episode: 8113, duration: 1.695s, episode steps: 108, steps per second: 64, episode reward: -86.125, mean reward: -0.797 [-100.000, 9.793], mean action: 1.491 [0.000, 3.000], mean observation: 0.099 [-0.998, 1.326], loss: 0.983706, mean_absolute_error: 33.994827, mean_q: 44.359452, mean_eps: 0.100000
 1727688/2000000: episode: 8114, duration: 5.981s, episode steps: 370, steps per second: 62, episode reward: 171.723, mean reward: 0.464 [-18.683, 100.000], mean action: 1.251 [0.000, 3.000], mean observation: 0.076 [-0.943, 1.000], loss: 1.171896, mean_absolute_error: 37.301811, mean_q: 46.980881, mean_eps: 0.100000
 1728688/2000000: episode: 8115, duration: 17.280s, episode steps: 1000, steps per second: 58, episode reward: 69.978, mean reward: 0.070 [-19.585, 20.583], mean action: 1.166 [0.000, 3.000], mean observation: 0.129 [-0.875, 1.000], loss: 1.238305, mean_absolute_error: 35.218102, mean_q: 44.863547, mean_eps: 0.100000
 1728776/2000000: episode: 8116, duration: 1.450s, episode steps: 88, steps per second: 61, episode reward: -30.359, mean reward: -0.345 [-100.000, 18.757], mean action: 1.648 [0.000, 3.000], mean observation: -0.018 [-1.007, 1.722], loss: 1.617109, mean_absolute_error: 35.728731, mean_q: 46.181153, mean_eps: 0.100000
 1729206/2000000: episode: 8117, duration: 6.990s, episode steps: 430, steps per second: 62, episode reward: 190.266, mean reward: 0.442 [-19.714, 100.000], mean action: 0.965 [0.000, 3.000], mean observation: 0.075 [-0.964, 1.000], loss: 1.363426, mean_absolute_error: 35.706235, mean_q: 45.415352, mean_eps: 0.100000
 1729425/2000000: episode: 8118, duration: 3.547s, episode steps: 219, steps per second: 62, episode reward: 2.863, mean reward: 0.013 [-100.000, 14.863], mean action: 1.868 [0.000, 3.000], mean observation: 0.068 [-0.859, 1.000], loss: 1.109323, mean_absolute_error: 35.427194, mean_q: 44.977932, mean_eps: 0.100000
 1729554/2000000: episode: 8119, duration: 2.021s, episode steps: 129, steps per second: 64, episode reward: -203.923, mean reward: -1.581 [-100.000, 44.070], mean action: 1.907 [0.000, 3.000], mean observation: -0.017 [-0.944, 1.551], loss: 1.464349, mean_absolute_error: 36.293919, mean_q: 44.996813, mean_eps: 0.100000
 1730090/2000000: episode: 8120, duration: 8.582s, episode steps: 536, steps per second: 62, episode reward: 239.985, mean reward: 0.448 [-18.144, 100.000], mean action: 0.696 [0.000, 3.000], mean observation: 0.186 [-0.849, 1.000], loss: 1.223516, mean_absolute_error: 35.259649, mean_q: 44.507220, mean_eps: 0.100000
 1730317/2000000: episode: 8121, duration: 3.581s, episode steps: 227, steps per second: 63, episode reward: 246.559, mean reward: 1.086 [-7.637, 100.000], mean action: 1.383 [0.000, 3.000], mean observation: 0.132 [-0.979, 1.000], loss: 1.238016, mean_absolute_error: 36.178760, mean_q: 46.159972, mean_eps: 0.100000
 1730899/2000000: episode: 8122, duration: 9.786s, episode steps: 582, steps per second: 59, episode reward: 168.930, mean reward: 0.290 [-9.460, 100.000], mean action: 1.790 [0.000, 3.000], mean observation: 0.008 [-0.906, 1.000], loss: 1.177321, mean_absolute_error: 35.385310, mean_q: 44.392930, mean_eps: 0.100000
 1731417/2000000: episode: 8123, duration: 8.362s, episode steps: 518, steps per second: 62, episode reward: 168.599, mean reward: 0.325 [-18.761, 100.000], mean action: 1.988 [0.000, 3.000], mean observation: 0.167 [-1.188, 1.000], loss: 1.120805, mean_absolute_error: 35.433174, mean_q: 44.711563, mean_eps: 0.100000
 1732324/2000000: episode: 8124, duration: 14.963s, episode steps: 907, steps per second: 61, episode reward: 177.465, mean reward: 0.196 [-22.074, 100.000], mean action: 1.974 [0.000, 3.000], mean observation: 0.181 [-0.878, 1.000], loss: 1.152633, mean_absolute_error: 35.532475, mean_q: 45.335775, mean_eps: 0.100000
 1732576/2000000: episode: 8125, duration: 4.044s, episode steps: 252, steps per second: 62, episode reward: 248.315, mean reward: 0.985 [-17.861, 100.000], mean action: 1.278 [0.000, 3.000], mean observation: 0.121 [-0.780, 1.000], loss: 1.061661, mean_absolute_error: 35.604424, mean_q: 45.357205, mean_eps: 0.100000
 1732878/2000000: episode: 8126, duration: 4.815s, episode steps: 302, steps per second: 63, episode reward: 203.293, mean reward: 0.673 [-18.106, 100.000], mean action: 1.219 [0.000, 3.000], mean observation: 0.056 [-0.845, 1.000], loss: 1.314980, mean_absolute_error: 35.114874, mean_q: 44.036634, mean_eps: 0.100000
 1733511/2000000: episode: 8127, duration: 10.214s, episode steps: 633, steps per second: 62, episode reward: 184.198, mean reward: 0.291 [-17.967, 100.000], mean action: 1.194 [0.000, 3.000], mean observation: 0.174 [-1.359, 1.000], loss: 1.152947, mean_absolute_error: 35.486813, mean_q: 44.656322, mean_eps: 0.100000
 1733617/2000000: episode: 8128, duration: 1.688s, episode steps: 106, steps per second: 63, episode reward: -54.806, mean reward: -0.517 [-100.000, 16.844], mean action: 1.557 [0.000, 3.000], mean observation: -0.117 [-1.034, 1.494], loss: 1.065183, mean_absolute_error: 35.964726, mean_q: 44.639371, mean_eps: 0.100000
 1733720/2000000: episode: 8129, duration: 1.649s, episode steps: 103, steps per second: 62, episode reward: -8.740, mean reward: -0.085 [-100.000, 14.389], mean action: 1.864 [0.000, 3.000], mean observation: -0.096 [-1.004, 1.509], loss: 1.057605, mean_absolute_error: 36.501768, mean_q: 44.790816, mean_eps: 0.100000
 1734139/2000000: episode: 8130, duration: 6.621s, episode steps: 419, steps per second: 63, episode reward: 193.001, mean reward: 0.461 [-17.719, 100.000], mean action: 1.086 [0.000, 3.000], mean observation: 0.127 [-1.072, 1.000], loss: 1.285355, mean_absolute_error: 35.958865, mean_q: 45.057364, mean_eps: 0.100000
 1735139/2000000: episode: 8131, duration: 16.103s, episode steps: 1000, steps per second: 62, episode reward: 114.752, mean reward: 0.115 [-19.434, 22.873], mean action: 1.576 [0.000, 3.000], mean observation: 0.197 [-0.784, 1.000], loss: 1.149523, mean_absolute_error: 36.082727, mean_q: 45.682002, mean_eps: 0.100000
 1736139/2000000: episode: 8132, duration: 16.360s, episode steps: 1000, steps per second: 61, episode reward: 88.222, mean reward: 0.088 [-20.156, 22.240], mean action: 0.904 [0.000, 3.000], mean observation: 0.233 [-0.867, 1.000], loss: 1.161565, mean_absolute_error: 35.728038, mean_q: 45.228118, mean_eps: 0.100000
 1736250/2000000: episode: 8133, duration: 1.774s, episode steps: 111, steps per second: 63, episode reward: -34.849, mean reward: -0.314 [-100.000, 16.979], mean action: 1.955 [0.000, 3.000], mean observation: -0.102 [-0.998, 1.572], loss: 1.059521, mean_absolute_error: 35.428265, mean_q: 46.280531, mean_eps: 0.100000
 1736493/2000000: episode: 8134, duration: 3.765s, episode steps: 243, steps per second: 65, episode reward: 196.198, mean reward: 0.807 [-8.295, 100.000], mean action: 1.181 [0.000, 3.000], mean observation: 0.073 [-0.956, 1.000], loss: 0.881894, mean_absolute_error: 34.249628, mean_q: 43.764913, mean_eps: 0.100000
 1736836/2000000: episode: 8135, duration: 5.520s, episode steps: 343, steps per second: 62, episode reward: 215.592, mean reward: 0.629 [-7.612, 100.000], mean action: 1.114 [0.000, 3.000], mean observation: 0.066 [-0.911, 1.000], loss: 1.296481, mean_absolute_error: 35.653622, mean_q: 45.291986, mean_eps: 0.100000
 1736975/2000000: episode: 8136, duration: 2.222s, episode steps: 139, steps per second: 63, episode reward: -164.320, mean reward: -1.182 [-100.000, 52.650], mean action: 1.324 [0.000, 3.000], mean observation: -0.119 [-1.787, 1.000], loss: 1.122520, mean_absolute_error: 35.207899, mean_q: 44.170430, mean_eps: 0.100000
 1737121/2000000: episode: 8137, duration: 2.271s, episode steps: 146, steps per second: 64, episode reward: -115.988, mean reward: -0.794 [-100.000, 14.436], mean action: 1.301 [0.000, 3.000], mean observation: 0.014 [-1.327, 1.000], loss: 1.242189, mean_absolute_error: 35.957953, mean_q: 46.037755, mean_eps: 0.100000
 1737279/2000000: episode: 8138, duration: 2.443s, episode steps: 158, steps per second: 65, episode reward: -201.336, mean reward: -1.274 [-100.000, 41.321], mean action: 2.044 [0.000, 3.000], mean observation: -0.004 [-0.893, 1.676], loss: 1.193276, mean_absolute_error: 36.591835, mean_q: 46.342617, mean_eps: 0.100000
 1737781/2000000: episode: 8139, duration: 8.047s, episode steps: 502, steps per second: 62, episode reward: 209.864, mean reward: 0.418 [-18.501, 100.000], mean action: 0.876 [0.000, 3.000], mean observation: 0.111 [-1.023, 1.182], loss: 1.360873, mean_absolute_error: 35.156214, mean_q: 44.275210, mean_eps: 0.100000
 1737938/2000000: episode: 8140, duration: 2.465s, episode steps: 157, steps per second: 64, episode reward: -93.993, mean reward: -0.599 [-100.000, 8.496], mean action: 1.847 [0.000, 3.000], mean observation: -0.091 [-1.424, 1.000], loss: 0.979398, mean_absolute_error: 36.246166, mean_q: 45.467918, mean_eps: 0.100000
 1738074/2000000: episode: 8141, duration: 2.134s, episode steps: 136, steps per second: 64, episode reward: -94.560, mean reward: -0.695 [-100.000, 15.051], mean action: 1.632 [0.000, 3.000], mean observation: -0.028 [-1.256, 1.000], loss: 1.213980, mean_absolute_error: 35.015006, mean_q: 44.146537, mean_eps: 0.100000
 1738307/2000000: episode: 8142, duration: 3.872s, episode steps: 233, steps per second: 60, episode reward: 239.265, mean reward: 1.027 [-10.565, 100.000], mean action: 1.481 [0.000, 3.000], mean observation: 0.067 [-0.855, 1.000], loss: 1.035739, mean_absolute_error: 36.587455, mean_q: 46.297117, mean_eps: 0.100000
 1738651/2000000: episode: 8143, duration: 5.458s, episode steps: 344, steps per second: 63, episode reward: 252.660, mean reward: 0.734 [-17.377, 100.000], mean action: 0.756 [0.000, 3.000], mean observation: 0.130 [-0.971, 1.000], loss: 1.555999, mean_absolute_error: 35.596000, mean_q: 45.502443, mean_eps: 0.100000
 1739028/2000000: episode: 8144, duration: 6.020s, episode steps: 377, steps per second: 63, episode reward: -81.087, mean reward: -0.215 [-100.000, 13.805], mean action: 1.785 [0.000, 3.000], mean observation: -0.028 [-0.972, 1.000], loss: 1.278524, mean_absolute_error: 35.851504, mean_q: 45.649754, mean_eps: 0.100000
 1739398/2000000: episode: 8145, duration: 6.201s, episode steps: 370, steps per second: 60, episode reward: -288.729, mean reward: -0.780 [-100.000, 4.787], mean action: 1.838 [0.000, 3.000], mean observation: 0.130 [-0.960, 1.027], loss: 1.058241, mean_absolute_error: 36.214949, mean_q: 46.387231, mean_eps: 0.100000
 1739526/2000000: episode: 8146, duration: 2.010s, episode steps: 128, steps per second: 64, episode reward: -219.978, mean reward: -1.719 [-100.000, 10.018], mean action: 1.617 [0.000, 3.000], mean observation: -0.140 [-2.415, 1.000], loss: 1.273132, mean_absolute_error: 35.465714, mean_q: 44.651093, mean_eps: 0.100000
 1739761/2000000: episode: 8147, duration: 3.739s, episode steps: 235, steps per second: 63, episode reward: 158.983, mean reward: 0.677 [-16.097, 100.000], mean action: 1.455 [0.000, 3.000], mean observation: 0.028 [-0.885, 1.000], loss: 1.224261, mean_absolute_error: 34.483550, mean_q: 43.704736, mean_eps: 0.100000
 1739879/2000000: episode: 8148, duration: 1.819s, episode steps: 118, steps per second: 65, episode reward: -40.823, mean reward: -0.346 [-100.000, 13.126], mean action: 2.042 [0.000, 3.000], mean observation: 0.026 [-1.657, 1.000], loss: 1.155556, mean_absolute_error: 36.844630, mean_q: 47.159973, mean_eps: 0.100000
 1739993/2000000: episode: 8149, duration: 1.792s, episode steps: 114, steps per second: 64, episode reward: -99.771, mean reward: -0.875 [-100.000, 19.592], mean action: 1.877 [0.000, 3.000], mean observation: 0.055 [-0.959, 1.262], loss: 0.864310, mean_absolute_error: 35.874501, mean_q: 45.401451, mean_eps: 0.100000
 1740198/2000000: episode: 8150, duration: 3.187s, episode steps: 205, steps per second: 64, episode reward: 203.425, mean reward: 0.992 [-3.238, 100.000], mean action: 1.312 [0.000, 3.000], mean observation: 0.075 [-0.921, 1.000], loss: 0.854028, mean_absolute_error: 35.931049, mean_q: 46.872625, mean_eps: 0.100000
 1740596/2000000: episode: 8151, duration: 6.411s, episode steps: 398, steps per second: 62, episode reward: -188.780, mean reward: -0.474 [-100.000, 4.579], mean action: 1.681 [0.000, 3.000], mean observation: 0.166 [-0.851, 1.003], loss: 1.172439, mean_absolute_error: 35.717666, mean_q: 45.901139, mean_eps: 0.100000
 1741596/2000000: episode: 8152, duration: 17.798s, episode steps: 1000, steps per second: 56, episode reward: -94.486, mean reward: -0.094 [-5.201, 5.561], mean action: 1.825 [0.000, 3.000], mean observation: 0.127 [-0.786, 0.937], loss: 1.329581, mean_absolute_error: 36.406404, mean_q: 45.858553, mean_eps: 0.100000
 1741835/2000000: episode: 8153, duration: 3.758s, episode steps: 239, steps per second: 64, episode reward: 188.855, mean reward: 0.790 [-16.576, 100.000], mean action: 2.297 [0.000, 3.000], mean observation: 0.070 [-0.966, 1.144], loss: 1.018569, mean_absolute_error: 35.980582, mean_q: 45.923901, mean_eps: 0.100000
 1742088/2000000: episode: 8154, duration: 3.996s, episode steps: 253, steps per second: 63, episode reward: 198.141, mean reward: 0.783 [-17.708, 100.000], mean action: 0.791 [0.000, 3.000], mean observation: 0.116 [-0.951, 1.000], loss: 1.476793, mean_absolute_error: 35.721598, mean_q: 45.176656, mean_eps: 0.100000
 1742295/2000000: episode: 8155, duration: 3.275s, episode steps: 207, steps per second: 63, episode reward: 200.469, mean reward: 0.968 [-8.106, 100.000], mean action: 1.333 [0.000, 3.000], mean observation: 0.086 [-0.918, 1.000], loss: 1.208068, mean_absolute_error: 36.075397, mean_q: 45.897813, mean_eps: 0.100000
 1742436/2000000: episode: 8156, duration: 2.227s, episode steps: 141, steps per second: 63, episode reward: -474.240, mean reward: -3.363 [-100.000, 13.478], mean action: 2.000 [0.000, 3.000], mean observation: 0.006 [-1.352, 2.681], loss: 1.059861, mean_absolute_error: 35.554368, mean_q: 45.902821, mean_eps: 0.100000
 1743091/2000000: episode: 8157, duration: 10.638s, episode steps: 655, steps per second: 62, episode reward: 198.288, mean reward: 0.303 [-18.905, 100.000], mean action: 0.823 [0.000, 3.000], mean observation: 0.157 [-1.295, 1.000], loss: 1.297744, mean_absolute_error: 35.996183, mean_q: 45.991884, mean_eps: 0.100000
 1743396/2000000: episode: 8158, duration: 4.861s, episode steps: 305, steps per second: 63, episode reward: 181.459, mean reward: 0.595 [-15.188, 100.000], mean action: 1.374 [0.000, 3.000], mean observation: 0.064 [-1.082, 1.090], loss: 1.195980, mean_absolute_error: 34.976253, mean_q: 44.977625, mean_eps: 0.100000
 1743629/2000000: episode: 8159, duration: 3.703s, episode steps: 233, steps per second: 63, episode reward: -57.971, mean reward: -0.249 [-100.000, 10.622], mean action: 1.678 [0.000, 3.000], mean observation: 0.090 [-0.968, 1.000], loss: 1.144610, mean_absolute_error: 35.570841, mean_q: 45.661037, mean_eps: 0.100000
 1743891/2000000: episode: 8160, duration: 4.426s, episode steps: 262, steps per second: 59, episode reward: 234.026, mean reward: 0.893 [-17.865, 100.000], mean action: 1.656 [0.000, 3.000], mean observation: 0.124 [-0.996, 1.132], loss: 1.223804, mean_absolute_error: 35.696847, mean_q: 44.824374, mean_eps: 0.100000
 1744291/2000000: episode: 8161, duration: 6.308s, episode steps: 400, steps per second: 63, episode reward: 175.789, mean reward: 0.439 [-20.415, 100.000], mean action: 0.828 [0.000, 3.000], mean observation: 0.195 [-0.952, 1.000], loss: 1.222111, mean_absolute_error: 35.147149, mean_q: 44.425678, mean_eps: 0.100000
 1744584/2000000: episode: 8162, duration: 4.639s, episode steps: 293, steps per second: 63, episode reward: 186.265, mean reward: 0.636 [-18.410, 100.000], mean action: 1.003 [0.000, 3.000], mean observation: 0.141 [-1.126, 1.000], loss: 1.261748, mean_absolute_error: 35.889232, mean_q: 45.777865, mean_eps: 0.100000
 1744923/2000000: episode: 8163, duration: 5.323s, episode steps: 339, steps per second: 64, episode reward: 232.000, mean reward: 0.684 [-19.825, 100.000], mean action: 1.342 [0.000, 3.000], mean observation: 0.156 [-0.847, 1.000], loss: 1.321799, mean_absolute_error: 35.898314, mean_q: 46.249280, mean_eps: 0.100000
 1745521/2000000: episode: 8164, duration: 9.644s, episode steps: 598, steps per second: 62, episode reward: 173.556, mean reward: 0.290 [-18.237, 100.000], mean action: 0.866 [0.000, 3.000], mean observation: 0.141 [-1.527, 1.000], loss: 1.314052, mean_absolute_error: 36.168369, mean_q: 45.920102, mean_eps: 0.100000
 1745820/2000000: episode: 8165, duration: 5.044s, episode steps: 299, steps per second: 59, episode reward: 204.463, mean reward: 0.684 [-10.077, 100.000], mean action: 1.482 [0.000, 3.000], mean observation: 0.027 [-0.915, 1.007], loss: 1.268840, mean_absolute_error: 34.984386, mean_q: 44.613271, mean_eps: 0.100000
 1746115/2000000: episode: 8166, duration: 4.778s, episode steps: 295, steps per second: 62, episode reward: 191.799, mean reward: 0.650 [-17.699, 100.000], mean action: 1.163 [0.000, 3.000], mean observation: 0.086 [-1.011, 1.000], loss: 1.065934, mean_absolute_error: 34.567452, mean_q: 44.566399, mean_eps: 0.100000
 1746233/2000000: episode: 8167, duration: 2.020s, episode steps: 118, steps per second: 58, episode reward: -42.244, mean reward: -0.358 [-100.000, 18.278], mean action: 1.822 [0.000, 3.000], mean observation: -0.086 [-0.990, 1.448], loss: 1.143123, mean_absolute_error: 35.986279, mean_q: 46.098136, mean_eps: 0.100000
 1746386/2000000: episode: 8168, duration: 2.373s, episode steps: 153, steps per second: 64, episode reward: 11.630, mean reward: 0.076 [-100.000, 12.556], mean action: 1.941 [0.000, 3.000], mean observation: -0.058 [-0.974, 1.000], loss: 1.382421, mean_absolute_error: 35.272923, mean_q: 45.541607, mean_eps: 0.100000
 1746757/2000000: episode: 8169, duration: 5.909s, episode steps: 371, steps per second: 63, episode reward: 227.463, mean reward: 0.613 [-18.532, 100.000], mean action: 1.453 [0.000, 3.000], mean observation: 0.052 [-0.746, 1.000], loss: 1.202112, mean_absolute_error: 35.719433, mean_q: 46.035037, mean_eps: 0.100000
 1747467/2000000: episode: 8170, duration: 11.248s, episode steps: 710, steps per second: 63, episode reward: 215.833, mean reward: 0.304 [-18.888, 100.000], mean action: 0.838 [0.000, 3.000], mean observation: 0.145 [-1.044, 1.000], loss: 1.163933, mean_absolute_error: 35.379449, mean_q: 44.749733, mean_eps: 0.100000
 1747653/2000000: episode: 8171, duration: 2.910s, episode steps: 186, steps per second: 64, episode reward: -47.485, mean reward: -0.255 [-100.000, 16.326], mean action: 1.812 [0.000, 3.000], mean observation: -0.035 [-1.208, 1.228], loss: 1.313088, mean_absolute_error: 35.304715, mean_q: 44.083876, mean_eps: 0.100000
 1748071/2000000: episode: 8172, duration: 6.687s, episode steps: 418, steps per second: 63, episode reward: 228.567, mean reward: 0.547 [-18.062, 100.000], mean action: 1.100 [0.000, 3.000], mean observation: 0.071 [-0.968, 1.000], loss: 1.143731, mean_absolute_error: 35.608723, mean_q: 45.547997, mean_eps: 0.100000
 1748341/2000000: episode: 8173, duration: 4.293s, episode steps: 270, steps per second: 63, episode reward: 134.767, mean reward: 0.499 [-4.910, 100.000], mean action: 2.067 [0.000, 3.000], mean observation: 0.044 [-0.951, 1.000], loss: 1.162028, mean_absolute_error: 35.017296, mean_q: 44.454413, mean_eps: 0.100000
 1748521/2000000: episode: 8174, duration: 2.883s, episode steps: 180, steps per second: 62, episode reward: -239.108, mean reward: -1.328 [-100.000, 5.259], mean action: 1.517 [0.000, 3.000], mean observation: 0.114 [-0.997, 1.082], loss: 1.211400, mean_absolute_error: 34.767183, mean_q: 44.635163, mean_eps: 0.100000
 1748891/2000000: episode: 8175, duration: 5.904s, episode steps: 370, steps per second: 63, episode reward: 194.753, mean reward: 0.526 [-19.312, 100.000], mean action: 1.084 [0.000, 3.000], mean observation: 0.111 [-1.156, 1.000], loss: 1.054796, mean_absolute_error: 34.835688, mean_q: 44.698875, mean_eps: 0.100000
 1749153/2000000: episode: 8176, duration: 4.112s, episode steps: 262, steps per second: 64, episode reward: 199.806, mean reward: 0.763 [-8.639, 100.000], mean action: 1.221 [0.000, 3.000], mean observation: 0.079 [-1.074, 1.000], loss: 1.401067, mean_absolute_error: 36.571382, mean_q: 45.300764, mean_eps: 0.100000
 1749643/2000000: episode: 8177, duration: 7.796s, episode steps: 490, steps per second: 63, episode reward: 195.747, mean reward: 0.399 [-18.717, 100.000], mean action: 1.088 [0.000, 3.000], mean observation: 0.064 [-1.348, 1.000], loss: 1.101743, mean_absolute_error: 35.913417, mean_q: 45.816615, mean_eps: 0.100000
 1749749/2000000: episode: 8178, duration: 1.686s, episode steps: 106, steps per second: 63, episode reward: -63.189, mean reward: -0.596 [-100.000, 9.493], mean action: 1.519 [0.000, 3.000], mean observation: -0.084 [-0.926, 1.660], loss: 1.310723, mean_absolute_error: 36.489843, mean_q: 45.040803, mean_eps: 0.100000
 1750192/2000000: episode: 8179, duration: 7.118s, episode steps: 443, steps per second: 62, episode reward: 183.683, mean reward: 0.415 [-20.891, 100.000], mean action: 1.052 [0.000, 3.000], mean observation: 0.142 [-0.992, 1.000], loss: 1.123159, mean_absolute_error: 36.194517, mean_q: 46.377038, mean_eps: 0.100000
 1750616/2000000: episode: 8180, duration: 6.715s, episode steps: 424, steps per second: 63, episode reward: 259.417, mean reward: 0.612 [-17.333, 100.000], mean action: 1.370 [0.000, 3.000], mean observation: 0.092 [-1.428, 1.000], loss: 1.401451, mean_absolute_error: 35.358504, mean_q: 44.084977, mean_eps: 0.100000
 1750906/2000000: episode: 8181, duration: 4.586s, episode steps: 290, steps per second: 63, episode reward: 262.575, mean reward: 0.905 [-11.587, 100.000], mean action: 1.276 [0.000, 3.000], mean observation: 0.089 [-0.919, 1.179], loss: 0.941299, mean_absolute_error: 35.189045, mean_q: 44.240251, mean_eps: 0.100000
 1751521/2000000: episode: 8182, duration: 9.959s, episode steps: 615, steps per second: 62, episode reward: 198.861, mean reward: 0.323 [-22.815, 100.000], mean action: 0.987 [0.000, 3.000], mean observation: 0.213 [-1.023, 1.015], loss: 1.123560, mean_absolute_error: 35.387970, mean_q: 45.138976, mean_eps: 0.100000
 1751931/2000000: episode: 8183, duration: 6.547s, episode steps: 410, steps per second: 63, episode reward: 225.380, mean reward: 0.550 [-18.978, 100.000], mean action: 0.868 [0.000, 3.000], mean observation: 0.161 [-0.942, 1.000], loss: 1.150439, mean_absolute_error: 36.462196, mean_q: 46.322110, mean_eps: 0.100000
 1752135/2000000: episode: 8184, duration: 3.236s, episode steps: 204, steps per second: 63, episode reward: -262.583, mean reward: -1.287 [-100.000, 27.195], mean action: 1.833 [0.000, 3.000], mean observation: 0.094 [-1.227, 1.005], loss: 1.281024, mean_absolute_error: 36.020088, mean_q: 45.081702, mean_eps: 0.100000
 1752455/2000000: episode: 8185, duration: 5.193s, episode steps: 320, steps per second: 62, episode reward: 126.100, mean reward: 0.394 [-18.607, 100.000], mean action: 1.319 [0.000, 3.000], mean observation: 0.074 [-1.398, 1.000], loss: 1.270116, mean_absolute_error: 35.320858, mean_q: 44.493128, mean_eps: 0.100000
 1752707/2000000: episode: 8186, duration: 3.983s, episode steps: 252, steps per second: 63, episode reward: 196.239, mean reward: 0.779 [-4.002, 100.000], mean action: 1.365 [0.000, 3.000], mean observation: 0.074 [-0.894, 1.000], loss: 1.637239, mean_absolute_error: 36.583637, mean_q: 45.558188, mean_eps: 0.100000
 1752997/2000000: episode: 8187, duration: 4.605s, episode steps: 290, steps per second: 63, episode reward: 170.630, mean reward: 0.588 [-7.106, 100.000], mean action: 1.390 [0.000, 3.000], mean observation: 0.115 [-1.967, 1.000], loss: 1.103694, mean_absolute_error: 34.917304, mean_q: 44.916845, mean_eps: 0.100000
 1753284/2000000: episode: 8188, duration: 4.721s, episode steps: 287, steps per second: 61, episode reward: 194.980, mean reward: 0.679 [-8.307, 100.000], mean action: 1.655 [0.000, 3.000], mean observation: 0.119 [-0.899, 1.000], loss: 1.100427, mean_absolute_error: 36.800336, mean_q: 47.082792, mean_eps: 0.100000
 1753861/2000000: episode: 8189, duration: 9.249s, episode steps: 577, steps per second: 62, episode reward: 196.893, mean reward: 0.341 [-17.777, 100.000], mean action: 1.712 [0.000, 3.000], mean observation: 0.136 [-0.937, 1.000], loss: 1.233082, mean_absolute_error: 36.065574, mean_q: 46.007570, mean_eps: 0.100000
 1754076/2000000: episode: 8190, duration: 3.406s, episode steps: 215, steps per second: 63, episode reward: -53.377, mean reward: -0.248 [-100.000, 15.530], mean action: 1.786 [0.000, 3.000], mean observation: -0.024 [-0.954, 1.000], loss: 0.997488, mean_absolute_error: 36.036347, mean_q: 45.728190, mean_eps: 0.100000
 1754729/2000000: episode: 8191, duration: 10.401s, episode steps: 653, steps per second: 63, episode reward: 219.990, mean reward: 0.337 [-18.240, 100.000], mean action: 0.645 [0.000, 3.000], mean observation: 0.188 [-1.133, 1.000], loss: 1.087043, mean_absolute_error: 35.998250, mean_q: 45.676006, mean_eps: 0.100000
 1755232/2000000: episode: 8192, duration: 7.999s, episode steps: 503, steps per second: 63, episode reward: 145.767, mean reward: 0.290 [-17.205, 100.000], mean action: 0.966 [0.000, 3.000], mean observation: 0.180 [-1.111, 1.000], loss: 1.227049, mean_absolute_error: 35.768562, mean_q: 45.397674, mean_eps: 0.100000
 1755340/2000000: episode: 8193, duration: 1.757s, episode steps: 108, steps per second: 61, episode reward: -172.956, mean reward: -1.601 [-100.000, 16.102], mean action: 1.907 [0.000, 3.000], mean observation: -0.067 [-1.930, 1.000], loss: 1.441212, mean_absolute_error: 36.324095, mean_q: 44.931482, mean_eps: 0.100000
 1755480/2000000: episode: 8194, duration: 2.256s, episode steps: 140, steps per second: 62, episode reward: -46.948, mean reward: -0.335 [-100.000, 12.063], mean action: 1.571 [0.000, 3.000], mean observation: -0.074 [-0.857, 1.000], loss: 1.199045, mean_absolute_error: 35.772536, mean_q: 45.550547, mean_eps: 0.100000
 1755593/2000000: episode: 8195, duration: 1.842s, episode steps: 113, steps per second: 61, episode reward: -205.574, mean reward: -1.819 [-100.000, 17.632], mean action: 1.903 [0.000, 3.000], mean observation: -0.053 [-2.100, 1.000], loss: 0.923787, mean_absolute_error: 34.561493, mean_q: 44.073389, mean_eps: 0.100000
 1755917/2000000: episode: 8196, duration: 5.140s, episode steps: 324, steps per second: 63, episode reward: 171.560, mean reward: 0.530 [-14.275, 100.000], mean action: 1.253 [0.000, 3.000], mean observation: 0.070 [-1.019, 1.000], loss: 1.237962, mean_absolute_error: 37.008153, mean_q: 47.332244, mean_eps: 0.100000
 1756345/2000000: episode: 8197, duration: 6.846s, episode steps: 428, steps per second: 63, episode reward: 201.540, mean reward: 0.471 [-19.888, 100.000], mean action: 0.944 [0.000, 3.000], mean observation: 0.161 [-1.202, 1.000], loss: 1.178509, mean_absolute_error: 35.554516, mean_q: 45.078649, mean_eps: 0.100000
 1756697/2000000: episode: 8198, duration: 5.526s, episode steps: 352, steps per second: 64, episode reward: 189.908, mean reward: 0.540 [-18.210, 100.000], mean action: 1.199 [0.000, 3.000], mean observation: 0.150 [-0.990, 1.000], loss: 1.398030, mean_absolute_error: 35.803233, mean_q: 46.075680, mean_eps: 0.100000
 1757272/2000000: episode: 8199, duration: 9.070s, episode steps: 575, steps per second: 63, episode reward: 196.005, mean reward: 0.341 [-17.574, 100.000], mean action: 0.892 [0.000, 3.000], mean observation: 0.131 [-1.258, 1.000], loss: 1.112021, mean_absolute_error: 34.963992, mean_q: 44.621909, mean_eps: 0.100000
 1757501/2000000: episode: 8200, duration: 3.636s, episode steps: 229, steps per second: 63, episode reward: -63.461, mean reward: -0.277 [-100.000, 14.674], mean action: 1.790 [0.000, 3.000], mean observation: 0.105 [-0.964, 1.071], loss: 1.100694, mean_absolute_error: 36.652690, mean_q: 46.005147, mean_eps: 0.100000
 1758019/2000000: episode: 8201, duration: 8.496s, episode steps: 518, steps per second: 61, episode reward: 185.754, mean reward: 0.359 [-19.430, 100.000], mean action: 1.160 [0.000, 3.000], mean observation: 0.108 [-1.039, 1.000], loss: 1.226286, mean_absolute_error: 35.249685, mean_q: 44.625323, mean_eps: 0.100000
 1758236/2000000: episode: 8202, duration: 3.439s, episode steps: 217, steps per second: 63, episode reward: 170.107, mean reward: 0.784 [-13.958, 100.000], mean action: 1.401 [0.000, 3.000], mean observation: 0.051 [-0.942, 1.000], loss: 1.340006, mean_absolute_error: 35.375633, mean_q: 44.353019, mean_eps: 0.100000
 1758621/2000000: episode: 8203, duration: 6.104s, episode steps: 385, steps per second: 63, episode reward: -101.635, mean reward: -0.264 [-100.000, 15.953], mean action: 1.862 [0.000, 3.000], mean observation: -0.083 [-0.976, 1.121], loss: 1.041524, mean_absolute_error: 36.507425, mean_q: 45.917115, mean_eps: 0.100000
 1758927/2000000: episode: 8204, duration: 4.799s, episode steps: 306, steps per second: 64, episode reward: -152.950, mean reward: -0.500 [-100.000, 5.962], mean action: 1.843 [0.000, 3.000], mean observation: 0.154 [-0.975, 1.001], loss: 1.167245, mean_absolute_error: 36.065298, mean_q: 46.413776, mean_eps: 0.100000
 1759165/2000000: episode: 8205, duration: 3.763s, episode steps: 238, steps per second: 63, episode reward: 212.030, mean reward: 0.891 [-10.644, 100.000], mean action: 1.097 [0.000, 3.000], mean observation: 0.127 [-1.312, 1.000], loss: 1.359045, mean_absolute_error: 34.896693, mean_q: 43.665418, mean_eps: 0.100000
 1759310/2000000: episode: 8206, duration: 2.247s, episode steps: 145, steps per second: 65, episode reward: -401.277, mean reward: -2.767 [-100.000, 15.182], mean action: 1.731 [0.000, 3.000], mean observation: 0.060 [-0.933, 2.427], loss: 1.032637, mean_absolute_error: 36.772973, mean_q: 46.730997, mean_eps: 0.100000
 1759461/2000000: episode: 8207, duration: 2.370s, episode steps: 151, steps per second: 64, episode reward: -103.030, mean reward: -0.682 [-100.000, 17.621], mean action: 1.861 [0.000, 3.000], mean observation: -0.028 [-0.969, 1.000], loss: 1.070405, mean_absolute_error: 35.445831, mean_q: 44.697078, mean_eps: 0.100000
 1759717/2000000: episode: 8208, duration: 4.017s, episode steps: 256, steps per second: 64, episode reward: 229.207, mean reward: 0.895 [-18.384, 100.000], mean action: 1.164 [0.000, 3.000], mean observation: 0.071 [-0.902, 1.000], loss: 1.308948, mean_absolute_error: 35.342621, mean_q: 44.919924, mean_eps: 0.100000
 1760180/2000000: episode: 8209, duration: 7.767s, episode steps: 463, steps per second: 60, episode reward: -70.115, mean reward: -0.151 [-100.000, 15.722], mean action: 1.799 [0.000, 3.000], mean observation: -0.072 [-0.887, 1.020], loss: 1.380260, mean_absolute_error: 35.354331, mean_q: 45.029671, mean_eps: 0.100000
 1760325/2000000: episode: 8210, duration: 2.325s, episode steps: 145, steps per second: 62, episode reward: -43.042, mean reward: -0.297 [-100.000, 15.581], mean action: 1.407 [0.000, 3.000], mean observation: -0.029 [-1.121, 1.000], loss: 1.540350, mean_absolute_error: 36.471686, mean_q: 45.051950, mean_eps: 0.100000
 1760670/2000000: episode: 8211, duration: 5.723s, episode steps: 345, steps per second: 60, episode reward: 158.795, mean reward: 0.460 [-17.856, 100.000], mean action: 1.243 [0.000, 3.000], mean observation: 0.137 [-1.023, 1.000], loss: 1.183929, mean_absolute_error: 34.228879, mean_q: 43.270202, mean_eps: 0.100000
 1760960/2000000: episode: 8212, duration: 4.618s, episode steps: 290, steps per second: 63, episode reward: 215.537, mean reward: 0.743 [-17.609, 100.000], mean action: 1.472 [0.000, 3.000], mean observation: 0.166 [-0.907, 1.387], loss: 1.253740, mean_absolute_error: 35.710802, mean_q: 45.364410, mean_eps: 0.100000
 1761129/2000000: episode: 8213, duration: 2.746s, episode steps: 169, steps per second: 62, episode reward: -14.429, mean reward: -0.085 [-100.000, 20.601], mean action: 1.710 [0.000, 3.000], mean observation: 0.068 [-0.975, 1.000], loss: 1.125852, mean_absolute_error: 36.256751, mean_q: 46.081867, mean_eps: 0.100000
 1761439/2000000: episode: 8214, duration: 4.990s, episode steps: 310, steps per second: 62, episode reward: -213.145, mean reward: -0.688 [-100.000, 5.356], mean action: 1.771 [0.000, 3.000], mean observation: 0.129 [-1.004, 1.003], loss: 1.216719, mean_absolute_error: 35.443177, mean_q: 45.642976, mean_eps: 0.100000
 1761951/2000000: episode: 8215, duration: 8.174s, episode steps: 512, steps per second: 63, episode reward: 183.753, mean reward: 0.359 [-12.483, 100.000], mean action: 2.518 [0.000, 3.000], mean observation: 0.088 [-0.984, 1.000], loss: 1.377275, mean_absolute_error: 35.222467, mean_q: 44.601893, mean_eps: 0.100000
 1762707/2000000: episode: 8216, duration: 12.134s, episode steps: 756, steps per second: 62, episode reward: 155.792, mean reward: 0.206 [-18.003, 100.000], mean action: 2.479 [0.000, 3.000], mean observation: 0.082 [-1.225, 1.000], loss: 1.180989, mean_absolute_error: 35.905289, mean_q: 45.483493, mean_eps: 0.100000
 1763027/2000000: episode: 8217, duration: 5.079s, episode steps: 320, steps per second: 63, episode reward: 206.432, mean reward: 0.645 [-12.482, 100.000], mean action: 1.188 [0.000, 3.000], mean observation: 0.089 [-0.902, 1.000], loss: 1.491617, mean_absolute_error: 36.075409, mean_q: 44.883038, mean_eps: 0.100000
 1763459/2000000: episode: 8218, duration: 6.867s, episode steps: 432, steps per second: 63, episode reward: -126.946, mean reward: -0.294 [-100.000, 15.734], mean action: 1.794 [0.000, 3.000], mean observation: -0.042 [-0.794, 1.000], loss: 1.111276, mean_absolute_error: 35.705137, mean_q: 45.204276, mean_eps: 0.100000
 1763605/2000000: episode: 8219, duration: 2.318s, episode steps: 146, steps per second: 63, episode reward: -40.822, mean reward: -0.280 [-100.000, 10.057], mean action: 1.534 [0.000, 3.000], mean observation: 0.039 [-1.745, 1.004], loss: 1.250939, mean_absolute_error: 36.387465, mean_q: 45.762408, mean_eps: 0.100000
 1763855/2000000: episode: 8220, duration: 3.887s, episode steps: 250, steps per second: 64, episode reward: 227.916, mean reward: 0.912 [-5.100, 100.000], mean action: 1.376 [0.000, 3.000], mean observation: 0.072 [-0.938, 1.000], loss: 1.110027, mean_absolute_error: 36.184727, mean_q: 46.341029, mean_eps: 0.100000
 1764163/2000000: episode: 8221, duration: 4.792s, episode steps: 308, steps per second: 64, episode reward: -87.192, mean reward: -0.283 [-100.000, 18.567], mean action: 1.841 [0.000, 3.000], mean observation: -0.051 [-0.961, 1.021], loss: 1.447500, mean_absolute_error: 35.583905, mean_q: 45.178964, mean_eps: 0.100000
 1764555/2000000: episode: 8222, duration: 6.219s, episode steps: 392, steps per second: 63, episode reward: 217.374, mean reward: 0.555 [-18.735, 100.000], mean action: 1.145 [0.000, 3.000], mean observation: 0.093 [-0.957, 1.000], loss: 1.395450, mean_absolute_error: 34.976456, mean_q: 43.906428, mean_eps: 0.100000
 1764649/2000000: episode: 8223, duration: 1.483s, episode steps: 94, steps per second: 63, episode reward: -37.012, mean reward: -0.394 [-100.000, 11.000], mean action: 1.691 [0.000, 3.000], mean observation: -0.129 [-1.008, 1.777], loss: 1.306715, mean_absolute_error: 36.053978, mean_q: 45.673462, mean_eps: 0.100000
 1765000/2000000: episode: 8224, duration: 5.492s, episode steps: 351, steps per second: 64, episode reward: 245.327, mean reward: 0.699 [-8.862, 100.000], mean action: 1.459 [0.000, 3.000], mean observation: 0.007 [-0.988, 1.000], loss: 1.162571, mean_absolute_error: 36.075195, mean_q: 45.439594, mean_eps: 0.100000
 1765344/2000000: episode: 8225, duration: 5.436s, episode steps: 344, steps per second: 63, episode reward: 221.381, mean reward: 0.644 [-18.321, 100.000], mean action: 1.235 [0.000, 3.000], mean observation: 0.098 [-0.965, 1.015], loss: 0.984862, mean_absolute_error: 36.002906, mean_q: 45.377249, mean_eps: 0.100000
 1765881/2000000: episode: 8226, duration: 8.722s, episode steps: 537, steps per second: 62, episode reward: 200.640, mean reward: 0.374 [-18.537, 100.000], mean action: 0.965 [0.000, 3.000], mean observation: 0.143 [-0.980, 1.000], loss: 1.389828, mean_absolute_error: 36.149738, mean_q: 45.900451, mean_eps: 0.100000
 1766348/2000000: episode: 8227, duration: 7.271s, episode steps: 467, steps per second: 64, episode reward: 191.776, mean reward: 0.411 [-17.751, 100.000], mean action: 0.737 [0.000, 3.000], mean observation: 0.168 [-1.418, 1.000], loss: 1.090438, mean_absolute_error: 35.775533, mean_q: 44.739488, mean_eps: 0.100000
 1766471/2000000: episode: 8228, duration: 1.905s, episode steps: 123, steps per second: 65, episode reward: -131.858, mean reward: -1.072 [-100.000, 17.603], mean action: 1.610 [0.000, 3.000], mean observation: -0.032 [-1.550, 1.000], loss: 1.008402, mean_absolute_error: 34.604176, mean_q: 42.842578, mean_eps: 0.100000
 1767174/2000000: episode: 8229, duration: 11.516s, episode steps: 703, steps per second: 61, episode reward: 109.477, mean reward: 0.156 [-18.446, 100.000], mean action: 2.145 [0.000, 3.000], mean observation: 0.042 [-0.852, 1.000], loss: 1.234838, mean_absolute_error: 35.736227, mean_q: 45.511351, mean_eps: 0.100000
 1767327/2000000: episode: 8230, duration: 2.374s, episode steps: 153, steps per second: 64, episode reward: -7.377, mean reward: -0.048 [-100.000, 15.270], mean action: 1.876 [0.000, 3.000], mean observation: 0.070 [-0.993, 1.006], loss: 1.110273, mean_absolute_error: 35.218205, mean_q: 45.183634, mean_eps: 0.100000
 1767742/2000000: episode: 8231, duration: 6.590s, episode steps: 415, steps per second: 63, episode reward: -204.612, mean reward: -0.493 [-100.000, 4.593], mean action: 1.557 [0.000, 3.000], mean observation: 0.165 [-0.990, 1.001], loss: 1.149863, mean_absolute_error: 36.701966, mean_q: 46.810525, mean_eps: 0.100000
 1767863/2000000: episode: 8232, duration: 1.847s, episode steps: 121, steps per second: 66, episode reward: -71.712, mean reward: -0.593 [-100.000, 14.983], mean action: 1.860 [0.000, 3.000], mean observation: -0.077 [-0.996, 1.000], loss: 1.259527, mean_absolute_error: 36.766258, mean_q: 46.783098, mean_eps: 0.100000
 1768152/2000000: episode: 8233, duration: 4.566s, episode steps: 289, steps per second: 63, episode reward: 202.978, mean reward: 0.702 [-8.287, 100.000], mean action: 1.166 [0.000, 3.000], mean observation: 0.105 [-0.958, 1.000], loss: 1.133128, mean_absolute_error: 35.479788, mean_q: 44.544052, mean_eps: 0.100000
 1768276/2000000: episode: 8234, duration: 2.092s, episode steps: 124, steps per second: 59, episode reward: -85.032, mean reward: -0.686 [-100.000, 18.817], mean action: 1.669 [0.000, 3.000], mean observation: 0.083 [-1.006, 1.546], loss: 1.228623, mean_absolute_error: 34.851597, mean_q: 44.541176, mean_eps: 0.100000
 1768383/2000000: episode: 8235, duration: 1.681s, episode steps: 107, steps per second: 64, episode reward: -4.653, mean reward: -0.043 [-100.000, 16.591], mean action: 1.813 [0.000, 3.000], mean observation: -0.087 [-1.043, 1.333], loss: 1.225108, mean_absolute_error: 34.835745, mean_q: 44.895746, mean_eps: 0.100000
 1768725/2000000: episode: 8236, duration: 5.394s, episode steps: 342, steps per second: 63, episode reward: 229.878, mean reward: 0.672 [-17.642, 100.000], mean action: 0.792 [0.000, 3.000], mean observation: 0.152 [-1.000, 1.000], loss: 1.434999, mean_absolute_error: 35.909805, mean_q: 45.462869, mean_eps: 0.100000
 1769212/2000000: episode: 8237, duration: 7.728s, episode steps: 487, steps per second: 63, episode reward: 193.092, mean reward: 0.396 [-18.175, 100.000], mean action: 1.211 [0.000, 3.000], mean observation: 0.058 [-1.056, 1.000], loss: 1.107279, mean_absolute_error: 36.098395, mean_q: 46.435919, mean_eps: 0.100000
 1769997/2000000: episode: 8238, duration: 12.864s, episode steps: 785, steps per second: 61, episode reward: 175.517, mean reward: 0.224 [-17.154, 100.000], mean action: 2.410 [0.000, 3.000], mean observation: 0.131 [-0.909, 1.000], loss: 1.214282, mean_absolute_error: 35.248286, mean_q: 45.561808, mean_eps: 0.100000
 1770354/2000000: episode: 8239, duration: 5.589s, episode steps: 357, steps per second: 64, episode reward: -235.107, mean reward: -0.659 [-100.000, 5.146], mean action: 1.591 [0.000, 3.000], mean observation: 0.179 [-0.846, 1.013], loss: 0.999872, mean_absolute_error: 34.580049, mean_q: 44.663102, mean_eps: 0.100000
 1770637/2000000: episode: 8240, duration: 4.448s, episode steps: 283, steps per second: 64, episode reward: -38.922, mean reward: -0.138 [-100.000, 15.061], mean action: 1.364 [0.000, 3.000], mean observation: -0.030 [-0.841, 1.000], loss: 1.337982, mean_absolute_error: 35.402477, mean_q: 45.460534, mean_eps: 0.100000
 1770848/2000000: episode: 8241, duration: 3.250s, episode steps: 211, steps per second: 65, episode reward: -287.614, mean reward: -1.363 [-100.000, 14.606], mean action: 1.555 [0.000, 3.000], mean observation: -0.090 [-2.301, 1.000], loss: 1.229772, mean_absolute_error: 34.588062, mean_q: 44.208411, mean_eps: 0.100000
 1771264/2000000: episode: 8242, duration: 6.569s, episode steps: 416, steps per second: 63, episode reward: 194.429, mean reward: 0.467 [-19.496, 100.000], mean action: 1.620 [0.000, 3.000], mean observation: 0.202 [-0.791, 1.000], loss: 1.549892, mean_absolute_error: 36.188503, mean_q: 46.298089, mean_eps: 0.100000
 1772025/2000000: episode: 8243, duration: 12.337s, episode steps: 761, steps per second: 62, episode reward: 204.135, mean reward: 0.268 [-18.618, 100.000], mean action: 2.016 [0.000, 3.000], mean observation: 0.195 [-1.285, 1.000], loss: 1.243188, mean_absolute_error: 35.832143, mean_q: 45.381479, mean_eps: 0.100000
 1772211/2000000: episode: 8244, duration: 2.868s, episode steps: 186, steps per second: 65, episode reward: 212.949, mean reward: 1.145 [-3.326, 100.000], mean action: 1.398 [0.000, 3.000], mean observation: 0.019 [-0.910, 1.000], loss: 1.208956, mean_absolute_error: 34.997044, mean_q: 44.777304, mean_eps: 0.100000
 1772732/2000000: episode: 8245, duration: 8.326s, episode steps: 521, steps per second: 63, episode reward: 195.703, mean reward: 0.376 [-21.079, 100.000], mean action: 0.781 [0.000, 3.000], mean observation: 0.146 [-1.452, 1.000], loss: 1.313997, mean_absolute_error: 35.365787, mean_q: 44.326290, mean_eps: 0.100000
 1772867/2000000: episode: 8246, duration: 2.097s, episode steps: 135, steps per second: 64, episode reward: -48.093, mean reward: -0.356 [-100.000, 17.289], mean action: 1.333 [0.000, 3.000], mean observation: 0.086 [-1.854, 1.011], loss: 1.381086, mean_absolute_error: 36.586669, mean_q: 46.121394, mean_eps: 0.100000
 1773032/2000000: episode: 8247, duration: 2.630s, episode steps: 165, steps per second: 63, episode reward: 5.974, mean reward: 0.036 [-100.000, 18.507], mean action: 1.903 [0.000, 3.000], mean observation: 0.078 [-0.829, 1.217], loss: 1.237120, mean_absolute_error: 35.580663, mean_q: 44.646077, mean_eps: 0.100000
 1773237/2000000: episode: 8248, duration: 3.234s, episode steps: 205, steps per second: 63, episode reward: 198.792, mean reward: 0.970 [-10.722, 100.000], mean action: 2.068 [0.000, 3.000], mean observation: 0.018 [-0.964, 1.108], loss: 1.245266, mean_absolute_error: 35.372571, mean_q: 45.446845, mean_eps: 0.100000
 1773384/2000000: episode: 8249, duration: 2.306s, episode steps: 147, steps per second: 64, episode reward: -329.590, mean reward: -2.242 [-100.000, 7.331], mean action: 1.871 [0.000, 3.000], mean observation: 0.119 [-2.110, 3.183], loss: 1.197684, mean_absolute_error: 34.326310, mean_q: 44.036703, mean_eps: 0.100000
 1773484/2000000: episode: 8250, duration: 1.617s, episode steps: 100, steps per second: 62, episode reward: -225.263, mean reward: -2.253 [-100.000, 6.765], mean action: 2.060 [0.000, 3.000], mean observation: -0.173 [-1.365, 1.185], loss: 0.942869, mean_absolute_error: 35.570631, mean_q: 45.079747, mean_eps: 0.100000
 1773601/2000000: episode: 8251, duration: 1.852s, episode steps: 117, steps per second: 63, episode reward: -63.110, mean reward: -0.539 [-100.000, 14.259], mean action: 1.923 [0.000, 3.000], mean observation: -0.071 [-0.910, 1.000], loss: 0.963146, mean_absolute_error: 35.515508, mean_q: 44.729088, mean_eps: 0.100000
 1774010/2000000: episode: 8252, duration: 6.444s, episode steps: 409, steps per second: 63, episode reward: 229.662, mean reward: 0.562 [-18.240, 100.000], mean action: 0.990 [0.000, 3.000], mean observation: 0.137 [-0.994, 1.018], loss: 1.163315, mean_absolute_error: 35.613430, mean_q: 45.777028, mean_eps: 0.100000
 1774123/2000000: episode: 8253, duration: 1.745s, episode steps: 113, steps per second: 65, episode reward: -26.225, mean reward: -0.232 [-100.000, 11.212], mean action: 1.752 [0.000, 3.000], mean observation: -0.068 [-0.911, 1.000], loss: 1.065210, mean_absolute_error: 33.431299, mean_q: 42.853413, mean_eps: 0.100000
 1774257/2000000: episode: 8254, duration: 2.123s, episode steps: 134, steps per second: 63, episode reward: -50.653, mean reward: -0.378 [-100.000, 11.979], mean action: 1.978 [0.000, 3.000], mean observation: -0.063 [-0.943, 1.361], loss: 1.205360, mean_absolute_error: 34.668087, mean_q: 43.851372, mean_eps: 0.100000
 1775257/2000000: episode: 8255, duration: 16.234s, episode steps: 1000, steps per second: 62, episode reward: 78.709, mean reward: 0.079 [-19.768, 21.077], mean action: 1.004 [0.000, 3.000], mean observation: 0.134 [-0.891, 1.000], loss: 1.302661, mean_absolute_error: 35.821463, mean_q: 45.245889, mean_eps: 0.100000
 1775824/2000000: episode: 8256, duration: 9.401s, episode steps: 567, steps per second: 60, episode reward: 194.580, mean reward: 0.343 [-18.691, 100.000], mean action: 0.818 [0.000, 3.000], mean observation: 0.141 [-0.932, 1.000], loss: 1.085055, mean_absolute_error: 35.088097, mean_q: 44.525008, mean_eps: 0.100000
 1776053/2000000: episode: 8257, duration: 3.631s, episode steps: 229, steps per second: 63, episode reward: 135.213, mean reward: 0.590 [-17.930, 100.000], mean action: 1.410 [0.000, 3.000], mean observation: 0.035 [-1.403, 1.000], loss: 1.021756, mean_absolute_error: 34.682647, mean_q: 45.027487, mean_eps: 0.100000
 1776181/2000000: episode: 8258, duration: 2.065s, episode steps: 128, steps per second: 62, episode reward: -270.300, mean reward: -2.112 [-100.000, 30.576], mean action: 1.531 [0.000, 3.000], mean observation: 0.107 [-1.213, 1.856], loss: 1.244051, mean_absolute_error: 34.751088, mean_q: 42.865946, mean_eps: 0.100000
 1776547/2000000: episode: 8259, duration: 5.929s, episode steps: 366, steps per second: 62, episode reward: -87.839, mean reward: -0.240 [-100.000, 21.340], mean action: 1.893 [0.000, 3.000], mean observation: -0.060 [-0.873, 1.939], loss: 1.385224, mean_absolute_error: 36.909469, mean_q: 47.180819, mean_eps: 0.100000
 1776677/2000000: episode: 8260, duration: 2.055s, episode steps: 130, steps per second: 63, episode reward: -2.473, mean reward: -0.019 [-100.000, 12.686], mean action: 1.938 [0.000, 3.000], mean observation: -0.023 [-0.957, 1.000], loss: 1.033113, mean_absolute_error: 34.730022, mean_q: 44.316055, mean_eps: 0.100000
 1777306/2000000: episode: 8261, duration: 10.057s, episode steps: 629, steps per second: 63, episode reward: 175.986, mean reward: 0.280 [-19.344, 100.000], mean action: 0.949 [0.000, 3.000], mean observation: 0.195 [-1.022, 1.000], loss: 1.112286, mean_absolute_error: 35.789129, mean_q: 45.972117, mean_eps: 0.100000
 1777624/2000000: episode: 8262, duration: 5.025s, episode steps: 318, steps per second: 63, episode reward: 230.954, mean reward: 0.726 [-17.603, 100.000], mean action: 1.132 [0.000, 3.000], mean observation: 0.085 [-0.980, 1.000], loss: 1.055449, mean_absolute_error: 35.065416, mean_q: 45.067848, mean_eps: 0.100000
 1777736/2000000: episode: 8263, duration: 1.790s, episode steps: 112, steps per second: 63, episode reward: -53.278, mean reward: -0.476 [-100.000, 16.784], mean action: 1.366 [0.000, 3.000], mean observation: 0.041 [-1.738, 1.000], loss: 1.132633, mean_absolute_error: 34.939298, mean_q: 45.130136, mean_eps: 0.100000
 1777984/2000000: episode: 8264, duration: 3.872s, episode steps: 248, steps per second: 64, episode reward: 215.258, mean reward: 0.868 [-18.566, 100.000], mean action: 0.835 [0.000, 3.000], mean observation: 0.116 [-0.989, 1.000], loss: 1.171362, mean_absolute_error: 35.900390, mean_q: 46.167543, mean_eps: 0.100000
 1778112/2000000: episode: 8265, duration: 2.056s, episode steps: 128, steps per second: 62, episode reward: -20.182, mean reward: -0.158 [-100.000, 13.219], mean action: 1.789 [0.000, 3.000], mean observation: -0.070 [-0.886, 1.264], loss: 1.120411, mean_absolute_error: 35.171801, mean_q: 45.159071, mean_eps: 0.100000
 1778553/2000000: episode: 8266, duration: 7.287s, episode steps: 441, steps per second: 61, episode reward: 176.565, mean reward: 0.400 [-17.903, 100.000], mean action: 2.268 [0.000, 3.000], mean observation: 0.225 [-1.004, 1.000], loss: 1.281761, mean_absolute_error: 34.946341, mean_q: 44.442264, mean_eps: 0.100000
 1778865/2000000: episode: 8267, duration: 4.946s, episode steps: 312, steps per second: 63, episode reward: 183.394, mean reward: 0.588 [-18.947, 100.000], mean action: 0.827 [0.000, 3.000], mean observation: 0.095 [-1.490, 1.000], loss: 1.262071, mean_absolute_error: 35.077208, mean_q: 44.678716, mean_eps: 0.100000
 1779010/2000000: episode: 8268, duration: 2.222s, episode steps: 145, steps per second: 65, episode reward: -145.726, mean reward: -1.005 [-100.000, 18.650], mean action: 1.248 [0.000, 3.000], mean observation: -0.048 [-1.798, 1.000], loss: 1.296878, mean_absolute_error: 36.108396, mean_q: 46.503342, mean_eps: 0.100000
 1779371/2000000: episode: 8269, duration: 5.784s, episode steps: 361, steps per second: 62, episode reward: 225.497, mean reward: 0.625 [-17.413, 100.000], mean action: 1.055 [0.000, 3.000], mean observation: 0.146 [-0.972, 1.000], loss: 1.248484, mean_absolute_error: 35.245222, mean_q: 44.182228, mean_eps: 0.100000
 1779673/2000000: episode: 8270, duration: 4.799s, episode steps: 302, steps per second: 63, episode reward: -96.045, mean reward: -0.318 [-100.000, 10.738], mean action: 1.623 [0.000, 3.000], mean observation: -0.044 [-0.916, 1.061], loss: 1.310754, mean_absolute_error: 35.287572, mean_q: 44.613417, mean_eps: 0.100000
 1779827/2000000: episode: 8271, duration: 2.366s, episode steps: 154, steps per second: 65, episode reward: -18.471, mean reward: -0.120 [-100.000, 16.296], mean action: 1.532 [0.000, 3.000], mean observation: -0.010 [-1.900, 1.000], loss: 1.416544, mean_absolute_error: 34.598573, mean_q: 43.714979, mean_eps: 0.100000
 1780569/2000000: episode: 8272, duration: 12.302s, episode steps: 742, steps per second: 60, episode reward: -247.440, mean reward: -0.333 [-100.000, 4.770], mean action: 1.744 [0.000, 3.000], mean observation: 0.155 [-0.984, 1.002], loss: 1.241080, mean_absolute_error: 36.138790, mean_q: 46.132088, mean_eps: 0.100000
 1780916/2000000: episode: 8273, duration: 5.412s, episode steps: 347, steps per second: 64, episode reward: -105.270, mean reward: -0.303 [-100.000, 5.568], mean action: 1.602 [0.000, 3.000], mean observation: -0.095 [-1.001, 1.010], loss: 1.314570, mean_absolute_error: 35.420280, mean_q: 44.598195, mean_eps: 0.100000
 1781104/2000000: episode: 8274, duration: 3.407s, episode steps: 188, steps per second: 55, episode reward: -75.101, mean reward: -0.399 [-100.000, 19.253], mean action: 1.340 [0.000, 3.000], mean observation: -0.061 [-1.023, 1.000], loss: 1.101755, mean_absolute_error: 35.300826, mean_q: 44.973250, mean_eps: 0.100000
 1781432/2000000: episode: 8275, duration: 5.162s, episode steps: 328, steps per second: 64, episode reward: -183.496, mean reward: -0.559 [-100.000, 4.571], mean action: 1.494 [0.000, 3.000], mean observation: 0.204 [-0.874, 1.003], loss: 1.438114, mean_absolute_error: 36.439755, mean_q: 45.365079, mean_eps: 0.100000
 1781550/2000000: episode: 8276, duration: 1.825s, episode steps: 118, steps per second: 65, episode reward: -63.856, mean reward: -0.541 [-100.000, 8.951], mean action: 1.525 [0.000, 3.000], mean observation: 0.011 [-1.743, 1.000], loss: 1.258484, mean_absolute_error: 35.774988, mean_q: 47.088681, mean_eps: 0.100000
 1781841/2000000: episode: 8277, duration: 4.677s, episode steps: 291, steps per second: 62, episode reward: -212.451, mean reward: -0.730 [-100.000, 4.786], mean action: 1.722 [0.000, 3.000], mean observation: 0.174 [-0.949, 1.005], loss: 1.368100, mean_absolute_error: 35.748081, mean_q: 45.559780, mean_eps: 0.100000
 1781937/2000000: episode: 8278, duration: 1.482s, episode steps: 96, steps per second: 65, episode reward: -73.950, mean reward: -0.770 [-100.000, 15.260], mean action: 1.792 [0.000, 3.000], mean observation: -0.146 [-1.451, 1.000], loss: 1.428457, mean_absolute_error: 35.814055, mean_q: 45.847195, mean_eps: 0.100000
 1782495/2000000: episode: 8279, duration: 9.108s, episode steps: 558, steps per second: 61, episode reward: -220.189, mean reward: -0.395 [-100.000, 4.707], mean action: 1.620 [0.000, 3.000], mean observation: 0.148 [-1.044, 1.122], loss: 1.200341, mean_absolute_error: 35.128412, mean_q: 45.186822, mean_eps: 0.100000
 1783495/2000000: episode: 8280, duration: 17.701s, episode steps: 1000, steps per second: 56, episode reward: -145.165, mean reward: -0.145 [-4.953, 5.069], mean action: 1.492 [0.000, 3.000], mean observation: 0.147 [-0.975, 1.022], loss: 1.256950, mean_absolute_error: 35.834910, mean_q: 46.007736, mean_eps: 0.100000
 1783770/2000000: episode: 8281, duration: 4.318s, episode steps: 275, steps per second: 64, episode reward: 233.967, mean reward: 0.851 [-12.828, 100.000], mean action: 1.255 [0.000, 3.000], mean observation: 0.146 [-0.908, 1.000], loss: 1.160382, mean_absolute_error: 35.974518, mean_q: 46.090364, mean_eps: 0.100000
 1784076/2000000: episode: 8282, duration: 4.874s, episode steps: 306, steps per second: 63, episode reward: -216.528, mean reward: -0.708 [-100.000, 4.908], mean action: 1.549 [0.000, 3.000], mean observation: 0.193 [-0.791, 1.152], loss: 1.529904, mean_absolute_error: 35.291846, mean_q: 45.092902, mean_eps: 0.100000
 1784194/2000000: episode: 8283, duration: 1.872s, episode steps: 118, steps per second: 63, episode reward: -89.225, mean reward: -0.756 [-100.000, 16.393], mean action: 1.983 [0.000, 3.000], mean observation: -0.021 [-1.863, 1.000], loss: 1.137729, mean_absolute_error: 35.949552, mean_q: 45.833902, mean_eps: 0.100000
 1784627/2000000: episode: 8284, duration: 6.832s, episode steps: 433, steps per second: 63, episode reward: -69.316, mean reward: -0.160 [-100.000, 18.065], mean action: 1.744 [0.000, 3.000], mean observation: -0.042 [-1.007, 1.000], loss: 1.249612, mean_absolute_error: 35.429181, mean_q: 44.875932, mean_eps: 0.100000
 1784736/2000000: episode: 8285, duration: 1.731s, episode steps: 109, steps per second: 63, episode reward: -320.584, mean reward: -2.941 [-100.000, 4.115], mean action: 1.560 [0.000, 3.000], mean observation: 0.171 [-1.964, 4.210], loss: 1.004038, mean_absolute_error: 37.418630, mean_q: 48.215861, mean_eps: 0.100000
 1785197/2000000: episode: 8286, duration: 7.313s, episode steps: 461, steps per second: 63, episode reward: -215.087, mean reward: -0.467 [-100.000, 4.664], mean action: 1.512 [0.000, 3.000], mean observation: 0.186 [-0.934, 1.121], loss: 1.145743, mean_absolute_error: 35.492003, mean_q: 44.933412, mean_eps: 0.100000
 1785823/2000000: episode: 8287, duration: 10.085s, episode steps: 626, steps per second: 62, episode reward: 185.720, mean reward: 0.297 [-19.348, 100.000], mean action: 0.899 [0.000, 3.000], mean observation: 0.049 [-0.936, 1.000], loss: 1.400436, mean_absolute_error: 35.362261, mean_q: 45.065356, mean_eps: 0.100000
 1786104/2000000: episode: 8288, duration: 4.538s, episode steps: 281, steps per second: 62, episode reward: 10.955, mean reward: 0.039 [-100.000, 15.693], mean action: 1.626 [0.000, 3.000], mean observation: -0.048 [-1.002, 1.000], loss: 1.235952, mean_absolute_error: 35.081031, mean_q: 44.516013, mean_eps: 0.100000
 1786220/2000000: episode: 8289, duration: 1.841s, episode steps: 116, steps per second: 63, episode reward: -321.305, mean reward: -2.770 [-100.000, 3.766], mean action: 1.241 [0.000, 3.000], mean observation: 0.190 [-1.468, 3.608], loss: 1.162127, mean_absolute_error: 36.767008, mean_q: 46.892957, mean_eps: 0.100000
 1786503/2000000: episode: 8290, duration: 4.428s, episode steps: 283, steps per second: 64, episode reward: -166.060, mean reward: -0.587 [-100.000, 4.872], mean action: 1.569 [0.000, 3.000], mean observation: 0.176 [-0.931, 1.001], loss: 1.295423, mean_absolute_error: 35.928447, mean_q: 45.450036, mean_eps: 0.100000
 1786764/2000000: episode: 8291, duration: 4.176s, episode steps: 261, steps per second: 63, episode reward: 218.266, mean reward: 0.836 [-9.152, 100.000], mean action: 1.157 [0.000, 3.000], mean observation: 0.077 [-0.959, 1.000], loss: 1.272866, mean_absolute_error: 35.320470, mean_q: 45.836773, mean_eps: 0.100000
 1787213/2000000: episode: 8292, duration: 7.155s, episode steps: 449, steps per second: 63, episode reward: 221.696, mean reward: 0.494 [-19.161, 100.000], mean action: 0.875 [0.000, 3.000], mean observation: 0.191 [-0.994, 1.012], loss: 1.102067, mean_absolute_error: 35.939404, mean_q: 46.174377, mean_eps: 0.100000
 1787903/2000000: episode: 8293, duration: 11.393s, episode steps: 690, steps per second: 61, episode reward: 138.250, mean reward: 0.200 [-10.865, 100.000], mean action: 0.786 [0.000, 3.000], mean observation: 0.094 [-0.980, 1.000], loss: 1.363705, mean_absolute_error: 35.767815, mean_q: 45.262578, mean_eps: 0.100000
 1788903/2000000: episode: 8294, duration: 16.542s, episode steps: 1000, steps per second: 60, episode reward: 59.868, mean reward: 0.060 [-19.297, 22.610], mean action: 0.811 [0.000, 3.000], mean observation: 0.131 [-1.069, 1.000], loss: 1.134606, mean_absolute_error: 35.047799, mean_q: 44.902458, mean_eps: 0.100000
 1789816/2000000: episode: 8295, duration: 15.014s, episode steps: 913, steps per second: 61, episode reward: 199.724, mean reward: 0.219 [-19.952, 100.000], mean action: 1.792 [0.000, 3.000], mean observation: 0.153 [-0.990, 1.000], loss: 1.311394, mean_absolute_error: 35.200150, mean_q: 45.062137, mean_eps: 0.100000
 1789974/2000000: episode: 8296, duration: 2.492s, episode steps: 158, steps per second: 63, episode reward: -24.224, mean reward: -0.153 [-100.000, 10.728], mean action: 1.614 [0.000, 3.000], mean observation: 0.022 [-1.843, 1.013], loss: 1.151715, mean_absolute_error: 35.814813, mean_q: 45.178298, mean_eps: 0.100000
 1790132/2000000: episode: 8297, duration: 2.512s, episode steps: 158, steps per second: 63, episode reward: -9.579, mean reward: -0.061 [-100.000, 24.512], mean action: 1.861 [0.000, 3.000], mean observation: 0.001 [-0.967, 1.000], loss: 1.371253, mean_absolute_error: 35.014958, mean_q: 45.144324, mean_eps: 0.100000
 1790570/2000000: episode: 8298, duration: 7.315s, episode steps: 438, steps per second: 60, episode reward: 174.574, mean reward: 0.399 [-17.119, 100.000], mean action: 2.283 [0.000, 3.000], mean observation: 0.232 [-1.001, 1.414], loss: 1.039575, mean_absolute_error: 34.809174, mean_q: 44.053067, mean_eps: 0.100000
 1791570/2000000: episode: 8299, duration: 17.139s, episode steps: 1000, steps per second: 58, episode reward: -200.333, mean reward: -0.200 [-4.859, 5.086], mean action: 1.765 [0.000, 3.000], mean observation: 0.196 [-0.983, 1.178], loss: 1.225966, mean_absolute_error: 34.758409, mean_q: 44.429178, mean_eps: 0.100000
 1791695/2000000: episode: 8300, duration: 1.927s, episode steps: 125, steps per second: 65, episode reward: -73.864, mean reward: -0.591 [-100.000, 10.635], mean action: 1.656 [0.000, 3.000], mean observation: -0.060 [-1.849, 1.000], loss: 0.986750, mean_absolute_error: 33.460225, mean_q: 43.126697, mean_eps: 0.100000
 1791831/2000000: episode: 8301, duration: 2.197s, episode steps: 136, steps per second: 62, episode reward: -118.844, mean reward: -0.874 [-100.000, 15.417], mean action: 1.434 [0.000, 3.000], mean observation: 0.005 [-1.420, 1.000], loss: 1.549401, mean_absolute_error: 33.835128, mean_q: 41.673953, mean_eps: 0.100000
 1792275/2000000: episode: 8302, duration: 7.206s, episode steps: 444, steps per second: 62, episode reward: -127.176, mean reward: -0.286 [-100.000, 9.918], mean action: 1.727 [0.000, 3.000], mean observation: -0.070 [-1.009, 1.331], loss: 1.393305, mean_absolute_error: 34.890273, mean_q: 44.437130, mean_eps: 0.100000
 1792389/2000000: episode: 8303, duration: 1.799s, episode steps: 114, steps per second: 63, episode reward: -349.617, mean reward: -3.067 [-100.000, 10.029], mean action: 1.421 [0.000, 3.000], mean observation: 0.135 [-1.623, 2.747], loss: 0.983159, mean_absolute_error: 34.235510, mean_q: 44.216861, mean_eps: 0.100000
 1792593/2000000: episode: 8304, duration: 3.207s, episode steps: 204, steps per second: 64, episode reward: -95.881, mean reward: -0.470 [-100.000, 12.032], mean action: 1.515 [0.000, 3.000], mean observation: -0.076 [-1.000, 1.159], loss: 1.295268, mean_absolute_error: 35.042901, mean_q: 44.489406, mean_eps: 0.100000
 1792799/2000000: episode: 8305, duration: 3.189s, episode steps: 206, steps per second: 65, episode reward: 190.867, mean reward: 0.927 [-15.216, 100.000], mean action: 2.078 [0.000, 3.000], mean observation: 0.028 [-0.992, 1.000], loss: 1.070125, mean_absolute_error: 35.700495, mean_q: 45.577154, mean_eps: 0.100000
 1792993/2000000: episode: 8306, duration: 3.037s, episode steps: 194, steps per second: 64, episode reward: 188.722, mean reward: 0.973 [-9.836, 100.000], mean action: 1.170 [0.000, 3.000], mean observation: 0.127 [-0.945, 1.000], loss: 1.378827, mean_absolute_error: 36.290213, mean_q: 46.637249, mean_eps: 0.100000
 1793145/2000000: episode: 8307, duration: 2.329s, episode steps: 152, steps per second: 65, episode reward: -38.931, mean reward: -0.256 [-100.000, 25.458], mean action: 1.553 [0.000, 3.000], mean observation: 0.091 [-1.343, 1.000], loss: 1.327054, mean_absolute_error: 35.301732, mean_q: 45.103105, mean_eps: 0.100000
 1793710/2000000: episode: 8308, duration: 9.289s, episode steps: 565, steps per second: 61, episode reward: -306.064, mean reward: -0.542 [-100.000, 14.594], mean action: 1.697 [0.000, 3.000], mean observation: 0.147 [-0.966, 1.121], loss: 1.300699, mean_absolute_error: 34.808413, mean_q: 44.641365, mean_eps: 0.100000
 1794087/2000000: episode: 8309, duration: 5.901s, episode steps: 377, steps per second: 64, episode reward: 221.244, mean reward: 0.587 [-4.308, 100.000], mean action: 1.690 [0.000, 3.000], mean observation: 0.077 [-0.858, 1.000], loss: 1.301297, mean_absolute_error: 35.512693, mean_q: 45.420476, mean_eps: 0.100000
 1794200/2000000: episode: 8310, duration: 1.809s, episode steps: 113, steps per second: 62, episode reward: -41.280, mean reward: -0.365 [-100.000, 18.284], mean action: 1.673 [0.000, 3.000], mean observation: -0.018 [-1.746, 1.000], loss: 0.988439, mean_absolute_error: 35.067828, mean_q: 45.631869, mean_eps: 0.100000
 1794599/2000000: episode: 8311, duration: 6.261s, episode steps: 399, steps per second: 64, episode reward: 199.056, mean reward: 0.499 [-18.695, 100.000], mean action: 1.632 [0.000, 3.000], mean observation: 0.138 [-0.915, 1.000], loss: 1.278220, mean_absolute_error: 35.870706, mean_q: 46.153238, mean_eps: 0.100000
 1794931/2000000: episode: 8312, duration: 5.198s, episode steps: 332, steps per second: 64, episode reward: 223.993, mean reward: 0.675 [-5.839, 100.000], mean action: 1.307 [0.000, 3.000], mean observation: 0.037 [-0.950, 1.000], loss: 1.064535, mean_absolute_error: 35.136746, mean_q: 44.837939, mean_eps: 0.100000
 1795042/2000000: episode: 8313, duration: 1.722s, episode steps: 111, steps per second: 64, episode reward: -33.549, mean reward: -0.302 [-100.000, 20.456], mean action: 1.532 [0.000, 3.000], mean observation: -0.091 [-0.954, 1.525], loss: 1.265973, mean_absolute_error: 36.792168, mean_q: 44.924769, mean_eps: 0.100000
 1795157/2000000: episode: 8314, duration: 1.797s, episode steps: 115, steps per second: 64, episode reward: -73.384, mean reward: -0.638 [-100.000, 9.124], mean action: 1.730 [0.000, 3.000], mean observation: -0.033 [-0.971, 1.000], loss: 1.156610, mean_absolute_error: 34.498825, mean_q: 43.622311, mean_eps: 0.100000
 1795295/2000000: episode: 8315, duration: 2.212s, episode steps: 138, steps per second: 62, episode reward: -5.336, mean reward: -0.039 [-100.000, 18.939], mean action: 1.529 [0.000, 3.000], mean observation: -0.010 [-1.586, 1.000], loss: 1.226751, mean_absolute_error: 36.125047, mean_q: 45.571788, mean_eps: 0.100000
 1795433/2000000: episode: 8316, duration: 2.309s, episode steps: 138, steps per second: 60, episode reward: -29.181, mean reward: -0.211 [-100.000, 15.505], mean action: 1.616 [0.000, 3.000], mean observation: 0.029 [-1.007, 1.000], loss: 1.138633, mean_absolute_error: 35.532398, mean_q: 45.940876, mean_eps: 0.100000
 1795662/2000000: episode: 8317, duration: 3.579s, episode steps: 229, steps per second: 64, episode reward: -19.190, mean reward: -0.084 [-100.000, 16.415], mean action: 1.528 [0.000, 3.000], mean observation: -0.032 [-0.892, 1.000], loss: 1.057583, mean_absolute_error: 35.093282, mean_q: 45.076477, mean_eps: 0.100000
 1795804/2000000: episode: 8318, duration: 2.215s, episode steps: 142, steps per second: 64, episode reward: -93.793, mean reward: -0.661 [-100.000, 23.675], mean action: 1.366 [0.000, 3.000], mean observation: 0.014 [-1.048, 1.136], loss: 1.371017, mean_absolute_error: 35.834246, mean_q: 45.640344, mean_eps: 0.100000
 1796042/2000000: episode: 8319, duration: 3.718s, episode steps: 238, steps per second: 64, episode reward: 232.178, mean reward: 0.976 [-18.090, 100.000], mean action: 1.504 [0.000, 3.000], mean observation: 0.135 [-0.972, 1.000], loss: 1.170619, mean_absolute_error: 35.903559, mean_q: 46.294433, mean_eps: 0.100000
 1796214/2000000: episode: 8320, duration: 2.658s, episode steps: 172, steps per second: 65, episode reward: -22.191, mean reward: -0.129 [-100.000, 22.662], mean action: 1.640 [0.000, 3.000], mean observation: -0.021 [-0.945, 1.168], loss: 1.424370, mean_absolute_error: 34.705697, mean_q: 43.714888, mean_eps: 0.100000
 1796478/2000000: episode: 8321, duration: 4.141s, episode steps: 264, steps per second: 64, episode reward: -232.975, mean reward: -0.882 [-100.000, 4.882], mean action: 1.712 [0.000, 3.000], mean observation: 0.166 [-0.977, 1.004], loss: 1.316239, mean_absolute_error: 35.355360, mean_q: 44.699232, mean_eps: 0.100000
 1796960/2000000: episode: 8322, duration: 7.525s, episode steps: 482, steps per second: 64, episode reward: 239.757, mean reward: 0.497 [-17.526, 100.000], mean action: 0.693 [0.000, 3.000], mean observation: 0.121 [-0.955, 1.000], loss: 1.260787, mean_absolute_error: 35.768326, mean_q: 46.284724, mean_eps: 0.100000
 1797092/2000000: episode: 8323, duration: 2.100s, episode steps: 132, steps per second: 63, episode reward: -37.476, mean reward: -0.284 [-100.000, 16.260], mean action: 1.295 [0.000, 3.000], mean observation: 0.032 [-1.588, 1.000], loss: 1.058304, mean_absolute_error: 34.877903, mean_q: 44.358927, mean_eps: 0.100000
 1797355/2000000: episode: 8324, duration: 4.088s, episode steps: 263, steps per second: 64, episode reward: 218.061, mean reward: 0.829 [-17.522, 100.000], mean action: 0.669 [0.000, 3.000], mean observation: 0.122 [-0.935, 1.000], loss: 1.293582, mean_absolute_error: 36.033963, mean_q: 46.725946, mean_eps: 0.100000
 1797436/2000000: episode: 8325, duration: 1.320s, episode steps: 81, steps per second: 61, episode reward: -169.145, mean reward: -2.088 [-100.000, 12.070], mean action: 1.605 [0.000, 3.000], mean observation: -0.157 [-1.464, 1.000], loss: 1.106978, mean_absolute_error: 34.997003, mean_q: 45.207083, mean_eps: 0.100000
 1797549/2000000: episode: 8326, duration: 1.769s, episode steps: 113, steps per second: 64, episode reward: -110.683, mean reward: -0.979 [-100.000, 6.156], mean action: 1.522 [0.000, 3.000], mean observation: -0.022 [-1.478, 1.000], loss: 1.381736, mean_absolute_error: 36.105640, mean_q: 45.482595, mean_eps: 0.100000
 1797726/2000000: episode: 8327, duration: 2.727s, episode steps: 177, steps per second: 65, episode reward: 220.057, mean reward: 1.243 [-10.618, 100.000], mean action: 1.497 [0.000, 3.000], mean observation: 0.087 [-0.959, 1.000], loss: 1.592294, mean_absolute_error: 35.944358, mean_q: 45.399286, mean_eps: 0.100000
 1797843/2000000: episode: 8328, duration: 1.835s, episode steps: 117, steps per second: 64, episode reward: -23.460, mean reward: -0.201 [-100.000, 19.893], mean action: 2.043 [0.000, 3.000], mean observation: -0.114 [-0.994, 1.191], loss: 1.327904, mean_absolute_error: 34.810363, mean_q: 44.938475, mean_eps: 0.100000
 1798412/2000000: episode: 8329, duration: 9.247s, episode steps: 569, steps per second: 62, episode reward: 206.764, mean reward: 0.363 [-18.214, 100.000], mean action: 1.132 [0.000, 3.000], mean observation: 0.186 [-0.927, 1.000], loss: 1.319738, mean_absolute_error: 34.907888, mean_q: 44.757769, mean_eps: 0.100000
 1798874/2000000: episode: 8330, duration: 7.514s, episode steps: 462, steps per second: 61, episode reward: -177.890, mean reward: -0.385 [-100.000, 5.312], mean action: 1.924 [0.000, 3.000], mean observation: 0.164 [-1.045, 1.007], loss: 1.073972, mean_absolute_error: 35.456026, mean_q: 45.243198, mean_eps: 0.100000
 1799473/2000000: episode: 8331, duration: 9.480s, episode steps: 599, steps per second: 63, episode reward: 196.171, mean reward: 0.327 [-18.935, 100.000], mean action: 1.030 [0.000, 3.000], mean observation: 0.118 [-0.958, 1.000], loss: 1.243688, mean_absolute_error: 35.793698, mean_q: 45.750271, mean_eps: 0.100000
 1800473/2000000: episode: 8332, duration: 16.326s, episode steps: 1000, steps per second: 61, episode reward: 47.563, mean reward: 0.048 [-20.959, 23.152], mean action: 2.100 [0.000, 3.000], mean observation: 0.120 [-1.653, 1.008], loss: 1.188018, mean_absolute_error: 34.905778, mean_q: 44.954350, mean_eps: 0.100000
 1800620/2000000: episode: 8333, duration: 2.391s, episode steps: 147, steps per second: 61, episode reward: -329.424, mean reward: -2.241 [-100.000, 37.874], mean action: 2.231 [0.000, 3.000], mean observation: 0.095 [-0.988, 2.466], loss: 0.991103, mean_absolute_error: 33.610469, mean_q: 43.138998, mean_eps: 0.100000
 1800753/2000000: episode: 8334, duration: 2.112s, episode steps: 133, steps per second: 63, episode reward: -12.349, mean reward: -0.093 [-100.000, 17.391], mean action: 1.744 [0.000, 3.000], mean observation: -0.071 [-0.979, 1.250], loss: 1.467554, mean_absolute_error: 36.976198, mean_q: 47.712047, mean_eps: 0.100000
 1801066/2000000: episode: 8335, duration: 5.009s, episode steps: 313, steps per second: 62, episode reward: 221.522, mean reward: 0.708 [-8.349, 100.000], mean action: 1.435 [0.000, 3.000], mean observation: 0.136 [-1.599, 1.000], loss: 1.108619, mean_absolute_error: 33.244275, mean_q: 42.137412, mean_eps: 0.100000
 1801627/2000000: episode: 8336, duration: 8.869s, episode steps: 561, steps per second: 63, episode reward: 225.194, mean reward: 0.401 [-19.805, 100.000], mean action: 0.592 [0.000, 3.000], mean observation: 0.147 [-0.923, 1.000], loss: 1.217512, mean_absolute_error: 34.730161, mean_q: 44.835615, mean_eps: 0.100000
 1801970/2000000: episode: 8337, duration: 5.444s, episode steps: 343, steps per second: 63, episode reward: -99.329, mean reward: -0.290 [-100.000, 10.134], mean action: 1.767 [0.000, 3.000], mean observation: -0.052 [-0.869, 1.116], loss: 1.133885, mean_absolute_error: 35.174709, mean_q: 45.369847, mean_eps: 0.100000
 1802322/2000000: episode: 8338, duration: 5.687s, episode steps: 352, steps per second: 62, episode reward: 125.682, mean reward: 0.357 [-19.999, 100.000], mean action: 1.165 [0.000, 3.000], mean observation: 0.127 [-0.928, 1.000], loss: 1.277338, mean_absolute_error: 34.388815, mean_q: 44.365758, mean_eps: 0.100000
 1802436/2000000: episode: 8339, duration: 1.808s, episode steps: 114, steps per second: 63, episode reward: -6.872, mean reward: -0.060 [-100.000, 33.547], mean action: 1.798 [0.000, 3.000], mean observation: -0.085 [-0.941, 1.306], loss: 1.173613, mean_absolute_error: 34.119631, mean_q: 44.538264, mean_eps: 0.100000
 1802704/2000000: episode: 8340, duration: 4.252s, episode steps: 268, steps per second: 63, episode reward: 241.263, mean reward: 0.900 [-8.881, 100.000], mean action: 1.556 [0.000, 3.000], mean observation: 0.112 [-1.674, 1.000], loss: 1.153306, mean_absolute_error: 34.923536, mean_q: 44.950866, mean_eps: 0.100000
 1802844/2000000: episode: 8341, duration: 2.242s, episode steps: 140, steps per second: 62, episode reward: -25.586, mean reward: -0.183 [-100.000, 14.372], mean action: 1.293 [0.000, 3.000], mean observation: -0.067 [-0.928, 1.000], loss: 1.124768, mean_absolute_error: 33.726369, mean_q: 43.356887, mean_eps: 0.100000
 1803129/2000000: episode: 8342, duration: 4.531s, episode steps: 285, steps per second: 63, episode reward: 226.061, mean reward: 0.793 [-4.666, 100.000], mean action: 1.481 [0.000, 3.000], mean observation: 0.015 [-0.903, 1.000], loss: 1.169719, mean_absolute_error: 35.433669, mean_q: 45.758697, mean_eps: 0.100000
 1804003/2000000: episode: 8343, duration: 15.671s, episode steps: 874, steps per second: 56, episode reward: -312.665, mean reward: -0.358 [-100.000, 5.451], mean action: 1.667 [0.000, 3.000], mean observation: 0.147 [-1.008, 1.419], loss: 1.251641, mean_absolute_error: 35.379747, mean_q: 45.457257, mean_eps: 0.100000
 1804255/2000000: episode: 8344, duration: 3.946s, episode steps: 252, steps per second: 64, episode reward: 205.898, mean reward: 0.817 [-18.438, 100.000], mean action: 1.135 [0.000, 3.000], mean observation: 0.082 [-0.896, 1.000], loss: 1.419099, mean_absolute_error: 35.404709, mean_q: 44.923534, mean_eps: 0.100000
 1804393/2000000: episode: 8345, duration: 2.183s, episode steps: 138, steps per second: 63, episode reward: -33.800, mean reward: -0.245 [-100.000, 25.836], mean action: 1.645 [0.000, 3.000], mean observation: 0.069 [-1.081, 1.000], loss: 1.726780, mean_absolute_error: 34.939146, mean_q: 44.328506, mean_eps: 0.100000
 1804827/2000000: episode: 8346, duration: 7.028s, episode steps: 434, steps per second: 62, episode reward: -213.686, mean reward: -0.492 [-100.000, 4.529], mean action: 1.523 [0.000, 3.000], mean observation: 0.198 [-0.964, 1.228], loss: 1.218872, mean_absolute_error: 34.839897, mean_q: 44.853549, mean_eps: 0.100000
 1805160/2000000: episode: 8347, duration: 5.263s, episode steps: 333, steps per second: 63, episode reward: 185.640, mean reward: 0.557 [-3.029, 100.000], mean action: 0.766 [0.000, 3.000], mean observation: 0.115 [-0.934, 1.000], loss: 1.208693, mean_absolute_error: 35.064161, mean_q: 45.810915, mean_eps: 0.100000
 1805394/2000000: episode: 8348, duration: 4.001s, episode steps: 234, steps per second: 58, episode reward: 223.364, mean reward: 0.955 [-5.660, 100.000], mean action: 1.350 [0.000, 3.000], mean observation: 0.086 [-0.965, 1.000], loss: 1.219816, mean_absolute_error: 34.706250, mean_q: 44.275438, mean_eps: 0.100000
 1805972/2000000: episode: 8349, duration: 9.283s, episode steps: 578, steps per second: 62, episode reward: 225.054, mean reward: 0.389 [-17.781, 100.000], mean action: 0.727 [0.000, 3.000], mean observation: 0.175 [-0.978, 1.000], loss: 1.187164, mean_absolute_error: 34.946434, mean_q: 44.974294, mean_eps: 0.100000
 1806195/2000000: episode: 8350, duration: 3.524s, episode steps: 223, steps per second: 63, episode reward: 225.044, mean reward: 1.009 [-18.136, 100.000], mean action: 1.587 [0.000, 3.000], mean observation: 0.110 [-0.981, 1.000], loss: 1.240187, mean_absolute_error: 34.311067, mean_q: 44.053024, mean_eps: 0.100000
 1806363/2000000: episode: 8351, duration: 2.622s, episode steps: 168, steps per second: 64, episode reward: -301.579, mean reward: -1.795 [-100.000, 12.988], mean action: 1.565 [0.000, 3.000], mean observation: 0.078 [-0.984, 1.292], loss: 1.402625, mean_absolute_error: 35.802481, mean_q: 46.119179, mean_eps: 0.100000
 1806597/2000000: episode: 8352, duration: 3.675s, episode steps: 234, steps per second: 64, episode reward: -26.126, mean reward: -0.112 [-100.000, 14.284], mean action: 1.872 [0.000, 3.000], mean observation: -0.036 [-0.927, 1.012], loss: 1.039940, mean_absolute_error: 34.086313, mean_q: 44.202404, mean_eps: 0.100000
 1807029/2000000: episode: 8353, duration: 7.134s, episode steps: 432, steps per second: 61, episode reward: 156.344, mean reward: 0.362 [-7.503, 100.000], mean action: 1.502 [0.000, 3.000], mean observation: 0.043 [-0.909, 1.000], loss: 1.379920, mean_absolute_error: 35.116459, mean_q: 44.956260, mean_eps: 0.100000
 1807153/2000000: episode: 8354, duration: 1.932s, episode steps: 124, steps per second: 64, episode reward: -97.716, mean reward: -0.788 [-100.000, 8.615], mean action: 1.935 [0.000, 3.000], mean observation: -0.040 [-0.974, 1.000], loss: 1.207979, mean_absolute_error: 34.297072, mean_q: 43.113242, mean_eps: 0.100000
 1808114/2000000: episode: 8355, duration: 16.196s, episode steps: 961, steps per second: 59, episode reward: 131.364, mean reward: 0.137 [-20.473, 100.000], mean action: 2.525 [0.000, 3.000], mean observation: 0.189 [-0.973, 1.000], loss: 1.300011, mean_absolute_error: 35.064790, mean_q: 44.790272, mean_eps: 0.100000
 1808482/2000000: episode: 8356, duration: 5.875s, episode steps: 368, steps per second: 63, episode reward: 160.410, mean reward: 0.436 [-9.455, 100.000], mean action: 1.690 [0.000, 3.000], mean observation: 0.050 [-0.960, 1.153], loss: 1.197548, mean_absolute_error: 34.920198, mean_q: 44.735339, mean_eps: 0.100000
 1809482/2000000: episode: 8357, duration: 16.232s, episode steps: 1000, steps per second: 62, episode reward: 72.734, mean reward: 0.073 [-19.010, 22.059], mean action: 0.714 [0.000, 3.000], mean observation: 0.237 [-0.996, 1.000], loss: 1.225000, mean_absolute_error: 35.142487, mean_q: 44.571634, mean_eps: 0.100000
 1809749/2000000: episode: 8358, duration: 4.244s, episode steps: 267, steps per second: 63, episode reward: -87.720, mean reward: -0.329 [-100.000, 21.086], mean action: 1.914 [0.000, 3.000], mean observation: -0.058 [-0.863, 1.000], loss: 1.164183, mean_absolute_error: 34.833320, mean_q: 44.827159, mean_eps: 0.100000
 1810043/2000000: episode: 8359, duration: 4.598s, episode steps: 294, steps per second: 64, episode reward: 243.612, mean reward: 0.829 [-12.148, 100.000], mean action: 1.425 [0.000, 3.000], mean observation: 0.117 [-0.906, 1.000], loss: 1.114929, mean_absolute_error: 35.078741, mean_q: 45.499623, mean_eps: 0.100000
 1810447/2000000: episode: 8360, duration: 6.544s, episode steps: 404, steps per second: 62, episode reward: 164.209, mean reward: 0.406 [-11.783, 100.000], mean action: 2.208 [0.000, 3.000], mean observation: 0.110 [-0.974, 1.000], loss: 1.149607, mean_absolute_error: 34.655705, mean_q: 44.883887, mean_eps: 0.100000
 1810721/2000000: episode: 8361, duration: 4.405s, episode steps: 274, steps per second: 62, episode reward: 215.512, mean reward: 0.787 [-8.388, 100.000], mean action: 1.303 [0.000, 3.000], mean observation: 0.139 [-0.982, 1.000], loss: 1.545143, mean_absolute_error: 35.035754, mean_q: 44.709294, mean_eps: 0.100000
 1810908/2000000: episode: 8362, duration: 3.108s, episode steps: 187, steps per second: 60, episode reward: -36.321, mean reward: -0.194 [-100.000, 13.372], mean action: 1.422 [0.000, 3.000], mean observation: -0.023 [-0.907, 1.000], loss: 1.092403, mean_absolute_error: 35.068732, mean_q: 45.452469, mean_eps: 0.100000
 1811052/2000000: episode: 8363, duration: 2.335s, episode steps: 144, steps per second: 62, episode reward: -34.162, mean reward: -0.237 [-100.000, 15.249], mean action: 1.764 [0.000, 3.000], mean observation: 0.113 [-1.110, 1.076], loss: 1.671379, mean_absolute_error: 34.296961, mean_q: 44.401069, mean_eps: 0.100000
 1811497/2000000: episode: 8364, duration: 7.241s, episode steps: 445, steps per second: 61, episode reward: 252.167, mean reward: 0.567 [-18.732, 100.000], mean action: 1.020 [0.000, 3.000], mean observation: 0.175 [-0.697, 1.000], loss: 1.218871, mean_absolute_error: 35.056748, mean_q: 44.952229, mean_eps: 0.100000
 1811763/2000000: episode: 8365, duration: 4.174s, episode steps: 266, steps per second: 64, episode reward: 223.824, mean reward: 0.841 [-10.739, 100.000], mean action: 1.117 [0.000, 3.000], mean observation: 0.098 [-0.980, 1.000], loss: 1.269028, mean_absolute_error: 34.878274, mean_q: 45.536858, mean_eps: 0.100000
 1811940/2000000: episode: 8366, duration: 2.830s, episode steps: 177, steps per second: 63, episode reward: 165.561, mean reward: 0.935 [-13.539, 100.000], mean action: 1.232 [0.000, 3.000], mean observation: 0.039 [-1.472, 1.000], loss: 1.068973, mean_absolute_error: 35.405073, mean_q: 45.979569, mean_eps: 0.100000
 1812337/2000000: episode: 8367, duration: 6.405s, episode steps: 397, steps per second: 62, episode reward: 201.589, mean reward: 0.508 [-13.386, 100.000], mean action: 1.456 [0.000, 3.000], mean observation: 0.020 [-0.986, 1.000], loss: 1.421722, mean_absolute_error: 35.630470, mean_q: 45.896086, mean_eps: 0.100000
 1812456/2000000: episode: 8368, duration: 1.875s, episode steps: 119, steps per second: 63, episode reward: -37.157, mean reward: -0.312 [-100.000, 12.368], mean action: 1.882 [0.000, 3.000], mean observation: -0.082 [-1.035, 1.340], loss: 1.051613, mean_absolute_error: 36.039670, mean_q: 47.640356, mean_eps: 0.100000
 1812560/2000000: episode: 8369, duration: 1.682s, episode steps: 104, steps per second: 62, episode reward: -11.237, mean reward: -0.108 [-100.000, 16.682], mean action: 1.433 [0.000, 3.000], mean observation: 0.006 [-1.472, 1.000], loss: 0.782596, mean_absolute_error: 34.084701, mean_q: 43.861583, mean_eps: 0.100000
 1812700/2000000: episode: 8370, duration: 2.299s, episode steps: 140, steps per second: 61, episode reward: -28.344, mean reward: -0.202 [-100.000, 19.304], mean action: 1.514 [0.000, 3.000], mean observation: 0.025 [-1.944, 1.000], loss: 0.939366, mean_absolute_error: 33.592486, mean_q: 43.327987, mean_eps: 0.100000
 1812819/2000000: episode: 8371, duration: 2.016s, episode steps: 119, steps per second: 59, episode reward: -23.484, mean reward: -0.197 [-100.000, 13.796], mean action: 1.395 [0.000, 3.000], mean observation: -0.079 [-0.987, 1.385], loss: 1.468813, mean_absolute_error: 33.778071, mean_q: 43.538012, mean_eps: 0.100000
 1812953/2000000: episode: 8372, duration: 2.135s, episode steps: 134, steps per second: 63, episode reward: -90.524, mean reward: -0.676 [-100.000, 18.670], mean action: 1.403 [0.000, 3.000], mean observation: 0.087 [-2.167, 1.000], loss: 1.344642, mean_absolute_error: 34.946634, mean_q: 45.731897, mean_eps: 0.100000
 1813278/2000000: episode: 8373, duration: 5.150s, episode steps: 325, steps per second: 63, episode reward: 206.449, mean reward: 0.635 [-9.989, 100.000], mean action: 0.972 [0.000, 3.000], mean observation: 0.105 [-1.260, 1.000], loss: 1.443491, mean_absolute_error: 34.710603, mean_q: 44.151311, mean_eps: 0.100000
 1813581/2000000: episode: 8374, duration: 4.791s, episode steps: 303, steps per second: 63, episode reward: -132.672, mean reward: -0.438 [-100.000, 5.662], mean action: 1.812 [0.000, 3.000], mean observation: 0.159 [-1.000, 1.002], loss: 1.358587, mean_absolute_error: 35.473655, mean_q: 45.138601, mean_eps: 0.100000
 1814115/2000000: episode: 8375, duration: 8.489s, episode steps: 534, steps per second: 63, episode reward: 216.864, mean reward: 0.406 [-19.864, 100.000], mean action: 1.084 [0.000, 3.000], mean observation: 0.052 [-1.311, 1.008], loss: 1.249797, mean_absolute_error: 34.934686, mean_q: 44.494824, mean_eps: 0.100000
 1814218/2000000: episode: 8376, duration: 1.630s, episode steps: 103, steps per second: 63, episode reward: -62.423, mean reward: -0.606 [-100.000, 11.877], mean action: 1.583 [0.000, 3.000], mean observation: 0.058 [-0.963, 1.419], loss: 1.678969, mean_absolute_error: 33.032108, mean_q: 42.267389, mean_eps: 0.100000
 1814579/2000000: episode: 8377, duration: 6.052s, episode steps: 361, steps per second: 60, episode reward: 229.302, mean reward: 0.635 [-17.579, 100.000], mean action: 0.778 [0.000, 3.000], mean observation: 0.110 [-1.064, 1.000], loss: 1.239186, mean_absolute_error: 34.861215, mean_q: 44.774919, mean_eps: 0.100000
 1815579/2000000: episode: 8378, duration: 18.127s, episode steps: 1000, steps per second: 55, episode reward: -181.773, mean reward: -0.182 [-5.371, 5.730], mean action: 1.740 [0.000, 3.000], mean observation: 0.138 [-0.917, 1.110], loss: 1.309078, mean_absolute_error: 34.434492, mean_q: 44.501164, mean_eps: 0.100000
 1815689/2000000: episode: 8379, duration: 1.773s, episode steps: 110, steps per second: 62, episode reward: -6.193, mean reward: -0.056 [-100.000, 15.192], mean action: 1.773 [0.000, 3.000], mean observation: 0.016 [-1.626, 1.000], loss: 1.341323, mean_absolute_error: 35.065308, mean_q: 45.708175, mean_eps: 0.100000
 1815992/2000000: episode: 8380, duration: 4.805s, episode steps: 303, steps per second: 63, episode reward: -102.400, mean reward: -0.338 [-100.000, 10.323], mean action: 1.891 [0.000, 3.000], mean observation: -0.081 [-0.928, 1.564], loss: 1.237389, mean_absolute_error: 34.149982, mean_q: 43.849429, mean_eps: 0.100000
 1816113/2000000: episode: 8381, duration: 1.925s, episode steps: 121, steps per second: 63, episode reward: -54.878, mean reward: -0.454 [-100.000, 11.858], mean action: 1.463 [0.000, 3.000], mean observation: 0.003 [-1.834, 1.000], loss: 1.130322, mean_absolute_error: 34.479153, mean_q: 44.429423, mean_eps: 0.100000
 1816362/2000000: episode: 8382, duration: 3.916s, episode steps: 249, steps per second: 64, episode reward: 186.161, mean reward: 0.748 [-16.483, 100.000], mean action: 2.241 [0.000, 3.000], mean observation: 0.209 [-0.985, 1.003], loss: 1.115960, mean_absolute_error: 35.926914, mean_q: 46.019279, mean_eps: 0.100000
 1816866/2000000: episode: 8383, duration: 8.086s, episode steps: 504, steps per second: 62, episode reward: 214.896, mean reward: 0.426 [-17.639, 100.000], mean action: 0.810 [0.000, 3.000], mean observation: 0.138 [-0.999, 1.186], loss: 1.238219, mean_absolute_error: 35.184799, mean_q: 45.484710, mean_eps: 0.100000
 1817022/2000000: episode: 8384, duration: 2.406s, episode steps: 156, steps per second: 65, episode reward: -27.662, mean reward: -0.177 [-100.000, 12.408], mean action: 1.577 [0.000, 3.000], mean observation: -0.032 [-0.965, 1.000], loss: 1.515636, mean_absolute_error: 35.464947, mean_q: 45.651931, mean_eps: 0.100000
 1817253/2000000: episode: 8385, duration: 3.637s, episode steps: 231, steps per second: 64, episode reward: 216.605, mean reward: 0.938 [-4.908, 100.000], mean action: 1.446 [0.000, 3.000], mean observation: 0.080 [-0.996, 1.000], loss: 0.900459, mean_absolute_error: 35.025086, mean_q: 44.723259, mean_eps: 0.100000
 1817636/2000000: episode: 8386, duration: 6.130s, episode steps: 383, steps per second: 62, episode reward: 261.835, mean reward: 0.684 [-15.910, 100.000], mean action: 1.723 [0.000, 3.000], mean observation: 0.151 [-0.841, 1.158], loss: 1.424813, mean_absolute_error: 35.036857, mean_q: 44.649925, mean_eps: 0.100000
 1818030/2000000: episode: 8387, duration: 6.853s, episode steps: 394, steps per second: 57, episode reward: -105.987, mean reward: -0.269 [-100.000, 9.917], mean action: 1.820 [0.000, 3.000], mean observation: -0.047 [-0.942, 1.000], loss: 1.554119, mean_absolute_error: 35.219989, mean_q: 44.749582, mean_eps: 0.100000
 1818547/2000000: episode: 8388, duration: 8.223s, episode steps: 517, steps per second: 63, episode reward: 249.828, mean reward: 0.483 [-19.257, 100.000], mean action: 0.841 [0.000, 3.000], mean observation: 0.129 [-0.764, 1.000], loss: 1.580370, mean_absolute_error: 35.125381, mean_q: 45.372980, mean_eps: 0.100000
 1818792/2000000: episode: 8389, duration: 4.058s, episode steps: 245, steps per second: 60, episode reward: -43.992, mean reward: -0.180 [-100.000, 18.037], mean action: 1.743 [0.000, 3.000], mean observation: -0.017 [-1.077, 1.000], loss: 1.364763, mean_absolute_error: 34.880006, mean_q: 44.532789, mean_eps: 0.100000
 1819040/2000000: episode: 8390, duration: 4.071s, episode steps: 248, steps per second: 61, episode reward: 222.084, mean reward: 0.896 [-8.981, 100.000], mean action: 1.851 [0.000, 3.000], mean observation: 0.126 [-0.924, 1.000], loss: 1.412774, mean_absolute_error: 34.892728, mean_q: 45.102516, mean_eps: 0.100000
 1819145/2000000: episode: 8391, duration: 1.703s, episode steps: 105, steps per second: 62, episode reward: -100.220, mean reward: -0.954 [-100.000, 14.520], mean action: 1.762 [0.000, 3.000], mean observation: -0.033 [-1.152, 1.000], loss: 1.430225, mean_absolute_error: 35.511591, mean_q: 45.683686, mean_eps: 0.100000
 1819401/2000000: episode: 8392, duration: 4.134s, episode steps: 256, steps per second: 62, episode reward: -146.227, mean reward: -0.571 [-100.000, 8.326], mean action: 1.852 [0.000, 3.000], mean observation: 0.159 [-0.969, 1.154], loss: 1.334360, mean_absolute_error: 35.986702, mean_q: 45.944374, mean_eps: 0.100000
 1819607/2000000: episode: 8393, duration: 3.207s, episode steps: 206, steps per second: 64, episode reward: 233.377, mean reward: 1.133 [-6.899, 100.000], mean action: 1.063 [0.000, 3.000], mean observation: 0.071 [-0.893, 1.000], loss: 1.401274, mean_absolute_error: 33.979751, mean_q: 44.057999, mean_eps: 0.100000
 1819774/2000000: episode: 8394, duration: 2.650s, episode steps: 167, steps per second: 63, episode reward: -57.556, mean reward: -0.345 [-100.000, 17.759], mean action: 1.922 [0.000, 3.000], mean observation: -0.073 [-0.792, 1.591], loss: 1.636914, mean_absolute_error: 34.626264, mean_q: 44.797975, mean_eps: 0.100000
 1820073/2000000: episode: 8395, duration: 5.033s, episode steps: 299, steps per second: 59, episode reward: -85.962, mean reward: -0.287 [-100.000, 11.946], mean action: 1.676 [0.000, 3.000], mean observation: -0.081 [-0.796, 1.347], loss: 1.167739, mean_absolute_error: 34.926964, mean_q: 44.930529, mean_eps: 0.100000
 1820527/2000000: episode: 8396, duration: 7.392s, episode steps: 454, steps per second: 61, episode reward: 206.240, mean reward: 0.454 [-18.147, 100.000], mean action: 0.971 [0.000, 3.000], mean observation: 0.120 [-1.009, 1.000], loss: 1.330889, mean_absolute_error: 34.779180, mean_q: 45.271504, mean_eps: 0.100000
 1820822/2000000: episode: 8397, duration: 4.733s, episode steps: 295, steps per second: 62, episode reward: 223.248, mean reward: 0.757 [-17.493, 100.000], mean action: 0.864 [0.000, 3.000], mean observation: 0.100 [-0.966, 1.000], loss: 1.426224, mean_absolute_error: 34.736995, mean_q: 44.676936, mean_eps: 0.100000
 1821822/2000000: episode: 8398, duration: 17.324s, episode steps: 1000, steps per second: 58, episode reward: -117.995, mean reward: -0.118 [-5.265, 5.161], mean action: 1.831 [0.000, 3.000], mean observation: 0.185 [-0.895, 0.944], loss: 1.148982, mean_absolute_error: 34.830510, mean_q: 45.229115, mean_eps: 0.100000
 1822051/2000000: episode: 8399, duration: 3.723s, episode steps: 229, steps per second: 62, episode reward: 257.555, mean reward: 1.125 [-3.570, 100.000], mean action: 1.428 [0.000, 3.000], mean observation: 0.144 [-0.775, 1.153], loss: 1.127111, mean_absolute_error: 34.769119, mean_q: 44.286302, mean_eps: 0.100000
 1822349/2000000: episode: 8400, duration: 4.677s, episode steps: 298, steps per second: 64, episode reward: -109.501, mean reward: -0.367 [-100.000, 26.937], mean action: 1.762 [0.000, 3.000], mean observation: -0.087 [-1.171, 1.601], loss: 0.950215, mean_absolute_error: 34.521115, mean_q: 44.124195, mean_eps: 0.100000
 1822455/2000000: episode: 8401, duration: 1.621s, episode steps: 106, steps per second: 65, episode reward: -19.966, mean reward: -0.188 [-100.000, 12.102], mean action: 1.906 [0.000, 3.000], mean observation: -0.065 [-0.921, 1.378], loss: 0.995057, mean_absolute_error: 35.550955, mean_q: 46.205281, mean_eps: 0.100000
 1822629/2000000: episode: 8402, duration: 2.737s, episode steps: 174, steps per second: 64, episode reward: -1.611, mean reward: -0.009 [-100.000, 15.676], mean action: 1.569 [0.000, 3.000], mean observation: -0.018 [-1.218, 1.000], loss: 0.966265, mean_absolute_error: 34.872754, mean_q: 45.444215, mean_eps: 0.100000
 1822750/2000000: episode: 8403, duration: 1.869s, episode steps: 121, steps per second: 65, episode reward: -116.545, mean reward: -0.963 [-100.000, 17.125], mean action: 1.843 [0.000, 3.000], mean observation: -0.004 [-1.782, 1.000], loss: 1.596969, mean_absolute_error: 35.128263, mean_q: 42.933687, mean_eps: 0.100000
 1822869/2000000: episode: 8404, duration: 1.858s, episode steps: 119, steps per second: 64, episode reward: -109.073, mean reward: -0.917 [-100.000, 16.033], mean action: 1.538 [0.000, 3.000], mean observation: -0.040 [-0.811, 1.759], loss: 1.454652, mean_absolute_error: 36.490398, mean_q: 46.166584, mean_eps: 0.100000
 1823869/2000000: episode: 8405, duration: 16.385s, episode steps: 1000, steps per second: 61, episode reward: -173.240, mean reward: -0.173 [-4.981, 5.335], mean action: 1.773 [0.000, 3.000], mean observation: 0.169 [-0.850, 1.058], loss: 1.251238, mean_absolute_error: 34.901683, mean_q: 45.345438, mean_eps: 0.100000
 1824034/2000000: episode: 8406, duration: 2.592s, episode steps: 165, steps per second: 64, episode reward: -12.530, mean reward: -0.076 [-100.000, 12.756], mean action: 1.552 [0.000, 3.000], mean observation: -0.045 [-0.953, 1.157], loss: 1.357852, mean_absolute_error: 35.020367, mean_q: 45.595135, mean_eps: 0.100000
 1824156/2000000: episode: 8407, duration: 1.947s, episode steps: 122, steps per second: 63, episode reward: -23.862, mean reward: -0.196 [-100.000, 30.493], mean action: 1.492 [0.000, 3.000], mean observation: 0.083 [-1.660, 1.000], loss: 1.260342, mean_absolute_error: 35.780545, mean_q: 47.051543, mean_eps: 0.100000
 1824453/2000000: episode: 8408, duration: 4.747s, episode steps: 297, steps per second: 63, episode reward: 190.016, mean reward: 0.640 [-17.754, 100.000], mean action: 0.855 [0.000, 3.000], mean observation: 0.087 [-0.961, 1.000], loss: 1.602616, mean_absolute_error: 35.575421, mean_q: 45.468038, mean_eps: 0.100000
 1824704/2000000: episode: 8409, duration: 3.941s, episode steps: 251, steps per second: 64, episode reward: -145.145, mean reward: -0.578 [-100.000, 18.284], mean action: 1.602 [0.000, 3.000], mean observation: -0.123 [-1.208, 1.836], loss: 1.205285, mean_absolute_error: 34.719825, mean_q: 45.491645, mean_eps: 0.100000
 1824988/2000000: episode: 8410, duration: 4.548s, episode steps: 284, steps per second: 62, episode reward: 254.084, mean reward: 0.895 [-3.551, 100.000], mean action: 1.430 [0.000, 3.000], mean observation: 0.148 [-0.902, 1.000], loss: 1.226082, mean_absolute_error: 35.002172, mean_q: 44.931766, mean_eps: 0.100000
 1825417/2000000: episode: 8411, duration: 6.789s, episode steps: 429, steps per second: 63, episode reward: 233.701, mean reward: 0.545 [-10.141, 100.000], mean action: 1.240 [0.000, 3.000], mean observation: 0.047 [-1.197, 1.000], loss: 1.087474, mean_absolute_error: 34.622915, mean_q: 45.298602, mean_eps: 0.100000
 1825811/2000000: episode: 8412, duration: 6.286s, episode steps: 394, steps per second: 63, episode reward: 209.579, mean reward: 0.532 [-12.386, 100.000], mean action: 1.647 [0.000, 3.000], mean observation: 0.040 [-0.962, 1.000], loss: 1.105993, mean_absolute_error: 35.226091, mean_q: 45.339438, mean_eps: 0.100000
 1825979/2000000: episode: 8413, duration: 2.644s, episode steps: 168, steps per second: 64, episode reward: 2.340, mean reward: 0.014 [-100.000, 16.166], mean action: 1.565 [0.000, 3.000], mean observation: 0.101 [-0.756, 1.000], loss: 1.315533, mean_absolute_error: 34.496525, mean_q: 44.464742, mean_eps: 0.100000
 1826395/2000000: episode: 8414, duration: 6.556s, episode steps: 416, steps per second: 63, episode reward: 239.446, mean reward: 0.576 [-19.630, 100.000], mean action: 1.101 [0.000, 3.000], mean observation: 0.152 [-0.892, 1.000], loss: 1.518683, mean_absolute_error: 34.446122, mean_q: 44.486312, mean_eps: 0.100000
 1827395/2000000: episode: 8415, duration: 16.686s, episode steps: 1000, steps per second: 60, episode reward: -135.173, mean reward: -0.135 [-5.464, 4.467], mean action: 1.656 [0.000, 3.000], mean observation: 0.131 [-1.022, 0.930], loss: 1.307654, mean_absolute_error: 35.311438, mean_q: 45.374248, mean_eps: 0.100000
 1827535/2000000: episode: 8416, duration: 2.286s, episode steps: 140, steps per second: 61, episode reward: -38.973, mean reward: -0.278 [-100.000, 18.505], mean action: 1.514 [0.000, 3.000], mean observation: 0.071 [-1.351, 1.000], loss: 1.578837, mean_absolute_error: 35.502744, mean_q: 45.765598, mean_eps: 0.100000
 1827664/2000000: episode: 8417, duration: 2.087s, episode steps: 129, steps per second: 62, episode reward: -24.302, mean reward: -0.188 [-100.000, 10.191], mean action: 1.550 [0.000, 3.000], mean observation: -0.066 [-0.890, 1.000], loss: 1.096302, mean_absolute_error: 33.812340, mean_q: 43.735486, mean_eps: 0.100000
 1827784/2000000: episode: 8418, duration: 1.929s, episode steps: 120, steps per second: 62, episode reward: -163.393, mean reward: -1.362 [-100.000, 14.508], mean action: 1.158 [0.000, 3.000], mean observation: 0.026 [-2.796, 1.000], loss: 1.759392, mean_absolute_error: 34.681583, mean_q: 44.689440, mean_eps: 0.100000
 1828123/2000000: episode: 8419, duration: 5.408s, episode steps: 339, steps per second: 63, episode reward: 163.833, mean reward: 0.483 [-17.413, 100.000], mean action: 1.236 [0.000, 3.000], mean observation: 0.066 [-1.412, 1.000], loss: 1.270687, mean_absolute_error: 35.783830, mean_q: 46.198030, mean_eps: 0.100000
 1828230/2000000: episode: 8420, duration: 1.677s, episode steps: 107, steps per second: 64, episode reward: -34.858, mean reward: -0.326 [-100.000, 16.220], mean action: 1.822 [0.000, 3.000], mean observation: -0.069 [-0.960, 1.397], loss: 1.639389, mean_absolute_error: 36.208622, mean_q: 46.428839, mean_eps: 0.100000
 1829032/2000000: episode: 8421, duration: 13.491s, episode steps: 802, steps per second: 59, episode reward: 167.015, mean reward: 0.208 [-21.202, 100.000], mean action: 2.099 [0.000, 3.000], mean observation: 0.108 [-0.861, 1.000], loss: 1.090730, mean_absolute_error: 34.935063, mean_q: 45.155173, mean_eps: 0.100000
 1829162/2000000: episode: 8422, duration: 2.059s, episode steps: 130, steps per second: 63, episode reward: -71.630, mean reward: -0.551 [-100.000, 17.798], mean action: 1.362 [0.000, 3.000], mean observation: 0.015 [-1.636, 1.000], loss: 1.348972, mean_absolute_error: 35.575873, mean_q: 46.890427, mean_eps: 0.100000
 1829616/2000000: episode: 8423, duration: 7.516s, episode steps: 454, steps per second: 60, episode reward: 136.505, mean reward: 0.301 [-10.943, 100.000], mean action: 1.236 [0.000, 3.000], mean observation: 0.003 [-0.975, 1.000], loss: 1.439000, mean_absolute_error: 34.431454, mean_q: 44.110285, mean_eps: 0.100000
 1829731/2000000: episode: 8424, duration: 1.816s, episode steps: 115, steps per second: 63, episode reward: -22.330, mean reward: -0.194 [-100.000, 10.494], mean action: 1.957 [0.000, 3.000], mean observation: -0.040 [-0.933, 1.195], loss: 1.046166, mean_absolute_error: 36.496143, mean_q: 47.252145, mean_eps: 0.100000
 1829865/2000000: episode: 8425, duration: 2.101s, episode steps: 134, steps per second: 64, episode reward: -61.471, mean reward: -0.459 [-100.000, 19.833], mean action: 1.448 [0.000, 3.000], mean observation: -0.019 [-1.590, 1.000], loss: 1.139172, mean_absolute_error: 33.834151, mean_q: 44.452765, mean_eps: 0.100000
 1830029/2000000: episode: 8426, duration: 2.534s, episode steps: 164, steps per second: 65, episode reward: -28.283, mean reward: -0.172 [-100.000, 9.529], mean action: 1.494 [0.000, 3.000], mean observation: 0.081 [-1.242, 1.000], loss: 1.122708, mean_absolute_error: 36.298944, mean_q: 47.442687, mean_eps: 0.100000
 1830153/2000000: episode: 8427, duration: 1.917s, episode steps: 124, steps per second: 65, episode reward: -110.658, mean reward: -0.892 [-100.000, 29.990], mean action: 1.468 [0.000, 3.000], mean observation: 0.013 [-1.576, 1.000], loss: 0.948452, mean_absolute_error: 32.737571, mean_q: 42.083983, mean_eps: 0.100000
 1830541/2000000: episode: 8428, duration: 6.169s, episode steps: 388, steps per second: 63, episode reward: 135.359, mean reward: 0.349 [-18.293, 100.000], mean action: 1.915 [0.000, 3.000], mean observation: 0.010 [-0.910, 1.000], loss: 1.447742, mean_absolute_error: 35.399706, mean_q: 45.164375, mean_eps: 0.100000
 1831541/2000000: episode: 8429, duration: 16.192s, episode steps: 1000, steps per second: 62, episode reward: 54.357, mean reward: 0.054 [-18.939, 22.339], mean action: 0.948 [0.000, 3.000], mean observation: 0.114 [-0.969, 1.000], loss: 1.195125, mean_absolute_error: 35.024936, mean_q: 45.524254, mean_eps: 0.100000
 1831667/2000000: episode: 8430, duration: 1.963s, episode steps: 126, steps per second: 64, episode reward: -43.618, mean reward: -0.346 [-100.000, 18.014], mean action: 1.873 [0.000, 3.000], mean observation: 0.080 [-1.562, 1.188], loss: 1.197632, mean_absolute_error: 36.681896, mean_q: 47.273756, mean_eps: 0.100000
 1831768/2000000: episode: 8431, duration: 1.633s, episode steps: 101, steps per second: 62, episode reward: -81.167, mean reward: -0.804 [-100.000, 27.125], mean action: 1.911 [0.000, 3.000], mean observation: 0.052 [-0.984, 1.835], loss: 1.503050, mean_absolute_error: 34.906734, mean_q: 45.103831, mean_eps: 0.100000
 1832131/2000000: episode: 8432, duration: 5.779s, episode steps: 363, steps per second: 63, episode reward: 241.146, mean reward: 0.664 [-17.645, 100.000], mean action: 1.427 [0.000, 3.000], mean observation: 0.071 [-0.893, 1.000], loss: 1.308739, mean_absolute_error: 34.932007, mean_q: 45.129805, mean_eps: 0.100000
 1832404/2000000: episode: 8433, duration: 4.499s, episode steps: 273, steps per second: 61, episode reward: 252.425, mean reward: 0.925 [-10.313, 100.000], mean action: 1.571 [0.000, 3.000], mean observation: 0.106 [-1.461, 1.000], loss: 1.171873, mean_absolute_error: 34.710102, mean_q: 44.682423, mean_eps: 0.100000
 1832484/2000000: episode: 8434, duration: 1.301s, episode steps: 80, steps per second: 61, episode reward: -79.732, mean reward: -0.997 [-100.000, 12.196], mean action: 1.587 [0.000, 3.000], mean observation: -0.147 [-1.688, 1.000], loss: 1.543695, mean_absolute_error: 36.002673, mean_q: 44.961646, mean_eps: 0.100000
 1832720/2000000: episode: 8435, duration: 3.776s, episode steps: 236, steps per second: 62, episode reward: 236.072, mean reward: 1.000 [-17.369, 100.000], mean action: 1.271 [0.000, 3.000], mean observation: 0.112 [-1.379, 1.000], loss: 1.331455, mean_absolute_error: 34.989441, mean_q: 44.798665, mean_eps: 0.100000
 1832967/2000000: episode: 8436, duration: 3.923s, episode steps: 247, steps per second: 63, episode reward: 223.657, mean reward: 0.905 [-10.261, 100.000], mean action: 1.356 [0.000, 3.000], mean observation: 0.080 [-0.956, 1.000], loss: 1.535968, mean_absolute_error: 34.648172, mean_q: 43.106584, mean_eps: 0.100000
 1833204/2000000: episode: 8437, duration: 3.787s, episode steps: 237, steps per second: 63, episode reward: 221.271, mean reward: 0.934 [-18.464, 100.000], mean action: 1.198 [0.000, 3.000], mean observation: 0.048 [-0.950, 1.000], loss: 1.276795, mean_absolute_error: 34.300896, mean_q: 44.596346, mean_eps: 0.100000
 1833313/2000000: episode: 8438, duration: 1.766s, episode steps: 109, steps per second: 62, episode reward: -40.713, mean reward: -0.374 [-100.000, 12.143], mean action: 1.734 [0.000, 3.000], mean observation: 0.092 [-0.973, 1.000], loss: 1.540120, mean_absolute_error: 34.101566, mean_q: 44.153000, mean_eps: 0.100000
 1833530/2000000: episode: 8439, duration: 3.395s, episode steps: 217, steps per second: 64, episode reward: 239.404, mean reward: 1.103 [-17.861, 100.000], mean action: 1.267 [0.000, 3.000], mean observation: 0.124 [-1.407, 1.000], loss: 1.177378, mean_absolute_error: 34.035909, mean_q: 44.129535, mean_eps: 0.100000
 1833659/2000000: episode: 8440, duration: 1.975s, episode steps: 129, steps per second: 65, episode reward: -96.544, mean reward: -0.748 [-100.000, 9.391], mean action: 1.581 [0.000, 3.000], mean observation: 0.021 [-2.624, 1.010], loss: 1.553817, mean_absolute_error: 34.694397, mean_q: 44.957304, mean_eps: 0.100000
 1833772/2000000: episode: 8441, duration: 1.785s, episode steps: 113, steps per second: 63, episode reward: -96.080, mean reward: -0.850 [-100.000, 8.373], mean action: 1.735 [0.000, 3.000], mean observation: -0.011 [-2.683, 1.000], loss: 1.271751, mean_absolute_error: 34.832066, mean_q: 45.409915, mean_eps: 0.100000
 1834772/2000000: episode: 8442, duration: 16.786s, episode steps: 1000, steps per second: 60, episode reward: 113.454, mean reward: 0.113 [-22.719, 22.654], mean action: 1.291 [0.000, 3.000], mean observation: 0.165 [-0.906, 1.000], loss: 1.303838, mean_absolute_error: 34.783072, mean_q: 44.899823, mean_eps: 0.100000
 1834874/2000000: episode: 8443, duration: 1.805s, episode steps: 102, steps per second: 56, episode reward: -97.160, mean reward: -0.953 [-100.000, 9.556], mean action: 1.755 [0.000, 3.000], mean observation: -0.047 [-1.921, 1.000], loss: 1.406021, mean_absolute_error: 35.155361, mean_q: 45.737066, mean_eps: 0.100000
 1834980/2000000: episode: 8444, duration: 1.728s, episode steps: 106, steps per second: 61, episode reward: -53.122, mean reward: -0.501 [-100.000, 19.070], mean action: 1.962 [1.000, 3.000], mean observation: 0.044 [-1.889, 1.000], loss: 1.207920, mean_absolute_error: 33.063652, mean_q: 41.274457, mean_eps: 0.100000
 1835281/2000000: episode: 8445, duration: 4.818s, episode steps: 301, steps per second: 62, episode reward: 217.360, mean reward: 0.722 [-8.039, 100.000], mean action: 1.173 [0.000, 3.000], mean observation: 0.137 [-0.998, 1.000], loss: 1.412501, mean_absolute_error: 35.099970, mean_q: 45.447483, mean_eps: 0.100000
 1835553/2000000: episode: 8446, duration: 4.373s, episode steps: 272, steps per second: 62, episode reward: 241.820, mean reward: 0.889 [-10.640, 100.000], mean action: 1.272 [0.000, 3.000], mean observation: 0.090 [-0.907, 1.000], loss: 1.260892, mean_absolute_error: 36.024603, mean_q: 46.485811, mean_eps: 0.100000
 1836475/2000000: episode: 8447, duration: 15.301s, episode steps: 922, steps per second: 60, episode reward: 165.125, mean reward: 0.179 [-18.590, 100.000], mean action: 0.843 [0.000, 3.000], mean observation: 0.148 [-0.906, 1.000], loss: 1.152768, mean_absolute_error: 34.918173, mean_q: 45.412681, mean_eps: 0.100000
 1836731/2000000: episode: 8448, duration: 4.020s, episode steps: 256, steps per second: 64, episode reward: 226.018, mean reward: 0.883 [-4.332, 100.000], mean action: 0.992 [0.000, 3.000], mean observation: 0.145 [-1.142, 1.000], loss: 1.273058, mean_absolute_error: 34.030686, mean_q: 43.833323, mean_eps: 0.100000
 1837731/2000000: episode: 8449, duration: 16.492s, episode steps: 1000, steps per second: 61, episode reward: -20.221, mean reward: -0.020 [-9.246, 14.100], mean action: 1.440 [0.000, 3.000], mean observation: 0.087 [-1.286, 1.000], loss: 1.475665, mean_absolute_error: 34.538542, mean_q: 44.035883, mean_eps: 0.100000
 1837861/2000000: episode: 8450, duration: 2.028s, episode steps: 130, steps per second: 64, episode reward: -79.293, mean reward: -0.610 [-100.000, 21.762], mean action: 1.469 [0.000, 3.000], mean observation: -0.023 [-1.847, 1.001], loss: 1.253512, mean_absolute_error: 36.761226, mean_q: 47.842342, mean_eps: 0.100000
 1838101/2000000: episode: 8451, duration: 3.716s, episode steps: 240, steps per second: 65, episode reward: 227.399, mean reward: 0.947 [-6.809, 100.000], mean action: 1.258 [0.000, 3.000], mean observation: 0.081 [-0.915, 1.000], loss: 1.512515, mean_absolute_error: 35.864419, mean_q: 45.638149, mean_eps: 0.100000
 1838267/2000000: episode: 8452, duration: 2.527s, episode steps: 166, steps per second: 66, episode reward: -19.579, mean reward: -0.118 [-100.000, 12.112], mean action: 1.614 [0.000, 3.000], mean observation: -0.045 [-1.022, 1.000], loss: 1.214301, mean_absolute_error: 35.236407, mean_q: 46.166943, mean_eps: 0.100000
 1838454/2000000: episode: 8453, duration: 2.931s, episode steps: 187, steps per second: 64, episode reward: -49.866, mean reward: -0.267 [-100.000, 16.694], mean action: 2.027 [0.000, 3.000], mean observation: -0.052 [-1.142, 1.000], loss: 1.115873, mean_absolute_error: 33.692077, mean_q: 42.791527, mean_eps: 0.100000
 1838737/2000000: episode: 8454, duration: 4.416s, episode steps: 283, steps per second: 64, episode reward: 244.176, mean reward: 0.863 [-19.991, 100.000], mean action: 1.106 [0.000, 3.000], mean observation: 0.147 [-1.436, 1.000], loss: 1.073008, mean_absolute_error: 34.579944, mean_q: 44.282276, mean_eps: 0.100000
 1838866/2000000: episode: 8455, duration: 1.999s, episode steps: 129, steps per second: 65, episode reward: -69.359, mean reward: -0.538 [-100.000, 51.582], mean action: 1.845 [0.000, 3.000], mean observation: -0.070 [-0.893, 1.246], loss: 1.377424, mean_absolute_error: 35.885108, mean_q: 46.998132, mean_eps: 0.100000
 1839357/2000000: episode: 8456, duration: 7.915s, episode steps: 491, steps per second: 62, episode reward: 214.305, mean reward: 0.436 [-17.864, 100.000], mean action: 1.004 [0.000, 3.000], mean observation: 0.081 [-0.810, 1.000], loss: 1.195103, mean_absolute_error: 34.501028, mean_q: 44.708810, mean_eps: 0.100000
 1839470/2000000: episode: 8457, duration: 1.744s, episode steps: 113, steps per second: 65, episode reward: -19.709, mean reward: -0.174 [-100.000, 19.494], mean action: 1.805 [0.000, 3.000], mean observation: 0.024 [-1.724, 1.000], loss: 1.111625, mean_absolute_error: 36.245970, mean_q: 46.512874, mean_eps: 0.100000
 1839944/2000000: episode: 8458, duration: 7.474s, episode steps: 474, steps per second: 63, episode reward: 218.901, mean reward: 0.462 [-19.022, 100.000], mean action: 0.962 [0.000, 3.000], mean observation: 0.136 [-1.516, 1.000], loss: 1.561511, mean_absolute_error: 34.150784, mean_q: 43.931051, mean_eps: 0.100000
 1840108/2000000: episode: 8459, duration: 2.583s, episode steps: 164, steps per second: 63, episode reward: -84.144, mean reward: -0.513 [-100.000, 7.244], mean action: 1.659 [0.000, 3.000], mean observation: -0.042 [-1.042, 1.275], loss: 1.331937, mean_absolute_error: 34.149845, mean_q: 44.259512, mean_eps: 0.100000
 1840229/2000000: episode: 8460, duration: 1.906s, episode steps: 121, steps per second: 63, episode reward: -87.988, mean reward: -0.727 [-100.000, 10.332], mean action: 1.645 [0.000, 3.000], mean observation: -0.057 [-1.600, 1.000], loss: 1.641407, mean_absolute_error: 36.081179, mean_q: 45.715329, mean_eps: 0.100000
 1840415/2000000: episode: 8461, duration: 2.856s, episode steps: 186, steps per second: 65, episode reward: -144.119, mean reward: -0.775 [-100.000, 8.077], mean action: 1.672 [0.000, 3.000], mean observation: -0.033 [-0.956, 1.000], loss: 1.247266, mean_absolute_error: 34.971926, mean_q: 44.942133, mean_eps: 0.100000
 1840573/2000000: episode: 8462, duration: 2.502s, episode steps: 158, steps per second: 63, episode reward: 12.892, mean reward: 0.082 [-100.000, 19.432], mean action: 1.791 [0.000, 3.000], mean observation: -0.044 [-1.480, 1.000], loss: 1.359703, mean_absolute_error: 35.188168, mean_q: 45.961195, mean_eps: 0.100000
 1840851/2000000: episode: 8463, duration: 4.358s, episode steps: 278, steps per second: 64, episode reward: 199.914, mean reward: 0.719 [-9.411, 100.000], mean action: 1.723 [0.000, 3.000], mean observation: 0.013 [-0.912, 1.000], loss: 1.106535, mean_absolute_error: 35.130192, mean_q: 45.097378, mean_eps: 0.100000
 1840987/2000000: episode: 8464, duration: 2.143s, episode steps: 136, steps per second: 63, episode reward: -40.846, mean reward: -0.300 [-100.000, 16.466], mean action: 1.456 [0.000, 3.000], mean observation: 0.044 [-1.878, 1.000], loss: 1.074036, mean_absolute_error: 35.192896, mean_q: 46.985774, mean_eps: 0.100000
 1841347/2000000: episode: 8465, duration: 5.732s, episode steps: 360, steps per second: 63, episode reward: 203.622, mean reward: 0.566 [-17.439, 100.000], mean action: 1.108 [0.000, 3.000], mean observation: 0.078 [-1.030, 1.000], loss: 1.184727, mean_absolute_error: 35.740468, mean_q: 45.991375, mean_eps: 0.100000
 1841459/2000000: episode: 8466, duration: 1.749s, episode steps: 112, steps per second: 64, episode reward: -84.048, mean reward: -0.750 [-100.000, 10.551], mean action: 1.205 [0.000, 3.000], mean observation: -0.056 [-1.626, 1.000], loss: 1.425419, mean_absolute_error: 33.458984, mean_q: 44.130494, mean_eps: 0.100000
 1841694/2000000: episode: 8467, duration: 3.674s, episode steps: 235, steps per second: 64, episode reward: 216.943, mean reward: 0.923 [-17.867, 100.000], mean action: 0.872 [0.000, 3.000], mean observation: 0.127 [-1.388, 1.000], loss: 1.221524, mean_absolute_error: 35.886665, mean_q: 46.003014, mean_eps: 0.100000
 1841846/2000000: episode: 8468, duration: 2.363s, episode steps: 152, steps per second: 64, episode reward: -10.241, mean reward: -0.067 [-100.000, 24.450], mean action: 1.579 [0.000, 3.000], mean observation: -0.048 [-2.213, 1.000], loss: 1.265127, mean_absolute_error: 34.440948, mean_q: 45.238482, mean_eps: 0.100000
 1841989/2000000: episode: 8469, duration: 2.231s, episode steps: 143, steps per second: 64, episode reward: -57.356, mean reward: -0.401 [-100.000, 10.146], mean action: 1.301 [0.000, 3.000], mean observation: -0.020 [-1.013, 1.000], loss: 0.991136, mean_absolute_error: 36.340981, mean_q: 46.852038, mean_eps: 0.100000
 1842135/2000000: episode: 8470, duration: 2.219s, episode steps: 146, steps per second: 66, episode reward: -7.167, mean reward: -0.049 [-100.000, 20.757], mean action: 1.733 [0.000, 3.000], mean observation: -0.032 [-1.504, 1.000], loss: 0.995817, mean_absolute_error: 36.143425, mean_q: 47.067934, mean_eps: 0.100000
 1842274/2000000: episode: 8471, duration: 2.201s, episode steps: 139, steps per second: 63, episode reward: -30.241, mean reward: -0.218 [-100.000, 13.423], mean action: 1.590 [0.000, 3.000], mean observation: -0.065 [-0.972, 1.219], loss: 1.179269, mean_absolute_error: 35.754904, mean_q: 47.202292, mean_eps: 0.100000
 1842411/2000000: episode: 8472, duration: 2.321s, episode steps: 137, steps per second: 59, episode reward: -72.689, mean reward: -0.531 [-100.000, 14.493], mean action: 1.336 [0.000, 3.000], mean observation: 0.013 [-1.474, 1.000], loss: 1.563313, mean_absolute_error: 35.167622, mean_q: 45.974298, mean_eps: 0.100000
 1842786/2000000: episode: 8473, duration: 5.977s, episode steps: 375, steps per second: 63, episode reward: 191.484, mean reward: 0.511 [-19.637, 100.000], mean action: 1.059 [0.000, 3.000], mean observation: 0.119 [-1.304, 1.000], loss: 1.158995, mean_absolute_error: 34.425570, mean_q: 44.173774, mean_eps: 0.100000
 1842996/2000000: episode: 8474, duration: 3.325s, episode steps: 210, steps per second: 63, episode reward: 254.348, mean reward: 1.211 [-17.509, 100.000], mean action: 1.433 [0.000, 3.000], mean observation: 0.132 [-0.925, 1.000], loss: 1.175680, mean_absolute_error: 36.074714, mean_q: 46.448769, mean_eps: 0.100000
 1843248/2000000: episode: 8475, duration: 4.024s, episode steps: 252, steps per second: 63, episode reward: 224.329, mean reward: 0.890 [-9.703, 100.000], mean action: 0.944 [0.000, 3.000], mean observation: -0.014 [-1.075, 1.581], loss: 1.350293, mean_absolute_error: 36.547859, mean_q: 46.940127, mean_eps: 0.100000
 1843590/2000000: episode: 8476, duration: 5.430s, episode steps: 342, steps per second: 63, episode reward: 155.794, mean reward: 0.456 [-17.145, 100.000], mean action: 1.240 [0.000, 3.000], mean observation: 0.055 [-0.960, 1.000], loss: 1.169657, mean_absolute_error: 34.330308, mean_q: 44.132328, mean_eps: 0.100000
 1843748/2000000: episode: 8477, duration: 2.520s, episode steps: 158, steps per second: 63, episode reward: -219.732, mean reward: -1.391 [-100.000, 13.791], mean action: 1.595 [0.000, 3.000], mean observation: -0.203 [-1.737, 1.000], loss: 1.314230, mean_absolute_error: 35.540657, mean_q: 44.201895, mean_eps: 0.100000
 1843912/2000000: episode: 8478, duration: 2.609s, episode steps: 164, steps per second: 63, episode reward: -5.364, mean reward: -0.033 [-100.000, 15.708], mean action: 1.720 [0.000, 3.000], mean observation: 0.105 [-0.799, 1.003], loss: 1.429559, mean_absolute_error: 35.272768, mean_q: 45.121873, mean_eps: 0.100000
 1844075/2000000: episode: 8479, duration: 2.560s, episode steps: 163, steps per second: 64, episode reward: -52.773, mean reward: -0.324 [-100.000, 11.487], mean action: 1.779 [0.000, 3.000], mean observation: -0.017 [-1.747, 1.000], loss: 1.404043, mean_absolute_error: 34.644137, mean_q: 44.362723, mean_eps: 0.100000
 1844161/2000000: episode: 8480, duration: 1.398s, episode steps: 86, steps per second: 62, episode reward: -58.004, mean reward: -0.674 [-100.000, 6.109], mean action: 1.814 [0.000, 3.000], mean observation: 0.074 [-1.937, 1.000], loss: 1.478611, mean_absolute_error: 34.944395, mean_q: 45.248608, mean_eps: 0.100000
 1844482/2000000: episode: 8481, duration: 5.200s, episode steps: 321, steps per second: 62, episode reward: 174.920, mean reward: 0.545 [-19.097, 100.000], mean action: 2.555 [0.000, 3.000], mean observation: 0.247 [-1.032, 1.483], loss: 1.310110, mean_absolute_error: 35.074564, mean_q: 44.905182, mean_eps: 0.100000
 1844583/2000000: episode: 8482, duration: 1.561s, episode steps: 101, steps per second: 65, episode reward: -66.257, mean reward: -0.656 [-100.000, 10.945], mean action: 1.257 [0.000, 3.000], mean observation: 0.052 [-0.970, 1.220], loss: 1.038696, mean_absolute_error: 35.799112, mean_q: 46.554300, mean_eps: 0.100000
 1844969/2000000: episode: 8483, duration: 6.112s, episode steps: 386, steps per second: 63, episode reward: 234.206, mean reward: 0.607 [-19.193, 100.000], mean action: 1.624 [0.000, 3.000], mean observation: 0.163 [-1.124, 1.032], loss: 1.105892, mean_absolute_error: 34.981601, mean_q: 45.582365, mean_eps: 0.100000
 1845070/2000000: episode: 8484, duration: 1.575s, episode steps: 101, steps per second: 64, episode reward: -47.310, mean reward: -0.468 [-100.000, 4.253], mean action: 1.525 [0.000, 3.000], mean observation: -0.136 [-1.017, 0.928], loss: 1.237579, mean_absolute_error: 35.381212, mean_q: 45.960314, mean_eps: 0.100000
 1845455/2000000: episode: 8485, duration: 5.995s, episode steps: 385, steps per second: 64, episode reward: 239.668, mean reward: 0.623 [-18.198, 100.000], mean action: 0.790 [0.000, 3.000], mean observation: 0.111 [-0.941, 1.000], loss: 1.430411, mean_absolute_error: 34.096959, mean_q: 43.674093, mean_eps: 0.100000
 1845817/2000000: episode: 8486, duration: 5.765s, episode steps: 362, steps per second: 63, episode reward: 195.736, mean reward: 0.541 [-9.973, 100.000], mean action: 2.080 [0.000, 3.000], mean observation: 0.030 [-0.914, 1.000], loss: 1.228928, mean_absolute_error: 34.915712, mean_q: 44.331670, mean_eps: 0.100000
 1845930/2000000: episode: 8487, duration: 1.753s, episode steps: 113, steps per second: 64, episode reward: -53.658, mean reward: -0.475 [-100.000, 17.657], mean action: 1.876 [0.000, 3.000], mean observation: -0.008 [-2.235, 1.000], loss: 1.464653, mean_absolute_error: 35.432458, mean_q: 44.745959, mean_eps: 0.100000
 1846220/2000000: episode: 8488, duration: 4.576s, episode steps: 290, steps per second: 63, episode reward: 212.946, mean reward: 0.734 [-19.554, 100.000], mean action: 0.979 [0.000, 3.000], mean observation: 0.067 [-1.577, 1.000], loss: 1.128704, mean_absolute_error: 34.944823, mean_q: 44.718457, mean_eps: 0.100000
 1846653/2000000: episode: 8489, duration: 6.883s, episode steps: 433, steps per second: 63, episode reward: 233.806, mean reward: 0.540 [-17.560, 100.000], mean action: 0.938 [0.000, 3.000], mean observation: 0.162 [-0.946, 1.007], loss: 1.416893, mean_absolute_error: 34.604932, mean_q: 44.588986, mean_eps: 0.100000
 1846887/2000000: episode: 8490, duration: 3.599s, episode steps: 234, steps per second: 65, episode reward: 232.824, mean reward: 0.995 [-16.318, 100.000], mean action: 1.051 [0.000, 3.000], mean observation: 0.137 [-1.598, 1.000], loss: 1.094859, mean_absolute_error: 34.774761, mean_q: 44.216043, mean_eps: 0.100000
 1846990/2000000: episode: 8491, duration: 1.611s, episode steps: 103, steps per second: 64, episode reward: -33.589, mean reward: -0.326 [-100.000, 11.267], mean action: 1.757 [0.000, 3.000], mean observation: -0.075 [-0.975, 1.494], loss: 1.116568, mean_absolute_error: 35.153637, mean_q: 46.138643, mean_eps: 0.100000
 1847361/2000000: episode: 8492, duration: 5.859s, episode steps: 371, steps per second: 63, episode reward: 191.445, mean reward: 0.516 [-19.513, 100.000], mean action: 1.210 [0.000, 3.000], mean observation: 0.089 [-0.877, 1.000], loss: 1.108472, mean_absolute_error: 35.522212, mean_q: 45.898902, mean_eps: 0.100000
 1847706/2000000: episode: 8493, duration: 5.543s, episode steps: 345, steps per second: 62, episode reward: 186.272, mean reward: 0.540 [-17.508, 100.000], mean action: 1.284 [0.000, 3.000], mean observation: 0.028 [-0.944, 1.000], loss: 1.370340, mean_absolute_error: 34.725486, mean_q: 44.242903, mean_eps: 0.100000
 1847832/2000000: episode: 8494, duration: 1.995s, episode steps: 126, steps per second: 63, episode reward: -13.271, mean reward: -0.105 [-100.000, 18.224], mean action: 1.341 [0.000, 3.000], mean observation: -0.018 [-0.995, 1.000], loss: 1.026381, mean_absolute_error: 34.145161, mean_q: 44.126307, mean_eps: 0.100000
 1847968/2000000: episode: 8495, duration: 2.163s, episode steps: 136, steps per second: 63, episode reward: -53.446, mean reward: -0.393 [-100.000, 19.255], mean action: 1.706 [0.000, 3.000], mean observation: -0.059 [-1.035, 1.635], loss: 1.424421, mean_absolute_error: 36.588169, mean_q: 46.176499, mean_eps: 0.100000
 1848512/2000000: episode: 8496, duration: 8.685s, episode steps: 544, steps per second: 63, episode reward: 236.458, mean reward: 0.435 [-17.885, 100.000], mean action: 1.026 [0.000, 3.000], mean observation: 0.090 [-0.822, 1.000], loss: 1.388946, mean_absolute_error: 34.666956, mean_q: 44.984314, mean_eps: 0.100000
 1848625/2000000: episode: 8497, duration: 1.823s, episode steps: 113, steps per second: 62, episode reward: -29.368, mean reward: -0.260 [-100.000, 9.377], mean action: 1.823 [0.000, 3.000], mean observation: 0.026 [-1.668, 1.000], loss: 1.153636, mean_absolute_error: 35.916162, mean_q: 46.158450, mean_eps: 0.100000
 1848969/2000000: episode: 8498, duration: 5.347s, episode steps: 344, steps per second: 64, episode reward: 234.140, mean reward: 0.681 [-19.717, 100.000], mean action: 1.163 [0.000, 3.000], mean observation: 0.114 [-0.775, 1.000], loss: 1.340782, mean_absolute_error: 35.263120, mean_q: 45.042797, mean_eps: 0.100000
 1849215/2000000: episode: 8499, duration: 3.792s, episode steps: 246, steps per second: 65, episode reward: 239.881, mean reward: 0.975 [-7.325, 100.000], mean action: 1.268 [0.000, 3.000], mean observation: 0.131 [-1.265, 1.002], loss: 1.154840, mean_absolute_error: 35.084578, mean_q: 45.467279, mean_eps: 0.100000
 1849325/2000000: episode: 8500, duration: 1.729s, episode steps: 110, steps per second: 64, episode reward: -59.075, mean reward: -0.537 [-100.000, 16.122], mean action: 1.518 [0.000, 3.000], mean observation: 0.029 [-2.101, 1.000], loss: 1.200994, mean_absolute_error: 33.564707, mean_q: 44.216500, mean_eps: 0.100000
 1849448/2000000: episode: 8501, duration: 1.894s, episode steps: 123, steps per second: 65, episode reward: -86.521, mean reward: -0.703 [-100.000, 10.817], mean action: 1.626 [0.000, 3.000], mean observation: 0.044 [-2.046, 1.000], loss: 1.173630, mean_absolute_error: 35.021761, mean_q: 45.506461, mean_eps: 0.100000
 1849562/2000000: episode: 8502, duration: 1.772s, episode steps: 114, steps per second: 64, episode reward: -54.029, mean reward: -0.474 [-100.000, 18.151], mean action: 1.789 [0.000, 3.000], mean observation: 0.003 [-1.827, 1.000], loss: 1.219064, mean_absolute_error: 36.523183, mean_q: 46.762086, mean_eps: 0.100000
 1849669/2000000: episode: 8503, duration: 1.649s, episode steps: 107, steps per second: 65, episode reward: -43.653, mean reward: -0.408 [-100.000, 18.377], mean action: 1.991 [0.000, 3.000], mean observation: 0.055 [-1.649, 1.000], loss: 1.175276, mean_absolute_error: 35.283740, mean_q: 46.320058, mean_eps: 0.100000
 1849920/2000000: episode: 8504, duration: 4.252s, episode steps: 251, steps per second: 59, episode reward: 240.078, mean reward: 0.956 [-8.354, 100.000], mean action: 1.163 [0.000, 3.000], mean observation: 0.081 [-0.938, 1.000], loss: 1.050335, mean_absolute_error: 35.317921, mean_q: 46.448942, mean_eps: 0.100000
 1850077/2000000: episode: 8505, duration: 2.500s, episode steps: 157, steps per second: 63, episode reward: -5.834, mean reward: -0.037 [-100.000, 21.896], mean action: 1.433 [0.000, 3.000], mean observation: -0.015 [-1.543, 1.000], loss: 1.265130, mean_absolute_error: 35.965181, mean_q: 46.444715, mean_eps: 0.100000
 1850207/2000000: episode: 8506, duration: 1.974s, episode steps: 130, steps per second: 66, episode reward: -53.836, mean reward: -0.414 [-100.000, 13.229], mean action: 1.238 [0.000, 3.000], mean observation: -0.043 [-1.572, 1.000], loss: 1.167626, mean_absolute_error: 33.572265, mean_q: 42.097367, mean_eps: 0.100000
 1850483/2000000: episode: 8507, duration: 4.441s, episode steps: 276, steps per second: 62, episode reward: 221.900, mean reward: 0.804 [-9.425, 100.000], mean action: 0.812 [0.000, 3.000], mean observation: 0.112 [-0.937, 1.000], loss: 1.238270, mean_absolute_error: 35.347702, mean_q: 45.872658, mean_eps: 0.100000
 1850608/2000000: episode: 8508, duration: 1.960s, episode steps: 125, steps per second: 64, episode reward: -93.359, mean reward: -0.747 [-100.000, 17.184], mean action: 1.840 [0.000, 3.000], mean observation: 0.022 [-1.449, 1.000], loss: 1.155323, mean_absolute_error: 35.268630, mean_q: 45.611863, mean_eps: 0.100000
 1850864/2000000: episode: 8509, duration: 3.982s, episode steps: 256, steps per second: 64, episode reward: 230.303, mean reward: 0.900 [-9.585, 100.000], mean action: 0.973 [0.000, 3.000], mean observation: 0.115 [-1.260, 1.000], loss: 1.346742, mean_absolute_error: 35.823196, mean_q: 44.862169, mean_eps: 0.100000
 1850962/2000000: episode: 8510, duration: 1.576s, episode steps: 98, steps per second: 62, episode reward: -50.314, mean reward: -0.513 [-100.000, 29.477], mean action: 1.939 [0.000, 3.000], mean observation: 0.053 [-2.249, 1.000], loss: 1.254031, mean_absolute_error: 35.807107, mean_q: 46.502648, mean_eps: 0.100000
 1851075/2000000: episode: 8511, duration: 1.754s, episode steps: 113, steps per second: 64, episode reward: -93.740, mean reward: -0.830 [-100.000, 18.836], mean action: 1.929 [0.000, 3.000], mean observation: -0.021 [-1.816, 1.000], loss: 1.150115, mean_absolute_error: 34.328753, mean_q: 45.362724, mean_eps: 0.100000
 1851483/2000000: episode: 8512, duration: 6.468s, episode steps: 408, steps per second: 63, episode reward: 141.600, mean reward: 0.347 [-19.534, 100.000], mean action: 1.289 [0.000, 3.000], mean observation: 0.053 [-0.862, 1.000], loss: 1.234391, mean_absolute_error: 35.145627, mean_q: 45.427835, mean_eps: 0.100000
 1851602/2000000: episode: 8513, duration: 1.863s, episode steps: 119, steps per second: 64, episode reward: -110.218, mean reward: -0.926 [-100.000, 15.141], mean action: 1.513 [0.000, 3.000], mean observation: 0.013 [-3.304, 1.000], loss: 1.007505, mean_absolute_error: 35.889348, mean_q: 46.993514, mean_eps: 0.100000
 1851762/2000000: episode: 8514, duration: 2.475s, episode steps: 160, steps per second: 65, episode reward: -45.086, mean reward: -0.282 [-100.000, 19.969], mean action: 1.475 [0.000, 3.000], mean observation: -0.011 [-1.294, 1.228], loss: 1.251604, mean_absolute_error: 36.076073, mean_q: 46.301284, mean_eps: 0.100000
 1852140/2000000: episode: 8515, duration: 6.057s, episode steps: 378, steps per second: 62, episode reward: 221.397, mean reward: 0.586 [-18.900, 100.000], mean action: 0.931 [0.000, 3.000], mean observation: 0.187 [-1.168, 1.000], loss: 1.146376, mean_absolute_error: 34.999692, mean_q: 45.154062, mean_eps: 0.100000
 1853140/2000000: episode: 8516, duration: 16.125s, episode steps: 1000, steps per second: 62, episode reward: 53.728, mean reward: 0.054 [-19.277, 22.679], mean action: 1.056 [0.000, 3.000], mean observation: 0.192 [-0.936, 1.000], loss: 1.276065, mean_absolute_error: 35.191873, mean_q: 45.440415, mean_eps: 0.100000
 1853261/2000000: episode: 8517, duration: 1.896s, episode steps: 121, steps per second: 64, episode reward: -39.074, mean reward: -0.323 [-100.000, 21.168], mean action: 1.256 [0.000, 3.000], mean observation: 0.020 [-1.708, 1.000], loss: 1.073418, mean_absolute_error: 34.103844, mean_q: 45.304542, mean_eps: 0.100000
 1853360/2000000: episode: 8518, duration: 1.547s, episode steps: 99, steps per second: 64, episode reward: -45.794, mean reward: -0.463 [-100.000, 18.713], mean action: 1.747 [0.000, 3.000], mean observation: -0.152 [-1.001, 1.068], loss: 1.118703, mean_absolute_error: 33.673607, mean_q: 44.176952, mean_eps: 0.100000
 1853529/2000000: episode: 8519, duration: 2.668s, episode steps: 169, steps per second: 63, episode reward: -43.878, mean reward: -0.260 [-100.000, 43.819], mean action: 1.893 [0.000, 3.000], mean observation: 0.090 [-1.182, 1.000], loss: 1.348070, mean_absolute_error: 35.352217, mean_q: 45.629070, mean_eps: 0.100000
 1853817/2000000: episode: 8520, duration: 4.504s, episode steps: 288, steps per second: 64, episode reward: 238.599, mean reward: 0.828 [-17.333, 100.000], mean action: 1.233 [0.000, 3.000], mean observation: 0.070 [-1.345, 1.000], loss: 1.193107, mean_absolute_error: 35.300372, mean_q: 45.970591, mean_eps: 0.100000
 1853988/2000000: episode: 8521, duration: 2.642s, episode steps: 171, steps per second: 65, episode reward: -29.281, mean reward: -0.171 [-100.000, 17.015], mean action: 1.942 [0.000, 3.000], mean observation: 0.053 [-1.191, 1.009], loss: 1.407399, mean_absolute_error: 34.104367, mean_q: 43.852421, mean_eps: 0.100000
 1854133/2000000: episode: 8522, duration: 2.257s, episode steps: 145, steps per second: 64, episode reward: -49.306, mean reward: -0.340 [-100.000, 14.015], mean action: 1.890 [0.000, 3.000], mean observation: -0.076 [-1.053, 1.326], loss: 1.450924, mean_absolute_error: 36.508491, mean_q: 46.472868, mean_eps: 0.100000
 1854234/2000000: episode: 8523, duration: 1.560s, episode steps: 101, steps per second: 65, episode reward: -4.688, mean reward: -0.046 [-100.000, 31.400], mean action: 1.743 [0.000, 3.000], mean observation: 0.061 [-1.264, 1.000], loss: 1.000212, mean_absolute_error: 33.402647, mean_q: 42.831630, mean_eps: 0.100000
 1854340/2000000: episode: 8524, duration: 1.646s, episode steps: 106, steps per second: 64, episode reward: -238.605, mean reward: -2.251 [-100.000, 62.541], mean action: 2.057 [0.000, 3.000], mean observation: 0.150 [-0.909, 2.200], loss: 1.076765, mean_absolute_error: 35.769635, mean_q: 46.944145, mean_eps: 0.100000
 1854578/2000000: episode: 8525, duration: 3.739s, episode steps: 238, steps per second: 64, episode reward: 136.699, mean reward: 0.574 [-12.287, 100.000], mean action: 1.395 [0.000, 3.000], mean observation: 0.051 [-1.565, 1.000], loss: 1.253669, mean_absolute_error: 35.143636, mean_q: 45.360220, mean_eps: 0.100000
 1854730/2000000: episode: 8526, duration: 2.374s, episode steps: 152, steps per second: 64, episode reward: -8.381, mean reward: -0.055 [-100.000, 14.353], mean action: 1.809 [0.000, 3.000], mean observation: -0.052 [-1.011, 1.000], loss: 1.177966, mean_absolute_error: 33.999462, mean_q: 43.618497, mean_eps: 0.100000
 1855090/2000000: episode: 8527, duration: 5.669s, episode steps: 360, steps per second: 64, episode reward: 115.940, mean reward: 0.322 [-17.780, 100.000], mean action: 1.767 [0.000, 3.000], mean observation: 0.022 [-0.941, 1.000], loss: 1.301594, mean_absolute_error: 35.289420, mean_q: 45.890504, mean_eps: 0.100000
 1855466/2000000: episode: 8528, duration: 5.835s, episode steps: 376, steps per second: 64, episode reward: -88.563, mean reward: -0.236 [-100.000, 11.453], mean action: 1.761 [0.000, 3.000], mean observation: -0.045 [-0.699, 1.171], loss: 1.110614, mean_absolute_error: 35.610972, mean_q: 46.194205, mean_eps: 0.100000
 1855789/2000000: episode: 8529, duration: 5.331s, episode steps: 323, steps per second: 61, episode reward: 255.638, mean reward: 0.791 [-18.243, 100.000], mean action: 1.285 [0.000, 3.000], mean observation: 0.062 [-1.480, 1.000], loss: 1.132705, mean_absolute_error: 35.110799, mean_q: 45.330779, mean_eps: 0.100000
 1856276/2000000: episode: 8530, duration: 7.832s, episode steps: 487, steps per second: 62, episode reward: 232.274, mean reward: 0.477 [-10.566, 100.000], mean action: 1.101 [0.000, 3.000], mean observation: 0.067 [-0.988, 1.000], loss: 1.181096, mean_absolute_error: 34.360891, mean_q: 44.362858, mean_eps: 0.100000
 1856473/2000000: episode: 8531, duration: 3.119s, episode steps: 197, steps per second: 63, episode reward: -24.435, mean reward: -0.124 [-100.000, 17.261], mean action: 1.695 [0.000, 3.000], mean observation: -0.019 [-0.907, 1.186], loss: 1.265412, mean_absolute_error: 36.709308, mean_q: 46.474857, mean_eps: 0.100000
 1856906/2000000: episode: 8532, duration: 6.891s, episode steps: 433, steps per second: 63, episode reward: 181.078, mean reward: 0.418 [-18.212, 100.000], mean action: 1.266 [0.000, 3.000], mean observation: 0.098 [-1.426, 1.016], loss: 1.377770, mean_absolute_error: 35.524630, mean_q: 45.412549, mean_eps: 0.100000
 1857207/2000000: episode: 8533, duration: 4.732s, episode steps: 301, steps per second: 64, episode reward: 220.137, mean reward: 0.731 [-8.583, 100.000], mean action: 1.050 [0.000, 3.000], mean observation: 0.141 [-0.794, 1.000], loss: 1.361338, mean_absolute_error: 34.193798, mean_q: 44.623531, mean_eps: 0.100000
 1857533/2000000: episode: 8534, duration: 5.419s, episode steps: 326, steps per second: 60, episode reward: 120.465, mean reward: 0.370 [-17.212, 100.000], mean action: 1.647 [0.000, 3.000], mean observation: 0.026 [-0.970, 1.000], loss: 1.175895, mean_absolute_error: 35.380989, mean_q: 45.769180, mean_eps: 0.100000
 1858089/2000000: episode: 8535, duration: 9.182s, episode steps: 556, steps per second: 61, episode reward: 169.566, mean reward: 0.305 [-17.630, 100.000], mean action: 1.261 [0.000, 3.000], mean observation: 0.078 [-0.751, 1.000], loss: 1.182888, mean_absolute_error: 34.641477, mean_q: 45.103215, mean_eps: 0.100000
 1858313/2000000: episode: 8536, duration: 3.482s, episode steps: 224, steps per second: 64, episode reward: 235.462, mean reward: 1.051 [-11.128, 100.000], mean action: 1.379 [0.000, 3.000], mean observation: 0.138 [-1.645, 1.000], loss: 1.201745, mean_absolute_error: 35.278321, mean_q: 45.418731, mean_eps: 0.100000
 1858484/2000000: episode: 8537, duration: 2.658s, episode steps: 171, steps per second: 64, episode reward: 22.993, mean reward: 0.134 [-100.000, 19.088], mean action: 1.608 [0.000, 3.000], mean observation: 0.071 [-0.702, 1.243], loss: 1.440810, mean_absolute_error: 35.038704, mean_q: 44.182425, mean_eps: 0.100000
 1858854/2000000: episode: 8538, duration: 5.878s, episode steps: 370, steps per second: 63, episode reward: 207.912, mean reward: 0.562 [-17.555, 100.000], mean action: 0.824 [0.000, 3.000], mean observation: 0.122 [-1.213, 1.000], loss: 1.274870, mean_absolute_error: 35.019918, mean_q: 44.782112, mean_eps: 0.100000
 1858985/2000000: episode: 8539, duration: 2.050s, episode steps: 131, steps per second: 64, episode reward: -50.850, mean reward: -0.388 [-100.000, 13.079], mean action: 1.573 [0.000, 3.000], mean observation: 0.019 [-1.832, 1.002], loss: 1.433240, mean_absolute_error: 35.815606, mean_q: 46.274009, mean_eps: 0.100000
 1859188/2000000: episode: 8540, duration: 3.195s, episode steps: 203, steps per second: 64, episode reward: 218.030, mean reward: 1.074 [-18.736, 100.000], mean action: 0.872 [0.000, 3.000], mean observation: 0.111 [-0.986, 1.000], loss: 1.256395, mean_absolute_error: 35.050318, mean_q: 44.280732, mean_eps: 0.100000
 1859688/2000000: episode: 8541, duration: 7.847s, episode steps: 500, steps per second: 64, episode reward: 242.133, mean reward: 0.484 [-18.671, 100.000], mean action: 0.846 [0.000, 3.000], mean observation: 0.171 [-0.968, 1.000], loss: 1.418630, mean_absolute_error: 35.196918, mean_q: 45.158106, mean_eps: 0.100000
 1859866/2000000: episode: 8542, duration: 2.788s, episode steps: 178, steps per second: 64, episode reward: -20.669, mean reward: -0.116 [-100.000, 12.426], mean action: 1.399 [0.000, 3.000], mean observation: -0.000 [-1.469, 1.014], loss: 1.608971, mean_absolute_error: 36.518113, mean_q: 47.185099, mean_eps: 0.100000
 1860441/2000000: episode: 8543, duration: 9.240s, episode steps: 575, steps per second: 62, episode reward: 246.831, mean reward: 0.429 [-17.978, 100.000], mean action: 1.174 [0.000, 3.000], mean observation: 0.051 [-1.796, 1.000], loss: 0.993423, mean_absolute_error: 34.227158, mean_q: 44.554890, mean_eps: 0.100000
 1860784/2000000: episode: 8544, duration: 5.435s, episode steps: 343, steps per second: 63, episode reward: 150.067, mean reward: 0.438 [-17.971, 100.000], mean action: 2.096 [0.000, 3.000], mean observation: 0.052 [-0.923, 1.000], loss: 1.182403, mean_absolute_error: 33.475178, mean_q: 43.371423, mean_eps: 0.100000
 1860874/2000000: episode: 8545, duration: 1.458s, episode steps: 90, steps per second: 62, episode reward: -70.235, mean reward: -0.780 [-100.000, 20.146], mean action: 1.867 [0.000, 3.000], mean observation: -0.112 [-1.019, 1.000], loss: 1.171137, mean_absolute_error: 33.570070, mean_q: 43.073579, mean_eps: 0.100000
 1861233/2000000: episode: 8546, duration: 5.653s, episode steps: 359, steps per second: 64, episode reward: 242.972, mean reward: 0.677 [-19.170, 100.000], mean action: 0.989 [0.000, 3.000], mean observation: 0.102 [-0.839, 1.000], loss: 1.253587, mean_absolute_error: 35.102321, mean_q: 45.361835, mean_eps: 0.100000
 1861773/2000000: episode: 8547, duration: 8.597s, episode steps: 540, steps per second: 63, episode reward: 253.820, mean reward: 0.470 [-18.198, 100.000], mean action: 0.748 [0.000, 3.000], mean observation: 0.213 [-0.971, 1.000], loss: 1.314258, mean_absolute_error: 34.479544, mean_q: 44.513856, mean_eps: 0.100000
 1862092/2000000: episode: 8548, duration: 5.071s, episode steps: 319, steps per second: 63, episode reward: 195.835, mean reward: 0.614 [-9.652, 100.000], mean action: 1.893 [0.000, 3.000], mean observation: 0.085 [-0.927, 1.041], loss: 1.255561, mean_absolute_error: 34.828948, mean_q: 45.692577, mean_eps: 0.100000
 1862191/2000000: episode: 8549, duration: 1.565s, episode steps: 99, steps per second: 63, episode reward: -31.408, mean reward: -0.317 [-100.000, 18.219], mean action: 1.505 [0.000, 3.000], mean observation: -0.080 [-1.001, 1.573], loss: 1.187486, mean_absolute_error: 36.058117, mean_q: 47.388977, mean_eps: 0.100000
 1862720/2000000: episode: 8550, duration: 8.699s, episode steps: 529, steps per second: 61, episode reward: 169.148, mean reward: 0.320 [-20.017, 100.000], mean action: 2.134 [0.000, 3.000], mean observation: 0.170 [-0.926, 1.000], loss: 1.258719, mean_absolute_error: 34.849046, mean_q: 44.334503, mean_eps: 0.100000
 1863037/2000000: episode: 8551, duration: 5.059s, episode steps: 317, steps per second: 63, episode reward: 217.452, mean reward: 0.686 [-20.110, 100.000], mean action: 1.095 [0.000, 3.000], mean observation: 0.147 [-1.169, 1.000], loss: 1.332312, mean_absolute_error: 34.873176, mean_q: 44.287891, mean_eps: 0.100000
 1863511/2000000: episode: 8552, duration: 7.475s, episode steps: 474, steps per second: 63, episode reward: 119.330, mean reward: 0.252 [-10.864, 100.000], mean action: 2.274 [0.000, 3.000], mean observation: 0.032 [-0.876, 1.168], loss: 1.084034, mean_absolute_error: 35.266826, mean_q: 45.997143, mean_eps: 0.100000
 1863665/2000000: episode: 8553, duration: 2.423s, episode steps: 154, steps per second: 64, episode reward: -86.992, mean reward: -0.565 [-100.000, 11.976], mean action: 1.578 [0.000, 3.000], mean observation: 0.042 [-1.090, 1.000], loss: 1.368898, mean_absolute_error: 35.468286, mean_q: 46.383156, mean_eps: 0.100000
 1863764/2000000: episode: 8554, duration: 1.535s, episode steps: 99, steps per second: 64, episode reward: -35.123, mean reward: -0.355 [-100.000, 17.859], mean action: 1.697 [0.000, 3.000], mean observation: 0.024 [-1.894, 1.000], loss: 1.157803, mean_absolute_error: 36.414338, mean_q: 46.459885, mean_eps: 0.100000
 1864325/2000000: episode: 8555, duration: 8.810s, episode steps: 561, steps per second: 64, episode reward: 254.241, mean reward: 0.453 [-18.581, 100.000], mean action: 0.578 [0.000, 3.000], mean observation: 0.178 [-0.986, 1.000], loss: 1.345139, mean_absolute_error: 34.688789, mean_q: 43.692130, mean_eps: 0.100000
 1864579/2000000: episode: 8556, duration: 3.960s, episode steps: 254, steps per second: 64, episode reward: 209.132, mean reward: 0.823 [-11.043, 100.000], mean action: 1.122 [0.000, 3.000], mean observation: 0.061 [-1.431, 1.000], loss: 1.110256, mean_absolute_error: 34.493459, mean_q: 44.751038, mean_eps: 0.100000
 1864978/2000000: episode: 8557, duration: 6.472s, episode steps: 399, steps per second: 62, episode reward: 235.662, mean reward: 0.591 [-10.748, 100.000], mean action: 1.060 [0.000, 3.000], mean observation: 0.151 [-1.024, 1.011], loss: 1.210577, mean_absolute_error: 34.306805, mean_q: 44.725944, mean_eps: 0.100000
 1865133/2000000: episode: 8558, duration: 2.492s, episode steps: 155, steps per second: 62, episode reward: -41.896, mean reward: -0.270 [-100.000, 14.095], mean action: 1.613 [0.000, 3.000], mean observation: 0.086 [-0.985, 1.024], loss: 1.165132, mean_absolute_error: 34.648337, mean_q: 45.185892, mean_eps: 0.100000
 1865227/2000000: episode: 8559, duration: 1.411s, episode steps: 94, steps per second: 67, episode reward: -159.255, mean reward: -1.694 [-100.000, 8.902], mean action: 0.979 [0.000, 3.000], mean observation: -0.064 [-3.118, 1.000], loss: 1.016054, mean_absolute_error: 33.579962, mean_q: 42.153317, mean_eps: 0.100000
 1865320/2000000: episode: 8560, duration: 1.438s, episode steps: 93, steps per second: 65, episode reward: -104.564, mean reward: -1.124 [-100.000, 15.295], mean action: 1.516 [0.000, 3.000], mean observation: -0.116 [-1.006, 3.059], loss: 1.175827, mean_absolute_error: 37.813676, mean_q: 47.869376, mean_eps: 0.100000
 1866229/2000000: episode: 8561, duration: 14.686s, episode steps: 909, steps per second: 62, episode reward: 155.367, mean reward: 0.171 [-19.257, 100.000], mean action: 1.383 [0.000, 3.000], mean observation: 0.141 [-0.892, 1.000], loss: 1.294257, mean_absolute_error: 34.356671, mean_q: 44.450412, mean_eps: 0.100000
 1866327/2000000: episode: 8562, duration: 1.516s, episode steps: 98, steps per second: 65, episode reward: 12.454, mean reward: 0.127 [-100.000, 13.407], mean action: 1.531 [0.000, 3.000], mean observation: -0.108 [-1.071, 1.000], loss: 1.369950, mean_absolute_error: 35.698859, mean_q: 46.200797, mean_eps: 0.100000
 1866430/2000000: episode: 8563, duration: 1.608s, episode steps: 103, steps per second: 64, episode reward: -30.029, mean reward: -0.292 [-100.000, 19.906], mean action: 1.515 [0.000, 3.000], mean observation: 0.015 [-2.120, 1.000], loss: 1.143498, mean_absolute_error: 34.595963, mean_q: 45.681222, mean_eps: 0.100000
 1866541/2000000: episode: 8564, duration: 1.749s, episode steps: 111, steps per second: 63, episode reward: -13.277, mean reward: -0.120 [-100.000, 10.803], mean action: 1.928 [0.000, 3.000], mean observation: 0.059 [-0.873, 1.000], loss: 1.353883, mean_absolute_error: 33.380989, mean_q: 43.243961, mean_eps: 0.100000
 1866641/2000000: episode: 8565, duration: 1.563s, episode steps: 100, steps per second: 64, episode reward: -16.664, mean reward: -0.167 [-100.000, 11.788], mean action: 1.780 [0.000, 3.000], mean observation: 0.087 [-1.420, 1.000], loss: 1.284102, mean_absolute_error: 32.151612, mean_q: 41.275971, mean_eps: 0.100000
 1866945/2000000: episode: 8566, duration: 4.752s, episode steps: 304, steps per second: 64, episode reward: 245.673, mean reward: 0.808 [-17.510, 100.000], mean action: 1.056 [0.000, 3.000], mean observation: 0.076 [-1.263, 1.000], loss: 1.482608, mean_absolute_error: 34.444855, mean_q: 44.457289, mean_eps: 0.100000
 1867178/2000000: episode: 8567, duration: 3.575s, episode steps: 233, steps per second: 65, episode reward: 227.620, mean reward: 0.977 [-17.829, 100.000], mean action: 0.991 [0.000, 3.000], mean observation: 0.055 [-1.016, 1.000], loss: 1.189461, mean_absolute_error: 34.542079, mean_q: 44.623541, mean_eps: 0.100000
 1867843/2000000: episode: 8568, duration: 10.844s, episode steps: 665, steps per second: 61, episode reward: 158.339, mean reward: 0.238 [-19.352, 100.000], mean action: 0.995 [0.000, 3.000], mean observation: 0.123 [-1.185, 1.000], loss: 1.331348, mean_absolute_error: 34.907512, mean_q: 44.739857, mean_eps: 0.100000
 1867969/2000000: episode: 8569, duration: 1.995s, episode steps: 126, steps per second: 63, episode reward: -57.731, mean reward: -0.458 [-100.000, 27.042], mean action: 1.643 [0.000, 3.000], mean observation: 0.043 [-2.065, 1.009], loss: 1.557260, mean_absolute_error: 33.577034, mean_q: 42.499522, mean_eps: 0.100000
 1868092/2000000: episode: 8570, duration: 1.925s, episode steps: 123, steps per second: 64, episode reward: -38.251, mean reward: -0.311 [-100.000, 6.113], mean action: 1.545 [0.000, 3.000], mean observation: -0.069 [-0.996, 1.408], loss: 1.254474, mean_absolute_error: 34.747274, mean_q: 43.910357, mean_eps: 0.100000
 1868232/2000000: episode: 8571, duration: 2.220s, episode steps: 140, steps per second: 63, episode reward: -30.897, mean reward: -0.221 [-100.000, 18.428], mean action: 1.693 [0.000, 3.000], mean observation: -0.094 [-1.023, 1.298], loss: 1.233289, mean_absolute_error: 34.644653, mean_q: 45.309225, mean_eps: 0.100000
 1868335/2000000: episode: 8572, duration: 1.613s, episode steps: 103, steps per second: 64, episode reward: -39.454, mean reward: -0.383 [-100.000, 28.034], mean action: 1.427 [0.000, 3.000], mean observation: -0.073 [-0.875, 1.631], loss: 0.995193, mean_absolute_error: 33.646558, mean_q: 44.072214, mean_eps: 0.100000
 1868418/2000000: episode: 8573, duration: 1.308s, episode steps: 83, steps per second: 63, episode reward: -183.624, mean reward: -2.212 [-100.000, 6.232], mean action: 1.819 [0.000, 3.000], mean observation: 0.041 [-1.244, 1.000], loss: 1.145886, mean_absolute_error: 33.877425, mean_q: 42.572937, mean_eps: 0.100000
 1868592/2000000: episode: 8574, duration: 2.725s, episode steps: 174, steps per second: 64, episode reward: -50.724, mean reward: -0.292 [-100.000, 47.244], mean action: 1.747 [0.000, 3.000], mean observation: 0.028 [-1.346, 1.019], loss: 1.040331, mean_absolute_error: 34.126870, mean_q: 44.576108, mean_eps: 0.100000
 1868837/2000000: episode: 8575, duration: 3.934s, episode steps: 245, steps per second: 62, episode reward: 171.360, mean reward: 0.699 [-12.573, 100.000], mean action: 1.482 [0.000, 3.000], mean observation: 0.075 [-1.716, 1.000], loss: 1.146215, mean_absolute_error: 33.894148, mean_q: 43.411514, mean_eps: 0.100000
 1869837/2000000: episode: 8576, duration: 16.447s, episode steps: 1000, steps per second: 61, episode reward: 59.026, mean reward: 0.059 [-19.149, 21.990], mean action: 0.878 [0.000, 3.000], mean observation: 0.133 [-0.947, 1.000], loss: 1.373580, mean_absolute_error: 34.354702, mean_q: 44.033594, mean_eps: 0.100000
 1869989/2000000: episode: 8577, duration: 2.428s, episode steps: 152, steps per second: 63, episode reward: -45.552, mean reward: -0.300 [-100.000, 17.074], mean action: 1.895 [0.000, 3.000], mean observation: 0.047 [-1.127, 1.000], loss: 1.203829, mean_absolute_error: 35.669472, mean_q: 45.976157, mean_eps: 0.100000
 1870130/2000000: episode: 8578, duration: 2.287s, episode steps: 141, steps per second: 62, episode reward: -44.404, mean reward: -0.315 [-100.000, 17.974], mean action: 1.702 [0.000, 3.000], mean observation: 0.061 [-1.276, 1.000], loss: 1.237621, mean_absolute_error: 34.657086, mean_q: 43.682969, mean_eps: 0.100000
 1870577/2000000: episode: 8579, duration: 7.186s, episode steps: 447, steps per second: 62, episode reward: 186.893, mean reward: 0.418 [-19.385, 100.000], mean action: 1.141 [0.000, 3.000], mean observation: 0.127 [-1.404, 1.000], loss: 0.967525, mean_absolute_error: 34.191656, mean_q: 44.495111, mean_eps: 0.100000
 1871577/2000000: episode: 8580, duration: 15.850s, episode steps: 1000, steps per second: 63, episode reward: 61.110, mean reward: 0.061 [-22.229, 22.367], mean action: 1.773 [0.000, 3.000], mean observation: 0.202 [-1.778, 1.000], loss: 1.300264, mean_absolute_error: 34.909868, mean_q: 44.757356, mean_eps: 0.100000
 1871748/2000000: episode: 8581, duration: 2.673s, episode steps: 171, steps per second: 64, episode reward: 7.496, mean reward: 0.044 [-100.000, 17.984], mean action: 1.392 [0.000, 3.000], mean observation: -0.006 [-1.222, 1.000], loss: 1.611754, mean_absolute_error: 32.896264, mean_q: 42.697163, mean_eps: 0.100000
 1872081/2000000: episode: 8582, duration: 5.379s, episode steps: 333, steps per second: 62, episode reward: 165.899, mean reward: 0.498 [-9.464, 100.000], mean action: 1.264 [0.000, 3.000], mean observation: 0.052 [-1.304, 1.000], loss: 1.334419, mean_absolute_error: 35.578022, mean_q: 45.894033, mean_eps: 0.100000
 1872206/2000000: episode: 8583, duration: 1.936s, episode steps: 125, steps per second: 65, episode reward: -23.111, mean reward: -0.185 [-100.000, 14.510], mean action: 1.208 [0.000, 3.000], mean observation: 0.020 [-1.595, 1.000], loss: 1.750752, mean_absolute_error: 35.926240, mean_q: 45.166969, mean_eps: 0.100000
 1872306/2000000: episode: 8584, duration: 1.575s, episode steps: 100, steps per second: 63, episode reward: 2.565, mean reward: 0.026 [-100.000, 17.295], mean action: 1.860 [0.000, 3.000], mean observation: -0.018 [-1.595, 1.000], loss: 1.434552, mean_absolute_error: 34.024687, mean_q: 44.072011, mean_eps: 0.100000
 1872425/2000000: episode: 8585, duration: 1.864s, episode steps: 119, steps per second: 64, episode reward: -87.642, mean reward: -0.736 [-100.000, 16.600], mean action: 1.580 [0.000, 3.000], mean observation: -0.030 [-1.897, 1.000], loss: 1.185284, mean_absolute_error: 34.754583, mean_q: 44.462040, mean_eps: 0.100000
 1872508/2000000: episode: 8586, duration: 1.526s, episode steps: 83, steps per second: 54, episode reward: -19.313, mean reward: -0.233 [-100.000, 19.653], mean action: 1.952 [0.000, 3.000], mean observation: -0.117 [-1.023, 1.000], loss: 1.474391, mean_absolute_error: 36.990326, mean_q: 47.231449, mean_eps: 0.100000
 1872612/2000000: episode: 8587, duration: 1.685s, episode steps: 104, steps per second: 62, episode reward: -24.680, mean reward: -0.237 [-100.000, 20.751], mean action: 1.471 [0.000, 3.000], mean observation: 0.035 [-1.304, 1.000], loss: 1.913836, mean_absolute_error: 36.428209, mean_q: 47.004168, mean_eps: 0.100000
 1872728/2000000: episode: 8588, duration: 1.869s, episode steps: 116, steps per second: 62, episode reward: -64.869, mean reward: -0.559 [-100.000, 21.085], mean action: 1.828 [0.000, 3.000], mean observation: 0.053 [-2.036, 1.000], loss: 1.269526, mean_absolute_error: 35.720319, mean_q: 46.347251, mean_eps: 0.100000
 1873224/2000000: episode: 8589, duration: 8.095s, episode steps: 496, steps per second: 61, episode reward: 180.646, mean reward: 0.364 [-17.759, 100.000], mean action: 1.242 [0.000, 3.000], mean observation: 0.137 [-0.931, 1.000], loss: 1.488795, mean_absolute_error: 34.631718, mean_q: 43.985707, mean_eps: 0.100000
 1873518/2000000: episode: 8590, duration: 4.660s, episode steps: 294, steps per second: 63, episode reward: 108.806, mean reward: 0.370 [-14.607, 100.000], mean action: 2.293 [0.000, 3.000], mean observation: 0.035 [-0.911, 1.000], loss: 1.344507, mean_absolute_error: 34.313593, mean_q: 44.082228, mean_eps: 0.100000
 1873608/2000000: episode: 8591, duration: 1.429s, episode steps: 90, steps per second: 63, episode reward: -69.306, mean reward: -0.770 [-100.000, 12.888], mean action: 1.633 [0.000, 3.000], mean observation: -0.114 [-0.954, 3.323], loss: 0.924101, mean_absolute_error: 34.873018, mean_q: 44.421144, mean_eps: 0.100000
 1873759/2000000: episode: 8592, duration: 2.354s, episode steps: 151, steps per second: 64, episode reward: -9.594, mean reward: -0.064 [-100.000, 13.393], mean action: 1.464 [0.000, 3.000], mean observation: -0.052 [-0.989, 1.000], loss: 1.740341, mean_absolute_error: 35.023978, mean_q: 44.987960, mean_eps: 0.100000
 1874292/2000000: episode: 8593, duration: 9.182s, episode steps: 533, steps per second: 58, episode reward: 153.618, mean reward: 0.288 [-17.741, 100.000], mean action: 1.777 [0.000, 3.000], mean observation: 0.097 [-1.121, 1.000], loss: 1.350414, mean_absolute_error: 34.431889, mean_q: 44.248546, mean_eps: 0.100000
 1874441/2000000: episode: 8594, duration: 2.429s, episode steps: 149, steps per second: 61, episode reward: -65.643, mean reward: -0.441 [-100.000, 21.649], mean action: 1.483 [0.000, 3.000], mean observation: 0.003 [-0.942, 1.000], loss: 1.061322, mean_absolute_error: 34.758802, mean_q: 45.309865, mean_eps: 0.100000
 1874548/2000000: episode: 8595, duration: 1.648s, episode steps: 107, steps per second: 65, episode reward: -36.801, mean reward: -0.344 [-100.000, 15.952], mean action: 1.720 [0.000, 3.000], mean observation: 0.004 [-1.882, 1.069], loss: 1.539539, mean_absolute_error: 36.351373, mean_q: 46.316185, mean_eps: 0.100000
 1874670/2000000: episode: 8596, duration: 1.912s, episode steps: 122, steps per second: 64, episode reward: -65.175, mean reward: -0.534 [-100.000, 10.443], mean action: 1.631 [0.000, 3.000], mean observation: -0.047 [-1.551, 1.000], loss: 1.601320, mean_absolute_error: 32.705011, mean_q: 40.622527, mean_eps: 0.100000
 1874811/2000000: episode: 8597, duration: 2.191s, episode steps: 141, steps per second: 64, episode reward: -75.659, mean reward: -0.537 [-100.000, 20.140], mean action: 1.745 [0.000, 3.000], mean observation: 0.047 [-2.078, 1.000], loss: 1.333192, mean_absolute_error: 33.437048, mean_q: 42.599442, mean_eps: 0.100000
 1875112/2000000: episode: 8598, duration: 4.833s, episode steps: 301, steps per second: 62, episode reward: 168.141, mean reward: 0.559 [-22.858, 100.000], mean action: 1.365 [0.000, 3.000], mean observation: 0.069 [-1.072, 1.000], loss: 1.167005, mean_absolute_error: 34.387317, mean_q: 44.229115, mean_eps: 0.100000
 1875199/2000000: episode: 8599, duration: 1.379s, episode steps: 87, steps per second: 63, episode reward: -85.425, mean reward: -0.982 [-100.000, 7.954], mean action: 1.828 [0.000, 3.000], mean observation: 0.106 [-1.560, 1.000], loss: 1.108341, mean_absolute_error: 35.263666, mean_q: 45.767496, mean_eps: 0.100000
 1875351/2000000: episode: 8600, duration: 2.367s, episode steps: 152, steps per second: 64, episode reward: -4.245, mean reward: -0.028 [-100.000, 16.521], mean action: 1.822 [0.000, 3.000], mean observation: -0.031 [-1.267, 1.000], loss: 1.283028, mean_absolute_error: 34.222654, mean_q: 43.497935, mean_eps: 0.100000
 1875582/2000000: episode: 8601, duration: 3.660s, episode steps: 231, steps per second: 63, episode reward: 218.729, mean reward: 0.947 [-12.160, 100.000], mean action: 1.485 [0.000, 3.000], mean observation: 0.049 [-1.600, 1.000], loss: 1.296272, mean_absolute_error: 35.428375, mean_q: 44.937964, mean_eps: 0.100000
 1875685/2000000: episode: 8602, duration: 1.614s, episode steps: 103, steps per second: 64, episode reward: -35.196, mean reward: -0.342 [-100.000, 18.753], mean action: 1.650 [0.000, 3.000], mean observation: 0.017 [-2.136, 1.000], loss: 1.400166, mean_absolute_error: 34.511310, mean_q: 44.576169, mean_eps: 0.100000
 1875781/2000000: episode: 8603, duration: 1.503s, episode steps: 96, steps per second: 64, episode reward: -70.705, mean reward: -0.737 [-100.000, 19.536], mean action: 1.844 [0.000, 3.000], mean observation: 0.032 [-1.868, 1.000], loss: 1.335991, mean_absolute_error: 34.851358, mean_q: 43.659499, mean_eps: 0.100000
 1875924/2000000: episode: 8604, duration: 2.238s, episode steps: 143, steps per second: 64, episode reward: -67.730, mean reward: -0.474 [-100.000, 19.522], mean action: 2.000 [0.000, 3.000], mean observation: 0.072 [-2.131, 1.000], loss: 1.032313, mean_absolute_error: 35.162832, mean_q: 44.317675, mean_eps: 0.100000
 1876582/2000000: episode: 8605, duration: 10.601s, episode steps: 658, steps per second: 62, episode reward: 166.378, mean reward: 0.253 [-18.300, 100.000], mean action: 0.892 [0.000, 3.000], mean observation: 0.142 [-1.275, 1.000], loss: 1.195765, mean_absolute_error: 35.118133, mean_q: 45.756573, mean_eps: 0.100000
 1876690/2000000: episode: 8606, duration: 1.699s, episode steps: 108, steps per second: 64, episode reward: -34.361, mean reward: -0.318 [-100.000, 9.728], mean action: 1.472 [0.000, 3.000], mean observation: 0.030 [-1.706, 1.000], loss: 1.209716, mean_absolute_error: 35.067999, mean_q: 44.866904, mean_eps: 0.100000
 1876914/2000000: episode: 8607, duration: 3.524s, episode steps: 224, steps per second: 64, episode reward: 238.556, mean reward: 1.065 [-8.800, 100.000], mean action: 1.513 [0.000, 3.000], mean observation: 0.094 [-0.883, 1.000], loss: 1.086242, mean_absolute_error: 35.675302, mean_q: 45.525966, mean_eps: 0.100000
 1877249/2000000: episode: 8608, duration: 5.329s, episode steps: 335, steps per second: 63, episode reward: 265.302, mean reward: 0.792 [-17.350, 100.000], mean action: 1.284 [0.000, 3.000], mean observation: 0.149 [-0.898, 1.000], loss: 1.338542, mean_absolute_error: 35.051583, mean_q: 45.809451, mean_eps: 0.100000
 1877553/2000000: episode: 8609, duration: 4.808s, episode steps: 304, steps per second: 63, episode reward: 193.635, mean reward: 0.637 [-7.646, 100.000], mean action: 1.421 [0.000, 3.000], mean observation: 0.035 [-0.892, 1.006], loss: 1.612446, mean_absolute_error: 35.380344, mean_q: 45.754247, mean_eps: 0.100000
 1877688/2000000: episode: 8610, duration: 2.101s, episode steps: 135, steps per second: 64, episode reward: -37.870, mean reward: -0.281 [-100.000, 16.981], mean action: 1.370 [0.000, 3.000], mean observation: 0.088 [-1.617, 1.003], loss: 1.243092, mean_absolute_error: 36.354501, mean_q: 44.405714, mean_eps: 0.100000
 1877808/2000000: episode: 8611, duration: 1.952s, episode steps: 120, steps per second: 61, episode reward: -126.921, mean reward: -1.058 [-100.000, 10.282], mean action: 1.350 [0.000, 3.000], mean observation: -0.027 [-1.542, 1.000], loss: 1.116576, mean_absolute_error: 34.928528, mean_q: 45.288294, mean_eps: 0.100000
 1877936/2000000: episode: 8612, duration: 2.053s, episode steps: 128, steps per second: 62, episode reward: -58.334, mean reward: -0.456 [-100.000, 21.914], mean action: 1.617 [0.000, 3.000], mean observation: 0.028 [-2.428, 1.000], loss: 1.486852, mean_absolute_error: 36.127284, mean_q: 47.451935, mean_eps: 0.100000
 1878262/2000000: episode: 8613, duration: 5.223s, episode steps: 326, steps per second: 62, episode reward: 154.052, mean reward: 0.473 [-19.097, 100.000], mean action: 2.350 [0.000, 3.000], mean observation: 0.097 [-1.084, 1.009], loss: 1.551857, mean_absolute_error: 34.452687, mean_q: 43.987334, mean_eps: 0.100000
 1878655/2000000: episode: 8614, duration: 6.260s, episode steps: 393, steps per second: 63, episode reward: 147.596, mean reward: 0.376 [-17.984, 100.000], mean action: 1.148 [0.000, 3.000], mean observation: 0.106 [-1.063, 1.000], loss: 1.248853, mean_absolute_error: 34.605751, mean_q: 43.983954, mean_eps: 0.100000
 1879057/2000000: episode: 8615, duration: 6.421s, episode steps: 402, steps per second: 63, episode reward: 126.039, mean reward: 0.314 [-13.282, 100.000], mean action: 1.542 [0.000, 3.000], mean observation: -0.024 [-1.204, 1.000], loss: 1.478024, mean_absolute_error: 34.282643, mean_q: 44.375114, mean_eps: 0.100000
 1879384/2000000: episode: 8616, duration: 5.150s, episode steps: 327, steps per second: 63, episode reward: 250.178, mean reward: 0.765 [-18.339, 100.000], mean action: 1.474 [0.000, 3.000], mean observation: 0.119 [-1.090, 1.000], loss: 1.261574, mean_absolute_error: 35.508282, mean_q: 45.477427, mean_eps: 0.100000
 1879649/2000000: episode: 8617, duration: 4.193s, episode steps: 265, steps per second: 63, episode reward: 239.073, mean reward: 0.902 [-17.481, 100.000], mean action: 1.268 [0.000, 3.000], mean observation: 0.136 [-0.940, 1.000], loss: 1.060955, mean_absolute_error: 34.086547, mean_q: 43.709130, mean_eps: 0.100000
 1879923/2000000: episode: 8618, duration: 4.363s, episode steps: 274, steps per second: 63, episode reward: 169.791, mean reward: 0.620 [-21.272, 100.000], mean action: 1.971 [0.000, 3.000], mean observation: 0.046 [-1.044, 1.201], loss: 1.380149, mean_absolute_error: 34.695024, mean_q: 45.567281, mean_eps: 0.100000
 1880290/2000000: episode: 8619, duration: 6.059s, episode steps: 367, steps per second: 61, episode reward: 244.958, mean reward: 0.667 [-17.881, 100.000], mean action: 0.959 [0.000, 3.000], mean observation: 0.100 [-0.961, 1.011], loss: 1.258047, mean_absolute_error: 35.215993, mean_q: 45.897425, mean_eps: 0.100000
 1880400/2000000: episode: 8620, duration: 1.767s, episode steps: 110, steps per second: 62, episode reward: -11.214, mean reward: -0.102 [-100.000, 14.344], mean action: 1.982 [0.000, 3.000], mean observation: -0.035 [-0.967, 1.000], loss: 1.193518, mean_absolute_error: 34.912449, mean_q: 42.174414, mean_eps: 0.100000
 1880695/2000000: episode: 8621, duration: 4.763s, episode steps: 295, steps per second: 62, episode reward: 227.525, mean reward: 0.771 [-4.400, 100.000], mean action: 1.220 [0.000, 3.000], mean observation: 0.087 [-1.351, 1.000], loss: 1.296454, mean_absolute_error: 34.850441, mean_q: 45.249198, mean_eps: 0.100000
 1881066/2000000: episode: 8622, duration: 5.802s, episode steps: 371, steps per second: 64, episode reward: 252.319, mean reward: 0.680 [-10.581, 100.000], mean action: 0.747 [0.000, 3.000], mean observation: 0.134 [-0.950, 1.000], loss: 1.292896, mean_absolute_error: 35.303299, mean_q: 45.605286, mean_eps: 0.100000
 1881184/2000000: episode: 8623, duration: 1.907s, episode steps: 118, steps per second: 62, episode reward: 2.977, mean reward: 0.025 [-100.000, 11.831], mean action: 1.814 [0.000, 3.000], mean observation: -0.055 [-1.001, 1.000], loss: 1.068811, mean_absolute_error: 34.064032, mean_q: 44.195950, mean_eps: 0.100000
 1881363/2000000: episode: 8624, duration: 2.790s, episode steps: 179, steps per second: 64, episode reward: -6.471, mean reward: -0.036 [-100.000, 11.389], mean action: 1.698 [0.000, 3.000], mean observation: -0.030 [-0.816, 1.000], loss: 1.096694, mean_absolute_error: 34.575615, mean_q: 45.190164, mean_eps: 0.100000
 1881501/2000000: episode: 8625, duration: 2.237s, episode steps: 138, steps per second: 62, episode reward: -109.403, mean reward: -0.793 [-100.000, 16.734], mean action: 1.601 [0.000, 3.000], mean observation: 0.007 [-1.608, 1.000], loss: 1.106692, mean_absolute_error: 33.900380, mean_q: 43.466929, mean_eps: 0.100000
 1881751/2000000: episode: 8626, duration: 4.145s, episode steps: 250, steps per second: 60, episode reward: 166.833, mean reward: 0.667 [-17.407, 100.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.088 [-1.669, 1.000], loss: 1.323204, mean_absolute_error: 35.010770, mean_q: 44.476738, mean_eps: 0.100000
 1881953/2000000: episode: 8627, duration: 3.142s, episode steps: 202, steps per second: 64, episode reward: -18.621, mean reward: -0.092 [-100.000, 19.317], mean action: 1.431 [0.000, 3.000], mean observation: -0.006 [-1.089, 1.000], loss: 1.457269, mean_absolute_error: 34.993004, mean_q: 44.403958, mean_eps: 0.100000
 1882197/2000000: episode: 8628, duration: 3.865s, episode steps: 244, steps per second: 63, episode reward: -81.452, mean reward: -0.334 [-100.000, 14.814], mean action: 1.951 [0.000, 3.000], mean observation: -0.071 [-0.782, 1.233], loss: 1.538333, mean_absolute_error: 34.603142, mean_q: 44.524237, mean_eps: 0.100000
 1882297/2000000: episode: 8629, duration: 1.577s, episode steps: 100, steps per second: 63, episode reward: 0.787, mean reward: 0.008 [-100.000, 18.300], mean action: 1.910 [0.000, 3.000], mean observation: -0.057 [-1.006, 1.000], loss: 1.263415, mean_absolute_error: 34.776619, mean_q: 45.211223, mean_eps: 0.100000
 1882399/2000000: episode: 8630, duration: 1.589s, episode steps: 102, steps per second: 64, episode reward: -106.936, mean reward: -1.048 [-100.000, 12.002], mean action: 1.804 [0.000, 3.000], mean observation: -0.048 [-1.023, 2.069], loss: 0.957458, mean_absolute_error: 33.830445, mean_q: 43.592174, mean_eps: 0.100000
 1882722/2000000: episode: 8631, duration: 5.123s, episode steps: 323, steps per second: 63, episode reward: 166.509, mean reward: 0.516 [-6.193, 100.000], mean action: 1.257 [0.000, 3.000], mean observation: 0.058 [-0.942, 1.000], loss: 1.410824, mean_absolute_error: 34.653387, mean_q: 44.518988, mean_eps: 0.100000
 1882838/2000000: episode: 8632, duration: 1.801s, episode steps: 116, steps per second: 64, episode reward: -119.337, mean reward: -1.029 [-100.000, 8.196], mean action: 1.810 [0.000, 3.000], mean observation: -0.036 [-0.860, 1.636], loss: 1.091359, mean_absolute_error: 34.750939, mean_q: 45.292705, mean_eps: 0.100000
 1882971/2000000: episode: 8633, duration: 2.063s, episode steps: 133, steps per second: 64, episode reward: -64.253, mean reward: -0.483 [-100.000, 16.869], mean action: 1.624 [0.000, 3.000], mean observation: 0.074 [-1.819, 1.000], loss: 1.160598, mean_absolute_error: 36.378048, mean_q: 47.186391, mean_eps: 0.100000
 1883234/2000000: episode: 8634, duration: 4.172s, episode steps: 263, steps per second: 63, episode reward: 173.685, mean reward: 0.660 [-5.132, 100.000], mean action: 2.236 [0.000, 3.000], mean observation: 0.012 [-1.042, 1.219], loss: 1.444466, mean_absolute_error: 33.564133, mean_q: 41.405299, mean_eps: 0.100000
 1883845/2000000: episode: 8635, duration: 10.043s, episode steps: 611, steps per second: 61, episode reward: 167.354, mean reward: 0.274 [-17.717, 100.000], mean action: 1.025 [0.000, 3.000], mean observation: 0.078 [-1.322, 1.000], loss: 1.343563, mean_absolute_error: 35.066801, mean_q: 44.821442, mean_eps: 0.100000
 1883952/2000000: episode: 8636, duration: 1.684s, episode steps: 107, steps per second: 64, episode reward: -62.539, mean reward: -0.584 [-100.000, 18.511], mean action: 1.916 [0.000, 3.000], mean observation: 0.018 [-1.908, 1.000], loss: 1.635713, mean_absolute_error: 35.261631, mean_q: 45.306812, mean_eps: 0.100000
 1884117/2000000: episode: 8637, duration: 2.632s, episode steps: 165, steps per second: 63, episode reward: -58.556, mean reward: -0.355 [-100.000, 12.238], mean action: 1.697 [0.000, 3.000], mean observation: -0.044 [-0.844, 1.108], loss: 1.379585, mean_absolute_error: 34.011935, mean_q: 43.738511, mean_eps: 0.100000
 1884405/2000000: episode: 8638, duration: 4.509s, episode steps: 288, steps per second: 64, episode reward: 226.418, mean reward: 0.786 [-6.814, 100.000], mean action: 1.156 [0.000, 3.000], mean observation: 0.069 [-1.431, 1.000], loss: 1.257570, mean_absolute_error: 34.499490, mean_q: 44.454672, mean_eps: 0.100000
 1884582/2000000: episode: 8639, duration: 2.736s, episode steps: 177, steps per second: 65, episode reward: -9.715, mean reward: -0.055 [-100.000, 14.433], mean action: 1.695 [0.000, 3.000], mean observation: 0.101 [-0.954, 1.000], loss: 1.444800, mean_absolute_error: 34.336187, mean_q: 44.074159, mean_eps: 0.100000
 1884687/2000000: episode: 8640, duration: 1.627s, episode steps: 105, steps per second: 65, episode reward: -53.229, mean reward: -0.507 [-100.000, 13.324], mean action: 1.905 [0.000, 3.000], mean observation: 0.007 [-2.134, 1.000], loss: 1.714483, mean_absolute_error: 33.956721, mean_q: 42.035146, mean_eps: 0.100000
 1885098/2000000: episode: 8641, duration: 6.566s, episode steps: 411, steps per second: 63, episode reward: 218.356, mean reward: 0.531 [-17.745, 100.000], mean action: 0.891 [0.000, 3.000], mean observation: 0.123 [-0.861, 1.000], loss: 1.216485, mean_absolute_error: 33.858687, mean_q: 44.039572, mean_eps: 0.100000
 1885258/2000000: episode: 8642, duration: 2.485s, episode steps: 160, steps per second: 64, episode reward: -30.761, mean reward: -0.192 [-100.000, 13.157], mean action: 1.606 [0.000, 3.000], mean observation: -0.093 [-1.426, 1.000], loss: 1.099631, mean_absolute_error: 34.688084, mean_q: 43.935381, mean_eps: 0.100000
 1885611/2000000: episode: 8643, duration: 5.511s, episode steps: 353, steps per second: 64, episode reward: 152.748, mean reward: 0.433 [-9.664, 100.000], mean action: 1.572 [0.000, 3.000], mean observation: 0.011 [-1.469, 1.000], loss: 1.338049, mean_absolute_error: 35.712807, mean_q: 45.178548, mean_eps: 0.100000
 1885773/2000000: episode: 8644, duration: 2.566s, episode steps: 162, steps per second: 63, episode reward: -77.752, mean reward: -0.480 [-100.000, 16.640], mean action: 1.265 [0.000, 3.000], mean observation: 0.062 [-1.456, 1.000], loss: 1.608161, mean_absolute_error: 33.588134, mean_q: 43.000305, mean_eps: 0.100000
 1885974/2000000: episode: 8645, duration: 3.092s, episode steps: 201, steps per second: 65, episode reward: -26.809, mean reward: -0.133 [-100.000, 14.031], mean action: 1.557 [0.000, 3.000], mean observation: 0.002 [-0.970, 1.076], loss: 0.877295, mean_absolute_error: 34.973698, mean_q: 45.955941, mean_eps: 0.100000
 1886086/2000000: episode: 8646, duration: 1.742s, episode steps: 112, steps per second: 64, episode reward: -105.063, mean reward: -0.938 [-100.000, 28.329], mean action: 1.848 [0.000, 3.000], mean observation: -0.058 [-2.706, 1.000], loss: 1.158044, mean_absolute_error: 34.380711, mean_q: 44.212027, mean_eps: 0.100000
 1886187/2000000: episode: 8647, duration: 1.564s, episode steps: 101, steps per second: 65, episode reward: -109.250, mean reward: -1.082 [-100.000, 14.568], mean action: 1.594 [0.000, 3.000], mean observation: 0.071 [-2.980, 1.000], loss: 1.592908, mean_absolute_error: 35.762942, mean_q: 43.737801, mean_eps: 0.100000
 1886297/2000000: episode: 8648, duration: 1.744s, episode steps: 110, steps per second: 63, episode reward: -110.353, mean reward: -1.003 [-100.000, 9.387], mean action: 1.873 [0.000, 3.000], mean observation: -0.051 [-1.786, 1.000], loss: 1.428430, mean_absolute_error: 34.670770, mean_q: 44.855531, mean_eps: 0.100000
 1886432/2000000: episode: 8649, duration: 2.115s, episode steps: 135, steps per second: 64, episode reward: -74.819, mean reward: -0.554 [-100.000, 9.861], mean action: 1.630 [0.000, 3.000], mean observation: -0.051 [-0.881, 1.569], loss: 1.046183, mean_absolute_error: 35.771647, mean_q: 46.606690, mean_eps: 0.100000
 1886705/2000000: episode: 8650, duration: 4.282s, episode steps: 273, steps per second: 64, episode reward: 233.638, mean reward: 0.856 [-11.964, 100.000], mean action: 0.996 [0.000, 3.000], mean observation: 0.129 [-1.307, 1.000], loss: 1.463819, mean_absolute_error: 35.208378, mean_q: 44.813231, mean_eps: 0.100000
 1886828/2000000: episode: 8651, duration: 1.914s, episode steps: 123, steps per second: 64, episode reward: -82.201, mean reward: -0.668 [-100.000, 11.223], mean action: 1.862 [0.000, 3.000], mean observation: -0.044 [-0.831, 1.000], loss: 1.270697, mean_absolute_error: 34.125528, mean_q: 44.489225, mean_eps: 0.100000
 1886908/2000000: episode: 8652, duration: 1.292s, episode steps: 80, steps per second: 62, episode reward: -40.350, mean reward: -0.504 [-100.000, 11.585], mean action: 1.637 [0.000, 3.000], mean observation: 0.080 [-0.995, 1.482], loss: 1.213268, mean_absolute_error: 33.296623, mean_q: 42.279741, mean_eps: 0.100000
 1887049/2000000: episode: 8653, duration: 2.244s, episode steps: 141, steps per second: 63, episode reward: 7.737, mean reward: 0.055 [-100.000, 15.145], mean action: 1.794 [0.000, 3.000], mean observation: -0.058 [-1.465, 1.000], loss: 1.294336, mean_absolute_error: 34.996875, mean_q: 45.482154, mean_eps: 0.100000
 1887147/2000000: episode: 8654, duration: 1.494s, episode steps: 98, steps per second: 66, episode reward: 16.834, mean reward: 0.172 [-100.000, 23.681], mean action: 1.408 [0.000, 3.000], mean observation: -0.005 [-0.948, 1.000], loss: 1.236756, mean_absolute_error: 35.440120, mean_q: 45.307133, mean_eps: 0.100000
 1887259/2000000: episode: 8655, duration: 1.738s, episode steps: 112, steps per second: 64, episode reward: -105.816, mean reward: -0.945 [-100.000, 14.475], mean action: 1.571 [0.000, 3.000], mean observation: 0.013 [-2.551, 1.000], loss: 1.162917, mean_absolute_error: 33.989758, mean_q: 43.286013, mean_eps: 0.100000
 1887395/2000000: episode: 8656, duration: 2.130s, episode steps: 136, steps per second: 64, episode reward: -40.063, mean reward: -0.295 [-100.000, 13.843], mean action: 1.772 [0.000, 3.000], mean observation: -0.017 [-0.888, 1.000], loss: 1.394653, mean_absolute_error: 35.147480, mean_q: 46.090096, mean_eps: 0.100000
 1887528/2000000: episode: 8657, duration: 2.235s, episode steps: 133, steps per second: 59, episode reward: -75.672, mean reward: -0.569 [-100.000, 21.146], mean action: 1.526 [0.000, 3.000], mean observation: 0.088 [-2.060, 1.000], loss: 0.973436, mean_absolute_error: 34.285155, mean_q: 44.020018, mean_eps: 0.100000
 1887633/2000000: episode: 8658, duration: 1.746s, episode steps: 105, steps per second: 60, episode reward: -90.179, mean reward: -0.859 [-100.000, 19.757], mean action: 1.543 [0.000, 3.000], mean observation: 0.013 [-1.797, 1.000], loss: 1.366086, mean_absolute_error: 35.568796, mean_q: 45.342907, mean_eps: 0.100000
 1888144/2000000: episode: 8659, duration: 8.308s, episode steps: 511, steps per second: 62, episode reward: 207.832, mean reward: 0.407 [-19.788, 100.000], mean action: 0.959 [0.000, 3.000], mean observation: 0.147 [-1.226, 1.212], loss: 1.210132, mean_absolute_error: 35.450266, mean_q: 45.913025, mean_eps: 0.100000
 1888477/2000000: episode: 8660, duration: 5.323s, episode steps: 333, steps per second: 63, episode reward: 228.659, mean reward: 0.687 [-17.278, 100.000], mean action: 1.348 [0.000, 3.000], mean observation: 0.004 [-1.031, 1.000], loss: 1.138309, mean_absolute_error: 34.826923, mean_q: 44.840041, mean_eps: 0.100000
 1888875/2000000: episode: 8661, duration: 6.295s, episode steps: 398, steps per second: 63, episode reward: 197.345, mean reward: 0.496 [-19.333, 100.000], mean action: 1.008 [0.000, 3.000], mean observation: 0.088 [-1.376, 1.000], loss: 1.396291, mean_absolute_error: 34.132382, mean_q: 43.994144, mean_eps: 0.100000
 1889006/2000000: episode: 8662, duration: 2.051s, episode steps: 131, steps per second: 64, episode reward: -46.380, mean reward: -0.354 [-100.000, 24.954], mean action: 1.527 [0.000, 3.000], mean observation: 0.054 [-2.250, 1.000], loss: 1.398252, mean_absolute_error: 35.223042, mean_q: 45.020433, mean_eps: 0.100000
 1889250/2000000: episode: 8663, duration: 3.923s, episode steps: 244, steps per second: 62, episode reward: 2.917, mean reward: 0.012 [-100.000, 9.373], mean action: 1.783 [0.000, 3.000], mean observation: 0.051 [-0.833, 1.516], loss: 1.399784, mean_absolute_error: 35.049975, mean_q: 45.195967, mean_eps: 0.100000
 1889352/2000000: episode: 8664, duration: 1.636s, episode steps: 102, steps per second: 62, episode reward: -62.316, mean reward: -0.611 [-100.000, 20.275], mean action: 1.765 [0.000, 3.000], mean observation: 0.059 [-2.131, 1.000], loss: 1.427121, mean_absolute_error: 33.571109, mean_q: 43.490089, mean_eps: 0.100000
 1889670/2000000: episode: 8665, duration: 5.051s, episode steps: 318, steps per second: 63, episode reward: 205.815, mean reward: 0.647 [-18.658, 100.000], mean action: 1.126 [0.000, 3.000], mean observation: 0.111 [-1.229, 1.018], loss: 1.246805, mean_absolute_error: 34.262622, mean_q: 44.133858, mean_eps: 0.100000
 1890175/2000000: episode: 8666, duration: 8.250s, episode steps: 505, steps per second: 61, episode reward: 125.697, mean reward: 0.249 [-19.973, 100.000], mean action: 1.424 [0.000, 3.000], mean observation: 0.052 [-0.894, 1.206], loss: 1.048069, mean_absolute_error: 35.169007, mean_q: 45.398061, mean_eps: 0.100000
 1890526/2000000: episode: 8667, duration: 5.594s, episode steps: 351, steps per second: 63, episode reward: 254.492, mean reward: 0.725 [-10.487, 100.000], mean action: 1.174 [0.000, 3.000], mean observation: 0.111 [-0.824, 1.000], loss: 1.259460, mean_absolute_error: 34.070407, mean_q: 44.655524, mean_eps: 0.100000
 1890646/2000000: episode: 8668, duration: 1.871s, episode steps: 120, steps per second: 64, episode reward: -67.897, mean reward: -0.566 [-100.000, 11.627], mean action: 1.658 [0.000, 3.000], mean observation: -0.057 [-1.149, 1.000], loss: 1.231152, mean_absolute_error: 35.982857, mean_q: 46.346547, mean_eps: 0.100000
 1890753/2000000: episode: 8669, duration: 1.689s, episode steps: 107, steps per second: 63, episode reward: -89.944, mean reward: -0.841 [-100.000, 18.584], mean action: 1.953 [0.000, 3.000], mean observation: -0.013 [-2.316, 1.000], loss: 1.696655, mean_absolute_error: 33.099629, mean_q: 43.130968, mean_eps: 0.100000
 1890884/2000000: episode: 8670, duration: 2.069s, episode steps: 131, steps per second: 63, episode reward: -90.388, mean reward: -0.690 [-100.000, 9.433], mean action: 1.863 [0.000, 3.000], mean observation: 0.094 [-2.553, 1.000], loss: 1.431819, mean_absolute_error: 36.636485, mean_q: 46.952480, mean_eps: 0.100000
 1891456/2000000: episode: 8671, duration: 9.816s, episode steps: 572, steps per second: 58, episode reward: 247.525, mean reward: 0.433 [-20.022, 100.000], mean action: 0.552 [0.000, 3.000], mean observation: 0.222 [-1.443, 1.000], loss: 1.404755, mean_absolute_error: 34.576463, mean_q: 44.613467, mean_eps: 0.100000
 1891630/2000000: episode: 8672, duration: 2.747s, episode steps: 174, steps per second: 63, episode reward: -17.165, mean reward: -0.099 [-100.000, 24.566], mean action: 1.540 [0.000, 3.000], mean observation: 0.004 [-0.932, 1.000], loss: 1.406422, mean_absolute_error: 33.829760, mean_q: 42.878262, mean_eps: 0.100000
 1891841/2000000: episode: 8673, duration: 3.290s, episode steps: 211, steps per second: 64, episode reward: 22.967, mean reward: 0.109 [-100.000, 13.712], mean action: 1.664 [0.000, 3.000], mean observation: 0.078 [-0.853, 1.504], loss: 1.169889, mean_absolute_error: 34.280159, mean_q: 44.010834, mean_eps: 0.100000
 1891966/2000000: episode: 8674, duration: 1.918s, episode steps: 125, steps per second: 65, episode reward: -97.233, mean reward: -0.778 [-100.000, 20.320], mean action: 1.744 [0.000, 3.000], mean observation: 0.005 [-2.075, 1.000], loss: 1.352352, mean_absolute_error: 34.374983, mean_q: 43.747435, mean_eps: 0.100000
 1892335/2000000: episode: 8675, duration: 5.812s, episode steps: 369, steps per second: 63, episode reward: 226.890, mean reward: 0.615 [-17.436, 100.000], mean action: 0.862 [0.000, 3.000], mean observation: 0.105 [-1.048, 1.000], loss: 1.374701, mean_absolute_error: 34.628153, mean_q: 44.723750, mean_eps: 0.100000
 1892435/2000000: episode: 8676, duration: 1.563s, episode steps: 100, steps per second: 64, episode reward: -60.728, mean reward: -0.607 [-100.000, 20.544], mean action: 1.440 [0.000, 3.000], mean observation: -0.066 [-0.930, 2.492], loss: 1.431078, mean_absolute_error: 34.848843, mean_q: 44.770499, mean_eps: 0.100000
 1892692/2000000: episode: 8677, duration: 4.099s, episode steps: 257, steps per second: 63, episode reward: 166.136, mean reward: 0.646 [-18.655, 100.000], mean action: 2.210 [0.000, 3.000], mean observation: 0.114 [-0.872, 1.000], loss: 1.229272, mean_absolute_error: 34.055881, mean_q: 42.910817, mean_eps: 0.100000
 1892795/2000000: episode: 8678, duration: 1.729s, episode steps: 103, steps per second: 60, episode reward: -14.572, mean reward: -0.141 [-100.000, 17.540], mean action: 1.466 [0.000, 3.000], mean observation: -0.003 [-0.936, 1.000], loss: 1.251278, mean_absolute_error: 35.857170, mean_q: 47.550565, mean_eps: 0.100000
 1892930/2000000: episode: 8679, duration: 2.126s, episode steps: 135, steps per second: 64, episode reward: 4.724, mean reward: 0.035 [-100.000, 22.985], mean action: 2.081 [0.000, 3.000], mean observation: -0.037 [-1.770, 1.000], loss: 1.437183, mean_absolute_error: 33.965147, mean_q: 44.257602, mean_eps: 0.100000
 1893164/2000000: episode: 8680, duration: 3.721s, episode steps: 234, steps per second: 63, episode reward: -99.341, mean reward: -0.425 [-100.000, 27.157], mean action: 1.684 [0.000, 3.000], mean observation: -0.058 [-0.926, 1.167], loss: 1.322322, mean_absolute_error: 34.502012, mean_q: 44.902439, mean_eps: 0.100000
 1893614/2000000: episode: 8681, duration: 7.175s, episode steps: 450, steps per second: 63, episode reward: 159.875, mean reward: 0.355 [-19.298, 100.000], mean action: 1.218 [0.000, 3.000], mean observation: 0.058 [-0.975, 1.000], loss: 1.202965, mean_absolute_error: 34.632307, mean_q: 44.826645, mean_eps: 0.100000
 1893926/2000000: episode: 8682, duration: 4.948s, episode steps: 312, steps per second: 63, episode reward: 145.733, mean reward: 0.467 [-17.943, 100.000], mean action: 2.410 [0.000, 3.000], mean observation: 0.117 [-0.969, 1.000], loss: 1.277072, mean_absolute_error: 34.575601, mean_q: 44.189055, mean_eps: 0.100000
 1894143/2000000: episode: 8683, duration: 3.363s, episode steps: 217, steps per second: 65, episode reward: 8.101, mean reward: 0.037 [-100.000, 13.037], mean action: 1.687 [0.000, 3.000], mean observation: -0.047 [-0.784, 1.336], loss: 1.063240, mean_absolute_error: 34.198447, mean_q: 44.017390, mean_eps: 0.100000
 1894416/2000000: episode: 8684, duration: 4.386s, episode steps: 273, steps per second: 62, episode reward: 191.496, mean reward: 0.701 [-10.386, 100.000], mean action: 1.509 [0.000, 3.000], mean observation: 0.094 [-0.914, 1.000], loss: 1.085651, mean_absolute_error: 34.172303, mean_q: 44.079218, mean_eps: 0.100000
 1894536/2000000: episode: 8685, duration: 1.934s, episode steps: 120, steps per second: 62, episode reward: -23.877, mean reward: -0.199 [-100.000, 19.768], mean action: 1.883 [0.000, 3.000], mean observation: 0.071 [-1.057, 1.000], loss: 1.148272, mean_absolute_error: 34.501934, mean_q: 44.643810, mean_eps: 0.100000
 1894695/2000000: episode: 8686, duration: 2.410s, episode steps: 159, steps per second: 66, episode reward: -21.945, mean reward: -0.138 [-100.000, 15.330], mean action: 1.553 [0.000, 3.000], mean observation: 0.069 [-0.959, 1.000], loss: 1.246970, mean_absolute_error: 34.695849, mean_q: 44.837222, mean_eps: 0.100000
 1894796/2000000: episode: 8687, duration: 1.654s, episode steps: 101, steps per second: 61, episode reward: -8.273, mean reward: -0.082 [-100.000, 19.279], mean action: 1.901 [0.000, 3.000], mean observation: 0.004 [-2.095, 1.000], loss: 1.854699, mean_absolute_error: 32.741965, mean_q: 42.442583, mean_eps: 0.100000
 1894984/2000000: episode: 8688, duration: 3.226s, episode steps: 188, steps per second: 58, episode reward: -25.247, mean reward: -0.134 [-100.000, 16.527], mean action: 1.574 [0.000, 3.000], mean observation: -0.035 [-1.635, 1.009], loss: 1.465580, mean_absolute_error: 34.186257, mean_q: 44.081909, mean_eps: 0.100000
 1895141/2000000: episode: 8689, duration: 2.594s, episode steps: 157, steps per second: 61, episode reward: -84.315, mean reward: -0.537 [-100.000, 8.466], mean action: 1.548 [0.000, 3.000], mean observation: 0.054 [-1.225, 1.010], loss: 1.334757, mean_absolute_error: 34.311228, mean_q: 44.579203, mean_eps: 0.100000
 1895494/2000000: episode: 8690, duration: 5.686s, episode steps: 353, steps per second: 62, episode reward: 154.384, mean reward: 0.437 [-19.140, 100.000], mean action: 1.637 [0.000, 3.000], mean observation: 0.109 [-0.844, 1.101], loss: 1.321423, mean_absolute_error: 34.941825, mean_q: 45.531065, mean_eps: 0.100000
 1895613/2000000: episode: 8691, duration: 1.873s, episode steps: 119, steps per second: 64, episode reward: -43.759, mean reward: -0.368 [-100.000, 14.281], mean action: 1.849 [0.000, 3.000], mean observation: 0.087 [-1.768, 1.000], loss: 1.719419, mean_absolute_error: 34.321594, mean_q: 44.829803, mean_eps: 0.100000
 1895704/2000000: episode: 8692, duration: 1.440s, episode steps: 91, steps per second: 63, episode reward: -271.493, mean reward: -2.983 [-100.000, 4.023], mean action: 1.407 [0.000, 3.000], mean observation: 0.153 [-1.193, 3.549], loss: 1.185156, mean_absolute_error: 36.091843, mean_q: 46.914479, mean_eps: 0.100000
 1895856/2000000: episode: 8693, duration: 2.442s, episode steps: 152, steps per second: 62, episode reward: -41.219, mean reward: -0.271 [-100.000, 13.396], mean action: 1.862 [0.000, 3.000], mean observation: -0.064 [-0.939, 1.000], loss: 0.986588, mean_absolute_error: 35.531346, mean_q: 45.733888, mean_eps: 0.100000
 1896665/2000000: episode: 8694, duration: 12.988s, episode steps: 809, steps per second: 62, episode reward: 232.970, mean reward: 0.288 [-19.677, 100.000], mean action: 0.622 [0.000, 3.000], mean observation: 0.157 [-1.034, 1.000], loss: 1.097969, mean_absolute_error: 35.089472, mean_q: 45.391369, mean_eps: 0.100000
 1896786/2000000: episode: 8695, duration: 1.876s, episode steps: 121, steps per second: 65, episode reward: -107.845, mean reward: -0.891 [-100.000, 16.762], mean action: 1.612 [0.000, 3.000], mean observation: -0.040 [-0.897, 1.000], loss: 1.201666, mean_absolute_error: 34.515667, mean_q: 44.335113, mean_eps: 0.100000
 1896993/2000000: episode: 8696, duration: 3.218s, episode steps: 207, steps per second: 64, episode reward: -145.464, mean reward: -0.703 [-100.000, 26.109], mean action: 2.140 [0.000, 3.000], mean observation: -0.009 [-1.324, 1.000], loss: 1.261518, mean_absolute_error: 34.834850, mean_q: 44.990636, mean_eps: 0.100000
 1897151/2000000: episode: 8697, duration: 2.461s, episode steps: 158, steps per second: 64, episode reward: -54.831, mean reward: -0.347 [-100.000, 10.275], mean action: 1.772 [0.000, 3.000], mean observation: -0.035 [-1.717, 1.000], loss: 1.150243, mean_absolute_error: 35.517764, mean_q: 46.081984, mean_eps: 0.100000
 1898151/2000000: episode: 8698, duration: 16.955s, episode steps: 1000, steps per second: 59, episode reward: 30.634, mean reward: 0.031 [-22.001, 22.220], mean action: 2.408 [0.000, 3.000], mean observation: 0.173 [-0.930, 1.000], loss: 1.298047, mean_absolute_error: 34.447170, mean_q: 44.905372, mean_eps: 0.100000
 1898257/2000000: episode: 8699, duration: 1.672s, episode steps: 106, steps per second: 63, episode reward: -38.687, mean reward: -0.365 [-100.000, 26.236], mean action: 1.387 [0.000, 3.000], mean observation: 0.038 [-2.052, 1.000], loss: 1.103988, mean_absolute_error: 34.708595, mean_q: 45.487589, mean_eps: 0.100000
 1898373/2000000: episode: 8700, duration: 1.818s, episode steps: 116, steps per second: 64, episode reward: -70.375, mean reward: -0.607 [-100.000, 22.610], mean action: 1.862 [0.000, 3.000], mean observation: 0.014 [-1.928, 1.000], loss: 1.275866, mean_absolute_error: 35.949236, mean_q: 45.919870, mean_eps: 0.100000
 1898557/2000000: episode: 8701, duration: 2.909s, episode steps: 184, steps per second: 63, episode reward: -129.639, mean reward: -0.705 [-100.000, 11.642], mean action: 1.870 [0.000, 3.000], mean observation: -0.061 [-0.846, 1.181], loss: 1.525728, mean_absolute_error: 35.029567, mean_q: 44.487797, mean_eps: 0.100000
 1898661/2000000: episode: 8702, duration: 1.636s, episode steps: 104, steps per second: 64, episode reward: -106.188, mean reward: -1.021 [-100.000, 14.398], mean action: 1.298 [0.000, 3.000], mean observation: -0.060 [-2.338, 1.000], loss: 1.564242, mean_absolute_error: 34.625986, mean_q: 44.630247, mean_eps: 0.100000
 1898759/2000000: episode: 8703, duration: 1.496s, episode steps: 98, steps per second: 66, episode reward: -39.713, mean reward: -0.405 [-100.000, 10.382], mean action: 2.061 [0.000, 3.000], mean observation: -0.081 [-0.906, 1.000], loss: 1.041734, mean_absolute_error: 35.007177, mean_q: 46.035281, mean_eps: 0.100000
 1898858/2000000: episode: 8704, duration: 1.567s, episode steps: 99, steps per second: 63, episode reward: -37.585, mean reward: -0.380 [-100.000, 20.673], mean action: 1.838 [0.000, 3.000], mean observation: 0.021 [-1.995, 1.000], loss: 1.176818, mean_absolute_error: 35.518295, mean_q: 44.715109, mean_eps: 0.100000
 1899230/2000000: episode: 8705, duration: 5.890s, episode steps: 372, steps per second: 63, episode reward: 226.566, mean reward: 0.609 [-17.419, 100.000], mean action: 0.933 [0.000, 3.000], mean observation: 0.120 [-1.153, 1.000], loss: 1.182518, mean_absolute_error: 33.900313, mean_q: 44.414691, mean_eps: 0.100000
 1899320/2000000: episode: 8706, duration: 1.454s, episode steps: 90, steps per second: 62, episode reward: -76.541, mean reward: -0.850 [-100.000, 15.972], mean action: 1.933 [0.000, 3.000], mean observation: 0.004 [-2.674, 1.000], loss: 1.461360, mean_absolute_error: 34.801776, mean_q: 45.165011, mean_eps: 0.100000
 1899615/2000000: episode: 8707, duration: 4.649s, episode steps: 295, steps per second: 63, episode reward: 239.667, mean reward: 0.812 [-8.604, 100.000], mean action: 1.431 [0.000, 3.000], mean observation: 0.090 [-0.909, 1.023], loss: 1.308380, mean_absolute_error: 34.402642, mean_q: 44.592947, mean_eps: 0.100000
 1899762/2000000: episode: 8708, duration: 2.316s, episode steps: 147, steps per second: 63, episode reward: -29.113, mean reward: -0.198 [-100.000, 17.521], mean action: 1.680 [0.000, 3.000], mean observation: 0.110 [-0.907, 1.000], loss: 0.998084, mean_absolute_error: 34.644040, mean_q: 44.889849, mean_eps: 0.100000
 1899872/2000000: episode: 8709, duration: 1.764s, episode steps: 110, steps per second: 62, episode reward: -63.315, mean reward: -0.576 [-100.000, 17.542], mean action: 1.845 [0.000, 3.000], mean observation: 0.053 [-2.499, 1.000], loss: 1.267567, mean_absolute_error: 35.122156, mean_q: 45.622258, mean_eps: 0.100000
 1900156/2000000: episode: 8710, duration: 4.519s, episode steps: 284, steps per second: 63, episode reward: 172.049, mean reward: 0.606 [-11.481, 100.000], mean action: 1.627 [0.000, 3.000], mean observation: 0.006 [-1.064, 1.200], loss: 1.136081, mean_absolute_error: 34.023389, mean_q: 44.555344, mean_eps: 0.100000
 1900255/2000000: episode: 8711, duration: 1.591s, episode steps: 99, steps per second: 62, episode reward: -80.126, mean reward: -0.809 [-100.000, 19.869], mean action: 1.657 [0.000, 3.000], mean observation: -0.027 [-1.834, 1.000], loss: 1.280258, mean_absolute_error: 33.254826, mean_q: 42.726915, mean_eps: 0.100000
 1900371/2000000: episode: 8712, duration: 1.803s, episode steps: 116, steps per second: 64, episode reward: -114.937, mean reward: -0.991 [-100.000, 16.424], mean action: 2.026 [0.000, 3.000], mean observation: -0.044 [-1.313, 1.000], loss: 1.840292, mean_absolute_error: 34.243537, mean_q: 43.060943, mean_eps: 0.100000
 1900865/2000000: episode: 8713, duration: 7.921s, episode steps: 494, steps per second: 62, episode reward: 173.696, mean reward: 0.352 [-23.387, 100.000], mean action: 0.953 [0.000, 3.000], mean observation: 0.109 [-1.581, 1.000], loss: 1.140606, mean_absolute_error: 34.178534, mean_q: 44.170576, mean_eps: 0.100000
 1900967/2000000: episode: 8714, duration: 1.567s, episode steps: 102, steps per second: 65, episode reward: -28.164, mean reward: -0.276 [-100.000, 11.049], mean action: 1.814 [0.000, 3.000], mean observation: -0.085 [-0.953, 1.000], loss: 0.978726, mean_absolute_error: 33.990030, mean_q: 45.004125, mean_eps: 0.100000
 1901078/2000000: episode: 8715, duration: 1.756s, episode steps: 111, steps per second: 63, episode reward: -101.682, mean reward: -0.916 [-100.000, 23.786], mean action: 1.477 [0.000, 3.000], mean observation: 0.031 [-2.553, 1.000], loss: 0.881794, mean_absolute_error: 33.673002, mean_q: 43.605944, mean_eps: 0.100000
 1901399/2000000: episode: 8716, duration: 5.039s, episode steps: 321, steps per second: 64, episode reward: 267.924, mean reward: 0.835 [-19.701, 100.000], mean action: 1.047 [0.000, 3.000], mean observation: 0.058 [-1.188, 1.000], loss: 1.096540, mean_absolute_error: 34.296621, mean_q: 44.087384, mean_eps: 0.100000
 1901633/2000000: episode: 8717, duration: 3.695s, episode steps: 234, steps per second: 63, episode reward: -51.356, mean reward: -0.219 [-100.000, 16.096], mean action: 1.632 [0.000, 3.000], mean observation: -0.083 [-0.807, 1.847], loss: 1.402149, mean_absolute_error: 34.728754, mean_q: 44.538315, mean_eps: 0.100000
 1901935/2000000: episode: 8718, duration: 4.772s, episode steps: 302, steps per second: 63, episode reward: 165.714, mean reward: 0.549 [-9.759, 100.000], mean action: 1.331 [0.000, 3.000], mean observation: 0.062 [-1.731, 1.000], loss: 1.503538, mean_absolute_error: 34.355188, mean_q: 42.807560, mean_eps: 0.100000
 1902205/2000000: episode: 8719, duration: 4.277s, episode steps: 270, steps per second: 63, episode reward: 228.152, mean reward: 0.845 [-11.503, 100.000], mean action: 1.256 [0.000, 3.000], mean observation: 0.077 [-0.858, 1.000], loss: 1.264608, mean_absolute_error: 34.643646, mean_q: 44.413463, mean_eps: 0.100000
 1902320/2000000: episode: 8720, duration: 1.822s, episode steps: 115, steps per second: 63, episode reward: -46.913, mean reward: -0.408 [-100.000, 22.261], mean action: 1.522 [0.000, 3.000], mean observation: 0.090 [-2.235, 1.000], loss: 1.479394, mean_absolute_error: 34.642730, mean_q: 43.186454, mean_eps: 0.100000
 1902746/2000000: episode: 8721, duration: 7.039s, episode steps: 426, steps per second: 61, episode reward: 175.720, mean reward: 0.412 [-19.897, 100.000], mean action: 0.915 [0.000, 3.000], mean observation: 0.142 [-1.487, 1.000], loss: 1.444580, mean_absolute_error: 34.295196, mean_q: 44.493111, mean_eps: 0.100000
 1902861/2000000: episode: 8722, duration: 1.801s, episode steps: 115, steps per second: 64, episode reward: -14.376, mean reward: -0.125 [-100.000, 19.470], mean action: 1.513 [0.000, 3.000], mean observation: 0.036 [-1.420, 1.000], loss: 1.377320, mean_absolute_error: 32.483325, mean_q: 41.122306, mean_eps: 0.100000
 1903376/2000000: episode: 8723, duration: 8.343s, episode steps: 515, steps per second: 62, episode reward: 178.655, mean reward: 0.347 [-17.905, 100.000], mean action: 0.922 [0.000, 3.000], mean observation: 0.126 [-0.988, 1.000], loss: 1.202858, mean_absolute_error: 34.981287, mean_q: 45.130195, mean_eps: 0.100000
 1904376/2000000: episode: 8724, duration: 17.004s, episode steps: 1000, steps per second: 59, episode reward: 72.582, mean reward: 0.073 [-20.231, 23.012], mean action: 0.896 [0.000, 3.000], mean observation: 0.166 [-1.733, 1.000], loss: 1.165462, mean_absolute_error: 34.081938, mean_q: 44.124887, mean_eps: 0.100000
 1904518/2000000: episode: 8725, duration: 2.337s, episode steps: 142, steps per second: 61, episode reward: -91.116, mean reward: -0.642 [-100.000, 14.693], mean action: 1.676 [0.000, 3.000], mean observation: 0.021 [-1.271, 1.000], loss: 1.210979, mean_absolute_error: 33.990035, mean_q: 44.739708, mean_eps: 0.100000
 1904799/2000000: episode: 8726, duration: 4.506s, episode steps: 281, steps per second: 62, episode reward: 159.906, mean reward: 0.569 [-8.270, 100.000], mean action: 2.278 [0.000, 3.000], mean observation: 0.046 [-0.893, 1.160], loss: 1.347565, mean_absolute_error: 33.236932, mean_q: 43.314490, mean_eps: 0.100000
 1904913/2000000: episode: 8727, duration: 1.807s, episode steps: 114, steps per second: 63, episode reward: -25.481, mean reward: -0.224 [-100.000, 19.522], mean action: 1.675 [0.000, 3.000], mean observation: -0.026 [-1.595, 1.000], loss: 1.392447, mean_absolute_error: 34.087103, mean_q: 42.664484, mean_eps: 0.100000
 1905041/2000000: episode: 8728, duration: 1.991s, episode steps: 128, steps per second: 64, episode reward: -33.065, mean reward: -0.258 [-100.000, 26.612], mean action: 1.742 [0.000, 3.000], mean observation: -0.046 [-0.908, 1.000], loss: 1.007146, mean_absolute_error: 33.894460, mean_q: 44.002661, mean_eps: 0.100000
 1905169/2000000: episode: 8729, duration: 1.994s, episode steps: 128, steps per second: 64, episode reward: -82.473, mean reward: -0.644 [-100.000, 26.162], mean action: 1.711 [0.000, 3.000], mean observation: -0.033 [-0.901, 1.000], loss: 1.562242, mean_absolute_error: 34.770246, mean_q: 44.265176, mean_eps: 0.100000
 1905386/2000000: episode: 8730, duration: 3.404s, episode steps: 217, steps per second: 64, episode reward: -27.324, mean reward: -0.126 [-100.000, 20.558], mean action: 1.700 [0.000, 3.000], mean observation: -0.029 [-0.737, 1.000], loss: 1.616451, mean_absolute_error: 33.614487, mean_q: 42.762237, mean_eps: 0.100000
 1905536/2000000: episode: 8731, duration: 2.369s, episode steps: 150, steps per second: 63, episode reward: 8.393, mean reward: 0.056 [-100.000, 21.039], mean action: 1.553 [0.000, 3.000], mean observation: -0.076 [-1.181, 1.000], loss: 1.372352, mean_absolute_error: 34.940110, mean_q: 44.858773, mean_eps: 0.100000
 1905644/2000000: episode: 8732, duration: 1.738s, episode steps: 108, steps per second: 62, episode reward: -13.727, mean reward: -0.127 [-100.000, 22.476], mean action: 1.583 [0.000, 3.000], mean observation: 0.029 [-2.046, 1.000], loss: 1.083411, mean_absolute_error: 36.856257, mean_q: 47.006357, mean_eps: 0.100000
 1906107/2000000: episode: 8733, duration: 7.440s, episode steps: 463, steps per second: 62, episode reward: 191.939, mean reward: 0.415 [-19.102, 100.000], mean action: 1.253 [0.000, 3.000], mean observation: 0.075 [-0.925, 1.000], loss: 1.201433, mean_absolute_error: 33.949419, mean_q: 43.568413, mean_eps: 0.100000
 1906212/2000000: episode: 8734, duration: 1.690s, episode steps: 105, steps per second: 62, episode reward: -40.558, mean reward: -0.386 [-100.000, 19.533], mean action: 1.610 [0.000, 3.000], mean observation: 0.015 [-2.005, 1.000], loss: 1.027136, mean_absolute_error: 35.616251, mean_q: 46.538585, mean_eps: 0.100000
 1906339/2000000: episode: 8735, duration: 1.981s, episode steps: 127, steps per second: 64, episode reward: -67.143, mean reward: -0.529 [-100.000, 21.949], mean action: 1.575 [0.000, 3.000], mean observation: 0.079 [-2.190, 1.000], loss: 1.192618, mean_absolute_error: 33.567527, mean_q: 42.496028, mean_eps: 0.100000
 1906504/2000000: episode: 8736, duration: 2.646s, episode steps: 165, steps per second: 62, episode reward: -36.855, mean reward: -0.223 [-100.000, 21.719], mean action: 1.776 [0.000, 3.000], mean observation: -0.038 [-1.336, 1.102], loss: 1.237727, mean_absolute_error: 33.664033, mean_q: 43.342813, mean_eps: 0.100000
 1906904/2000000: episode: 8737, duration: 6.456s, episode steps: 400, steps per second: 62, episode reward: 122.305, mean reward: 0.306 [-18.315, 100.000], mean action: 1.387 [0.000, 3.000], mean observation: 0.043 [-0.712, 1.185], loss: 1.267526, mean_absolute_error: 34.306806, mean_q: 44.516435, mean_eps: 0.100000
 1907024/2000000: episode: 8738, duration: 1.915s, episode steps: 120, steps per second: 63, episode reward: -106.904, mean reward: -0.891 [-100.000, 18.967], mean action: 1.342 [0.000, 3.000], mean observation: 0.125 [-1.665, 1.000], loss: 1.174371, mean_absolute_error: 34.802390, mean_q: 44.025231, mean_eps: 0.100000
 1907128/2000000: episode: 8739, duration: 1.679s, episode steps: 104, steps per second: 62, episode reward: -247.716, mean reward: -2.382 [-100.000, 14.557], mean action: 1.587 [0.000, 3.000], mean observation: 0.150 [-1.169, 3.633], loss: 1.368949, mean_absolute_error: 33.728720, mean_q: 43.016059, mean_eps: 0.100000
 1907243/2000000: episode: 8740, duration: 1.817s, episode steps: 115, steps per second: 63, episode reward: -103.074, mean reward: -0.896 [-100.000, 13.909], mean action: 2.052 [0.000, 3.000], mean observation: -0.065 [-1.228, 1.000], loss: 1.157234, mean_absolute_error: 34.927968, mean_q: 44.840195, mean_eps: 0.100000
 1907361/2000000: episode: 8741, duration: 1.900s, episode steps: 118, steps per second: 62, episode reward: -86.423, mean reward: -0.732 [-100.000, 11.860], mean action: 1.932 [0.000, 3.000], mean observation: 0.011 [-2.113, 1.000], loss: 1.351871, mean_absolute_error: 32.552780, mean_q: 42.104185, mean_eps: 0.100000
 1907548/2000000: episode: 8742, duration: 3.221s, episode steps: 187, steps per second: 58, episode reward: 267.865, mean reward: 1.432 [-2.869, 100.000], mean action: 1.428 [0.000, 3.000], mean observation: 0.051 [-0.840, 1.000], loss: 0.991747, mean_absolute_error: 32.637686, mean_q: 42.563585, mean_eps: 0.100000
 1907662/2000000: episode: 8743, duration: 1.811s, episode steps: 114, steps per second: 63, episode reward: -6.588, mean reward: -0.058 [-100.000, 11.940], mean action: 1.798 [0.000, 3.000], mean observation: -0.061 [-0.828, 1.000], loss: 0.982464, mean_absolute_error: 35.221229, mean_q: 45.442470, mean_eps: 0.100000
 1907970/2000000: episode: 8744, duration: 4.873s, episode steps: 308, steps per second: 63, episode reward: 223.831, mean reward: 0.727 [-10.713, 100.000], mean action: 1.321 [0.000, 3.000], mean observation: 0.071 [-0.838, 1.000], loss: 1.259633, mean_absolute_error: 34.992349, mean_q: 44.555767, mean_eps: 0.100000
 1908068/2000000: episode: 8745, duration: 1.578s, episode steps: 98, steps per second: 62, episode reward: -102.915, mean reward: -1.050 [-100.000, 17.043], mean action: 1.755 [0.000, 3.000], mean observation: 0.026 [-3.098, 1.000], loss: 0.975877, mean_absolute_error: 35.353312, mean_q: 44.839635, mean_eps: 0.100000
 1908166/2000000: episode: 8746, duration: 1.522s, episode steps: 98, steps per second: 64, episode reward: -5.130, mean reward: -0.052 [-100.000, 18.811], mean action: 1.398 [0.000, 3.000], mean observation: 0.039 [-0.939, 1.000], loss: 0.949833, mean_absolute_error: 32.894250, mean_q: 41.617418, mean_eps: 0.100000
 1908296/2000000: episode: 8747, duration: 2.055s, episode steps: 130, steps per second: 63, episode reward: -48.704, mean reward: -0.375 [-100.000, 32.504], mean action: 1.992 [0.000, 3.000], mean observation: 0.041 [-2.893, 1.007], loss: 1.303541, mean_absolute_error: 34.318093, mean_q: 44.920904, mean_eps: 0.100000
 1908532/2000000: episode: 8748, duration: 3.775s, episode steps: 236, steps per second: 63, episode reward: 167.513, mean reward: 0.710 [-17.260, 100.000], mean action: 1.297 [0.000, 3.000], mean observation: 0.064 [-1.144, 1.000], loss: 1.172273, mean_absolute_error: 33.358165, mean_q: 43.520935, mean_eps: 0.100000
 1908653/2000000: episode: 8749, duration: 1.958s, episode steps: 121, steps per second: 62, episode reward: -43.336, mean reward: -0.358 [-100.000, 14.555], mean action: 1.851 [0.000, 3.000], mean observation: -0.012 [-1.250, 1.000], loss: 1.297987, mean_absolute_error: 34.335110, mean_q: 45.275778, mean_eps: 0.100000
 1909218/2000000: episode: 8750, duration: 9.280s, episode steps: 565, steps per second: 61, episode reward: 198.072, mean reward: 0.351 [-18.622, 100.000], mean action: 1.135 [0.000, 3.000], mean observation: 0.103 [-1.144, 1.000], loss: 1.274612, mean_absolute_error: 34.350874, mean_q: 43.486148, mean_eps: 0.100000
 1909581/2000000: episode: 8751, duration: 5.747s, episode steps: 363, steps per second: 63, episode reward: 263.099, mean reward: 0.725 [-17.531, 100.000], mean action: 1.201 [0.000, 3.000], mean observation: 0.080 [-0.765, 1.000], loss: 1.190824, mean_absolute_error: 34.344977, mean_q: 43.983252, mean_eps: 0.100000
 1909747/2000000: episode: 8752, duration: 2.564s, episode steps: 166, steps per second: 65, episode reward: 237.383, mean reward: 1.430 [-3.242, 100.000], mean action: 1.259 [0.000, 3.000], mean observation: 0.065 [-1.033, 1.000], loss: 1.223867, mean_absolute_error: 36.048363, mean_q: 45.690226, mean_eps: 0.100000
 1909842/2000000: episode: 8753, duration: 1.614s, episode steps: 95, steps per second: 59, episode reward: 2.850, mean reward: 0.030 [-100.000, 17.129], mean action: 1.874 [0.000, 3.000], mean observation: -0.093 [-0.992, 1.000], loss: 1.794794, mean_absolute_error: 35.170611, mean_q: 45.508414, mean_eps: 0.100000
 1910225/2000000: episode: 8754, duration: 6.241s, episode steps: 383, steps per second: 61, episode reward: 212.900, mean reward: 0.556 [-16.988, 100.000], mean action: 1.175 [0.000, 3.000], mean observation: 0.061 [-1.731, 1.000], loss: 1.214929, mean_absolute_error: 33.614573, mean_q: 43.274172, mean_eps: 0.100000
 1910590/2000000: episode: 8755, duration: 5.803s, episode steps: 365, steps per second: 63, episode reward: 133.839, mean reward: 0.367 [-17.751, 100.000], mean action: 2.427 [0.000, 3.000], mean observation: 0.145 [-0.902, 1.052], loss: 1.610671, mean_absolute_error: 34.015074, mean_q: 43.576665, mean_eps: 0.100000
 1910937/2000000: episode: 8756, duration: 5.512s, episode steps: 347, steps per second: 63, episode reward: 175.960, mean reward: 0.507 [-20.432, 100.000], mean action: 1.127 [0.000, 3.000], mean observation: 0.106 [-1.360, 1.000], loss: 1.102997, mean_absolute_error: 33.423274, mean_q: 43.650438, mean_eps: 0.100000
 1911095/2000000: episode: 8757, duration: 2.453s, episode steps: 158, steps per second: 64, episode reward: -52.552, mean reward: -0.333 [-100.000, 12.725], mean action: 1.741 [0.000, 3.000], mean observation: 0.057 [-0.881, 1.000], loss: 0.981715, mean_absolute_error: 33.203995, mean_q: 42.731498, mean_eps: 0.100000
 1911204/2000000: episode: 8758, duration: 1.736s, episode steps: 109, steps per second: 63, episode reward: -37.701, mean reward: -0.346 [-100.000, 18.014], mean action: 1.624 [0.000, 3.000], mean observation: 0.075 [-1.807, 1.000], loss: 1.756665, mean_absolute_error: 35.875614, mean_q: 45.683923, mean_eps: 0.100000
 1911310/2000000: episode: 8759, duration: 1.712s, episode steps: 106, steps per second: 62, episode reward: -124.003, mean reward: -1.170 [-100.000, 10.337], mean action: 1.745 [0.000, 3.000], mean observation: 0.019 [-1.784, 1.000], loss: 1.613142, mean_absolute_error: 34.179638, mean_q: 43.901846, mean_eps: 0.100000
 1911526/2000000: episode: 8760, duration: 3.371s, episode steps: 216, steps per second: 64, episode reward: -23.888, mean reward: -0.111 [-100.000, 14.607], mean action: 1.412 [0.000, 3.000], mean observation: -0.003 [-0.943, 1.026], loss: 1.206202, mean_absolute_error: 34.476332, mean_q: 44.052744, mean_eps: 0.100000
 1911641/2000000: episode: 8761, duration: 1.820s, episode steps: 115, steps per second: 63, episode reward: -444.531, mean reward: -3.865 [-100.000, 37.091], mean action: 1.791 [0.000, 3.000], mean observation: 0.064 [-1.146, 2.643], loss: 0.786732, mean_absolute_error: 35.290860, mean_q: 45.660310, mean_eps: 0.100000
 1911751/2000000: episode: 8762, duration: 1.700s, episode steps: 110, steps per second: 65, episode reward: -38.181, mean reward: -0.347 [-100.000, 6.086], mean action: 2.073 [0.000, 3.000], mean observation: 0.010 [-0.800, 1.000], loss: 1.027933, mean_absolute_error: 34.206335, mean_q: 44.825788, mean_eps: 0.100000
 1912079/2000000: episode: 8763, duration: 5.194s, episode steps: 328, steps per second: 63, episode reward: 178.685, mean reward: 0.545 [-9.858, 100.000], mean action: 1.662 [0.000, 3.000], mean observation: 0.044 [-0.967, 1.289], loss: 1.277405, mean_absolute_error: 34.492801, mean_q: 45.267798, mean_eps: 0.100000
 1912180/2000000: episode: 8764, duration: 1.627s, episode steps: 101, steps per second: 62, episode reward: -44.626, mean reward: -0.442 [-100.000, 19.765], mean action: 1.733 [0.000, 3.000], mean observation: -0.138 [-1.075, 1.000], loss: 1.383861, mean_absolute_error: 33.238005, mean_q: 43.802035, mean_eps: 0.100000
 1912303/2000000: episode: 8765, duration: 1.917s, episode steps: 123, steps per second: 64, episode reward: -157.348, mean reward: -1.279 [-100.000, 8.959], mean action: 1.789 [0.000, 3.000], mean observation: 0.047 [-1.133, 1.000], loss: 1.035512, mean_absolute_error: 34.335724, mean_q: 43.763633, mean_eps: 0.100000
 1912421/2000000: episode: 8766, duration: 1.888s, episode steps: 118, steps per second: 62, episode reward: -50.539, mean reward: -0.428 [-100.000, 44.533], mean action: 1.915 [0.000, 3.000], mean observation: -0.011 [-1.358, 1.000], loss: 1.195031, mean_absolute_error: 33.377282, mean_q: 44.009595, mean_eps: 0.100000
 1913084/2000000: episode: 8767, duration: 10.728s, episode steps: 663, steps per second: 62, episode reward: 165.879, mean reward: 0.250 [-18.697, 100.000], mean action: 1.665 [0.000, 3.000], mean observation: 0.107 [-1.012, 1.012], loss: 1.296189, mean_absolute_error: 33.993116, mean_q: 43.761007, mean_eps: 0.100000
 1913338/2000000: episode: 8768, duration: 3.989s, episode steps: 254, steps per second: 64, episode reward: 177.594, mean reward: 0.699 [-9.663, 100.000], mean action: 1.732 [0.000, 3.000], mean observation: 0.067 [-1.109, 1.000], loss: 1.395370, mean_absolute_error: 34.402565, mean_q: 44.436184, mean_eps: 0.100000
 1913673/2000000: episode: 8769, duration: 5.326s, episode steps: 335, steps per second: 63, episode reward: 166.507, mean reward: 0.497 [-18.685, 100.000], mean action: 1.701 [0.000, 3.000], mean observation: 0.052 [-0.880, 1.045], loss: 1.528508, mean_absolute_error: 33.448911, mean_q: 43.335356, mean_eps: 0.100000
 1913786/2000000: episode: 8770, duration: 1.779s, episode steps: 113, steps per second: 64, episode reward: -111.532, mean reward: -0.987 [-100.000, 9.882], mean action: 1.619 [0.000, 3.000], mean observation: -0.020 [-1.811, 1.000], loss: 1.711825, mean_absolute_error: 33.672878, mean_q: 42.815725, mean_eps: 0.100000
 1913936/2000000: episode: 8771, duration: 2.348s, episode steps: 150, steps per second: 64, episode reward: 0.046, mean reward: 0.000 [-100.000, 20.537], mean action: 1.553 [0.000, 3.000], mean observation: -0.036 [-0.923, 1.006], loss: 1.562885, mean_absolute_error: 33.509136, mean_q: 43.147356, mean_eps: 0.100000
 1914098/2000000: episode: 8772, duration: 2.578s, episode steps: 162, steps per second: 63, episode reward: -1.472, mean reward: -0.009 [-100.000, 19.340], mean action: 1.920 [0.000, 3.000], mean observation: -0.034 [-1.392, 1.000], loss: 1.468973, mean_absolute_error: 34.305071, mean_q: 44.117764, mean_eps: 0.100000
 1914209/2000000: episode: 8773, duration: 1.768s, episode steps: 111, steps per second: 63, episode reward: -73.471, mean reward: -0.662 [-100.000, 37.048], mean action: 1.568 [0.000, 3.000], mean observation: -0.046 [-1.456, 1.000], loss: 1.363970, mean_absolute_error: 33.161744, mean_q: 43.092162, mean_eps: 0.100000
 1914320/2000000: episode: 8774, duration: 1.724s, episode steps: 111, steps per second: 64, episode reward: -122.788, mean reward: -1.106 [-100.000, 21.218], mean action: 1.604 [0.000, 3.000], mean observation: -0.062 [-1.761, 1.000], loss: 1.261488, mean_absolute_error: 34.306388, mean_q: 44.674798, mean_eps: 0.100000
 1914424/2000000: episode: 8775, duration: 1.677s, episode steps: 104, steps per second: 62, episode reward: -38.907, mean reward: -0.374 [-100.000, 18.052], mean action: 1.788 [0.000, 3.000], mean observation: 0.064 [-1.633, 1.000], loss: 2.308783, mean_absolute_error: 33.727357, mean_q: 42.020315, mean_eps: 0.100000
 1914682/2000000: episode: 8776, duration: 4.047s, episode steps: 258, steps per second: 64, episode reward: -98.155, mean reward: -0.380 [-100.000, 17.549], mean action: 1.647 [0.000, 3.000], mean observation: -0.058 [-1.062, 1.936], loss: 1.234132, mean_absolute_error: 35.002622, mean_q: 45.714568, mean_eps: 0.100000
 1914966/2000000: episode: 8777, duration: 4.550s, episode steps: 284, steps per second: 62, episode reward: 147.649, mean reward: 0.520 [-18.580, 100.000], mean action: 1.736 [0.000, 3.000], mean observation: -0.020 [-1.157, 1.000], loss: 1.261096, mean_absolute_error: 33.576813, mean_q: 43.221471, mean_eps: 0.100000
 1915094/2000000: episode: 8778, duration: 1.986s, episode steps: 128, steps per second: 64, episode reward: -122.320, mean reward: -0.956 [-100.000, 10.550], mean action: 1.484 [0.000, 3.000], mean observation: 0.010 [-2.786, 1.000], loss: 1.177071, mean_absolute_error: 34.371853, mean_q: 43.559700, mean_eps: 0.100000
 1915203/2000000: episode: 8779, duration: 1.707s, episode steps: 109, steps per second: 64, episode reward: -10.407, mean reward: -0.095 [-100.000, 13.538], mean action: 1.963 [1.000, 3.000], mean observation: -0.081 [-0.903, 1.383], loss: 1.599874, mean_absolute_error: 34.203170, mean_q: 44.652746, mean_eps: 0.100000
 1915409/2000000: episode: 8780, duration: 3.258s, episode steps: 206, steps per second: 63, episode reward: -14.814, mean reward: -0.072 [-100.000, 12.964], mean action: 1.568 [0.000, 3.000], mean observation: -0.015 [-1.534, 1.003], loss: 1.668096, mean_absolute_error: 35.229615, mean_q: 45.040528, mean_eps: 0.100000
 1915495/2000000: episode: 8781, duration: 1.317s, episode steps: 86, steps per second: 65, episode reward: -71.650, mean reward: -0.833 [-100.000, 11.511], mean action: 1.581 [0.000, 3.000], mean observation: -0.094 [-1.808, 1.000], loss: 1.506787, mean_absolute_error: 33.827141, mean_q: 44.502934, mean_eps: 0.100000
 1915705/2000000: episode: 8782, duration: 3.313s, episode steps: 210, steps per second: 63, episode reward: 220.324, mean reward: 1.049 [-12.429, 100.000], mean action: 1.481 [0.000, 3.000], mean observation: 0.112 [-1.548, 1.000], loss: 1.318243, mean_absolute_error: 35.209679, mean_q: 45.892073, mean_eps: 0.100000
 1915845/2000000: episode: 8783, duration: 2.170s, episode steps: 140, steps per second: 65, episode reward: -95.713, mean reward: -0.684 [-100.000, 20.363], mean action: 1.229 [0.000, 3.000], mean observation: 0.059 [-2.869, 1.000], loss: 1.123474, mean_absolute_error: 34.058211, mean_q: 43.775850, mean_eps: 0.100000
 1916038/2000000: episode: 8784, duration: 3.017s, episode steps: 193, steps per second: 64, episode reward: -89.668, mean reward: -0.465 [-100.000, 10.199], mean action: 1.637 [0.000, 3.000], mean observation: -0.031 [-0.973, 1.007], loss: 1.552460, mean_absolute_error: 33.230781, mean_q: 43.668357, mean_eps: 0.100000
 1916255/2000000: episode: 8785, duration: 3.398s, episode steps: 217, steps per second: 64, episode reward: 8.693, mean reward: 0.040 [-100.000, 21.149], mean action: 1.839 [0.000, 3.000], mean observation: -0.002 [-0.969, 1.000], loss: 1.147624, mean_absolute_error: 33.919417, mean_q: 44.552883, mean_eps: 0.100000
 1916504/2000000: episode: 8786, duration: 3.960s, episode steps: 249, steps per second: 63, episode reward: -180.632, mean reward: -0.725 [-100.000, 46.052], mean action: 2.068 [0.000, 3.000], mean observation: 0.076 [-0.789, 1.551], loss: 1.005544, mean_absolute_error: 34.295361, mean_q: 44.373479, mean_eps: 0.100000
 1916617/2000000: episode: 8787, duration: 1.825s, episode steps: 113, steps per second: 62, episode reward: -70.402, mean reward: -0.623 [-100.000, 16.136], mean action: 1.664 [0.000, 3.000], mean observation: 0.039 [-1.510, 1.000], loss: 1.429317, mean_absolute_error: 33.971091, mean_q: 43.225782, mean_eps: 0.100000
 1916730/2000000: episode: 8788, duration: 1.762s, episode steps: 113, steps per second: 64, episode reward: -61.645, mean reward: -0.546 [-100.000, 13.786], mean action: 1.991 [0.000, 3.000], mean observation: -0.078 [-1.007, 1.083], loss: 1.694623, mean_absolute_error: 33.991735, mean_q: 43.395828, mean_eps: 0.100000
 1917112/2000000: episode: 8789, duration: 6.025s, episode steps: 382, steps per second: 63, episode reward: 257.439, mean reward: 0.674 [-19.058, 100.000], mean action: 0.948 [0.000, 3.000], mean observation: 0.164 [-1.411, 1.000], loss: 1.504491, mean_absolute_error: 34.259359, mean_q: 44.452464, mean_eps: 0.100000
 1917238/2000000: episode: 8790, duration: 1.994s, episode steps: 126, steps per second: 63, episode reward: -58.406, mean reward: -0.464 [-100.000, 11.801], mean action: 1.579 [0.000, 3.000], mean observation: -0.050 [-1.563, 1.000], loss: 1.716068, mean_absolute_error: 33.998079, mean_q: 44.022182, mean_eps: 0.100000
 1917355/2000000: episode: 8791, duration: 1.817s, episode steps: 117, steps per second: 64, episode reward: -73.174, mean reward: -0.625 [-100.000, 17.169], mean action: 1.624 [0.000, 3.000], mean observation: -0.087 [-1.786, 1.000], loss: 2.397882, mean_absolute_error: 33.488396, mean_q: 42.402680, mean_eps: 0.100000
 1917469/2000000: episode: 8792, duration: 2.029s, episode steps: 114, steps per second: 56, episode reward: -64.841, mean reward: -0.569 [-100.000, 15.077], mean action: 1.702 [0.000, 3.000], mean observation: -0.059 [-1.859, 1.000], loss: 1.401701, mean_absolute_error: 32.756164, mean_q: 42.665043, mean_eps: 0.100000
 1917585/2000000: episode: 8793, duration: 1.811s, episode steps: 116, steps per second: 64, episode reward: -111.980, mean reward: -0.965 [-100.000, 10.146], mean action: 1.302 [0.000, 3.000], mean observation: -0.039 [-1.721, 1.000], loss: 2.003153, mean_absolute_error: 34.073813, mean_q: 42.950779, mean_eps: 0.100000
 1917758/2000000: episode: 8794, duration: 2.696s, episode steps: 173, steps per second: 64, episode reward: 216.816, mean reward: 1.253 [-13.828, 100.000], mean action: 1.803 [0.000, 3.000], mean observation: 0.073 [-0.741, 1.092], loss: 1.271663, mean_absolute_error: 34.031442, mean_q: 44.055481, mean_eps: 0.100000
 1917877/2000000: episode: 8795, duration: 1.910s, episode steps: 119, steps per second: 62, episode reward: -41.706, mean reward: -0.350 [-100.000, 11.151], mean action: 1.882 [0.000, 3.000], mean observation: 0.002 [-1.912, 1.000], loss: 1.489848, mean_absolute_error: 33.484594, mean_q: 41.810439, mean_eps: 0.100000
 1917984/2000000: episode: 8796, duration: 1.674s, episode steps: 107, steps per second: 64, episode reward: -158.450, mean reward: -1.481 [-100.000, 12.898], mean action: 1.738 [0.000, 3.000], mean observation: -0.014 [-3.168, 1.000], loss: 1.460586, mean_absolute_error: 35.076970, mean_q: 45.503479, mean_eps: 0.100000
 1918101/2000000: episode: 8797, duration: 1.888s, episode steps: 117, steps per second: 62, episode reward: -96.625, mean reward: -0.826 [-100.000, 41.203], mean action: 1.615 [0.000, 3.000], mean observation: -0.006 [-1.884, 1.000], loss: 1.805760, mean_absolute_error: 33.249822, mean_q: 42.428394, mean_eps: 0.100000
 1918219/2000000: episode: 8798, duration: 1.817s, episode steps: 118, steps per second: 65, episode reward: -21.275, mean reward: -0.180 [-100.000, 10.608], mean action: 1.780 [0.000, 3.000], mean observation: -0.049 [-0.989, 1.000], loss: 1.250660, mean_absolute_error: 35.159414, mean_q: 43.755751, mean_eps: 0.100000
 1918524/2000000: episode: 8799, duration: 4.889s, episode steps: 305, steps per second: 62, episode reward: 180.210, mean reward: 0.591 [-11.355, 100.000], mean action: 1.292 [0.000, 3.000], mean observation: 0.034 [-0.837, 1.149], loss: 1.559229, mean_absolute_error: 34.735989, mean_q: 44.749873, mean_eps: 0.100000
 1918737/2000000: episode: 8800, duration: 3.414s, episode steps: 213, steps per second: 62, episode reward: 240.035, mean reward: 1.127 [-11.386, 100.000], mean action: 1.559 [0.000, 3.000], mean observation: 0.102 [-1.222, 1.000], loss: 1.416702, mean_absolute_error: 32.958930, mean_q: 42.918943, mean_eps: 0.100000
 1919399/2000000: episode: 8801, duration: 10.672s, episode steps: 662, steps per second: 62, episode reward: 150.324, mean reward: 0.227 [-19.404, 100.000], mean action: 1.855 [0.000, 3.000], mean observation: 0.035 [-0.800, 1.000], loss: 1.181945, mean_absolute_error: 34.036293, mean_q: 43.912610, mean_eps: 0.100000
 1919566/2000000: episode: 8802, duration: 2.622s, episode steps: 167, steps per second: 64, episode reward: -58.552, mean reward: -0.351 [-100.000, 15.990], mean action: 1.509 [0.000, 3.000], mean observation: 0.046 [-1.420, 1.009], loss: 1.424378, mean_absolute_error: 34.826669, mean_q: 44.610902, mean_eps: 0.100000
 1919839/2000000: episode: 8803, duration: 4.242s, episode steps: 273, steps per second: 64, episode reward: -66.878, mean reward: -0.245 [-100.000, 17.295], mean action: 1.509 [0.000, 3.000], mean observation: -0.042 [-0.967, 1.000], loss: 1.213223, mean_absolute_error: 34.993898, mean_q: 45.524992, mean_eps: 0.100000
 1920412/2000000: episode: 8804, duration: 9.380s, episode steps: 573, steps per second: 61, episode reward: 186.588, mean reward: 0.326 [-18.981, 100.000], mean action: 1.037 [0.000, 3.000], mean observation: 0.141 [-0.933, 1.000], loss: 1.243648, mean_absolute_error: 34.515097, mean_q: 44.353586, mean_eps: 0.100000
 1920602/2000000: episode: 8805, duration: 3.036s, episode steps: 190, steps per second: 63, episode reward: 261.269, mean reward: 1.375 [-3.504, 100.000], mean action: 1.589 [0.000, 3.000], mean observation: 0.111 [-1.605, 1.000], loss: 1.785933, mean_absolute_error: 34.289770, mean_q: 42.709820, mean_eps: 0.100000
 1920752/2000000: episode: 8806, duration: 2.362s, episode steps: 150, steps per second: 64, episode reward: -94.326, mean reward: -0.629 [-100.000, 19.141], mean action: 1.513 [0.000, 3.000], mean observation: 0.022 [-1.583, 1.012], loss: 1.480918, mean_absolute_error: 33.374002, mean_q: 43.491190, mean_eps: 0.100000
 1920875/2000000: episode: 8807, duration: 1.945s, episode steps: 123, steps per second: 63, episode reward: -85.754, mean reward: -0.697 [-100.000, 20.220], mean action: 1.813 [0.000, 3.000], mean observation: 0.082 [-2.245, 1.000], loss: 1.466791, mean_absolute_error: 33.210596, mean_q: 41.796329, mean_eps: 0.100000
 1921351/2000000: episode: 8808, duration: 8.023s, episode steps: 476, steps per second: 59, episode reward: 179.195, mean reward: 0.376 [-17.138, 100.000], mean action: 1.359 [0.000, 3.000], mean observation: 0.041 [-0.835, 1.000], loss: 1.211497, mean_absolute_error: 35.125837, mean_q: 45.573764, mean_eps: 0.100000
 1921470/2000000: episode: 8809, duration: 1.861s, episode steps: 119, steps per second: 64, episode reward: -59.790, mean reward: -0.502 [-100.000, 12.370], mean action: 1.487 [0.000, 3.000], mean observation: 0.035 [-1.786, 1.000], loss: 1.100155, mean_absolute_error: 35.742506, mean_q: 45.947255, mean_eps: 0.100000
 1921675/2000000: episode: 8810, duration: 3.191s, episode steps: 205, steps per second: 64, episode reward: 237.293, mean reward: 1.158 [-2.422, 100.000], mean action: 0.917 [0.000, 3.000], mean observation: 0.127 [-0.891, 1.000], loss: 1.149888, mean_absolute_error: 34.634000, mean_q: 44.280352, mean_eps: 0.100000
 1921757/2000000: episode: 8811, duration: 1.345s, episode steps: 82, steps per second: 61, episode reward: -82.326, mean reward: -1.004 [-100.000, 21.618], mean action: 1.890 [0.000, 3.000], mean observation: -0.064 [-2.451, 1.000], loss: 0.980014, mean_absolute_error: 33.753426, mean_q: 42.100432, mean_eps: 0.100000
 1922471/2000000: episode: 8812, duration: 11.662s, episode steps: 714, steps per second: 61, episode reward: 186.271, mean reward: 0.261 [-20.972, 100.000], mean action: 1.029 [0.000, 3.000], mean observation: 0.126 [-1.144, 1.014], loss: 1.350851, mean_absolute_error: 34.158310, mean_q: 44.288478, mean_eps: 0.100000
 1922748/2000000: episode: 8813, duration: 4.460s, episode steps: 277, steps per second: 62, episode reward: 145.301, mean reward: 0.525 [-16.889, 100.000], mean action: 1.560 [0.000, 3.000], mean observation: -0.059 [-1.642, 1.000], loss: 1.078966, mean_absolute_error: 34.124604, mean_q: 43.897225, mean_eps: 0.100000
 1922847/2000000: episode: 8814, duration: 1.558s, episode steps: 99, steps per second: 64, episode reward: 7.467, mean reward: 0.075 [-100.000, 18.038], mean action: 1.414 [0.000, 3.000], mean observation: -0.021 [-0.957, 1.000], loss: 0.966093, mean_absolute_error: 34.499494, mean_q: 44.966087, mean_eps: 0.100000
 1922954/2000000: episode: 8815, duration: 1.690s, episode steps: 107, steps per second: 63, episode reward: -83.265, mean reward: -0.778 [-100.000, 27.369], mean action: 1.944 [0.000, 3.000], mean observation: 0.006 [-1.648, 1.000], loss: 1.380667, mean_absolute_error: 35.223801, mean_q: 45.992239, mean_eps: 0.100000
 1923112/2000000: episode: 8816, duration: 2.523s, episode steps: 158, steps per second: 63, episode reward: -117.294, mean reward: -0.742 [-100.000, 19.085], mean action: 1.589 [0.000, 3.000], mean observation: 0.029 [-1.738, 1.000], loss: 0.882782, mean_absolute_error: 33.959655, mean_q: 44.618478, mean_eps: 0.100000
 1923233/2000000: episode: 8817, duration: 1.919s, episode steps: 121, steps per second: 63, episode reward: -78.762, mean reward: -0.651 [-100.000, 10.341], mean action: 1.851 [0.000, 3.000], mean observation: -0.029 [-0.795, 1.127], loss: 1.188814, mean_absolute_error: 34.538019, mean_q: 44.450943, mean_eps: 0.100000
 1923353/2000000: episode: 8818, duration: 1.867s, episode steps: 120, steps per second: 64, episode reward: -93.550, mean reward: -0.780 [-100.000, 28.592], mean action: 1.567 [0.000, 3.000], mean observation: 0.044 [-1.345, 1.000], loss: 1.503933, mean_absolute_error: 33.912112, mean_q: 43.339729, mean_eps: 0.100000
 1923473/2000000: episode: 8819, duration: 1.910s, episode steps: 120, steps per second: 63, episode reward: -67.156, mean reward: -0.560 [-100.000, 19.394], mean action: 1.658 [0.000, 3.000], mean observation: 0.038 [-2.091, 1.000], loss: 1.149519, mean_absolute_error: 33.741205, mean_q: 43.984768, mean_eps: 0.100000
 1923595/2000000: episode: 8820, duration: 1.888s, episode steps: 122, steps per second: 65, episode reward: -117.172, mean reward: -0.960 [-100.000, 15.129], mean action: 1.516 [0.000, 3.000], mean observation: 0.046 [-1.595, 1.000], loss: 1.314963, mean_absolute_error: 33.414741, mean_q: 43.265030, mean_eps: 0.100000
 1923691/2000000: episode: 8821, duration: 1.501s, episode steps: 96, steps per second: 64, episode reward: -105.294, mean reward: -1.097 [-100.000, 13.264], mean action: 1.656 [0.000, 3.000], mean observation: -0.173 [-1.084, 1.000], loss: 1.014913, mean_absolute_error: 33.312357, mean_q: 42.952608, mean_eps: 0.100000
 1923805/2000000: episode: 8822, duration: 1.810s, episode steps: 114, steps per second: 63, episode reward: -69.445, mean reward: -0.609 [-100.000, 19.890], mean action: 1.737 [0.000, 3.000], mean observation: 0.054 [-2.047, 1.000], loss: 1.384823, mean_absolute_error: 34.934558, mean_q: 45.385245, mean_eps: 0.100000
 1923920/2000000: episode: 8823, duration: 1.806s, episode steps: 115, steps per second: 64, episode reward: -67.230, mean reward: -0.585 [-100.000, 15.311], mean action: 1.974 [0.000, 3.000], mean observation: 0.043 [-0.992, 1.000], loss: 1.556569, mean_absolute_error: 35.331369, mean_q: 45.941886, mean_eps: 0.100000
 1924065/2000000: episode: 8824, duration: 2.292s, episode steps: 145, steps per second: 63, episode reward: -66.028, mean reward: -0.455 [-100.000, 15.036], mean action: 1.745 [0.000, 3.000], mean observation: 0.009 [-1.324, 1.000], loss: 1.375239, mean_absolute_error: 32.920371, mean_q: 42.429794, mean_eps: 0.100000
 1924170/2000000: episode: 8825, duration: 1.647s, episode steps: 105, steps per second: 64, episode reward: -10.038, mean reward: -0.096 [-100.000, 19.998], mean action: 2.019 [1.000, 3.000], mean observation: 0.074 [-1.813, 1.021], loss: 1.901911, mean_absolute_error: 35.257136, mean_q: 44.643602, mean_eps: 0.100000
 1924265/2000000: episode: 8826, duration: 1.504s, episode steps: 95, steps per second: 63, episode reward: -3.420, mean reward: -0.036 [-100.000, 18.081], mean action: 1.674 [0.000, 3.000], mean observation: -0.094 [-0.991, 1.262], loss: 1.508284, mean_absolute_error: 32.273098, mean_q: 41.158846, mean_eps: 0.100000
 1924688/2000000: episode: 8827, duration: 6.803s, episode steps: 423, steps per second: 62, episode reward: 226.269, mean reward: 0.535 [-17.663, 100.000], mean action: 1.170 [0.000, 3.000], mean observation: 0.125 [-0.991, 1.000], loss: 1.321846, mean_absolute_error: 34.543771, mean_q: 44.855872, mean_eps: 0.100000
 1924877/2000000: episode: 8828, duration: 3.141s, episode steps: 189, steps per second: 60, episode reward: -52.341, mean reward: -0.277 [-100.000, 16.547], mean action: 1.508 [0.000, 3.000], mean observation: -0.052 [-0.869, 1.000], loss: 1.212195, mean_absolute_error: 34.583365, mean_q: 44.587812, mean_eps: 0.100000
 1925010/2000000: episode: 8829, duration: 2.188s, episode steps: 133, steps per second: 61, episode reward: -250.307, mean reward: -1.882 [-100.000, 3.472], mean action: 1.624 [0.000, 3.000], mean observation: 0.114 [-1.640, 1.599], loss: 1.097671, mean_absolute_error: 34.984679, mean_q: 44.336215, mean_eps: 0.100000
 1925362/2000000: episode: 8830, duration: 5.705s, episode steps: 352, steps per second: 62, episode reward: 99.135, mean reward: 0.282 [-12.982, 100.000], mean action: 2.432 [0.000, 3.000], mean observation: 0.098 [-0.878, 1.000], loss: 1.196455, mean_absolute_error: 34.576746, mean_q: 44.393385, mean_eps: 0.100000
 1925474/2000000: episode: 8831, duration: 2.000s, episode steps: 112, steps per second: 56, episode reward: -53.116, mean reward: -0.474 [-100.000, 13.299], mean action: 1.884 [0.000, 3.000], mean observation: 0.019 [-0.867, 1.000], loss: 1.242768, mean_absolute_error: 33.856381, mean_q: 43.725514, mean_eps: 0.100000
 1925642/2000000: episode: 8832, duration: 2.605s, episode steps: 168, steps per second: 64, episode reward: -8.047, mean reward: -0.048 [-100.000, 12.694], mean action: 1.857 [0.000, 3.000], mean observation: -0.027 [-0.714, 1.000], loss: 1.164046, mean_absolute_error: 35.123328, mean_q: 45.382043, mean_eps: 0.100000
 1926106/2000000: episode: 8833, duration: 7.484s, episode steps: 464, steps per second: 62, episode reward: 200.142, mean reward: 0.431 [-18.461, 100.000], mean action: 1.377 [0.000, 3.000], mean observation: 0.104 [-0.929, 1.000], loss: 1.271776, mean_absolute_error: 33.982019, mean_q: 44.046519, mean_eps: 0.100000
 1926245/2000000: episode: 8834, duration: 2.191s, episode steps: 139, steps per second: 63, episode reward: -190.304, mean reward: -1.369 [-100.000, 62.388], mean action: 1.899 [0.000, 3.000], mean observation: 0.072 [-0.989, 1.561], loss: 1.449122, mean_absolute_error: 33.398908, mean_q: 41.478447, mean_eps: 0.100000
 1926378/2000000: episode: 8835, duration: 2.084s, episode steps: 133, steps per second: 64, episode reward: -97.330, mean reward: -0.732 [-100.000, 39.719], mean action: 1.850 [0.000, 3.000], mean observation: 0.021 [-1.938, 1.000], loss: 1.182808, mean_absolute_error: 33.097092, mean_q: 43.238906, mean_eps: 0.100000
 1926835/2000000: episode: 8836, duration: 7.822s, episode steps: 457, steps per second: 58, episode reward: 169.443, mean reward: 0.371 [-18.107, 100.000], mean action: 1.175 [0.000, 3.000], mean observation: 0.057 [-0.905, 1.000], loss: 1.280152, mean_absolute_error: 35.022611, mean_q: 45.552917, mean_eps: 0.100000
 1927116/2000000: episode: 8837, duration: 4.490s, episode steps: 281, steps per second: 63, episode reward: 155.832, mean reward: 0.555 [-4.389, 100.000], mean action: 1.530 [0.000, 3.000], mean observation: -0.001 [-0.756, 1.000], loss: 1.268945, mean_absolute_error: 34.341397, mean_q: 44.134363, mean_eps: 0.100000
 1927251/2000000: episode: 8838, duration: 2.152s, episode steps: 135, steps per second: 63, episode reward: -94.143, mean reward: -0.697 [-100.000, 10.757], mean action: 1.941 [0.000, 3.000], mean observation: -0.006 [-1.660, 1.000], loss: 1.339196, mean_absolute_error: 34.683259, mean_q: 43.146712, mean_eps: 0.100000
 1927566/2000000: episode: 8839, duration: 5.007s, episode steps: 315, steps per second: 63, episode reward: 171.492, mean reward: 0.544 [-6.613, 100.000], mean action: 1.517 [0.000, 3.000], mean observation: 0.000 [-0.928, 1.078], loss: 1.200579, mean_absolute_error: 33.684607, mean_q: 44.000458, mean_eps: 0.100000
 1927661/2000000: episode: 8840, duration: 1.506s, episode steps: 95, steps per second: 63, episode reward: -34.501, mean reward: -0.363 [-100.000, 11.492], mean action: 1.505 [0.000, 3.000], mean observation: 0.042 [-1.900, 1.000], loss: 1.337035, mean_absolute_error: 34.880793, mean_q: 45.699404, mean_eps: 0.100000
 1927795/2000000: episode: 8841, duration: 2.079s, episode steps: 134, steps per second: 64, episode reward: -170.882, mean reward: -1.275 [-100.000, 8.556], mean action: 1.649 [0.000, 3.000], mean observation: -0.021 [-1.743, 1.000], loss: 1.076680, mean_absolute_error: 35.027312, mean_q: 45.578482, mean_eps: 0.100000
 1928184/2000000: episode: 8842, duration: 6.297s, episode steps: 389, steps per second: 62, episode reward: 223.879, mean reward: 0.576 [-20.863, 100.000], mean action: 1.069 [0.000, 3.000], mean observation: 0.077 [-1.578, 1.000], loss: 1.353276, mean_absolute_error: 34.833024, mean_q: 45.106217, mean_eps: 0.100000
 1928517/2000000: episode: 8843, duration: 5.273s, episode steps: 333, steps per second: 63, episode reward: 250.706, mean reward: 0.753 [-18.722, 100.000], mean action: 1.024 [0.000, 3.000], mean observation: 0.168 [-1.521, 1.000], loss: 1.161389, mean_absolute_error: 34.458539, mean_q: 44.733544, mean_eps: 0.100000
 1928683/2000000: episode: 8844, duration: 2.593s, episode steps: 166, steps per second: 64, episode reward: -7.891, mean reward: -0.048 [-100.000, 17.718], mean action: 1.651 [0.000, 3.000], mean observation: -0.017 [-1.082, 1.000], loss: 1.382254, mean_absolute_error: 34.023865, mean_q: 44.434810, mean_eps: 0.100000
 1928831/2000000: episode: 8845, duration: 2.321s, episode steps: 148, steps per second: 64, episode reward: 3.137, mean reward: 0.021 [-100.000, 20.440], mean action: 1.622 [0.000, 3.000], mean observation: -0.041 [-1.565, 1.000], loss: 1.904430, mean_absolute_error: 33.871102, mean_q: 43.685913, mean_eps: 0.100000
 1929259/2000000: episode: 8846, duration: 6.822s, episode steps: 428, steps per second: 63, episode reward: 248.862, mean reward: 0.581 [-19.029, 100.000], mean action: 1.133 [0.000, 3.000], mean observation: 0.111 [-1.789, 1.000], loss: 1.673646, mean_absolute_error: 34.058657, mean_q: 43.293725, mean_eps: 0.100000
 1929377/2000000: episode: 8847, duration: 1.832s, episode steps: 118, steps per second: 64, episode reward: -166.679, mean reward: -1.413 [-100.000, 12.972], mean action: 1.314 [0.000, 3.000], mean observation: 0.002 [-3.257, 1.002], loss: 1.456460, mean_absolute_error: 35.285154, mean_q: 46.174237, mean_eps: 0.100000
 1929522/2000000: episode: 8848, duration: 2.256s, episode steps: 145, steps per second: 64, episode reward: 6.254, mean reward: 0.043 [-100.000, 13.210], mean action: 1.476 [0.000, 3.000], mean observation: -0.039 [-0.999, 1.000], loss: 1.325429, mean_absolute_error: 32.645844, mean_q: 42.047705, mean_eps: 0.100000
 1929636/2000000: episode: 8849, duration: 1.810s, episode steps: 114, steps per second: 63, episode reward: -143.471, mean reward: -1.259 [-100.000, 20.601], mean action: 1.596 [0.000, 3.000], mean observation: 0.009 [-2.536, 1.000], loss: 1.371419, mean_absolute_error: 33.013357, mean_q: 43.161664, mean_eps: 0.100000
 1930005/2000000: episode: 8850, duration: 5.937s, episode steps: 369, steps per second: 62, episode reward: 87.507, mean reward: 0.237 [-19.379, 100.000], mean action: 1.997 [0.000, 3.000], mean observation: 0.038 [-0.976, 1.000], loss: 1.393944, mean_absolute_error: 34.092116, mean_q: 43.198311, mean_eps: 0.100000
 1930143/2000000: episode: 8851, duration: 2.158s, episode steps: 138, steps per second: 64, episode reward: -135.564, mean reward: -0.982 [-100.000, 14.724], mean action: 1.297 [0.000, 3.000], mean observation: 0.064 [-3.013, 1.000], loss: 1.346004, mean_absolute_error: 32.770698, mean_q: 42.171483, mean_eps: 0.100000
 1930600/2000000: episode: 8852, duration: 7.388s, episode steps: 457, steps per second: 62, episode reward: 172.191, mean reward: 0.377 [-18.323, 100.000], mean action: 0.746 [0.000, 3.000], mean observation: 0.135 [-1.644, 1.000], loss: 1.259356, mean_absolute_error: 34.503563, mean_q: 44.829329, mean_eps: 0.100000
 1931013/2000000: episode: 8853, duration: 6.644s, episode steps: 413, steps per second: 62, episode reward: -102.013, mean reward: -0.247 [-100.000, 13.158], mean action: 1.838 [0.000, 3.000], mean observation: -0.091 [-1.178, 1.014], loss: 1.038104, mean_absolute_error: 34.275084, mean_q: 44.619797, mean_eps: 0.100000
 1931356/2000000: episode: 8854, duration: 5.491s, episode steps: 343, steps per second: 62, episode reward: 180.152, mean reward: 0.525 [-20.496, 100.000], mean action: 1.172 [0.000, 3.000], mean observation: 0.044 [-0.929, 1.000], loss: 1.346708, mean_absolute_error: 34.287365, mean_q: 44.335650, mean_eps: 0.100000
 1931523/2000000: episode: 8855, duration: 2.644s, episode steps: 167, steps per second: 63, episode reward: 193.279, mean reward: 1.157 [-3.194, 100.000], mean action: 1.701 [0.000, 3.000], mean observation: 0.015 [-0.931, 1.000], loss: 1.479307, mean_absolute_error: 33.409974, mean_q: 43.553622, mean_eps: 0.100000
 1931737/2000000: episode: 8856, duration: 3.426s, episode steps: 214, steps per second: 62, episode reward: 267.341, mean reward: 1.249 [-2.666, 100.000], mean action: 1.192 [0.000, 3.000], mean observation: 0.081 [-0.966, 1.000], loss: 1.134515, mean_absolute_error: 33.988164, mean_q: 43.786262, mean_eps: 0.100000
 1931863/2000000: episode: 8857, duration: 1.941s, episode steps: 126, steps per second: 65, episode reward: -18.559, mean reward: -0.147 [-100.000, 18.621], mean action: 1.556 [0.000, 3.000], mean observation: 0.082 [-1.808, 1.000], loss: 1.392425, mean_absolute_error: 35.452472, mean_q: 45.646072, mean_eps: 0.100000
 1931981/2000000: episode: 8858, duration: 1.857s, episode steps: 118, steps per second: 64, episode reward: -79.268, mean reward: -0.672 [-100.000, 39.637], mean action: 1.720 [0.000, 3.000], mean observation: 0.093 [-2.265, 1.000], loss: 1.409578, mean_absolute_error: 34.462718, mean_q: 45.142265, mean_eps: 0.100000
 1932105/2000000: episode: 8859, duration: 1.937s, episode steps: 124, steps per second: 64, episode reward: -129.689, mean reward: -1.046 [-100.000, 15.938], mean action: 1.258 [0.000, 3.000], mean observation: -0.012 [-1.444, 1.069], loss: 1.631361, mean_absolute_error: 33.121409, mean_q: 44.111700, mean_eps: 0.100000
 1932218/2000000: episode: 8860, duration: 1.738s, episode steps: 113, steps per second: 65, episode reward: -113.640, mean reward: -1.006 [-100.000, 21.147], mean action: 1.752 [0.000, 3.000], mean observation: -0.006 [-1.807, 1.000], loss: 1.044768, mean_absolute_error: 32.741239, mean_q: 41.815660, mean_eps: 0.100000
 1932384/2000000: episode: 8861, duration: 2.758s, episode steps: 166, steps per second: 60, episode reward: -8.064, mean reward: -0.049 [-100.000, 10.369], mean action: 1.825 [0.000, 3.000], mean observation: -0.033 [-0.716, 1.000], loss: 1.049675, mean_absolute_error: 32.433557, mean_q: 42.912515, mean_eps: 0.100000
 1932524/2000000: episode: 8862, duration: 2.323s, episode steps: 140, steps per second: 60, episode reward: -79.235, mean reward: -0.566 [-100.000, 17.866], mean action: 1.557 [0.000, 3.000], mean observation: 0.008 [-1.404, 1.000], loss: 1.266765, mean_absolute_error: 34.706924, mean_q: 44.989462, mean_eps: 0.100000
 1932624/2000000: episode: 8863, duration: 1.630s, episode steps: 100, steps per second: 61, episode reward: -7.715, mean reward: -0.077 [-100.000, 19.066], mean action: 1.640 [0.000, 3.000], mean observation: -0.087 [-0.962, 1.489], loss: 1.469564, mean_absolute_error: 34.550558, mean_q: 45.181586, mean_eps: 0.100000
 1932811/2000000: episode: 8864, duration: 2.930s, episode steps: 187, steps per second: 64, episode reward: 193.482, mean reward: 1.035 [-8.494, 100.000], mean action: 1.305 [0.000, 3.000], mean observation: 0.046 [-0.833, 1.000], loss: 1.275241, mean_absolute_error: 33.994059, mean_q: 43.239637, mean_eps: 0.100000
 1932907/2000000: episode: 8865, duration: 1.534s, episode steps: 96, steps per second: 63, episode reward: -24.951, mean reward: -0.260 [-100.000, 17.443], mean action: 1.677 [0.000, 3.000], mean observation: -0.091 [-0.986, 1.893], loss: 0.958564, mean_absolute_error: 32.447323, mean_q: 42.054451, mean_eps: 0.100000
 1933132/2000000: episode: 8866, duration: 3.765s, episode steps: 225, steps per second: 60, episode reward: -12.361, mean reward: -0.055 [-100.000, 14.342], mean action: 1.778 [0.000, 3.000], mean observation: -0.057 [-0.734, 1.370], loss: 1.156421, mean_absolute_error: 34.153394, mean_q: 44.825391, mean_eps: 0.100000
 1933329/2000000: episode: 8867, duration: 3.139s, episode steps: 197, steps per second: 63, episode reward: 8.612, mean reward: 0.044 [-100.000, 11.529], mean action: 1.832 [0.000, 3.000], mean observation: 0.062 [-0.806, 1.416], loss: 1.170282, mean_absolute_error: 34.427524, mean_q: 44.300902, mean_eps: 0.100000
 1933524/2000000: episode: 8868, duration: 3.106s, episode steps: 195, steps per second: 63, episode reward: -47.279, mean reward: -0.242 [-100.000, 17.019], mean action: 1.687 [0.000, 3.000], mean observation: -0.071 [-0.763, 1.000], loss: 1.233263, mean_absolute_error: 34.081762, mean_q: 44.774848, mean_eps: 0.100000
 1933666/2000000: episode: 8869, duration: 2.292s, episode steps: 142, steps per second: 62, episode reward: -111.100, mean reward: -0.782 [-100.000, 13.740], mean action: 1.958 [0.000, 3.000], mean observation: 0.017 [-1.594, 1.000], loss: 1.185575, mean_absolute_error: 33.609839, mean_q: 43.481051, mean_eps: 0.100000
 1933773/2000000: episode: 8870, duration: 1.702s, episode steps: 107, steps per second: 63, episode reward: -16.105, mean reward: -0.151 [-100.000, 15.963], mean action: 1.907 [0.000, 3.000], mean observation: 0.025 [-1.117, 1.000], loss: 1.119905, mean_absolute_error: 32.515516, mean_q: 41.706388, mean_eps: 0.100000
 1934130/2000000: episode: 8871, duration: 5.748s, episode steps: 357, steps per second: 62, episode reward: 161.061, mean reward: 0.451 [-23.250, 100.000], mean action: 1.297 [0.000, 3.000], mean observation: 0.063 [-1.102, 1.000], loss: 1.202868, mean_absolute_error: 33.665506, mean_q: 44.411058, mean_eps: 0.100000
 1934241/2000000: episode: 8872, duration: 1.800s, episode steps: 111, steps per second: 62, episode reward: -50.233, mean reward: -0.453 [-100.000, 15.425], mean action: 1.784 [0.000, 3.000], mean observation: 0.092 [-2.500, 1.000], loss: 1.540381, mean_absolute_error: 37.005917, mean_q: 46.335503, mean_eps: 0.100000
 1934358/2000000: episode: 8873, duration: 1.832s, episode steps: 117, steps per second: 64, episode reward: -40.022, mean reward: -0.342 [-100.000, 20.371], mean action: 1.607 [0.000, 3.000], mean observation: -0.092 [-0.782, 1.000], loss: 1.351694, mean_absolute_error: 34.447193, mean_q: 44.578136, mean_eps: 0.100000
 1934463/2000000: episode: 8874, duration: 1.654s, episode steps: 105, steps per second: 63, episode reward: -48.244, mean reward: -0.459 [-100.000, 16.685], mean action: 1.867 [0.000, 3.000], mean observation: 0.012 [-1.638, 1.000], loss: 1.281472, mean_absolute_error: 32.593696, mean_q: 42.198435, mean_eps: 0.100000
 1934588/2000000: episode: 8875, duration: 2.000s, episode steps: 125, steps per second: 63, episode reward: -95.120, mean reward: -0.761 [-100.000, 7.450], mean action: 1.968 [0.000, 3.000], mean observation: 0.035 [-0.873, 1.114], loss: 0.980860, mean_absolute_error: 35.085630, mean_q: 46.285388, mean_eps: 0.100000
 1934700/2000000: episode: 8876, duration: 1.825s, episode steps: 112, steps per second: 61, episode reward: -122.708, mean reward: -1.096 [-100.000, 10.884], mean action: 1.741 [0.000, 3.000], mean observation: -0.022 [-1.856, 1.000], loss: 1.206723, mean_absolute_error: 35.080362, mean_q: 46.061610, mean_eps: 0.100000
 1934800/2000000: episode: 8877, duration: 1.627s, episode steps: 100, steps per second: 61, episode reward: -34.743, mean reward: -0.347 [-100.000, 19.765], mean action: 1.880 [0.000, 3.000], mean observation: 0.058 [-1.788, 1.000], loss: 1.623358, mean_absolute_error: 34.130417, mean_q: 43.891063, mean_eps: 0.100000
 1934921/2000000: episode: 8878, duration: 1.948s, episode steps: 121, steps per second: 62, episode reward: -214.733, mean reward: -1.775 [-100.000, 6.094], mean action: 1.322 [0.000, 3.000], mean observation: 0.083 [-1.367, 1.000], loss: 1.321929, mean_absolute_error: 35.069032, mean_q: 46.295097, mean_eps: 0.100000
 1935049/2000000: episode: 8879, duration: 1.996s, episode steps: 128, steps per second: 64, episode reward: -51.506, mean reward: -0.402 [-100.000, 17.541], mean action: 1.922 [0.000, 3.000], mean observation: 0.061 [-1.620, 1.000], loss: 0.930310, mean_absolute_error: 35.029867, mean_q: 45.597338, mean_eps: 0.100000
 1935277/2000000: episode: 8880, duration: 3.575s, episode steps: 228, steps per second: 64, episode reward: 217.920, mean reward: 0.956 [-3.613, 100.000], mean action: 1.171 [0.000, 3.000], mean observation: 0.130 [-1.307, 1.000], loss: 1.598684, mean_absolute_error: 33.545312, mean_q: 43.712416, mean_eps: 0.100000
 1935483/2000000: episode: 8881, duration: 3.228s, episode steps: 206, steps per second: 64, episode reward: -70.377, mean reward: -0.342 [-100.000, 9.807], mean action: 1.558 [0.000, 3.000], mean observation: -0.022 [-0.893, 1.000], loss: 1.491887, mean_absolute_error: 32.895727, mean_q: 43.021070, mean_eps: 0.100000
 1935801/2000000: episode: 8882, duration: 5.074s, episode steps: 318, steps per second: 63, episode reward: 214.277, mean reward: 0.674 [-19.542, 100.000], mean action: 0.701 [0.000, 3.000], mean observation: 0.128 [-0.981, 1.000], loss: 1.286572, mean_absolute_error: 34.232030, mean_q: 44.232529, mean_eps: 0.100000
 1936179/2000000: episode: 8883, duration: 6.079s, episode steps: 378, steps per second: 62, episode reward: 190.347, mean reward: 0.504 [-20.373, 100.000], mean action: 1.228 [0.000, 3.000], mean observation: 0.114 [-1.456, 1.000], loss: 1.748279, mean_absolute_error: 33.053148, mean_q: 42.235174, mean_eps: 0.100000
 1936336/2000000: episode: 8884, duration: 2.476s, episode steps: 157, steps per second: 63, episode reward: -154.739, mean reward: -0.986 [-100.000, 11.048], mean action: 1.255 [0.000, 3.000], mean observation: 0.072 [-1.456, 1.197], loss: 1.286357, mean_absolute_error: 34.291300, mean_q: 43.896078, mean_eps: 0.100000
 1936465/2000000: episode: 8885, duration: 2.059s, episode steps: 129, steps per second: 63, episode reward: -88.316, mean reward: -0.685 [-100.000, 14.333], mean action: 2.031 [0.000, 3.000], mean observation: 0.040 [-1.155, 1.000], loss: 1.383884, mean_absolute_error: 34.124464, mean_q: 44.595636, mean_eps: 0.100000
 1936606/2000000: episode: 8886, duration: 2.193s, episode steps: 141, steps per second: 64, episode reward: -125.173, mean reward: -0.888 [-100.000, 17.444], mean action: 1.823 [0.000, 3.000], mean observation: 0.020 [-2.190, 1.000], loss: 1.857812, mean_absolute_error: 34.135055, mean_q: 42.741262, mean_eps: 0.100000
 1936738/2000000: episode: 8887, duration: 2.086s, episode steps: 132, steps per second: 63, episode reward: -115.339, mean reward: -0.874 [-100.000, 10.643], mean action: 1.886 [0.000, 3.000], mean observation: -0.031 [-0.905, 1.000], loss: 1.563925, mean_absolute_error: 34.327117, mean_q: 43.130071, mean_eps: 0.100000
 1936843/2000000: episode: 8888, duration: 1.641s, episode steps: 105, steps per second: 64, episode reward: -18.608, mean reward: -0.177 [-100.000, 33.776], mean action: 1.819 [0.000, 3.000], mean observation: 0.003 [-1.315, 1.000], loss: 1.278868, mean_absolute_error: 33.492381, mean_q: 43.901512, mean_eps: 0.100000
 1936974/2000000: episode: 8889, duration: 2.066s, episode steps: 131, steps per second: 63, episode reward: -18.570, mean reward: -0.142 [-100.000, 17.115], mean action: 1.916 [0.000, 3.000], mean observation: -0.081 [-1.049, 1.387], loss: 1.398547, mean_absolute_error: 34.132994, mean_q: 43.626534, mean_eps: 0.100000
 1937103/2000000: episode: 8890, duration: 2.022s, episode steps: 129, steps per second: 64, episode reward: -325.009, mean reward: -2.519 [-100.000, 16.032], mean action: 1.620 [0.000, 3.000], mean observation: 0.161 [-1.320, 3.062], loss: 1.191438, mean_absolute_error: 32.542336, mean_q: 42.733503, mean_eps: 0.100000
 1937220/2000000: episode: 8891, duration: 1.872s, episode steps: 117, steps per second: 63, episode reward: -120.004, mean reward: -1.026 [-100.000, 14.195], mean action: 1.427 [0.000, 3.000], mean observation: 0.103 [-3.366, 1.000], loss: 1.404316, mean_absolute_error: 34.664729, mean_q: 44.180541, mean_eps: 0.100000
 1937341/2000000: episode: 8892, duration: 1.944s, episode steps: 121, steps per second: 62, episode reward: -76.913, mean reward: -0.636 [-100.000, 17.705], mean action: 1.769 [0.000, 3.000], mean observation: 0.060 [-1.825, 1.056], loss: 1.512917, mean_absolute_error: 33.181328, mean_q: 43.189539, mean_eps: 0.100000
 1937637/2000000: episode: 8893, duration: 4.664s, episode steps: 296, steps per second: 63, episode reward: 158.696, mean reward: 0.536 [-18.252, 100.000], mean action: 1.652 [0.000, 3.000], mean observation: 0.068 [-1.000, 1.225], loss: 1.684390, mean_absolute_error: 33.842401, mean_q: 43.260205, mean_eps: 0.100000
 1937759/2000000: episode: 8894, duration: 1.892s, episode steps: 122, steps per second: 64, episode reward: -109.231, mean reward: -0.895 [-100.000, 18.422], mean action: 1.525 [0.000, 3.000], mean observation: 0.027 [-1.456, 1.000], loss: 1.389054, mean_absolute_error: 35.817714, mean_q: 46.987886, mean_eps: 0.100000
 1937898/2000000: episode: 8895, duration: 2.184s, episode steps: 139, steps per second: 64, episode reward: -98.416, mean reward: -0.708 [-100.000, 19.920], mean action: 1.446 [0.000, 3.000], mean observation: 0.084 [-2.957, 1.006], loss: 1.104254, mean_absolute_error: 34.276216, mean_q: 44.744227, mean_eps: 0.100000
 1938253/2000000: episode: 8896, duration: 5.663s, episode steps: 355, steps per second: 63, episode reward: 135.629, mean reward: 0.382 [-12.130, 100.000], mean action: 1.445 [0.000, 3.000], mean observation: 0.019 [-0.898, 1.000], loss: 1.455936, mean_absolute_error: 35.019777, mean_q: 45.819640, mean_eps: 0.100000
 1938364/2000000: episode: 8897, duration: 1.758s, episode steps: 111, steps per second: 63, episode reward: -298.652, mean reward: -2.691 [-100.000, 53.356], mean action: 1.450 [0.000, 3.000], mean observation: 0.041 [-1.260, 2.524], loss: 1.204324, mean_absolute_error: 35.188048, mean_q: 46.351377, mean_eps: 0.100000
 1938491/2000000: episode: 8898, duration: 2.009s, episode steps: 127, steps per second: 63, episode reward: -173.693, mean reward: -1.368 [-100.000, 9.077], mean action: 1.937 [0.000, 3.000], mean observation: -0.107 [-1.704, 1.000], loss: 1.000487, mean_absolute_error: 34.998301, mean_q: 45.718710, mean_eps: 0.100000
 1938635/2000000: episode: 8899, duration: 2.254s, episode steps: 144, steps per second: 64, episode reward: -85.513, mean reward: -0.594 [-100.000, 11.363], mean action: 1.646 [0.000, 3.000], mean observation: 0.003 [-1.368, 1.000], loss: 1.082635, mean_absolute_error: 33.497774, mean_q: 44.521283, mean_eps: 0.100000
 1939102/2000000: episode: 8900, duration: 7.416s, episode steps: 467, steps per second: 63, episode reward: 231.803, mean reward: 0.496 [-19.610, 100.000], mean action: 1.101 [0.000, 3.000], mean observation: 0.066 [-1.175, 1.000], loss: 1.405123, mean_absolute_error: 34.428809, mean_q: 45.075286, mean_eps: 0.100000
 1939494/2000000: episode: 8901, duration: 6.329s, episode steps: 392, steps per second: 62, episode reward: 159.156, mean reward: 0.406 [-18.665, 100.000], mean action: 1.161 [0.000, 3.000], mean observation: 0.019 [-0.920, 1.000], loss: 1.241505, mean_absolute_error: 34.438080, mean_q: 44.812019, mean_eps: 0.100000
 1939761/2000000: episode: 8902, duration: 4.276s, episode steps: 267, steps per second: 62, episode reward: -68.781, mean reward: -0.258 [-100.000, 18.047], mean action: 1.719 [0.000, 3.000], mean observation: -0.088 [-0.934, 1.000], loss: 1.072358, mean_absolute_error: 34.965955, mean_q: 45.533876, mean_eps: 0.100000
 1939888/2000000: episode: 8903, duration: 2.250s, episode steps: 127, steps per second: 56, episode reward: -133.822, mean reward: -1.054 [-100.000, 15.024], mean action: 1.756 [0.000, 3.000], mean observation: 0.034 [-1.475, 1.072], loss: 1.132346, mean_absolute_error: 34.004556, mean_q: 44.330731, mean_eps: 0.100000
 1940185/2000000: episode: 8904, duration: 4.874s, episode steps: 297, steps per second: 61, episode reward: 195.372, mean reward: 0.658 [-19.367, 100.000], mean action: 1.670 [0.000, 3.000], mean observation: 0.021 [-0.886, 1.000], loss: 1.342938, mean_absolute_error: 34.812902, mean_q: 45.696950, mean_eps: 0.100000
 1940293/2000000: episode: 8905, duration: 1.687s, episode steps: 108, steps per second: 64, episode reward: -86.218, mean reward: -0.798 [-100.000, 27.628], mean action: 1.343 [0.000, 3.000], mean observation: 0.018 [-1.656, 1.000], loss: 1.362753, mean_absolute_error: 36.275927, mean_q: 47.452558, mean_eps: 0.100000
 1940402/2000000: episode: 8906, duration: 1.807s, episode steps: 109, steps per second: 60, episode reward: -60.705, mean reward: -0.557 [-100.000, 18.548], mean action: 1.606 [0.000, 3.000], mean observation: -0.016 [-1.314, 1.000], loss: 1.347666, mean_absolute_error: 34.102008, mean_q: 43.768732, mean_eps: 0.100000
 1940570/2000000: episode: 8907, duration: 2.695s, episode steps: 168, steps per second: 62, episode reward: -66.784, mean reward: -0.398 [-100.000, 12.485], mean action: 1.988 [0.000, 3.000], mean observation: -0.005 [-0.940, 1.144], loss: 1.468164, mean_absolute_error: 33.633383, mean_q: 43.876118, mean_eps: 0.100000
 1940686/2000000: episode: 8908, duration: 1.831s, episode steps: 116, steps per second: 63, episode reward: -136.146, mean reward: -1.174 [-100.000, 10.844], mean action: 1.836 [0.000, 3.000], mean observation: -0.013 [-2.159, 1.000], loss: 1.766649, mean_absolute_error: 33.998599, mean_q: 43.004779, mean_eps: 0.100000
 1940809/2000000: episode: 8909, duration: 1.952s, episode steps: 123, steps per second: 63, episode reward: -111.314, mean reward: -0.905 [-100.000, 10.742], mean action: 1.951 [0.000, 3.000], mean observation: -0.019 [-1.272, 1.000], loss: 0.972960, mean_absolute_error: 35.024651, mean_q: 45.731566, mean_eps: 0.100000
 1940904/2000000: episode: 8910, duration: 1.510s, episode steps: 95, steps per second: 63, episode reward: -26.192, mean reward: -0.276 [-100.000, 22.454], mean action: 1.758 [0.000, 3.000], mean observation: 0.049 [-2.146, 1.000], loss: 1.387957, mean_absolute_error: 33.843292, mean_q: 42.933212, mean_eps: 0.100000
 1941005/2000000: episode: 8911, duration: 1.616s, episode steps: 101, steps per second: 63, episode reward: -107.879, mean reward: -1.068 [-100.000, 21.370], mean action: 1.396 [0.000, 3.000], mean observation: -0.057 [-1.786, 1.000], loss: 1.777494, mean_absolute_error: 34.729681, mean_q: 44.241646, mean_eps: 0.100000
 1941094/2000000: episode: 8912, duration: 1.398s, episode steps: 89, steps per second: 64, episode reward: -90.153, mean reward: -1.013 [-100.000, 15.409], mean action: 1.910 [0.000, 3.000], mean observation: -0.079 [-1.632, 1.000], loss: 1.337366, mean_absolute_error: 35.154237, mean_q: 45.723517, mean_eps: 0.100000
 1941190/2000000: episode: 8913, duration: 1.513s, episode steps: 96, steps per second: 63, episode reward: -32.726, mean reward: -0.341 [-100.000, 14.601], mean action: 2.010 [1.000, 3.000], mean observation: 0.061 [-2.510, 1.000], loss: 1.597799, mean_absolute_error: 34.934322, mean_q: 46.873925, mean_eps: 0.100000
 1941544/2000000: episode: 8914, duration: 5.707s, episode steps: 354, steps per second: 62, episode reward: -115.275, mean reward: -0.326 [-100.000, 18.340], mean action: 1.709 [0.000, 3.000], mean observation: -0.062 [-0.928, 1.077], loss: 1.398914, mean_absolute_error: 33.593121, mean_q: 43.946369, mean_eps: 0.100000
 1941814/2000000: episode: 8915, duration: 4.295s, episode steps: 270, steps per second: 63, episode reward: 145.576, mean reward: 0.539 [-14.906, 100.000], mean action: 1.674 [0.000, 3.000], mean observation: 0.040 [-0.864, 1.000], loss: 1.257331, mean_absolute_error: 34.777395, mean_q: 44.838404, mean_eps: 0.100000
 1941934/2000000: episode: 8916, duration: 1.897s, episode steps: 120, steps per second: 63, episode reward: -67.613, mean reward: -0.563 [-100.000, 23.024], mean action: 1.642 [0.000, 3.000], mean observation: 0.117 [-2.490, 1.000], loss: 1.116584, mean_absolute_error: 32.615051, mean_q: 42.362568, mean_eps: 0.100000
 1942061/2000000: episode: 8917, duration: 2.022s, episode steps: 127, steps per second: 63, episode reward: -110.057, mean reward: -0.867 [-100.000, 8.579], mean action: 1.772 [0.000, 3.000], mean observation: 0.020 [-1.560, 1.000], loss: 0.922088, mean_absolute_error: 36.290092, mean_q: 46.144773, mean_eps: 0.100000
 1942552/2000000: episode: 8918, duration: 8.095s, episode steps: 491, steps per second: 61, episode reward: 142.997, mean reward: 0.291 [-17.977, 100.000], mean action: 1.984 [0.000, 3.000], mean observation: 0.027 [-1.049, 1.122], loss: 1.352654, mean_absolute_error: 34.644024, mean_q: 44.743950, mean_eps: 0.100000
 1942685/2000000: episode: 8919, duration: 2.113s, episode steps: 133, steps per second: 63, episode reward: -160.309, mean reward: -1.205 [-100.000, 9.096], mean action: 1.857 [0.000, 3.000], mean observation: -0.071 [-1.930, 1.000], loss: 1.613776, mean_absolute_error: 34.002240, mean_q: 44.059334, mean_eps: 0.100000
 1942788/2000000: episode: 8920, duration: 1.650s, episode steps: 103, steps per second: 62, episode reward: -35.103, mean reward: -0.341 [-100.000, 16.067], mean action: 2.029 [0.000, 3.000], mean observation: 0.016 [-1.714, 1.000], loss: 1.458984, mean_absolute_error: 34.309078, mean_q: 45.313885, mean_eps: 0.100000
 1942875/2000000: episode: 8921, duration: 1.381s, episode steps: 87, steps per second: 63, episode reward: -91.486, mean reward: -1.052 [-100.000, 9.376], mean action: 2.057 [0.000, 3.000], mean observation: -0.097 [-0.946, 1.785], loss: 1.637641, mean_absolute_error: 35.016137, mean_q: 45.475539, mean_eps: 0.100000
 1943005/2000000: episode: 8922, duration: 2.076s, episode steps: 130, steps per second: 63, episode reward: -78.788, mean reward: -0.606 [-100.000, 14.384], mean action: 1.477 [0.000, 3.000], mean observation: 0.083 [-2.855, 1.000], loss: 1.356463, mean_absolute_error: 34.563786, mean_q: 44.915617, mean_eps: 0.100000
 1943136/2000000: episode: 8923, duration: 2.065s, episode steps: 131, steps per second: 63, episode reward: -136.845, mean reward: -1.045 [-100.000, 10.478], mean action: 1.947 [0.000, 3.000], mean observation: 0.016 [-1.065, 1.000], loss: 0.963798, mean_absolute_error: 34.479122, mean_q: 45.404092, mean_eps: 0.100000
 1943240/2000000: episode: 8924, duration: 1.684s, episode steps: 104, steps per second: 62, episode reward: -30.259, mean reward: -0.291 [-100.000, 29.511], mean action: 1.644 [0.000, 3.000], mean observation: -0.047 [-1.852, 1.000], loss: 1.946141, mean_absolute_error: 35.738537, mean_q: 46.080702, mean_eps: 0.100000
 1943353/2000000: episode: 8925, duration: 1.806s, episode steps: 113, steps per second: 63, episode reward: -134.649, mean reward: -1.192 [-100.000, 16.575], mean action: 1.504 [0.000, 3.000], mean observation: -0.019 [-1.361, 1.000], loss: 0.785404, mean_absolute_error: 35.149798, mean_q: 45.811032, mean_eps: 0.100000
 1943499/2000000: episode: 8926, duration: 2.246s, episode steps: 146, steps per second: 65, episode reward: -30.894, mean reward: -0.212 [-100.000, 16.665], mean action: 1.527 [0.000, 3.000], mean observation: 0.069 [-0.937, 1.000], loss: 1.032623, mean_absolute_error: 35.383362, mean_q: 45.399221, mean_eps: 0.100000
 1943753/2000000: episode: 8927, duration: 4.017s, episode steps: 254, steps per second: 63, episode reward: 232.754, mean reward: 0.916 [-9.007, 100.000], mean action: 0.929 [0.000, 3.000], mean observation: 0.046 [-1.004, 1.000], loss: 1.188568, mean_absolute_error: 34.561267, mean_q: 45.635292, mean_eps: 0.100000
 1943874/2000000: episode: 8928, duration: 1.881s, episode steps: 121, steps per second: 64, episode reward: -84.423, mean reward: -0.698 [-100.000, 28.855], mean action: 1.975 [0.000, 3.000], mean observation: 0.013 [-2.728, 1.016], loss: 0.946019, mean_absolute_error: 33.467202, mean_q: 44.768113, mean_eps: 0.100000
 1943983/2000000: episode: 8929, duration: 1.680s, episode steps: 109, steps per second: 65, episode reward: -173.519, mean reward: -1.592 [-100.000, 12.061], mean action: 1.844 [0.000, 3.000], mean observation: -0.018 [-1.137, 1.000], loss: 1.535481, mean_absolute_error: 33.062260, mean_q: 43.153346, mean_eps: 0.100000
 1944125/2000000: episode: 8930, duration: 2.271s, episode steps: 142, steps per second: 63, episode reward: -307.774, mean reward: -2.167 [-100.000, 25.358], mean action: 1.479 [0.000, 3.000], mean observation: 0.157 [-1.193, 2.399], loss: 1.111479, mean_absolute_error: 32.705475, mean_q: 43.050029, mean_eps: 0.100000
 1944345/2000000: episode: 8931, duration: 3.454s, episode steps: 220, steps per second: 64, episode reward: 200.062, mean reward: 0.909 [-4.947, 100.000], mean action: 1.518 [0.000, 3.000], mean observation: 0.054 [-0.879, 1.000], loss: 0.995707, mean_absolute_error: 34.645424, mean_q: 45.442381, mean_eps: 0.100000
 1944554/2000000: episode: 8932, duration: 3.291s, episode steps: 209, steps per second: 64, episode reward: 168.896, mean reward: 0.808 [-10.562, 100.000], mean action: 1.273 [0.000, 3.000], mean observation: 0.066 [-1.560, 1.000], loss: 1.227721, mean_absolute_error: 34.670362, mean_q: 44.646930, mean_eps: 0.100000
 1944675/2000000: episode: 8933, duration: 1.882s, episode steps: 121, steps per second: 64, episode reward: -83.739, mean reward: -0.692 [-100.000, 14.647], mean action: 1.711 [0.000, 3.000], mean observation: -0.039 [-1.174, 1.042], loss: 1.430632, mean_absolute_error: 35.857375, mean_q: 46.335364, mean_eps: 0.100000
 1944897/2000000: episode: 8934, duration: 3.500s, episode steps: 222, steps per second: 63, episode reward: 257.269, mean reward: 1.159 [-10.308, 100.000], mean action: 1.847 [0.000, 3.000], mean observation: 0.147 [-1.476, 1.000], loss: 0.938091, mean_absolute_error: 35.504859, mean_q: 46.784916, mean_eps: 0.100000
 1945218/2000000: episode: 8935, duration: 5.272s, episode steps: 321, steps per second: 61, episode reward: 174.073, mean reward: 0.542 [-18.063, 100.000], mean action: 1.364 [0.000, 3.000], mean observation: 0.085 [-1.385, 1.000], loss: 1.251509, mean_absolute_error: 33.781200, mean_q: 43.538937, mean_eps: 0.100000
 1945426/2000000: episode: 8936, duration: 3.278s, episode steps: 208, steps per second: 63, episode reward: -80.564, mean reward: -0.387 [-100.000, 17.418], mean action: 1.553 [0.000, 3.000], mean observation: -0.093 [-0.873, 1.401], loss: 1.677356, mean_absolute_error: 34.243437, mean_q: 44.616981, mean_eps: 0.100000
 1945543/2000000: episode: 8937, duration: 1.819s, episode steps: 117, steps per second: 64, episode reward: -55.074, mean reward: -0.471 [-100.000, 18.196], mean action: 1.735 [0.000, 3.000], mean observation: -0.014 [-1.714, 1.000], loss: 1.545440, mean_absolute_error: 34.567565, mean_q: 45.541022, mean_eps: 0.100000
 1945660/2000000: episode: 8938, duration: 1.861s, episode steps: 117, steps per second: 63, episode reward: -205.171, mean reward: -1.754 [-100.000, 6.936], mean action: 1.846 [0.000, 3.000], mean observation: -0.012 [-1.210, 1.002], loss: 1.378188, mean_absolute_error: 34.680707, mean_q: 45.477673, mean_eps: 0.100000
 1945736/2000000: episode: 8939, duration: 1.254s, episode steps: 76, steps per second: 61, episode reward: -60.647, mean reward: -0.798 [-100.000, 21.660], mean action: 1.526 [0.000, 3.000], mean observation: 0.106 [-2.975, 1.000], loss: 1.351959, mean_absolute_error: 34.129642, mean_q: 43.730970, mean_eps: 0.100000
 1945838/2000000: episode: 8940, duration: 1.580s, episode steps: 102, steps per second: 65, episode reward: 15.393, mean reward: 0.151 [-100.000, 14.723], mean action: 1.755 [0.000, 3.000], mean observation: -0.094 [-0.943, 1.000], loss: 1.182322, mean_absolute_error: 35.001666, mean_q: 44.970149, mean_eps: 0.100000
 1945933/2000000: episode: 8941, duration: 1.477s, episode steps: 95, steps per second: 64, episode reward: -113.567, mean reward: -1.195 [-100.000, 13.228], mean action: 1.505 [0.000, 3.000], mean observation: -0.053 [-2.890, 1.000], loss: 1.131713, mean_absolute_error: 32.846362, mean_q: 42.424038, mean_eps: 0.100000
 1946234/2000000: episode: 8942, duration: 4.739s, episode steps: 301, steps per second: 64, episode reward: 124.679, mean reward: 0.414 [-12.076, 100.000], mean action: 1.678 [0.000, 3.000], mean observation: 0.007 [-0.850, 1.034], loss: 1.131446, mean_absolute_error: 33.927168, mean_q: 44.184905, mean_eps: 0.100000
 1946351/2000000: episode: 8943, duration: 1.800s, episode steps: 117, steps per second: 65, episode reward: -66.125, mean reward: -0.565 [-100.000, 11.626], mean action: 1.581 [0.000, 3.000], mean observation: 0.051 [-1.974, 1.000], loss: 1.196010, mean_absolute_error: 33.940721, mean_q: 44.254881, mean_eps: 0.100000
 1946514/2000000: episode: 8944, duration: 2.550s, episode steps: 163, steps per second: 64, episode reward: -105.278, mean reward: -0.646 [-100.000, 16.434], mean action: 1.767 [0.000, 3.000], mean observation: 0.009 [-1.562, 1.002], loss: 1.220789, mean_absolute_error: 34.956785, mean_q: 45.751431, mean_eps: 0.100000
 1947116/2000000: episode: 8945, duration: 9.559s, episode steps: 602, steps per second: 63, episode reward: 192.794, mean reward: 0.320 [-18.625, 100.000], mean action: 0.605 [0.000, 3.000], mean observation: 0.143 [-0.916, 1.000], loss: 1.245205, mean_absolute_error: 34.510420, mean_q: 44.830048, mean_eps: 0.100000
 1947247/2000000: episode: 8946, duration: 2.076s, episode steps: 131, steps per second: 63, episode reward: -8.803, mean reward: -0.067 [-100.000, 17.049], mean action: 1.878 [0.000, 3.000], mean observation: 0.087 [-0.870, 1.197], loss: 1.405410, mean_absolute_error: 33.978650, mean_q: 43.688865, mean_eps: 0.100000
 1947508/2000000: episode: 8947, duration: 4.440s, episode steps: 261, steps per second: 59, episode reward: -292.742, mean reward: -1.122 [-100.000, 27.937], mean action: 1.874 [0.000, 3.000], mean observation: 0.114 [-0.958, 2.041], loss: 1.413415, mean_absolute_error: 34.916896, mean_q: 45.660850, mean_eps: 0.100000
 1947641/2000000: episode: 8948, duration: 2.125s, episode steps: 133, steps per second: 63, episode reward: -71.648, mean reward: -0.539 [-100.000, 21.595], mean action: 1.481 [0.000, 3.000], mean observation: 0.083 [-2.232, 1.280], loss: 1.147325, mean_absolute_error: 34.346939, mean_q: 44.240580, mean_eps: 0.100000
 1947793/2000000: episode: 8949, duration: 2.348s, episode steps: 152, steps per second: 65, episode reward: -121.499, mean reward: -0.799 [-100.000, 18.093], mean action: 1.454 [0.000, 3.000], mean observation: 0.034 [-1.448, 1.000], loss: 1.613512, mean_absolute_error: 35.213889, mean_q: 45.440976, mean_eps: 0.100000
 1947909/2000000: episode: 8950, duration: 1.847s, episode steps: 116, steps per second: 63, episode reward: -64.610, mean reward: -0.557 [-100.000, 12.716], mean action: 1.905 [0.000, 3.000], mean observation: 0.031 [-0.880, 1.000], loss: 1.114283, mean_absolute_error: 34.244248, mean_q: 43.849494, mean_eps: 0.100000
 1948169/2000000: episode: 8951, duration: 4.098s, episode steps: 260, steps per second: 63, episode reward: 159.545, mean reward: 0.614 [-18.608, 100.000], mean action: 1.273 [0.000, 3.000], mean observation: 0.092 [-1.953, 1.000], loss: 1.551359, mean_absolute_error: 33.281529, mean_q: 43.020708, mean_eps: 0.100000
 1948274/2000000: episode: 8952, duration: 1.619s, episode steps: 105, steps per second: 65, episode reward: -130.945, mean reward: -1.247 [-100.000, 8.233], mean action: 1.590 [0.000, 3.000], mean observation: -0.025 [-0.912, 1.000], loss: 1.708760, mean_absolute_error: 34.498552, mean_q: 45.847119, mean_eps: 0.100000
 1948410/2000000: episode: 8953, duration: 2.133s, episode steps: 136, steps per second: 64, episode reward: -115.236, mean reward: -0.847 [-100.000, 23.675], mean action: 1.471 [0.000, 3.000], mean observation: 0.051 [-3.594, 1.000], loss: 1.071614, mean_absolute_error: 34.134013, mean_q: 45.319716, mean_eps: 0.100000
 1948516/2000000: episode: 8954, duration: 1.704s, episode steps: 106, steps per second: 62, episode reward: -101.337, mean reward: -0.956 [-100.000, 9.839], mean action: 1.915 [0.000, 3.000], mean observation: -0.003 [-1.803, 1.000], loss: 1.680528, mean_absolute_error: 35.602042, mean_q: 46.306120, mean_eps: 0.100000
 1948628/2000000: episode: 8955, duration: 1.796s, episode steps: 112, steps per second: 62, episode reward: -82.286, mean reward: -0.735 [-100.000, 12.869], mean action: 1.571 [0.000, 3.000], mean observation: -0.056 [-1.287, 1.000], loss: 1.247577, mean_absolute_error: 34.085301, mean_q: 44.482276, mean_eps: 0.100000
 1948771/2000000: episode: 8956, duration: 2.272s, episode steps: 143, steps per second: 63, episode reward: -104.638, mean reward: -0.732 [-100.000, 8.505], mean action: 1.657 [0.000, 3.000], mean observation: -0.013 [-0.756, 1.326], loss: 1.519100, mean_absolute_error: 34.678563, mean_q: 45.058145, mean_eps: 0.100000
 1948888/2000000: episode: 8957, duration: 1.872s, episode steps: 117, steps per second: 62, episode reward: -124.127, mean reward: -1.061 [-100.000, 11.897], mean action: 2.068 [0.000, 3.000], mean observation: -0.003 [-0.891, 1.000], loss: 1.258051, mean_absolute_error: 34.183059, mean_q: 44.225311, mean_eps: 0.100000
 1949006/2000000: episode: 8958, duration: 1.877s, episode steps: 118, steps per second: 63, episode reward: -58.064, mean reward: -0.492 [-100.000, 18.881], mean action: 2.008 [0.000, 3.000], mean observation: 0.048 [-1.709, 1.000], loss: 1.503171, mean_absolute_error: 33.539598, mean_q: 43.639691, mean_eps: 0.100000
 1949121/2000000: episode: 8959, duration: 1.810s, episode steps: 115, steps per second: 64, episode reward: -120.911, mean reward: -1.051 [-100.000, 16.397], mean action: 1.470 [0.000, 3.000], mean observation: 0.013 [-3.011, 1.000], loss: 1.430534, mean_absolute_error: 36.390135, mean_q: 47.232692, mean_eps: 0.100000
 1949244/2000000: episode: 8960, duration: 1.936s, episode steps: 123, steps per second: 64, episode reward: -59.515, mean reward: -0.484 [-100.000, 55.342], mean action: 1.593 [0.000, 3.000], mean observation: 0.019 [-1.466, 1.000], loss: 1.203763, mean_absolute_error: 35.612574, mean_q: 47.487614, mean_eps: 0.100000
 1949339/2000000: episode: 8961, duration: 1.515s, episode steps: 95, steps per second: 63, episode reward: -73.917, mean reward: -0.778 [-100.000, 19.901], mean action: 2.011 [0.000, 3.000], mean observation: 0.023 [-2.992, 1.000], loss: 1.415957, mean_absolute_error: 35.252356, mean_q: 45.035224, mean_eps: 0.100000
 1949476/2000000: episode: 8962, duration: 2.181s, episode steps: 137, steps per second: 63, episode reward: -54.478, mean reward: -0.398 [-100.000, 41.004], mean action: 1.883 [0.000, 3.000], mean observation: 0.086 [-2.361, 1.000], loss: 1.495023, mean_absolute_error: 34.651074, mean_q: 45.174605, mean_eps: 0.100000
 1949588/2000000: episode: 8963, duration: 1.797s, episode steps: 112, steps per second: 62, episode reward: -235.133, mean reward: -2.099 [-100.000, 7.252], mean action: 1.893 [0.000, 3.000], mean observation: -0.076 [-1.264, 1.000], loss: 1.242399, mean_absolute_error: 34.946075, mean_q: 46.628021, mean_eps: 0.100000
 1949876/2000000: episode: 8964, duration: 4.644s, episode steps: 288, steps per second: 62, episode reward: 127.829, mean reward: 0.444 [-18.040, 100.000], mean action: 1.462 [0.000, 3.000], mean observation: 0.041 [-1.117, 1.140], loss: 1.454981, mean_absolute_error: 34.361163, mean_q: 45.232124, mean_eps: 0.100000
 1950792/2000000: episode: 8965, duration: 15.418s, episode steps: 916, steps per second: 59, episode reward: 154.119, mean reward: 0.168 [-24.169, 100.000], mean action: 1.179 [0.000, 3.000], mean observation: 0.082 [-1.088, 1.000], loss: 1.381968, mean_absolute_error: 35.361463, mean_q: 45.797140, mean_eps: 0.100000
 1951241/2000000: episode: 8966, duration: 7.232s, episode steps: 449, steps per second: 62, episode reward: 241.448, mean reward: 0.538 [-19.236, 100.000], mean action: 1.109 [0.000, 3.000], mean observation: 0.109 [-0.783, 1.000], loss: 1.224873, mean_absolute_error: 35.052438, mean_q: 45.500756, mean_eps: 0.100000
 1951342/2000000: episode: 8967, duration: 1.588s, episode steps: 101, steps per second: 64, episode reward: -92.245, mean reward: -0.913 [-100.000, 21.787], mean action: 1.772 [0.000, 3.000], mean observation: -0.055 [-1.755, 1.000], loss: 1.106245, mean_absolute_error: 35.265831, mean_q: 46.673061, mean_eps: 0.100000
 1951449/2000000: episode: 8968, duration: 1.683s, episode steps: 107, steps per second: 64, episode reward: -55.555, mean reward: -0.519 [-100.000, 14.845], mean action: 1.963 [0.000, 3.000], mean observation: 0.001 [-1.091, 1.000], loss: 0.934748, mean_absolute_error: 36.136491, mean_q: 47.731279, mean_eps: 0.100000
 1951546/2000000: episode: 8969, duration: 1.541s, episode steps: 97, steps per second: 63, episode reward: -66.796, mean reward: -0.689 [-100.000, 18.333], mean action: 1.887 [0.000, 3.000], mean observation: -0.004 [-1.650, 1.000], loss: 1.288910, mean_absolute_error: 34.727506, mean_q: 45.817332, mean_eps: 0.100000
 1951808/2000000: episode: 8970, duration: 4.229s, episode steps: 262, steps per second: 62, episode reward: 197.948, mean reward: 0.756 [-8.478, 100.000], mean action: 1.626 [0.000, 3.000], mean observation: -0.024 [-1.053, 1.000], loss: 1.291782, mean_absolute_error: 35.088209, mean_q: 46.262250, mean_eps: 0.100000
 1951920/2000000: episode: 8971, duration: 1.815s, episode steps: 112, steps per second: 62, episode reward: -103.623, mean reward: -0.925 [-100.000, 26.365], mean action: 1.491 [0.000, 3.000], mean observation: 0.037 [-2.830, 1.000], loss: 1.444253, mean_absolute_error: 34.490575, mean_q: 43.422261, mean_eps: 0.100000
 1952107/2000000: episode: 8972, duration: 2.926s, episode steps: 187, steps per second: 64, episode reward: -191.627, mean reward: -1.025 [-100.000, 23.240], mean action: 1.422 [0.000, 3.000], mean observation: -0.070 [-1.004, 1.000], loss: 1.299307, mean_absolute_error: 35.017450, mean_q: 45.330461, mean_eps: 0.100000
 1952258/2000000: episode: 8973, duration: 2.387s, episode steps: 151, steps per second: 63, episode reward: -141.040, mean reward: -0.934 [-100.000, 11.901], mean action: 2.046 [0.000, 3.000], mean observation: -0.000 [-0.804, 1.000], loss: 1.135838, mean_absolute_error: 33.777181, mean_q: 44.555386, mean_eps: 0.100000
 1952366/2000000: episode: 8974, duration: 1.689s, episode steps: 108, steps per second: 64, episode reward: -84.805, mean reward: -0.785 [-100.000, 9.018], mean action: 1.981 [0.000, 3.000], mean observation: -0.077 [-0.830, 1.000], loss: 1.447326, mean_absolute_error: 35.814244, mean_q: 45.829476, mean_eps: 0.100000
 1952473/2000000: episode: 8975, duration: 1.667s, episode steps: 107, steps per second: 64, episode reward: -98.402, mean reward: -0.920 [-100.000, 13.814], mean action: 1.383 [0.000, 3.000], mean observation: -0.064 [-1.069, 1.000], loss: 1.051129, mean_absolute_error: 33.952001, mean_q: 45.283226, mean_eps: 0.100000
 1952622/2000000: episode: 8976, duration: 2.343s, episode steps: 149, steps per second: 64, episode reward: -54.888, mean reward: -0.368 [-100.000, 15.307], mean action: 1.718 [0.000, 3.000], mean observation: -0.090 [-0.825, 1.617], loss: 1.368659, mean_absolute_error: 35.910798, mean_q: 46.804288, mean_eps: 0.100000
 1952740/2000000: episode: 8977, duration: 1.877s, episode steps: 118, steps per second: 63, episode reward: -174.548, mean reward: -1.479 [-100.000, 10.077], mean action: 1.305 [0.000, 3.000], mean observation: 0.100 [-1.170, 1.000], loss: 1.548504, mean_absolute_error: 35.179804, mean_q: 46.274599, mean_eps: 0.100000
 1952854/2000000: episode: 8978, duration: 1.798s, episode steps: 114, steps per second: 63, episode reward: -34.750, mean reward: -0.305 [-100.000, 20.659], mean action: 1.351 [0.000, 3.000], mean observation: 0.050 [-1.924, 1.000], loss: 1.383176, mean_absolute_error: 35.529618, mean_q: 46.431711, mean_eps: 0.100000
 1952980/2000000: episode: 8979, duration: 1.997s, episode steps: 126, steps per second: 63, episode reward: -117.227, mean reward: -0.930 [-100.000, 15.279], mean action: 1.952 [0.000, 3.000], mean observation: -0.036 [-3.917, 1.003], loss: 0.942042, mean_absolute_error: 35.584968, mean_q: 46.411191, mean_eps: 0.100000
 1953082/2000000: episode: 8980, duration: 1.640s, episode steps: 102, steps per second: 62, episode reward: -11.278, mean reward: -0.111 [-100.000, 32.574], mean action: 1.686 [0.000, 3.000], mean observation: 0.077 [-1.690, 1.000], loss: 1.026876, mean_absolute_error: 35.463594, mean_q: 46.703511, mean_eps: 0.100000
 1953207/2000000: episode: 8981, duration: 1.948s, episode steps: 125, steps per second: 64, episode reward: -121.128, mean reward: -0.969 [-100.000, 13.308], mean action: 1.560 [0.000, 3.000], mean observation: 0.011 [-0.936, 1.000], loss: 1.285613, mean_absolute_error: 35.168642, mean_q: 45.572776, mean_eps: 0.100000
 1953317/2000000: episode: 8982, duration: 1.759s, episode steps: 110, steps per second: 63, episode reward: -97.383, mean reward: -0.885 [-100.000, 17.883], mean action: 1.973 [0.000, 3.000], mean observation: -0.038 [-1.506, 1.000], loss: 1.212451, mean_absolute_error: 35.989064, mean_q: 47.832405, mean_eps: 0.100000
 1953430/2000000: episode: 8983, duration: 1.770s, episode steps: 113, steps per second: 64, episode reward: -54.443, mean reward: -0.482 [-100.000, 19.074], mean action: 1.752 [0.000, 3.000], mean observation: 0.071 [-1.842, 1.000], loss: 1.533460, mean_absolute_error: 32.902363, mean_q: 42.392574, mean_eps: 0.100000
 1953685/2000000: episode: 8984, duration: 4.052s, episode steps: 255, steps per second: 63, episode reward: 187.027, mean reward: 0.733 [-17.536, 100.000], mean action: 1.145 [0.000, 3.000], mean observation: 0.074 [-0.910, 1.000], loss: 1.684254, mean_absolute_error: 34.414524, mean_q: 45.008539, mean_eps: 0.100000
 1953821/2000000: episode: 8985, duration: 2.126s, episode steps: 136, steps per second: 64, episode reward: -344.317, mean reward: -2.532 [-100.000, 28.401], mean action: 1.412 [0.000, 3.000], mean observation: 0.188 [-1.416, 3.675], loss: 1.048749, mean_absolute_error: 33.711216, mean_q: 44.875001, mean_eps: 0.100000
 1954132/2000000: episode: 8986, duration: 4.969s, episode steps: 311, steps per second: 63, episode reward: 161.855, mean reward: 0.520 [-17.833, 100.000], mean action: 1.296 [0.000, 3.000], mean observation: 0.067 [-0.894, 1.000], loss: 0.925590, mean_absolute_error: 36.003391, mean_q: 46.256310, mean_eps: 0.100000
 1954269/2000000: episode: 8987, duration: 2.185s, episode steps: 137, steps per second: 63, episode reward: -204.817, mean reward: -1.495 [-100.000, 9.662], mean action: 1.920 [0.000, 3.000], mean observation: -0.066 [-1.274, 1.000], loss: 1.824743, mean_absolute_error: 34.592730, mean_q: 45.096771, mean_eps: 0.100000
 1954403/2000000: episode: 8988, duration: 2.054s, episode steps: 134, steps per second: 65, episode reward: -59.340, mean reward: -0.443 [-100.000, 18.138], mean action: 1.366 [0.000, 3.000], mean observation: 0.050 [-1.886, 1.000], loss: 1.567546, mean_absolute_error: 37.138359, mean_q: 48.195267, mean_eps: 0.100000
 1954509/2000000: episode: 8989, duration: 1.718s, episode steps: 106, steps per second: 62, episode reward: -233.406, mean reward: -2.202 [-100.000, 24.346], mean action: 1.792 [0.000, 3.000], mean observation: 0.047 [-1.076, 3.974], loss: 1.072078, mean_absolute_error: 35.365843, mean_q: 44.293315, mean_eps: 0.100000
 1954726/2000000: episode: 8990, duration: 3.379s, episode steps: 217, steps per second: 64, episode reward: 168.154, mean reward: 0.775 [-18.714, 100.000], mean action: 1.931 [0.000, 3.000], mean observation: 0.057 [-0.890, 1.000], loss: 1.468541, mean_absolute_error: 36.493548, mean_q: 47.932953, mean_eps: 0.100000
 1954864/2000000: episode: 8991, duration: 2.234s, episode steps: 138, steps per second: 62, episode reward: 12.682, mean reward: 0.092 [-100.000, 21.875], mean action: 1.732 [0.000, 3.000], mean observation: -0.046 [-1.460, 1.000], loss: 1.520760, mean_absolute_error: 33.783899, mean_q: 43.506170, mean_eps: 0.100000
 1954970/2000000: episode: 8992, duration: 1.921s, episode steps: 106, steps per second: 55, episode reward: -15.605, mean reward: -0.147 [-100.000, 10.586], mean action: 1.557 [0.000, 3.000], mean observation: 0.036 [-1.624, 1.000], loss: 1.225680, mean_absolute_error: 36.193498, mean_q: 46.568817, mean_eps: 0.100000
 1955085/2000000: episode: 8993, duration: 1.828s, episode steps: 115, steps per second: 63, episode reward: -60.402, mean reward: -0.525 [-100.000, 18.084], mean action: 1.661 [0.000, 3.000], mean observation: -0.004 [-1.691, 1.000], loss: 1.544071, mean_absolute_error: 35.818114, mean_q: 45.815242, mean_eps: 0.100000
 1955201/2000000: episode: 8994, duration: 1.783s, episode steps: 116, steps per second: 65, episode reward: -29.659, mean reward: -0.256 [-100.000, 19.551], mean action: 1.466 [0.000, 3.000], mean observation: -0.032 [-1.654, 1.000], loss: 1.075776, mean_absolute_error: 35.944408, mean_q: 46.895577, mean_eps: 0.100000
 1955343/2000000: episode: 8995, duration: 2.186s, episode steps: 142, steps per second: 65, episode reward: -130.742, mean reward: -0.921 [-100.000, 13.209], mean action: 1.542 [0.000, 3.000], mean observation: 0.059 [-3.494, 1.006], loss: 1.461864, mean_absolute_error: 35.063137, mean_q: 46.033700, mean_eps: 0.100000
 1955439/2000000: episode: 8996, duration: 1.659s, episode steps: 96, steps per second: 58, episode reward: -59.592, mean reward: -0.621 [-100.000, 10.135], mean action: 1.990 [0.000, 3.000], mean observation: -0.000 [-1.841, 1.000], loss: 1.393200, mean_absolute_error: 34.647667, mean_q: 45.167760, mean_eps: 0.100000
 1955607/2000000: episode: 8997, duration: 2.677s, episode steps: 168, steps per second: 63, episode reward: -172.325, mean reward: -1.026 [-100.000, 13.527], mean action: 1.845 [0.000, 3.000], mean observation: -0.081 [-0.963, 1.321], loss: 1.154298, mean_absolute_error: 35.309368, mean_q: 45.827923, mean_eps: 0.100000
 1955710/2000000: episode: 8998, duration: 1.618s, episode steps: 103, steps per second: 64, episode reward: -93.053, mean reward: -0.903 [-100.000, 19.472], mean action: 1.573 [0.000, 3.000], mean observation: -0.010 [-1.680, 1.000], loss: 1.615427, mean_absolute_error: 33.384909, mean_q: 43.782329, mean_eps: 0.100000
 1955848/2000000: episode: 8999, duration: 2.152s, episode steps: 138, steps per second: 64, episode reward: -92.608, mean reward: -0.671 [-100.000, 27.523], mean action: 1.399 [0.000, 3.000], mean observation: -0.039 [-1.530, 1.000], loss: 1.393456, mean_absolute_error: 34.989719, mean_q: 46.281270, mean_eps: 0.100000
 1955980/2000000: episode: 9000, duration: 2.122s, episode steps: 132, steps per second: 62, episode reward: -28.163, mean reward: -0.213 [-100.000, 43.936], mean action: 1.712 [0.000, 3.000], mean observation: -0.039 [-1.111, 1.469], loss: 1.158075, mean_absolute_error: 33.827334, mean_q: 43.753593, mean_eps: 0.100000
 1956116/2000000: episode: 9001, duration: 2.207s, episode steps: 136, steps per second: 62, episode reward: -54.442, mean reward: -0.400 [-100.000, 16.357], mean action: 1.801 [0.000, 3.000], mean observation: 0.071 [-1.424, 1.000], loss: 1.366119, mean_absolute_error: 34.414030, mean_q: 44.734376, mean_eps: 0.100000
 1956225/2000000: episode: 9002, duration: 1.733s, episode steps: 109, steps per second: 63, episode reward: -252.799, mean reward: -2.319 [-100.000, 4.330], mean action: 1.211 [0.000, 3.000], mean observation: 0.082 [-1.411, 3.927], loss: 1.534775, mean_absolute_error: 36.521509, mean_q: 48.133775, mean_eps: 0.100000
 1956342/2000000: episode: 9003, duration: 1.799s, episode steps: 117, steps per second: 65, episode reward: -74.911, mean reward: -0.640 [-100.000, 25.919], mean action: 1.547 [0.000, 3.000], mean observation: 0.042 [-2.107, 1.000], loss: 1.358319, mean_absolute_error: 34.459499, mean_q: 45.073312, mean_eps: 0.100000
 1956942/2000000: episode: 9004, duration: 9.525s, episode steps: 600, steps per second: 63, episode reward: 110.839, mean reward: 0.185 [-18.196, 100.000], mean action: 1.197 [0.000, 3.000], mean observation: 0.030 [-0.934, 1.011], loss: 1.509750, mean_absolute_error: 34.354909, mean_q: 44.896804, mean_eps: 0.100000
 1957942/2000000: episode: 9005, duration: 16.354s, episode steps: 1000, steps per second: 61, episode reward: 98.937, mean reward: 0.099 [-18.239, 21.758], mean action: 0.965 [0.000, 3.000], mean observation: 0.157 [-1.628, 1.000], loss: 1.491500, mean_absolute_error: 35.002178, mean_q: 45.779417, mean_eps: 0.100000
 1958035/2000000: episode: 9006, duration: 1.451s, episode steps: 93, steps per second: 64, episode reward: -33.076, mean reward: -0.356 [-100.000, 22.462], mean action: 1.968 [0.000, 3.000], mean observation: 0.036 [-2.128, 1.000], loss: 1.298109, mean_absolute_error: 34.818162, mean_q: 45.339682, mean_eps: 0.100000
 1958149/2000000: episode: 9007, duration: 1.800s, episode steps: 114, steps per second: 63, episode reward: -165.132, mean reward: -1.449 [-100.000, 15.046], mean action: 1.623 [0.000, 3.000], mean observation: -0.027 [-1.313, 1.000], loss: 1.298422, mean_absolute_error: 35.497825, mean_q: 46.992316, mean_eps: 0.100000
 1958565/2000000: episode: 9008, duration: 6.574s, episode steps: 416, steps per second: 63, episode reward: 181.000, mean reward: 0.435 [-8.264, 100.000], mean action: 1.079 [0.000, 3.000], mean observation: 0.110 [-1.247, 1.000], loss: 1.455351, mean_absolute_error: 34.257791, mean_q: 45.227707, mean_eps: 0.100000
 1959078/2000000: episode: 9009, duration: 8.268s, episode steps: 513, steps per second: 62, episode reward: 160.790, mean reward: 0.313 [-19.497, 100.000], mean action: 0.889 [0.000, 3.000], mean observation: 0.043 [-0.902, 1.000], loss: 1.320373, mean_absolute_error: 34.402203, mean_q: 44.765363, mean_eps: 0.100000
 1959224/2000000: episode: 9010, duration: 2.308s, episode steps: 146, steps per second: 63, episode reward: -79.160, mean reward: -0.542 [-100.000, 33.412], mean action: 1.801 [0.000, 3.000], mean observation: 0.096 [-1.508, 1.000], loss: 1.151993, mean_absolute_error: 34.778149, mean_q: 45.955265, mean_eps: 0.100000
 1959348/2000000: episode: 9011, duration: 1.972s, episode steps: 124, steps per second: 63, episode reward: -124.807, mean reward: -1.007 [-100.000, 49.670], mean action: 1.589 [0.000, 3.000], mean observation: -0.011 [-1.852, 1.000], loss: 1.660049, mean_absolute_error: 36.938000, mean_q: 47.619602, mean_eps: 0.100000
 1959457/2000000: episode: 9012, duration: 1.746s, episode steps: 109, steps per second: 62, episode reward: -36.582, mean reward: -0.336 [-100.000, 17.716], mean action: 1.440 [0.000, 3.000], mean observation: -0.018 [-1.301, 1.000], loss: 1.194991, mean_absolute_error: 35.420994, mean_q: 46.066268, mean_eps: 0.100000
 1960457/2000000: episode: 9013, duration: 16.401s, episode steps: 1000, steps per second: 61, episode reward: 92.704, mean reward: 0.093 [-20.361, 24.323], mean action: 1.019 [0.000, 3.000], mean observation: 0.227 [-1.188, 1.000], loss: 1.417018, mean_absolute_error: 34.379367, mean_q: 44.595355, mean_eps: 0.100000
 1960619/2000000: episode: 9014, duration: 2.536s, episode steps: 162, steps per second: 64, episode reward: -85.017, mean reward: -0.525 [-100.000, 7.766], mean action: 1.537 [0.000, 3.000], mean observation: 0.028 [-0.817, 1.445], loss: 1.117650, mean_absolute_error: 34.259059, mean_q: 45.094027, mean_eps: 0.100000
 1960750/2000000: episode: 9015, duration: 2.046s, episode steps: 131, steps per second: 64, episode reward: -115.487, mean reward: -0.882 [-100.000, 46.450], mean action: 1.489 [0.000, 3.000], mean observation: 0.103 [-2.584, 1.000], loss: 1.355827, mean_absolute_error: 35.677912, mean_q: 46.466133, mean_eps: 0.100000
 1960844/2000000: episode: 9016, duration: 1.503s, episode steps: 94, steps per second: 63, episode reward: -27.279, mean reward: -0.290 [-100.000, 17.005], mean action: 1.553 [0.000, 3.000], mean observation: 0.090 [-1.468, 1.000], loss: 0.779327, mean_absolute_error: 35.488826, mean_q: 45.745924, mean_eps: 0.100000
 1961202/2000000: episode: 9017, duration: 5.738s, episode steps: 358, steps per second: 62, episode reward: 236.189, mean reward: 0.660 [-17.788, 100.000], mean action: 0.897 [0.000, 3.000], mean observation: 0.113 [-1.117, 1.000], loss: 1.169880, mean_absolute_error: 34.078677, mean_q: 44.746834, mean_eps: 0.100000
 1961493/2000000: episode: 9018, duration: 4.673s, episode steps: 291, steps per second: 62, episode reward: 147.130, mean reward: 0.506 [-11.035, 100.000], mean action: 1.179 [0.000, 3.000], mean observation: -0.023 [-0.853, 1.000], loss: 1.127701, mean_absolute_error: 33.489801, mean_q: 44.402200, mean_eps: 0.100000
 1961621/2000000: episode: 9019, duration: 2.011s, episode steps: 128, steps per second: 64, episode reward: -32.214, mean reward: -0.252 [-100.000, 13.968], mean action: 1.930 [0.000, 3.000], mean observation: 0.037 [-0.737, 1.404], loss: 1.321199, mean_absolute_error: 35.751538, mean_q: 45.545233, mean_eps: 0.100000
 1961718/2000000: episode: 9020, duration: 1.501s, episode steps: 97, steps per second: 65, episode reward: -114.358, mean reward: -1.179 [-100.000, 9.125], mean action: 1.454 [0.000, 3.000], mean observation: 0.003 [-2.886, 1.000], loss: 1.774329, mean_absolute_error: 34.298330, mean_q: 43.466641, mean_eps: 0.100000
 1961826/2000000: episode: 9021, duration: 1.696s, episode steps: 108, steps per second: 64, episode reward: -82.778, mean reward: -0.766 [-100.000, 12.863], mean action: 1.463 [0.000, 3.000], mean observation: 0.047 [-2.670, 1.000], loss: 1.619157, mean_absolute_error: 35.255394, mean_q: 45.236177, mean_eps: 0.100000
 1961958/2000000: episode: 9022, duration: 2.059s, episode steps: 132, steps per second: 64, episode reward: -80.823, mean reward: -0.612 [-100.000, 76.405], mean action: 1.576 [0.000, 3.000], mean observation: 0.003 [-0.953, 1.000], loss: 1.021213, mean_absolute_error: 34.638740, mean_q: 46.039970, mean_eps: 0.100000
 1962072/2000000: episode: 9023, duration: 1.816s, episode steps: 114, steps per second: 63, episode reward: -145.324, mean reward: -1.275 [-100.000, 11.887], mean action: 1.816 [0.000, 3.000], mean observation: -0.011 [-1.749, 1.000], loss: 1.072249, mean_absolute_error: 35.296766, mean_q: 46.735075, mean_eps: 0.100000
 1962574/2000000: episode: 9024, duration: 8.221s, episode steps: 502, steps per second: 61, episode reward: 184.221, mean reward: 0.367 [-17.798, 100.000], mean action: 1.048 [0.000, 3.000], mean observation: 0.017 [-1.766, 1.012], loss: 1.418646, mean_absolute_error: 35.182838, mean_q: 46.474681, mean_eps: 0.100000
 1962672/2000000: episode: 9025, duration: 1.551s, episode steps: 98, steps per second: 63, episode reward: -55.948, mean reward: -0.571 [-100.000, 11.309], mean action: 1.806 [0.000, 3.000], mean observation: -0.123 [-1.191, 1.000], loss: 1.075288, mean_absolute_error: 34.868847, mean_q: 45.473524, mean_eps: 0.100000
 1963005/2000000: episode: 9026, duration: 5.275s, episode steps: 333, steps per second: 63, episode reward: 245.882, mean reward: 0.738 [-19.066, 100.000], mean action: 1.009 [0.000, 3.000], mean observation: 0.079 [-0.988, 1.000], loss: 1.037905, mean_absolute_error: 34.374775, mean_q: 45.030949, mean_eps: 0.100000
 1963120/2000000: episode: 9027, duration: 1.812s, episode steps: 115, steps per second: 63, episode reward: -142.085, mean reward: -1.236 [-100.000, 15.860], mean action: 1.478 [0.000, 3.000], mean observation: -0.043 [-1.376, 1.000], loss: 1.533197, mean_absolute_error: 35.121204, mean_q: 46.642806, mean_eps: 0.100000
 1963508/2000000: episode: 9028, duration: 6.271s, episode steps: 388, steps per second: 62, episode reward: 189.000, mean reward: 0.487 [-18.404, 100.000], mean action: 1.219 [0.000, 3.000], mean observation: 0.096 [-0.678, 1.000], loss: 1.542060, mean_absolute_error: 34.538260, mean_q: 45.165842, mean_eps: 0.100000
 1964012/2000000: episode: 9029, duration: 8.464s, episode steps: 504, steps per second: 60, episode reward: 123.368, mean reward: 0.245 [-20.617, 100.000], mean action: 2.333 [0.000, 3.000], mean observation: 0.078 [-0.846, 1.000], loss: 1.316982, mean_absolute_error: 33.401942, mean_q: 44.524225, mean_eps: 0.100000
 1964609/2000000: episode: 9030, duration: 9.652s, episode steps: 597, steps per second: 62, episode reward: 142.115, mean reward: 0.238 [-19.630, 100.000], mean action: 1.303 [0.000, 3.000], mean observation: 0.061 [-0.847, 1.000], loss: 1.380024, mean_absolute_error: 34.235825, mean_q: 44.802135, mean_eps: 0.100000
 1964707/2000000: episode: 9031, duration: 1.562s, episode steps: 98, steps per second: 63, episode reward: -40.690, mean reward: -0.415 [-100.000, 21.554], mean action: 1.704 [0.000, 3.000], mean observation: 0.016 [-1.867, 1.000], loss: 1.508004, mean_absolute_error: 34.839433, mean_q: 44.985207, mean_eps: 0.100000
 1964830/2000000: episode: 9032, duration: 1.940s, episode steps: 123, steps per second: 63, episode reward: -96.633, mean reward: -0.786 [-100.000, 22.809], mean action: 1.602 [0.000, 3.000], mean observation: 0.119 [-2.723, 1.000], loss: 1.297117, mean_absolute_error: 35.393402, mean_q: 46.378051, mean_eps: 0.100000
 1964969/2000000: episode: 9033, duration: 2.179s, episode steps: 139, steps per second: 64, episode reward: -224.940, mean reward: -1.618 [-100.000, 11.432], mean action: 1.540 [0.000, 3.000], mean observation: 0.155 [-1.503, 4.658], loss: 1.264770, mean_absolute_error: 34.923000, mean_q: 44.761655, mean_eps: 0.100000
 1965212/2000000: episode: 9034, duration: 3.811s, episode steps: 243, steps per second: 64, episode reward: -107.118, mean reward: -0.441 [-100.000, 14.038], mean action: 1.654 [0.000, 3.000], mean observation: -0.093 [-0.889, 1.022], loss: 1.670866, mean_absolute_error: 34.831350, mean_q: 45.275783, mean_eps: 0.100000
 1965421/2000000: episode: 9035, duration: 3.339s, episode steps: 209, steps per second: 63, episode reward: 185.170, mean reward: 0.886 [-5.939, 100.000], mean action: 1.536 [0.000, 3.000], mean observation: 0.075 [-1.627, 1.000], loss: 0.970601, mean_absolute_error: 34.847535, mean_q: 45.614056, mean_eps: 0.100000
 1965648/2000000: episode: 9036, duration: 4.018s, episode steps: 227, steps per second: 56, episode reward: -10.046, mean reward: -0.044 [-100.000, 12.968], mean action: 1.665 [0.000, 3.000], mean observation: -0.049 [-0.926, 1.000], loss: 1.202705, mean_absolute_error: 34.123656, mean_q: 44.568720, mean_eps: 0.100000
 1965804/2000000: episode: 9037, duration: 2.537s, episode steps: 156, steps per second: 61, episode reward: -82.294, mean reward: -0.528 [-100.000, 9.111], mean action: 1.891 [0.000, 3.000], mean observation: 0.028 [-0.719, 1.469], loss: 1.302742, mean_absolute_error: 35.522852, mean_q: 45.357928, mean_eps: 0.100000
 1966091/2000000: episode: 9038, duration: 4.629s, episode steps: 287, steps per second: 62, episode reward: 190.775, mean reward: 0.665 [-19.389, 100.000], mean action: 1.279 [0.000, 3.000], mean observation: 0.039 [-0.796, 1.000], loss: 1.493238, mean_absolute_error: 35.146471, mean_q: 46.230942, mean_eps: 0.100000
 1966609/2000000: episode: 9039, duration: 8.544s, episode steps: 518, steps per second: 61, episode reward: 156.875, mean reward: 0.303 [-11.779, 100.000], mean action: 1.996 [0.000, 3.000], mean observation: -0.000 [-0.882, 1.000], loss: 1.295150, mean_absolute_error: 34.398401, mean_q: 45.176236, mean_eps: 0.100000
 1966726/2000000: episode: 9040, duration: 1.838s, episode steps: 117, steps per second: 64, episode reward: -111.137, mean reward: -0.950 [-100.000, 20.227], mean action: 1.590 [0.000, 3.000], mean observation: 0.088 [-2.554, 1.000], loss: 1.067716, mean_absolute_error: 34.846973, mean_q: 45.180664, mean_eps: 0.100000
 1967070/2000000: episode: 9041, duration: 5.479s, episode steps: 344, steps per second: 63, episode reward: 194.974, mean reward: 0.567 [-18.695, 100.000], mean action: 2.430 [0.000, 3.000], mean observation: 0.120 [-0.780, 1.000], loss: 1.408538, mean_absolute_error: 34.668290, mean_q: 45.372560, mean_eps: 0.100000
 1967178/2000000: episode: 9042, duration: 1.706s, episode steps: 108, steps per second: 63, episode reward: -49.271, mean reward: -0.456 [-100.000, 17.982], mean action: 1.630 [0.000, 3.000], mean observation: 0.033 [-1.493, 1.000], loss: 1.288606, mean_absolute_error: 35.724954, mean_q: 47.181166, mean_eps: 0.100000
 1967271/2000000: episode: 9043, duration: 1.447s, episode steps: 93, steps per second: 64, episode reward: -92.603, mean reward: -0.996 [-100.000, 14.914], mean action: 1.968 [0.000, 3.000], mean observation: -0.103 [-1.005, 1.000], loss: 0.905401, mean_absolute_error: 34.203101, mean_q: 44.992637, mean_eps: 0.100000
 1967380/2000000: episode: 9044, duration: 1.746s, episode steps: 109, steps per second: 62, episode reward: -146.521, mean reward: -1.344 [-100.000, 11.459], mean action: 1.826 [0.000, 3.000], mean observation: -0.013 [-0.934, 1.163], loss: 0.850631, mean_absolute_error: 34.692971, mean_q: 44.952235, mean_eps: 0.100000
 1967507/2000000: episode: 9045, duration: 2.005s, episode steps: 127, steps per second: 63, episode reward: 9.485, mean reward: 0.075 [-100.000, 9.638], mean action: 1.677 [0.000, 3.000], mean observation: -0.078 [-0.928, 1.000], loss: 1.012517, mean_absolute_error: 34.104600, mean_q: 45.291207, mean_eps: 0.100000
 1967614/2000000: episode: 9046, duration: 1.699s, episode steps: 107, steps per second: 63, episode reward: -98.429, mean reward: -0.920 [-100.000, 18.760], mean action: 1.402 [0.000, 3.000], mean observation: -0.016 [-1.717, 1.000], loss: 1.796871, mean_absolute_error: 34.948297, mean_q: 45.740159, mean_eps: 0.100000
 1967757/2000000: episode: 9047, duration: 2.230s, episode steps: 143, steps per second: 64, episode reward: -42.950, mean reward: -0.300 [-100.000, 14.467], mean action: 1.832 [0.000, 3.000], mean observation: 0.043 [-0.816, 1.000], loss: 1.180565, mean_absolute_error: 33.775060, mean_q: 44.784445, mean_eps: 0.100000
 1968116/2000000: episode: 9048, duration: 5.750s, episode steps: 359, steps per second: 62, episode reward: 183.084, mean reward: 0.510 [-19.611, 100.000], mean action: 1.281 [0.000, 3.000], mean observation: 0.040 [-0.878, 1.065], loss: 1.390973, mean_absolute_error: 34.327363, mean_q: 44.751705, mean_eps: 0.100000
 1968259/2000000: episode: 9049, duration: 2.223s, episode steps: 143, steps per second: 64, episode reward: -100.085, mean reward: -0.700 [-100.000, 16.046], mean action: 1.238 [0.000, 3.000], mean observation: 0.055 [-1.625, 1.000], loss: 1.366769, mean_absolute_error: 34.463974, mean_q: 45.014890, mean_eps: 0.100000
 1968336/2000000: episode: 9050, duration: 1.379s, episode steps: 77, steps per second: 56, episode reward: -64.006, mean reward: -0.831 [-100.000, 14.729], mean action: 1.584 [0.000, 3.000], mean observation: -0.157 [-1.304, 1.000], loss: 1.073241, mean_absolute_error: 35.703661, mean_q: 46.953476, mean_eps: 0.100000
 1968477/2000000: episode: 9051, duration: 2.544s, episode steps: 141, steps per second: 55, episode reward: -152.875, mean reward: -1.084 [-100.000, 10.044], mean action: 1.929 [0.000, 3.000], mean observation: -0.091 [-2.138, 1.000], loss: 1.173094, mean_absolute_error: 34.197814, mean_q: 42.952783, mean_eps: 0.100000
 1968648/2000000: episode: 9052, duration: 2.650s, episode steps: 171, steps per second: 65, episode reward: -67.228, mean reward: -0.393 [-100.000, 8.052], mean action: 1.696 [0.000, 3.000], mean observation: -0.018 [-0.776, 1.447], loss: 1.445798, mean_absolute_error: 35.060378, mean_q: 45.807254, mean_eps: 0.100000
 1968917/2000000: episode: 9053, duration: 4.364s, episode steps: 269, steps per second: 62, episode reward: 176.965, mean reward: 0.658 [-10.276, 100.000], mean action: 1.465 [0.000, 3.000], mean observation: 0.027 [-1.120, 1.071], loss: 1.602927, mean_absolute_error: 33.511388, mean_q: 43.871991, mean_eps: 0.100000
 1969054/2000000: episode: 9054, duration: 2.138s, episode steps: 137, steps per second: 64, episode reward: 8.233, mean reward: 0.060 [-100.000, 18.627], mean action: 1.635 [0.000, 3.000], mean observation: -0.027 [-1.439, 1.000], loss: 1.209636, mean_absolute_error: 34.480086, mean_q: 45.141751, mean_eps: 0.100000
 1969396/2000000: episode: 9055, duration: 5.521s, episode steps: 342, steps per second: 62, episode reward: 211.653, mean reward: 0.619 [-19.361, 100.000], mean action: 1.129 [0.000, 3.000], mean observation: 0.110 [-1.525, 1.000], loss: 1.308647, mean_absolute_error: 34.599162, mean_q: 45.651935, mean_eps: 0.100000
 1969551/2000000: episode: 9056, duration: 2.422s, episode steps: 155, steps per second: 64, episode reward: -80.571, mean reward: -0.520 [-100.000, 12.763], mean action: 1.665 [0.000, 3.000], mean observation: 0.080 [-1.515, 1.000], loss: 0.963541, mean_absolute_error: 34.855924, mean_q: 46.082999, mean_eps: 0.100000
 1969769/2000000: episode: 9057, duration: 3.621s, episode steps: 218, steps per second: 60, episode reward: -140.777, mean reward: -0.646 [-100.000, 4.766], mean action: 1.647 [0.000, 3.000], mean observation: -0.050 [-1.003, 1.011], loss: 1.302672, mean_absolute_error: 33.992603, mean_q: 44.821896, mean_eps: 0.100000
 1969997/2000000: episode: 9058, duration: 3.760s, episode steps: 228, steps per second: 61, episode reward: 191.209, mean reward: 0.839 [-17.285, 100.000], mean action: 2.386 [0.000, 3.000], mean observation: 0.121 [-1.113, 1.000], loss: 1.407486, mean_absolute_error: 34.826036, mean_q: 45.314074, mean_eps: 0.100000
 1970088/2000000: episode: 9059, duration: 1.454s, episode steps: 91, steps per second: 63, episode reward: -3.381, mean reward: -0.037 [-100.000, 19.146], mean action: 1.835 [0.000, 3.000], mean observation: 0.067 [-1.988, 1.000], loss: 1.131659, mean_absolute_error: 34.533573, mean_q: 45.619314, mean_eps: 0.100000
 1970282/2000000: episode: 9060, duration: 3.127s, episode steps: 194, steps per second: 62, episode reward: 30.833, mean reward: 0.159 [-100.000, 19.372], mean action: 1.892 [0.000, 3.000], mean observation: 0.004 [-0.878, 1.000], loss: 1.480884, mean_absolute_error: 34.571008, mean_q: 45.235314, mean_eps: 0.100000
 1970407/2000000: episode: 9061, duration: 2.078s, episode steps: 125, steps per second: 60, episode reward: -113.541, mean reward: -0.908 [-100.000, 14.957], mean action: 1.928 [0.000, 3.000], mean observation: -0.013 [-0.803, 1.000], loss: 0.884914, mean_absolute_error: 35.299248, mean_q: 46.564057, mean_eps: 0.100000
 1970780/2000000: episode: 9062, duration: 5.954s, episode steps: 373, steps per second: 63, episode reward: 187.908, mean reward: 0.504 [-17.373, 100.000], mean action: 1.630 [0.000, 3.000], mean observation: 0.100 [-0.776, 1.000], loss: 1.291619, mean_absolute_error: 35.397024, mean_q: 46.266871, mean_eps: 0.100000
 1970908/2000000: episode: 9063, duration: 2.053s, episode steps: 128, steps per second: 62, episode reward: -120.689, mean reward: -0.943 [-100.000, 10.961], mean action: 1.508 [0.000, 3.000], mean observation: 0.011 [-1.694, 1.000], loss: 1.956037, mean_absolute_error: 35.308363, mean_q: 45.222041, mean_eps: 0.100000
 1971017/2000000: episode: 9064, duration: 1.744s, episode steps: 109, steps per second: 62, episode reward: -135.600, mean reward: -1.244 [-100.000, 14.706], mean action: 1.532 [0.000, 3.000], mean observation: 0.041 [-2.490, 1.000], loss: 1.123989, mean_absolute_error: 33.112345, mean_q: 44.317029, mean_eps: 0.100000
 1971129/2000000: episode: 9065, duration: 1.731s, episode steps: 112, steps per second: 65, episode reward: -46.607, mean reward: -0.416 [-100.000, 17.689], mean action: 1.714 [0.000, 3.000], mean observation: 0.038 [-1.749, 1.000], loss: 1.055515, mean_absolute_error: 33.551568, mean_q: 44.098160, mean_eps: 0.100000
 1971265/2000000: episode: 9066, duration: 2.099s, episode steps: 136, steps per second: 65, episode reward: -142.473, mean reward: -1.048 [-100.000, 8.256], mean action: 1.897 [0.000, 3.000], mean observation: -0.031 [-1.700, 1.000], loss: 1.413920, mean_absolute_error: 34.448430, mean_q: 45.095732, mean_eps: 0.100000
 1971375/2000000: episode: 9067, duration: 1.677s, episode steps: 110, steps per second: 66, episode reward: -67.100, mean reward: -0.610 [-100.000, 21.322], mean action: 1.682 [0.000, 3.000], mean observation: 0.010 [-2.368, 1.000], loss: 1.027163, mean_absolute_error: 35.853544, mean_q: 46.932220, mean_eps: 0.100000
 1971598/2000000: episode: 9068, duration: 3.533s, episode steps: 223, steps per second: 63, episode reward: 205.008, mean reward: 0.919 [-18.324, 100.000], mean action: 1.247 [0.000, 3.000], mean observation: 0.092 [-1.377, 1.000], loss: 1.158716, mean_absolute_error: 34.773874, mean_q: 45.893368, mean_eps: 0.100000
 1971686/2000000: episode: 9069, duration: 1.374s, episode steps: 88, steps per second: 64, episode reward: -63.328, mean reward: -0.720 [-100.000, 22.696], mean action: 1.807 [0.000, 3.000], mean observation: -0.068 [-2.614, 1.000], loss: 1.070832, mean_absolute_error: 33.483723, mean_q: 43.768865, mean_eps: 0.100000
 1971784/2000000: episode: 9070, duration: 1.556s, episode steps: 98, steps per second: 63, episode reward: -90.853, mean reward: -0.927 [-100.000, 7.961], mean action: 1.612 [0.000, 3.000], mean observation: -0.043 [-2.892, 1.000], loss: 0.853565, mean_absolute_error: 33.493568, mean_q: 43.853820, mean_eps: 0.100000
 1971897/2000000: episode: 9071, duration: 1.797s, episode steps: 113, steps per second: 63, episode reward: -89.999, mean reward: -0.796 [-100.000, 11.253], mean action: 1.664 [0.000, 3.000], mean observation: -0.027 [-2.550, 1.000], loss: 1.534324, mean_absolute_error: 32.927533, mean_q: 43.293138, mean_eps: 0.100000
 1972095/2000000: episode: 9072, duration: 3.087s, episode steps: 198, steps per second: 64, episode reward: -205.270, mean reward: -1.037 [-100.000, 12.711], mean action: 2.030 [0.000, 3.000], mean observation: -0.024 [-1.003, 1.148], loss: 1.103731, mean_absolute_error: 34.300294, mean_q: 43.859594, mean_eps: 0.100000
 1972447/2000000: episode: 9073, duration: 5.713s, episode steps: 352, steps per second: 62, episode reward: 191.382, mean reward: 0.544 [-19.873, 100.000], mean action: 1.159 [0.000, 3.000], mean observation: 0.132 [-0.800, 1.000], loss: 1.356032, mean_absolute_error: 34.296017, mean_q: 44.629433, mean_eps: 0.100000
 1972522/2000000: episode: 9074, duration: 1.293s, episode steps: 75, steps per second: 58, episode reward: -47.373, mean reward: -0.632 [-100.000, 9.221], mean action: 1.827 [0.000, 3.000], mean observation: -0.136 [-1.317, 1.000], loss: 0.718460, mean_absolute_error: 34.426740, mean_q: 45.602040, mean_eps: 0.100000
 1973142/2000000: episode: 9075, duration: 10.122s, episode steps: 620, steps per second: 61, episode reward: 115.265, mean reward: 0.186 [-18.959, 100.000], mean action: 1.389 [0.000, 3.000], mean observation: 0.058 [-0.906, 1.268], loss: 1.511344, mean_absolute_error: 35.030694, mean_q: 45.717511, mean_eps: 0.100000
 1973254/2000000: episode: 9076, duration: 1.744s, episode steps: 112, steps per second: 64, episode reward: -72.608, mean reward: -0.648 [-100.000, 10.587], mean action: 2.036 [0.000, 3.000], mean observation: 0.028 [-1.882, 1.000], loss: 1.489934, mean_absolute_error: 35.794932, mean_q: 47.352637, mean_eps: 0.100000
 1973510/2000000: episode: 9077, duration: 4.041s, episode steps: 256, steps per second: 63, episode reward: 194.116, mean reward: 0.758 [-17.348, 100.000], mean action: 1.672 [0.000, 3.000], mean observation: 0.058 [-0.750, 1.000], loss: 1.202224, mean_absolute_error: 35.134119, mean_q: 46.761496, mean_eps: 0.100000
 1973614/2000000: episode: 9078, duration: 1.646s, episode steps: 104, steps per second: 63, episode reward: -15.697, mean reward: -0.151 [-100.000, 21.432], mean action: 1.721 [0.000, 3.000], mean observation: 0.016 [-1.835, 1.000], loss: 1.408689, mean_absolute_error: 33.879065, mean_q: 45.423595, mean_eps: 0.100000
 1973722/2000000: episode: 9079, duration: 1.683s, episode steps: 108, steps per second: 64, episode reward: -108.400, mean reward: -1.004 [-100.000, 15.453], mean action: 1.574 [0.000, 3.000], mean observation: 0.007 [-1.378, 1.000], loss: 2.305532, mean_absolute_error: 34.661227, mean_q: 45.355244, mean_eps: 0.100000
 1973843/2000000: episode: 9080, duration: 1.863s, episode steps: 121, steps per second: 65, episode reward: -26.135, mean reward: -0.216 [-100.000, 19.835], mean action: 1.711 [0.000, 3.000], mean observation: 0.063 [-1.592, 1.000], loss: 0.905085, mean_absolute_error: 35.087542, mean_q: 46.534930, mean_eps: 0.100000
 1973948/2000000: episode: 9081, duration: 1.646s, episode steps: 105, steps per second: 64, episode reward: -130.515, mean reward: -1.243 [-100.000, 8.202], mean action: 1.143 [0.000, 3.000], mean observation: -0.011 [-3.054, 1.000], loss: 1.703763, mean_absolute_error: 33.804580, mean_q: 42.690659, mean_eps: 0.100000
 1974261/2000000: episode: 9082, duration: 5.016s, episode steps: 313, steps per second: 62, episode reward: 158.605, mean reward: 0.507 [-13.130, 100.000], mean action: 1.629 [0.000, 3.000], mean observation: -0.010 [-0.949, 1.000], loss: 1.158425, mean_absolute_error: 34.708118, mean_q: 45.027935, mean_eps: 0.100000
 1974374/2000000: episode: 9083, duration: 1.761s, episode steps: 113, steps per second: 64, episode reward: -77.387, mean reward: -0.685 [-100.000, 9.598], mean action: 1.345 [0.000, 3.000], mean observation: 0.030 [-1.891, 1.000], loss: 0.992914, mean_absolute_error: 33.580084, mean_q: 44.489778, mean_eps: 0.100000
 1974498/2000000: episode: 9084, duration: 1.931s, episode steps: 124, steps per second: 64, episode reward: -140.758, mean reward: -1.135 [-100.000, 17.384], mean action: 1.476 [0.000, 3.000], mean observation: 0.002 [-2.504, 1.000], loss: 1.483401, mean_absolute_error: 33.841229, mean_q: 43.648729, mean_eps: 0.100000
 1974645/2000000: episode: 9085, duration: 2.299s, episode steps: 147, steps per second: 64, episode reward: -21.504, mean reward: -0.146 [-100.000, 12.266], mean action: 2.007 [0.000, 3.000], mean observation: 0.018 [-0.725, 1.000], loss: 1.377657, mean_absolute_error: 34.315931, mean_q: 44.641604, mean_eps: 0.100000
 1975163/2000000: episode: 9086, duration: 8.328s, episode steps: 518, steps per second: 62, episode reward: 179.509, mean reward: 0.347 [-20.962, 100.000], mean action: 1.129 [0.000, 3.000], mean observation: 0.143 [-1.253, 1.000], loss: 1.400905, mean_absolute_error: 34.875946, mean_q: 45.246665, mean_eps: 0.100000
 1975264/2000000: episode: 9087, duration: 1.638s, episode steps: 101, steps per second: 62, episode reward: -5.956, mean reward: -0.059 [-100.000, 20.265], mean action: 1.960 [0.000, 3.000], mean observation: 0.034 [-1.436, 1.000], loss: 1.169393, mean_absolute_error: 34.111752, mean_q: 45.587304, mean_eps: 0.100000
 1975417/2000000: episode: 9088, duration: 2.447s, episode steps: 153, steps per second: 63, episode reward: -65.545, mean reward: -0.428 [-100.000, 10.904], mean action: 1.542 [0.000, 3.000], mean observation: 0.035 [-0.848, 1.000], loss: 1.301297, mean_absolute_error: 34.585744, mean_q: 44.443724, mean_eps: 0.100000
 1975521/2000000: episode: 9089, duration: 1.620s, episode steps: 104, steps per second: 64, episode reward: -32.798, mean reward: -0.315 [-100.000, 18.698], mean action: 1.779 [0.000, 3.000], mean observation: 0.052 [-1.758, 1.000], loss: 1.551100, mean_absolute_error: 35.488680, mean_q: 47.374313, mean_eps: 0.100000
 1975733/2000000: episode: 9090, duration: 3.308s, episode steps: 212, steps per second: 64, episode reward: 205.201, mean reward: 0.968 [-18.467, 100.000], mean action: 1.580 [0.000, 3.000], mean observation: 0.054 [-0.882, 1.000], loss: 1.313677, mean_absolute_error: 33.743382, mean_q: 44.300175, mean_eps: 0.100000
 1975927/2000000: episode: 9091, duration: 3.023s, episode steps: 194, steps per second: 64, episode reward: -74.351, mean reward: -0.383 [-100.000, 16.375], mean action: 1.711 [0.000, 3.000], mean observation: -0.052 [-1.611, 1.000], loss: 1.408085, mean_absolute_error: 35.614518, mean_q: 47.228046, mean_eps: 0.100000
 1976072/2000000: episode: 9092, duration: 2.322s, episode steps: 145, steps per second: 62, episode reward: -247.591, mean reward: -1.708 [-100.000, 89.262], mean action: 1.745 [0.000, 3.000], mean observation: -0.007 [-0.993, 1.804], loss: 1.248362, mean_absolute_error: 35.205764, mean_q: 45.491150, mean_eps: 0.100000
 1976182/2000000: episode: 9093, duration: 1.766s, episode steps: 110, steps per second: 62, episode reward: -64.854, mean reward: -0.590 [-100.000, 20.598], mean action: 1.655 [0.000, 3.000], mean observation: 0.007 [-0.868, 1.000], loss: 1.754614, mean_absolute_error: 34.791130, mean_q: 46.349164, mean_eps: 0.100000
 1976308/2000000: episode: 9094, duration: 1.993s, episode steps: 126, steps per second: 63, episode reward: -115.076, mean reward: -0.913 [-100.000, 30.710], mean action: 1.683 [0.000, 3.000], mean observation: 0.004 [-1.897, 1.000], loss: 1.199500, mean_absolute_error: 34.263088, mean_q: 45.521635, mean_eps: 0.100000
 1976409/2000000: episode: 9095, duration: 1.624s, episode steps: 101, steps per second: 62, episode reward: -75.864, mean reward: -0.751 [-100.000, 8.958], mean action: 1.960 [0.000, 3.000], mean observation: -0.080 [-0.849, 1.000], loss: 1.719961, mean_absolute_error: 33.599424, mean_q: 43.578846, mean_eps: 0.100000
 1976501/2000000: episode: 9096, duration: 1.463s, episode steps: 92, steps per second: 63, episode reward: -39.946, mean reward: -0.434 [-100.000, 18.478], mean action: 1.750 [0.000, 3.000], mean observation: 0.049 [-1.849, 1.000], loss: 1.920286, mean_absolute_error: 33.300613, mean_q: 44.154447, mean_eps: 0.100000
 1977501/2000000: episode: 9097, duration: 16.373s, episode steps: 1000, steps per second: 61, episode reward: 3.651, mean reward: 0.004 [-20.753, 22.013], mean action: 0.865 [0.000, 3.000], mean observation: 0.060 [-1.024, 1.000], loss: 1.320643, mean_absolute_error: 34.473931, mean_q: 45.261260, mean_eps: 0.100000
 1977605/2000000: episode: 9098, duration: 1.624s, episode steps: 104, steps per second: 64, episode reward: -40.811, mean reward: -0.392 [-100.000, 20.037], mean action: 1.952 [0.000, 3.000], mean observation: 0.013 [-1.700, 1.000], loss: 1.732851, mean_absolute_error: 35.616902, mean_q: 45.676104, mean_eps: 0.100000
 1977710/2000000: episode: 9099, duration: 1.627s, episode steps: 105, steps per second: 65, episode reward: -28.021, mean reward: -0.267 [-100.000, 19.489], mean action: 2.010 [0.000, 3.000], mean observation: 0.040 [-1.771, 1.000], loss: 1.010161, mean_absolute_error: 32.395798, mean_q: 43.092817, mean_eps: 0.100000
 1978096/2000000: episode: 9100, duration: 6.101s, episode steps: 386, steps per second: 63, episode reward: 120.142, mean reward: 0.311 [-20.015, 100.000], mean action: 1.319 [0.000, 3.000], mean observation: 0.011 [-1.082, 1.000], loss: 1.275636, mean_absolute_error: 34.051434, mean_q: 44.732864, mean_eps: 0.100000
 1978192/2000000: episode: 9101, duration: 1.559s, episode steps: 96, steps per second: 62, episode reward: -43.712, mean reward: -0.455 [-100.000, 18.187], mean action: 1.750 [0.000, 3.000], mean observation: 0.071 [-1.798, 1.000], loss: 1.732259, mean_absolute_error: 34.989438, mean_q: 45.539101, mean_eps: 0.100000
 1978303/2000000: episode: 9102, duration: 1.740s, episode steps: 111, steps per second: 64, episode reward: -150.240, mean reward: -1.354 [-100.000, 16.029], mean action: 1.586 [0.000, 3.000], mean observation: 0.028 [-1.825, 1.280], loss: 1.073778, mean_absolute_error: 32.820788, mean_q: 42.285983, mean_eps: 0.100000
 1978426/2000000: episode: 9103, duration: 1.950s, episode steps: 123, steps per second: 63, episode reward: -56.025, mean reward: -0.455 [-100.000, 13.258], mean action: 1.805 [0.000, 3.000], mean observation: 0.020 [-0.856, 1.000], loss: 1.268726, mean_absolute_error: 34.964682, mean_q: 46.160609, mean_eps: 0.100000
 1978563/2000000: episode: 9104, duration: 2.232s, episode steps: 137, steps per second: 61, episode reward: -61.561, mean reward: -0.449 [-100.000, 17.010], mean action: 1.467 [0.000, 3.000], mean observation: 0.089 [-1.617, 1.000], loss: 1.544320, mean_absolute_error: 35.208323, mean_q: 45.362013, mean_eps: 0.100000
 1978828/2000000: episode: 9105, duration: 4.731s, episode steps: 265, steps per second: 56, episode reward: 146.365, mean reward: 0.552 [-17.296, 100.000], mean action: 2.385 [0.000, 3.000], mean observation: 0.088 [-0.929, 1.000], loss: 1.347492, mean_absolute_error: 34.777761, mean_q: 45.787287, mean_eps: 0.100000
 1978920/2000000: episode: 9106, duration: 1.656s, episode steps: 92, steps per second: 56, episode reward: -13.140, mean reward: -0.143 [-100.000, 20.647], mean action: 1.717 [0.000, 3.000], mean observation: 0.027 [-1.811, 1.000], loss: 1.367441, mean_absolute_error: 34.083336, mean_q: 45.151648, mean_eps: 0.100000
 1979306/2000000: episode: 9107, duration: 6.655s, episode steps: 386, steps per second: 58, episode reward: 175.031, mean reward: 0.453 [-18.007, 100.000], mean action: 1.277 [0.000, 3.000], mean observation: 0.037 [-1.019, 1.019], loss: 1.450552, mean_absolute_error: 34.499272, mean_q: 45.221705, mean_eps: 0.100000
 1979408/2000000: episode: 9108, duration: 1.610s, episode steps: 102, steps per second: 63, episode reward: -122.447, mean reward: -1.200 [-100.000, 10.887], mean action: 1.971 [0.000, 3.000], mean observation: -0.055 [-1.670, 1.000], loss: 1.552869, mean_absolute_error: 36.666236, mean_q: 48.762883, mean_eps: 0.100000
 1979497/2000000: episode: 9109, duration: 1.423s, episode steps: 89, steps per second: 63, episode reward: -53.034, mean reward: -0.596 [-100.000, 25.072], mean action: 1.966 [0.000, 3.000], mean observation: 0.050 [-2.334, 1.000], loss: 0.734298, mean_absolute_error: 33.529930, mean_q: 45.096566, mean_eps: 0.100000
 1979606/2000000: episode: 9110, duration: 1.711s, episode steps: 109, steps per second: 64, episode reward: -31.256, mean reward: -0.287 [-100.000, 13.570], mean action: 1.972 [0.000, 3.000], mean observation: 0.008 [-0.846, 1.000], loss: 1.503499, mean_absolute_error: 34.773826, mean_q: 45.953040, mean_eps: 0.100000
 1979800/2000000: episode: 9111, duration: 3.063s, episode steps: 194, steps per second: 63, episode reward: -16.030, mean reward: -0.083 [-100.000, 12.505], mean action: 1.680 [0.000, 3.000], mean observation: -0.024 [-1.116, 1.000], loss: 1.102731, mean_absolute_error: 33.844555, mean_q: 44.783490, mean_eps: 0.100000
 1980800/2000000: episode: 9112, duration: 15.824s, episode steps: 1000, steps per second: 63, episode reward: 111.319, mean reward: 0.111 [-20.074, 21.653], mean action: 1.475 [0.000, 3.000], mean observation: 0.193 [-0.943, 1.000], loss: 1.050900, mean_absolute_error: 34.734073, mean_q: 46.150441, mean_eps: 0.100000
 1981241/2000000: episode: 9113, duration: 7.103s, episode steps: 441, steps per second: 62, episode reward: 179.700, mean reward: 0.407 [-17.482, 100.000], mean action: 1.195 [0.000, 3.000], mean observation: 0.100 [-0.739, 1.000], loss: 1.212209, mean_absolute_error: 34.116818, mean_q: 45.076541, mean_eps: 0.100000
 1981364/2000000: episode: 9114, duration: 1.940s, episode steps: 123, steps per second: 63, episode reward: -156.814, mean reward: -1.275 [-100.000, 14.116], mean action: 1.520 [0.000, 3.000], mean observation: -0.032 [-1.326, 1.000], loss: 1.800394, mean_absolute_error: 34.756116, mean_q: 44.712364, mean_eps: 0.100000
 1981518/2000000: episode: 9115, duration: 2.421s, episode steps: 154, steps per second: 64, episode reward: -66.938, mean reward: -0.435 [-100.000, 53.453], mean action: 1.526 [0.000, 3.000], mean observation: 0.005 [-0.962, 1.005], loss: 1.190760, mean_absolute_error: 35.377876, mean_q: 46.988532, mean_eps: 0.100000
 1981758/2000000: episode: 9116, duration: 3.788s, episode steps: 240, steps per second: 63, episode reward: 193.498, mean reward: 0.806 [-9.969, 100.000], mean action: 1.829 [0.000, 3.000], mean observation: 0.071 [-0.832, 1.000], loss: 1.484345, mean_absolute_error: 33.835091, mean_q: 45.050215, mean_eps: 0.100000
 1982107/2000000: episode: 9117, duration: 5.579s, episode steps: 349, steps per second: 63, episode reward: 169.550, mean reward: 0.486 [-19.275, 100.000], mean action: 1.430 [0.000, 3.000], mean observation: 0.091 [-1.408, 1.000], loss: 1.476480, mean_absolute_error: 34.894165, mean_q: 45.759883, mean_eps: 0.100000
 1982426/2000000: episode: 9118, duration: 5.071s, episode steps: 319, steps per second: 63, episode reward: 196.683, mean reward: 0.617 [-20.529, 100.000], mean action: 1.527 [0.000, 3.000], mean observation: 0.068 [-1.431, 1.000], loss: 1.290461, mean_absolute_error: 34.349426, mean_q: 45.509880, mean_eps: 0.100000
 1982725/2000000: episode: 9119, duration: 4.904s, episode steps: 299, steps per second: 61, episode reward: 218.868, mean reward: 0.732 [-18.190, 100.000], mean action: 0.873 [0.000, 3.000], mean observation: 0.102 [-1.139, 1.000], loss: 1.360182, mean_absolute_error: 34.412073, mean_q: 45.750918, mean_eps: 0.100000
 1982845/2000000: episode: 9120, duration: 1.899s, episode steps: 120, steps per second: 63, episode reward: -118.813, mean reward: -0.990 [-100.000, 15.890], mean action: 1.658 [0.000, 3.000], mean observation: -0.003 [-1.254, 1.000], loss: 0.954298, mean_absolute_error: 36.207171, mean_q: 47.417471, mean_eps: 0.100000
 1982983/2000000: episode: 9121, duration: 2.100s, episode steps: 138, steps per second: 66, episode reward: -81.159, mean reward: -0.588 [-100.000, 13.793], mean action: 1.471 [0.000, 3.000], mean observation: 0.108 [-2.687, 1.000], loss: 1.542353, mean_absolute_error: 35.678192, mean_q: 47.262542, mean_eps: 0.100000
 1983223/2000000: episode: 9122, duration: 3.727s, episode steps: 240, steps per second: 64, episode reward: 100.296, mean reward: 0.418 [-13.271, 100.000], mean action: 1.704 [0.000, 3.000], mean observation: -0.018 [-1.291, 1.085], loss: 1.547830, mean_absolute_error: 34.675105, mean_q: 45.464471, mean_eps: 0.100000
 1983579/2000000: episode: 9123, duration: 5.638s, episode steps: 356, steps per second: 63, episode reward: 216.633, mean reward: 0.609 [-18.333, 100.000], mean action: 1.301 [0.000, 3.000], mean observation: 0.117 [-1.510, 1.000], loss: 1.350472, mean_absolute_error: 35.142226, mean_q: 46.293305, mean_eps: 0.100000
 1983727/2000000: episode: 9124, duration: 2.297s, episode steps: 148, steps per second: 64, episode reward: -118.299, mean reward: -0.799 [-100.000, 8.760], mean action: 1.791 [0.000, 3.000], mean observation: 0.022 [-1.337, 1.000], loss: 1.197789, mean_absolute_error: 35.386280, mean_q: 46.685803, mean_eps: 0.100000
 1984153/2000000: episode: 9125, duration: 6.952s, episode steps: 426, steps per second: 61, episode reward: 188.855, mean reward: 0.443 [-15.766, 100.000], mean action: 1.232 [0.000, 3.000], mean observation: 0.035 [-1.293, 1.000], loss: 1.250822, mean_absolute_error: 35.295960, mean_q: 46.477311, mean_eps: 0.100000
 1984271/2000000: episode: 9126, duration: 1.833s, episode steps: 118, steps per second: 64, episode reward: -43.924, mean reward: -0.372 [-100.000, 19.566], mean action: 1.458 [0.000, 3.000], mean observation: 0.079 [-1.882, 1.000], loss: 1.026469, mean_absolute_error: 33.996928, mean_q: 45.677264, mean_eps: 0.100000
 1984366/2000000: episode: 9127, duration: 1.490s, episode steps: 95, steps per second: 64, episode reward: -72.627, mean reward: -0.764 [-100.000, 20.807], mean action: 1.411 [0.000, 3.000], mean observation: -0.040 [-1.899, 1.000], loss: 1.175966, mean_absolute_error: 34.187104, mean_q: 43.865403, mean_eps: 0.100000
 1984687/2000000: episode: 9128, duration: 5.077s, episode steps: 321, steps per second: 63, episode reward: 232.418, mean reward: 0.724 [-17.390, 100.000], mean action: 0.798 [0.000, 3.000], mean observation: 0.121 [-1.341, 1.000], loss: 1.660767, mean_absolute_error: 34.136487, mean_q: 44.940476, mean_eps: 0.100000
 1984793/2000000: episode: 9129, duration: 1.969s, episode steps: 106, steps per second: 54, episode reward: -49.020, mean reward: -0.462 [-100.000, 19.061], mean action: 1.670 [0.000, 3.000], mean observation: 0.066 [-2.174, 1.000], loss: 1.112899, mean_absolute_error: 33.781592, mean_q: 44.002495, mean_eps: 0.100000
 1984936/2000000: episode: 9130, duration: 2.284s, episode steps: 143, steps per second: 63, episode reward: -146.077, mean reward: -1.022 [-100.000, 17.567], mean action: 1.322 [0.000, 3.000], mean observation: -0.042 [-2.780, 1.022], loss: 1.196616, mean_absolute_error: 33.953253, mean_q: 44.923930, mean_eps: 0.100000
 1985026/2000000: episode: 9131, duration: 1.424s, episode steps: 90, steps per second: 63, episode reward: -63.877, mean reward: -0.710 [-100.000, 30.570], mean action: 1.778 [0.000, 3.000], mean observation: -0.121 [-0.952, 3.203], loss: 1.031412, mean_absolute_error: 35.905898, mean_q: 48.003454, mean_eps: 0.100000
 1985160/2000000: episode: 9132, duration: 2.109s, episode steps: 134, steps per second: 64, episode reward: -78.340, mean reward: -0.585 [-100.000, 18.463], mean action: 1.410 [0.000, 3.000], mean observation: 0.082 [-1.394, 1.000], loss: 0.996301, mean_absolute_error: 34.039853, mean_q: 44.602215, mean_eps: 0.100000
 1985340/2000000: episode: 9133, duration: 3.087s, episode steps: 180, steps per second: 58, episode reward: -45.682, mean reward: -0.254 [-100.000, 17.594], mean action: 1.561 [0.000, 3.000], mean observation: -0.019 [-1.075, 1.000], loss: 1.525298, mean_absolute_error: 35.179239, mean_q: 46.406388, mean_eps: 0.100000
 1985744/2000000: episode: 9134, duration: 6.627s, episode steps: 404, steps per second: 61, episode reward: 146.933, mean reward: 0.364 [-12.963, 100.000], mean action: 1.698 [0.000, 3.000], mean observation: 0.014 [-0.864, 1.102], loss: 1.281211, mean_absolute_error: 35.080150, mean_q: 46.291711, mean_eps: 0.100000
 1985828/2000000: episode: 9135, duration: 1.382s, episode steps: 84, steps per second: 61, episode reward: -67.417, mean reward: -0.803 [-100.000, 10.102], mean action: 1.869 [0.000, 3.000], mean observation: -0.064 [-1.709, 1.000], loss: 1.366426, mean_absolute_error: 33.691313, mean_q: 43.936479, mean_eps: 0.100000
 1985948/2000000: episode: 9136, duration: 1.930s, episode steps: 120, steps per second: 62, episode reward: -110.035, mean reward: -0.917 [-100.000, 10.622], mean action: 1.642 [0.000, 3.000], mean observation: -0.026 [-1.103, 1.000], loss: 0.982695, mean_absolute_error: 34.011161, mean_q: 45.170004, mean_eps: 0.100000
 1986033/2000000: episode: 9137, duration: 1.375s, episode steps: 85, steps per second: 62, episode reward: -103.916, mean reward: -1.223 [-100.000, 4.831], mean action: 1.906 [0.000, 3.000], mean observation: -0.093 [-0.853, 0.924], loss: 1.472756, mean_absolute_error: 36.692183, mean_q: 48.337864, mean_eps: 0.100000
 1986150/2000000: episode: 9138, duration: 1.817s, episode steps: 117, steps per second: 64, episode reward: -138.883, mean reward: -1.187 [-100.000, 12.322], mean action: 1.299 [0.000, 3.000], mean observation: 0.019 [-1.191, 1.000], loss: 1.262280, mean_absolute_error: 34.614676, mean_q: 45.414240, mean_eps: 0.100000
 1986256/2000000: episode: 9139, duration: 1.673s, episode steps: 106, steps per second: 63, episode reward: -128.887, mean reward: -1.216 [-100.000, 15.477], mean action: 1.462 [0.000, 3.000], mean observation: -0.038 [-3.167, 1.011], loss: 1.495685, mean_absolute_error: 33.455108, mean_q: 43.409641, mean_eps: 0.100000
 1986376/2000000: episode: 9140, duration: 1.913s, episode steps: 120, steps per second: 63, episode reward: -189.403, mean reward: -1.578 [-100.000, 14.807], mean action: 1.808 [0.000, 3.000], mean observation: -0.076 [-1.300, 1.000], loss: 1.215669, mean_absolute_error: 34.446201, mean_q: 45.411157, mean_eps: 0.100000
 1986491/2000000: episode: 9141, duration: 1.850s, episode steps: 115, steps per second: 62, episode reward: -7.580, mean reward: -0.066 [-100.000, 17.384], mean action: 1.930 [0.000, 3.000], mean observation: 0.061 [-1.174, 1.000], loss: 1.407135, mean_absolute_error: 33.054999, mean_q: 43.782154, mean_eps: 0.100000
 1986769/2000000: episode: 9142, duration: 4.440s, episode steps: 278, steps per second: 63, episode reward: 164.948, mean reward: 0.593 [-17.485, 100.000], mean action: 2.101 [0.000, 3.000], mean observation: 0.073 [-0.889, 1.000], loss: 1.466734, mean_absolute_error: 34.955657, mean_q: 46.028947, mean_eps: 0.100000
 1986893/2000000: episode: 9143, duration: 1.952s, episode steps: 124, steps per second: 64, episode reward: -157.698, mean reward: -1.272 [-100.000, 14.437], mean action: 1.548 [0.000, 3.000], mean observation: 0.010 [-1.434, 1.000], loss: 1.038959, mean_absolute_error: 33.383383, mean_q: 43.976668, mean_eps: 0.100000
 1986994/2000000: episode: 9144, duration: 1.593s, episode steps: 101, steps per second: 63, episode reward: -16.068, mean reward: -0.159 [-100.000, 12.832], mean action: 1.911 [0.000, 3.000], mean observation: -0.072 [-0.818, 1.449], loss: 1.032584, mean_absolute_error: 35.529730, mean_q: 47.323344, mean_eps: 0.100000
 1987136/2000000: episode: 9145, duration: 2.260s, episode steps: 142, steps per second: 63, episode reward: -31.386, mean reward: -0.221 [-100.000, 69.744], mean action: 1.373 [0.000, 3.000], mean observation: 0.032 [-1.278, 1.000], loss: 1.550022, mean_absolute_error: 33.635822, mean_q: 44.151008, mean_eps: 0.100000
 1987250/2000000: episode: 9146, duration: 1.815s, episode steps: 114, steps per second: 63, episode reward: -159.847, mean reward: -1.402 [-100.000, 18.594], mean action: 1.202 [0.000, 3.000], mean observation: -0.004 [-3.353, 1.000], loss: 1.253556, mean_absolute_error: 34.903526, mean_q: 45.543334, mean_eps: 0.100000
 1987349/2000000: episode: 9147, duration: 1.574s, episode steps: 99, steps per second: 63, episode reward: -29.479, mean reward: -0.298 [-100.000, 15.161], mean action: 1.919 [0.000, 3.000], mean observation: -0.040 [-1.326, 1.000], loss: 1.585320, mean_absolute_error: 34.568740, mean_q: 44.974666, mean_eps: 0.100000
 1987465/2000000: episode: 9148, duration: 1.818s, episode steps: 116, steps per second: 64, episode reward: -195.637, mean reward: -1.687 [-100.000, 12.522], mean action: 1.422 [0.000, 3.000], mean observation: -0.039 [-1.396, 1.000], loss: 1.216758, mean_absolute_error: 35.340611, mean_q: 46.087575, mean_eps: 0.100000
 1987662/2000000: episode: 9149, duration: 3.085s, episode steps: 197, steps per second: 64, episode reward: 177.174, mean reward: 0.899 [-17.869, 100.000], mean action: 1.482 [0.000, 3.000], mean observation: 0.048 [-1.002, 1.000], loss: 1.175690, mean_absolute_error: 33.586633, mean_q: 43.606387, mean_eps: 0.100000
 1988073/2000000: episode: 9150, duration: 6.602s, episode steps: 411, steps per second: 62, episode reward: 189.018, mean reward: 0.460 [-10.237, 100.000], mean action: 0.723 [0.000, 3.000], mean observation: 0.112 [-0.848, 1.000], loss: 1.536045, mean_absolute_error: 35.059339, mean_q: 45.490277, mean_eps: 0.100000
 1988239/2000000: episode: 9151, duration: 2.587s, episode steps: 166, steps per second: 64, episode reward: 4.332, mean reward: 0.026 [-100.000, 13.597], mean action: 1.922 [0.000, 3.000], mean observation: 0.061 [-0.680, 1.000], loss: 0.862966, mean_absolute_error: 34.738108, mean_q: 45.857323, mean_eps: 0.100000
 1988372/2000000: episode: 9152, duration: 2.122s, episode steps: 133, steps per second: 63, episode reward: -99.769, mean reward: -0.750 [-100.000, 22.500], mean action: 1.737 [0.000, 3.000], mean observation: -0.042 [-1.847, 1.000], loss: 1.444276, mean_absolute_error: 35.431353, mean_q: 45.900411, mean_eps: 0.100000
 1988492/2000000: episode: 9153, duration: 1.965s, episode steps: 120, steps per second: 61, episode reward: -144.132, mean reward: -1.201 [-100.000, 17.379], mean action: 1.650 [0.000, 3.000], mean observation: -0.003 [-1.566, 1.000], loss: 1.300791, mean_absolute_error: 35.175316, mean_q: 46.466095, mean_eps: 0.100000
 1988621/2000000: episode: 9154, duration: 2.053s, episode steps: 129, steps per second: 63, episode reward: -67.260, mean reward: -0.521 [-100.000, 9.100], mean action: 1.984 [0.000, 3.000], mean observation: 0.046 [-0.932, 1.000], loss: 1.132564, mean_absolute_error: 34.153341, mean_q: 45.536783, mean_eps: 0.100000
 1988924/2000000: episode: 9155, duration: 4.835s, episode steps: 303, steps per second: 63, episode reward: 226.661, mean reward: 0.748 [-19.030, 100.000], mean action: 1.116 [0.000, 3.000], mean observation: 0.061 [-0.757, 1.000], loss: 1.339842, mean_absolute_error: 34.115117, mean_q: 45.110154, mean_eps: 0.100000
 1989093/2000000: episode: 9156, duration: 2.721s, episode steps: 169, steps per second: 62, episode reward: -104.432, mean reward: -0.618 [-100.000, 12.954], mean action: 1.811 [0.000, 3.000], mean observation: -0.012 [-1.036, 1.000], loss: 1.404038, mean_absolute_error: 34.195961, mean_q: 44.946644, mean_eps: 0.100000
 1989489/2000000: episode: 9157, duration: 6.387s, episode steps: 396, steps per second: 62, episode reward: 114.421, mean reward: 0.289 [-20.433, 100.000], mean action: 1.659 [0.000, 3.000], mean observation: 0.044 [-0.789, 1.000], loss: 1.477260, mean_absolute_error: 34.784786, mean_q: 45.533875, mean_eps: 0.100000
 1989611/2000000: episode: 9158, duration: 1.908s, episode steps: 122, steps per second: 64, episode reward: -98.086, mean reward: -0.804 [-100.000, 12.903], mean action: 1.762 [0.000, 3.000], mean observation: -0.027 [-1.724, 1.000], loss: 1.154862, mean_absolute_error: 33.650826, mean_q: 44.510884, mean_eps: 0.100000
 1989847/2000000: episode: 9159, duration: 3.736s, episode steps: 236, steps per second: 63, episode reward: 171.280, mean reward: 0.726 [-9.939, 100.000], mean action: 1.301 [0.000, 3.000], mean observation: -0.013 [-1.322, 1.000], loss: 1.323155, mean_absolute_error: 34.808260, mean_q: 44.652695, mean_eps: 0.100000
 1989987/2000000: episode: 9160, duration: 2.209s, episode steps: 140, steps per second: 63, episode reward: -124.959, mean reward: -0.893 [-100.000, 13.539], mean action: 1.600 [0.000, 3.000], mean observation: -0.008 [-0.899, 1.204], loss: 1.065609, mean_absolute_error: 34.045545, mean_q: 45.240340, mean_eps: 0.100000
 1990138/2000000: episode: 9161, duration: 2.392s, episode steps: 151, steps per second: 63, episode reward: -129.906, mean reward: -0.860 [-100.000, 11.377], mean action: 1.947 [0.000, 3.000], mean observation: 0.003 [-0.734, 1.103], loss: 1.247505, mean_absolute_error: 35.609996, mean_q: 47.080433, mean_eps: 0.100000
 1990243/2000000: episode: 9162, duration: 1.640s, episode steps: 105, steps per second: 64, episode reward: -32.036, mean reward: -0.305 [-100.000, 11.031], mean action: 1.705 [0.000, 3.000], mean observation: 0.048 [-1.642, 1.000], loss: 1.094985, mean_absolute_error: 34.189485, mean_q: 45.786156, mean_eps: 0.100000
 1990359/2000000: episode: 9163, duration: 1.842s, episode steps: 116, steps per second: 63, episode reward: -177.108, mean reward: -1.527 [-100.000, 17.758], mean action: 1.733 [0.000, 3.000], mean observation: -0.034 [-2.449, 1.000], loss: 1.184770, mean_absolute_error: 34.040101, mean_q: 43.783081, mean_eps: 0.100000
 1990748/2000000: episode: 9164, duration: 6.243s, episode steps: 389, steps per second: 62, episode reward: 197.853, mean reward: 0.509 [-11.447, 100.000], mean action: 2.157 [0.000, 3.000], mean observation: 0.088 [-0.745, 1.000], loss: 1.304827, mean_absolute_error: 33.732345, mean_q: 44.493461, mean_eps: 0.100000
 1990877/2000000: episode: 9165, duration: 2.067s, episode steps: 129, steps per second: 62, episode reward: -113.527, mean reward: -0.880 [-100.000, 9.911], mean action: 1.535 [0.000, 3.000], mean observation: -0.061 [-1.832, 1.000], loss: 1.152140, mean_absolute_error: 34.456241, mean_q: 44.941338, mean_eps: 0.100000
 1991023/2000000: episode: 9166, duration: 2.279s, episode steps: 146, steps per second: 64, episode reward: -18.963, mean reward: -0.130 [-100.000, 10.103], mean action: 1.932 [0.000, 3.000], mean observation: 0.081 [-0.859, 1.000], loss: 0.991611, mean_absolute_error: 34.161068, mean_q: 45.295475, mean_eps: 0.100000
 1991365/2000000: episode: 9167, duration: 5.455s, episode steps: 342, steps per second: 63, episode reward: -153.366, mean reward: -0.448 [-100.000, 6.618], mean action: 1.906 [0.000, 3.000], mean observation: -0.071 [-1.001, 0.967], loss: 1.210910, mean_absolute_error: 33.728607, mean_q: 44.486112, mean_eps: 0.100000
 1991707/2000000: episode: 9168, duration: 5.451s, episode steps: 342, steps per second: 63, episode reward: 187.043, mean reward: 0.547 [-16.221, 100.000], mean action: 1.482 [0.000, 3.000], mean observation: 0.099 [-0.995, 1.508], loss: 1.390674, mean_absolute_error: 34.042722, mean_q: 44.217200, mean_eps: 0.100000
 1991822/2000000: episode: 9169, duration: 1.812s, episode steps: 115, steps per second: 63, episode reward: -149.897, mean reward: -1.303 [-100.000, 8.831], mean action: 1.217 [0.000, 3.000], mean observation: -0.004 [-2.672, 1.000], loss: 2.185950, mean_absolute_error: 34.464045, mean_q: 45.691082, mean_eps: 0.100000
 1991972/2000000: episode: 9170, duration: 2.364s, episode steps: 150, steps per second: 63, episode reward: -22.523, mean reward: -0.150 [-100.000, 10.566], mean action: 1.573 [0.000, 3.000], mean observation: -0.014 [-1.553, 1.000], loss: 1.650962, mean_absolute_error: 33.791408, mean_q: 43.898521, mean_eps: 0.100000
 1992546/2000000: episode: 9171, duration: 9.552s, episode steps: 574, steps per second: 60, episode reward: 177.085, mean reward: 0.309 [-19.304, 100.000], mean action: 0.817 [0.000, 3.000], mean observation: 0.121 [-0.896, 1.000], loss: 1.094485, mean_absolute_error: 33.645848, mean_q: 44.706637, mean_eps: 0.100000
 1992621/2000000: episode: 9172, duration: 1.202s, episode steps: 75, steps per second: 62, episode reward: -75.255, mean reward: -1.003 [-100.000, 18.087], mean action: 1.840 [0.000, 3.000], mean observation: -0.114 [-1.323, 1.454], loss: 1.282213, mean_absolute_error: 35.077579, mean_q: 45.504327, mean_eps: 0.100000
 1993003/2000000: episode: 9173, duration: 6.079s, episode steps: 382, steps per second: 63, episode reward: 258.091, mean reward: 0.676 [-9.376, 100.000], mean action: 1.495 [0.000, 3.000], mean observation: 0.099 [-1.060, 1.000], loss: 1.256653, mean_absolute_error: 34.764523, mean_q: 45.828133, mean_eps: 0.100000
 1993133/2000000: episode: 9174, duration: 2.053s, episode steps: 130, steps per second: 63, episode reward: -110.441, mean reward: -0.850 [-100.000, 17.025], mean action: 1.700 [0.000, 3.000], mean observation: -0.034 [-1.688, 1.000], loss: 1.143952, mean_absolute_error: 34.346962, mean_q: 45.593248, mean_eps: 0.100000
 1993269/2000000: episode: 9175, duration: 2.136s, episode steps: 136, steps per second: 64, episode reward: -134.296, mean reward: -0.987 [-100.000, 17.805], mean action: 1.316 [0.000, 3.000], mean observation: -0.052 [-1.056, 1.012], loss: 1.287882, mean_absolute_error: 36.128119, mean_q: 48.167589, mean_eps: 0.100000
 1993481/2000000: episode: 9176, duration: 3.353s, episode steps: 212, steps per second: 63, episode reward: -121.787, mean reward: -0.574 [-100.000, 14.221], mean action: 1.632 [0.000, 3.000], mean observation: -0.042 [-0.895, 1.235], loss: 1.184880, mean_absolute_error: 34.193564, mean_q: 45.346817, mean_eps: 0.100000
 1993782/2000000: episode: 9177, duration: 4.960s, episode steps: 301, steps per second: 61, episode reward: -375.367, mean reward: -1.247 [-100.000, 8.802], mean action: 1.870 [0.000, 3.000], mean observation: -0.041 [-1.011, 1.950], loss: 1.075664, mean_absolute_error: 34.271752, mean_q: 45.341751, mean_eps: 0.100000
 1993914/2000000: episode: 9178, duration: 2.033s, episode steps: 132, steps per second: 65, episode reward: -115.968, mean reward: -0.879 [-100.000, 73.330], mean action: 1.439 [0.000, 3.000], mean observation: -0.040 [-1.510, 1.000], loss: 1.309846, mean_absolute_error: 34.277720, mean_q: 44.814304, mean_eps: 0.100000
 1994190/2000000: episode: 9179, duration: 4.435s, episode steps: 276, steps per second: 62, episode reward: 199.597, mean reward: 0.723 [-17.629, 100.000], mean action: 1.417 [0.000, 3.000], mean observation: 0.058 [-1.675, 1.000], loss: 1.062675, mean_absolute_error: 35.082500, mean_q: 46.508326, mean_eps: 0.100000
 1994656/2000000: episode: 9180, duration: 7.396s, episode steps: 466, steps per second: 63, episode reward: 243.753, mean reward: 0.523 [-8.894, 100.000], mean action: 0.755 [0.000, 3.000], mean observation: 0.121 [-0.694, 1.078], loss: 1.104824, mean_absolute_error: 34.529022, mean_q: 45.621727, mean_eps: 0.100000
 1994914/2000000: episode: 9181, duration: 4.160s, episode steps: 258, steps per second: 62, episode reward: 179.160, mean reward: 0.694 [-17.538, 100.000], mean action: 1.434 [0.000, 3.000], mean observation: 0.071 [-1.492, 1.000], loss: 1.329530, mean_absolute_error: 34.198557, mean_q: 44.538323, mean_eps: 0.100000
 1995400/2000000: episode: 9182, duration: 7.929s, episode steps: 486, steps per second: 61, episode reward: 204.493, mean reward: 0.421 [-17.604, 100.000], mean action: 1.117 [0.000, 3.000], mean observation: 0.079 [-0.754, 1.000], loss: 1.432523, mean_absolute_error: 33.959774, mean_q: 44.554384, mean_eps: 0.100000
 1995546/2000000: episode: 9183, duration: 2.338s, episode steps: 146, steps per second: 62, episode reward: -32.771, mean reward: -0.224 [-100.000, 9.972], mean action: 1.658 [0.000, 3.000], mean observation: 0.031 [-0.868, 1.208], loss: 1.646401, mean_absolute_error: 34.268579, mean_q: 44.704081, mean_eps: 0.100000
 1995667/2000000: episode: 9184, duration: 1.869s, episode steps: 121, steps per second: 65, episode reward: -256.253, mean reward: -2.118 [-100.000, 5.431], mean action: 1.314 [0.000, 3.000], mean observation: 0.111 [-1.433, 4.479], loss: 1.653301, mean_absolute_error: 34.817001, mean_q: 45.341230, mean_eps: 0.100000
 1995776/2000000: episode: 9185, duration: 1.744s, episode steps: 109, steps per second: 63, episode reward: -82.456, mean reward: -0.756 [-100.000, 10.865], mean action: 1.743 [0.000, 3.000], mean observation: -0.049 [-1.790, 1.000], loss: 0.950044, mean_absolute_error: 35.827345, mean_q: 47.909861, mean_eps: 0.100000
 1996002/2000000: episode: 9186, duration: 3.623s, episode steps: 226, steps per second: 62, episode reward: 193.745, mean reward: 0.857 [-18.966, 100.000], mean action: 1.695 [0.000, 3.000], mean observation: 0.064 [-1.219, 1.000], loss: 1.328044, mean_absolute_error: 33.501750, mean_q: 43.957584, mean_eps: 0.100000
 1996277/2000000: episode: 9187, duration: 4.352s, episode steps: 275, steps per second: 63, episode reward: 184.834, mean reward: 0.672 [-8.524, 100.000], mean action: 1.375 [0.000, 3.000], mean observation: 0.113 [-1.418, 1.000], loss: 1.229262, mean_absolute_error: 34.161412, mean_q: 45.675091, mean_eps: 0.100000
 1996881/2000000: episode: 9188, duration: 9.858s, episode steps: 604, steps per second: 61, episode reward: 244.337, mean reward: 0.405 [-18.540, 100.000], mean action: 0.866 [0.000, 3.000], mean observation: 0.093 [-1.331, 1.000], loss: 1.341502, mean_absolute_error: 34.131739, mean_q: 45.282060, mean_eps: 0.100000
 1997132/2000000: episode: 9189, duration: 4.044s, episode steps: 251, steps per second: 62, episode reward: 215.541, mean reward: 0.859 [-11.894, 100.000], mean action: 1.371 [0.000, 3.000], mean observation: 0.022 [-0.973, 1.000], loss: 1.282924, mean_absolute_error: 33.943938, mean_q: 44.639629, mean_eps: 0.100000
 1997252/2000000: episode: 9190, duration: 1.954s, episode steps: 120, steps per second: 61, episode reward: -117.028, mean reward: -0.975 [-100.000, 11.778], mean action: 1.467 [0.000, 3.000], mean observation: 0.002 [-0.849, 1.000], loss: 1.236350, mean_absolute_error: 33.924458, mean_q: 44.825168, mean_eps: 0.100000
 1997567/2000000: episode: 9191, duration: 5.055s, episode steps: 315, steps per second: 62, episode reward: 151.140, mean reward: 0.480 [-18.691, 100.000], mean action: 1.257 [0.000, 3.000], mean observation: 0.109 [-1.438, 1.000], loss: 1.417602, mean_absolute_error: 34.523640, mean_q: 45.601074, mean_eps: 0.100000
 1998035/2000000: episode: 9192, duration: 7.545s, episode steps: 468, steps per second: 62, episode reward: 249.211, mean reward: 0.533 [-19.856, 100.000], mean action: 0.947 [0.000, 3.000], mean observation: 0.148 [-1.293, 1.000], loss: 1.152403, mean_absolute_error: 33.881040, mean_q: 44.922266, mean_eps: 0.100000
 1998139/2000000: episode: 9193, duration: 1.624s, episode steps: 104, steps per second: 64, episode reward: -70.743, mean reward: -0.680 [-100.000, 10.931], mean action: 1.683 [0.000, 3.000], mean observation: -0.079 [-1.602, 1.000], loss: 1.394074, mean_absolute_error: 32.957265, mean_q: 44.020867, mean_eps: 0.100000
 1998234/2000000: episode: 9194, duration: 1.511s, episode steps: 95, steps per second: 63, episode reward: -70.748, mean reward: -0.745 [-100.000, 17.311], mean action: 1.674 [0.000, 3.000], mean observation: -0.001 [-1.628, 1.000], loss: 1.503505, mean_absolute_error: 34.571940, mean_q: 44.233046, mean_eps: 0.100000
 1998348/2000000: episode: 9195, duration: 1.806s, episode steps: 114, steps per second: 63, episode reward: -40.611, mean reward: -0.356 [-100.000, 18.525], mean action: 2.018 [0.000, 3.000], mean observation: 0.065 [-1.491, 1.000], loss: 1.100069, mean_absolute_error: 35.881150, mean_q: 46.978061, mean_eps: 0.100000
 1998679/2000000: episode: 9196, duration: 5.257s, episode steps: 331, steps per second: 63, episode reward: 196.127, mean reward: 0.593 [-9.747, 100.000], mean action: 1.299 [0.000, 3.000], mean observation: 0.044 [-0.860, 1.000], loss: 1.225794, mean_absolute_error: 34.014412, mean_q: 44.710477, mean_eps: 0.100000
 1998838/2000000: episode: 9197, duration: 2.507s, episode steps: 159, steps per second: 63, episode reward: -153.775, mean reward: -0.967 [-100.000, 22.616], mean action: 1.925 [0.000, 3.000], mean observation: -0.095 [-1.717, 1.000], loss: 1.413087, mean_absolute_error: 35.151083, mean_q: 46.508672, mean_eps: 0.100000
 1999151/2000000: episode: 9198, duration: 4.990s, episode steps: 313, steps per second: 63, episode reward: 246.326, mean reward: 0.787 [-9.050, 100.000], mean action: 1.339 [0.000, 3.000], mean observation: 0.099 [-0.804, 1.000], loss: 1.627854, mean_absolute_error: 34.326188, mean_q: 45.392157, mean_eps: 0.100000
 1999598/2000000: episode: 9199, duration: 7.144s, episode steps: 447, steps per second: 63, episode reward: 175.171, mean reward: 0.392 [-11.716, 100.000], mean action: 1.018 [0.000, 3.000], mean observation: 0.028 [-0.855, 1.000], loss: 1.120314, mean_absolute_error: 34.088150, mean_q: 45.109706, mean_eps: 0.100000
 1999722/2000000: episode: 9200, duration: 2.211s, episode steps: 124, steps per second: 56, episode reward: -26.515, mean reward: -0.214 [-100.000, 17.178], mean action: 1.742 [0.000, 3.000], mean observation: 0.047 [-0.906, 1.000], loss: 1.205616, mean_absolute_error: 35.293239, mean_q: 46.267406, mean_eps: 0.100000
 1999836/2000000: episode: 9201, duration: 1.864s, episode steps: 114, steps per second: 61, episode reward: -86.778, mean reward: -0.761 [-100.000, 10.371], mean action: 2.000 [0.000, 3.000], mean observation: -0.036 [-1.654, 1.000], loss: 1.309091, mean_absolute_error: 33.044610, mean_q: 43.184522, mean_eps: 0.100000
 1999951/2000000: episode: 9202, duration: 1.837s, episode steps: 115, steps per second: 63, episode reward: -182.713, mean reward: -1.589 [-100.000, 13.331], mean action: 1.730 [0.000, 3.000], mean observation: -0.047 [-1.298, 1.000], loss: 0.947210, mean_absolute_error: 35.258800, mean_q: 46.977118, mean_eps: 0.100000
done, took 31574.361 seconds
Testing for 5 episodes ...
Episode 1: reward: -106.470, steps: 255
Episode 2: reward: 246.034, steps: 275
Episode 3: reward: -274.409, steps: 299
Episode 4: reward: -85.481, steps: 285
Episode 5: reward: -115.335, steps: 239
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In hindsight, using a window length of 100 was redundant as the 8 high-level representations which defines the current state are enough, and it is not like there is any hidden information.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dqn</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">nb_episodes</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">visualize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;train_history_dqn.pckl&#39;</span><span class="p">,</span><span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">fitted</span><span class="o">.</span><span class="n">history</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
<span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;train_history_dqn.pckl&#39;</span><span class="p">,</span><span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">obj0</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="k">if</span> <span class="n">obj0</span><span class="o">==</span><span class="n">fitted</span><span class="o">.</span><span class="n">history</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;saved&quot;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Heuristic:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">heuristic</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="c1"># Heuristic for:</span>
    <span class="c1"># 1. Testing.</span>
    <span class="c1"># 2. Demonstration rollout.</span>
    <span class="n">angle_targ</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mf">0.5</span> <span class="o">+</span> <span class="n">s</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="mf">1.0</span>         <span class="c1"># angle should point towards center (s[0] is horizontal coordinate, s[2] hor speed)</span>
    <span class="k">if</span> <span class="n">angle_targ</span> <span class="o">&gt;</span>  <span class="mf">0.4</span><span class="p">:</span> <span class="n">angle_targ</span> <span class="o">=</span>  <span class="mf">0.4</span>  <span class="c1"># more than 0.4 radians (22 degrees) is bad</span>
    <span class="k">if</span> <span class="n">angle_targ</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">:</span> <span class="n">angle_targ</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.4</span>
    <span class="n">hover_targ</span> <span class="o">=</span> <span class="mf">0.55</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>           <span class="c1"># target y should be proporional to horizontal offset</span>

    <span class="c1"># PID controller: s[4] angle, s[5] angularSpeed</span>
    <span class="n">angle_todo</span> <span class="o">=</span> <span class="p">(</span><span class="n">angle_targ</span> <span class="o">-</span> <span class="n">s</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span><span class="o">*</span><span class="mf">0.5</span> <span class="o">-</span> <span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span><span class="o">*</span><span class="mf">1.0</span>
    <span class="c1">#print(&quot;angle_targ=%0.2f, angle_todo=%0.2f&quot; % (angle_targ, angle_todo))</span>

    <span class="c1"># PID controller: s[1] vertical coordinate s[3] vertical speed</span>
    <span class="n">hover_todo</span> <span class="o">=</span> <span class="p">(</span><span class="n">hover_targ</span> <span class="o">-</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">*</span><span class="mf">0.5</span> <span class="o">-</span> <span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span><span class="o">*</span><span class="mf">0.5</span>
    <span class="c1">#print(&quot;hover_targ=%0.2f, hover_todo=%0.2f&quot; % (hover_targ, hover_todo))</span>

    <span class="k">if</span> <span class="n">s</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span> <span class="ow">or</span> <span class="n">s</span><span class="p">[</span><span class="mi">7</span><span class="p">]:</span> <span class="c1"># legs have contact</span>
        <span class="n">angle_todo</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">hover_todo</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span><span class="o">*</span><span class="mf">0.5</span>  <span class="c1"># override to reduce fall speed, that&#39;s all we need after contact</span>

    <span class="k">if</span> <span class="kc">False</span><span class="p">:</span> <span class="c1">#env.continuous:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span> <span class="p">[</span><span class="n">hover_todo</span><span class="o">*</span><span class="mi">20</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="n">angle_todo</span><span class="o">*</span><span class="mi">20</span><span class="p">]</span> <span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">hover_todo</span> <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">angle_todo</span><span class="p">)</span> <span class="ow">and</span> <span class="n">hover_todo</span> <span class="o">&gt;</span> <span class="mf">0.05</span><span class="p">:</span> <span class="n">a</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="k">elif</span> <span class="n">angle_todo</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mf">0.05</span><span class="p">:</span> <span class="n">a</span> <span class="o">=</span> <span class="mi">3</span>
        <span class="k">elif</span> <span class="n">angle_todo</span> <span class="o">&gt;</span> <span class="o">+</span><span class="mf">0.05</span><span class="p">:</span> <span class="n">a</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">a</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[60]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dqn_his</span> <span class="o">=</span> <span class="n">dqn</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">nb_episodes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">visualize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;test_history_dqn.pckl&#39;</span><span class="p">,</span><span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">dqn_his</span><span class="o">.</span><span class="n">history</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
<span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Testing for 1000 episodes ...
Episode 1: reward: -105.517, steps: 219
Episode 2: reward: 172.804, steps: 384
Episode 3: reward: 209.412, steps: 427
Episode 4: reward: 159.072, steps: 407
Episode 5: reward: -41.756, steps: 205
Episode 6: reward: -25.036, steps: 246
Episode 7: reward: -197.747, steps: 182
Episode 8: reward: -144.150, steps: 144
Episode 9: reward: -166.756, steps: 133
Episode 10: reward: -75.653, steps: 272
Episode 11: reward: 151.761, steps: 315
Episode 12: reward: -82.215, steps: 266
Episode 13: reward: 206.094, steps: 332
Episode 14: reward: 241.473, steps: 280
Episode 15: reward: 242.180, steps: 302
Episode 16: reward: -104.046, steps: 115
Episode 17: reward: 193.107, steps: 313
Episode 18: reward: -139.096, steps: 214
Episode 19: reward: 194.473, steps: 316
Episode 20: reward: 217.790, steps: 390
Episode 21: reward: 224.596, steps: 521
Episode 22: reward: 231.357, steps: 325
Episode 23: reward: -19.230, steps: 202
Episode 24: reward: 217.013, steps: 304
Episode 25: reward: 240.155, steps: 289
Episode 26: reward: -132.701, steps: 134
Episode 27: reward: -22.962, steps: 444
Episode 28: reward: -65.675, steps: 240
Episode 29: reward: 227.506, steps: 269
Episode 30: reward: -22.659, steps: 351
Episode 31: reward: -22.565, steps: 216
Episode 32: reward: 213.347, steps: 270
Episode 33: reward: 119.693, steps: 225
Episode 34: reward: 214.955, steps: 329
Episode 35: reward: 175.017, steps: 512
Episode 36: reward: -109.560, steps: 228
Episode 37: reward: -26.888, steps: 225
Episode 38: reward: 225.206, steps: 361
Episode 39: reward: 234.279, steps: 383
Episode 40: reward: 111.307, steps: 1000
Episode 41: reward: -60.809, steps: 338
Episode 42: reward: 216.770, steps: 499
Episode 43: reward: 174.362, steps: 415
Episode 44: reward: 134.580, steps: 232
Episode 45: reward: -64.336, steps: 223
Episode 46: reward: -270.312, steps: 207
Episode 47: reward: 151.534, steps: 402
Episode 48: reward: -224.097, steps: 181
Episode 49: reward: 188.924, steps: 366
Episode 50: reward: 228.081, steps: 269
Episode 51: reward: 173.462, steps: 381
Episode 52: reward: 197.169, steps: 277
Episode 53: reward: -84.258, steps: 196
Episode 54: reward: -159.028, steps: 135
Episode 55: reward: 232.573, steps: 284
Episode 56: reward: -57.700, steps: 226
Episode 57: reward: -107.556, steps: 218
Episode 58: reward: 195.044, steps: 374
Episode 59: reward: 98.484, steps: 419
Episode 60: reward: 216.940, steps: 309
Episode 61: reward: -88.917, steps: 157
Episode 62: reward: 191.308, steps: 627
Episode 63: reward: -20.068, steps: 203
Episode 64: reward: -89.113, steps: 198
Episode 65: reward: -110.534, steps: 224
Episode 66: reward: -123.265, steps: 111
Episode 67: reward: 223.537, steps: 312
Episode 68: reward: -128.225, steps: 207
Episode 69: reward: -118.516, steps: 344
Episode 70: reward: 212.351, steps: 377
Episode 71: reward: 215.137, steps: 327
Episode 72: reward: -65.241, steps: 272
Episode 73: reward: -59.692, steps: 203
Episode 74: reward: -127.591, steps: 193
Episode 75: reward: -80.972, steps: 146
Episode 76: reward: 179.752, steps: 287
Episode 77: reward: 171.761, steps: 330
Episode 78: reward: -120.636, steps: 229
Episode 79: reward: 204.580, steps: 272
Episode 80: reward: 193.238, steps: 308
Episode 81: reward: -57.459, steps: 181
Episode 82: reward: -48.523, steps: 222
Episode 83: reward: 196.913, steps: 239
Episode 84: reward: -67.212, steps: 224
Episode 85: reward: 244.140, steps: 268
Episode 86: reward: -103.246, steps: 147
Episode 87: reward: -59.320, steps: 230
Episode 88: reward: -16.634, steps: 378
Episode 89: reward: 207.211, steps: 397
Episode 90: reward: -74.376, steps: 358
Episode 91: reward: -110.049, steps: 290
Episode 92: reward: 190.470, steps: 424
Episode 93: reward: 236.684, steps: 254
Episode 94: reward: 225.489, steps: 386
Episode 95: reward: 234.946, steps: 250
Episode 96: reward: 150.847, steps: 364
Episode 97: reward: -117.195, steps: 192
Episode 98: reward: 220.875, steps: 327
Episode 99: reward: 233.944, steps: 311
Episode 100: reward: 111.386, steps: 361
Episode 101: reward: -6.491, steps: 455
Episode 102: reward: 183.249, steps: 440
Episode 103: reward: 245.057, steps: 285
Episode 104: reward: 206.671, steps: 314
Episode 105: reward: 212.707, steps: 343
Episode 106: reward: 160.252, steps: 426
Episode 107: reward: 175.139, steps: 260
Episode 108: reward: 212.011, steps: 281
Episode 109: reward: 159.131, steps: 391
Episode 110: reward: 12.194, steps: 178
Episode 111: reward: 228.574, steps: 313
Episode 112: reward: 160.116, steps: 414
Episode 113: reward: -143.581, steps: 137
Episode 114: reward: 231.827, steps: 300
Episode 115: reward: -87.812, steps: 217
Episode 116: reward: 111.407, steps: 489
Episode 117: reward: -159.576, steps: 190
Episode 118: reward: -123.518, steps: 175
Episode 119: reward: 175.946, steps: 382
Episode 120: reward: -119.429, steps: 218
Episode 121: reward: -166.761, steps: 136
Episode 122: reward: -58.686, steps: 252
Episode 123: reward: 233.007, steps: 259
Episode 124: reward: 121.694, steps: 1000
Episode 125: reward: 137.300, steps: 205
Episode 126: reward: -30.326, steps: 223
Episode 127: reward: -77.484, steps: 202
Episode 128: reward: 227.635, steps: 316
Episode 129: reward: 223.269, steps: 284
Episode 130: reward: 148.506, steps: 557
Episode 131: reward: 230.349, steps: 283
Episode 132: reward: 241.114, steps: 276
Episode 133: reward: 220.705, steps: 316
Episode 134: reward: -247.899, steps: 179
Episode 135: reward: 228.266, steps: 396
Episode 136: reward: 217.762, steps: 278
Episode 137: reward: -105.460, steps: 210
Episode 138: reward: 139.180, steps: 278
Episode 139: reward: -72.949, steps: 137
Episode 140: reward: -109.467, steps: 206
Episode 141: reward: 104.833, steps: 435
Episode 142: reward: 222.002, steps: 424
Episode 143: reward: 222.285, steps: 330
Episode 144: reward: 238.498, steps: 266
Episode 145: reward: 173.081, steps: 345
Episode 146: reward: -75.033, steps: 207
Episode 147: reward: 211.461, steps: 351
Episode 148: reward: -88.918, steps: 206
Episode 149: reward: 211.968, steps: 423
Episode 150: reward: 242.263, steps: 525
Episode 151: reward: -102.305, steps: 148
Episode 152: reward: 186.654, steps: 287
Episode 153: reward: 151.610, steps: 322
Episode 154: reward: 207.820, steps: 375
Episode 155: reward: 179.215, steps: 267
Episode 156: reward: -76.433, steps: 263
Episode 157: reward: 238.250, steps: 261
Episode 158: reward: 170.014, steps: 231
Episode 159: reward: 247.960, steps: 280
Episode 160: reward: -84.304, steps: 193
Episode 161: reward: 202.966, steps: 302
Episode 162: reward: 187.429, steps: 403
Episode 163: reward: 190.115, steps: 304
Episode 164: reward: -73.232, steps: 237
Episode 165: reward: 212.786, steps: 360
Episode 166: reward: -162.762, steps: 205
Episode 167: reward: 226.292, steps: 308
Episode 168: reward: -76.757, steps: 209
Episode 169: reward: -102.145, steps: 115
Episode 170: reward: -61.110, steps: 128
Episode 171: reward: -133.192, steps: 199
Episode 172: reward: 186.654, steps: 332
Episode 173: reward: 212.912, steps: 473
Episode 174: reward: 29.256, steps: 208
Episode 175: reward: 173.641, steps: 284
Episode 176: reward: -4.958, steps: 141
Episode 177: reward: -50.266, steps: 336
Episode 178: reward: 200.997, steps: 342
Episode 179: reward: -181.137, steps: 154
Episode 180: reward: 182.300, steps: 308
Episode 181: reward: 231.138, steps: 280
Episode 182: reward: 201.711, steps: 460
Episode 183: reward: -210.367, steps: 676
Episode 184: reward: -103.815, steps: 210
Episode 185: reward: -166.541, steps: 214
Episode 186: reward: 135.977, steps: 347
Episode 187: reward: 210.471, steps: 296
Episode 188: reward: -146.278, steps: 129
Episode 189: reward: -25.740, steps: 346
Episode 190: reward: -24.089, steps: 198
Episode 191: reward: -114.841, steps: 324
Episode 192: reward: -96.882, steps: 219
Episode 193: reward: -119.249, steps: 209
Episode 194: reward: -20.678, steps: 215
Episode 195: reward: -111.942, steps: 208
Episode 196: reward: -155.789, steps: 458
Episode 197: reward: 226.811, steps: 376
Episode 198: reward: -344.162, steps: 262
Episode 199: reward: 232.349, steps: 364
Episode 200: reward: -149.897, steps: 125
Episode 201: reward: 223.393, steps: 412
Episode 202: reward: 195.585, steps: 279
Episode 203: reward: -43.290, steps: 296
Episode 204: reward: 206.893, steps: 303
Episode 205: reward: 65.805, steps: 1000
Episode 206: reward: -95.540, steps: 190
Episode 207: reward: 240.043, steps: 269
Episode 208: reward: -85.277, steps: 266
Episode 209: reward: 227.560, steps: 268
Episode 210: reward: -158.085, steps: 268
Episode 211: reward: -118.077, steps: 211
Episode 212: reward: -59.743, steps: 207
Episode 213: reward: -120.583, steps: 322
Episode 214: reward: -57.678, steps: 216
Episode 215: reward: 246.869, steps: 302
Episode 216: reward: -76.724, steps: 200
Episode 217: reward: -156.338, steps: 272
Episode 218: reward: 153.617, steps: 318
Episode 219: reward: -52.159, steps: 207
Episode 220: reward: 199.397, steps: 337
Episode 221: reward: -101.665, steps: 206
Episode 222: reward: -89.620, steps: 219
Episode 223: reward: -31.595, steps: 245
Episode 224: reward: 115.640, steps: 322
Episode 225: reward: 206.702, steps: 317
Episode 226: reward: 106.221, steps: 1000
Episode 227: reward: 228.024, steps: 271
Episode 228: reward: -143.115, steps: 123
Episode 229: reward: 97.254, steps: 241
Episode 230: reward: -65.471, steps: 264
Episode 231: reward: 214.421, steps: 423
Episode 232: reward: 163.476, steps: 249
Episode 233: reward: 128.839, steps: 359
Episode 234: reward: -160.008, steps: 126
Episode 235: reward: -6.739, steps: 260
Episode 236: reward: -92.209, steps: 189
Episode 237: reward: -232.778, steps: 178
Episode 238: reward: 108.983, steps: 1000
Episode 239: reward: 232.014, steps: 306
Episode 240: reward: 246.642, steps: 285
Episode 241: reward: 209.997, steps: 279
Episode 242: reward: 178.685, steps: 284
Episode 243: reward: 85.776, steps: 1000
Episode 244: reward: -164.271, steps: 206
Episode 245: reward: -119.749, steps: 349
Episode 246: reward: 195.535, steps: 365
Episode 247: reward: -0.643, steps: 215
Episode 248: reward: 208.696, steps: 373
Episode 249: reward: -34.711, steps: 249
Episode 250: reward: 171.213, steps: 303
Episode 251: reward: 236.262, steps: 287
Episode 252: reward: -71.495, steps: 219
Episode 253: reward: -9.670, steps: 230
Episode 254: reward: 237.065, steps: 340
Episode 255: reward: -80.775, steps: 127
Episode 256: reward: -16.424, steps: 400
Episode 257: reward: 207.797, steps: 366
Episode 258: reward: 235.604, steps: 289
Episode 259: reward: 258.547, steps: 275
Episode 260: reward: 158.009, steps: 299
Episode 261: reward: 225.709, steps: 345
Episode 262: reward: 217.677, steps: 358
Episode 263: reward: 216.433, steps: 402
Episode 264: reward: 104.237, steps: 833
Episode 265: reward: 239.922, steps: 264
Episode 266: reward: 113.137, steps: 274
Episode 267: reward: 160.113, steps: 280
Episode 268: reward: -117.605, steps: 226
Episode 269: reward: 222.663, steps: 261
Episode 270: reward: 226.835, steps: 361
Episode 271: reward: 212.930, steps: 327
Episode 272: reward: -25.502, steps: 231
Episode 273: reward: 140.507, steps: 319
Episode 274: reward: -173.384, steps: 120
Episode 275: reward: -125.201, steps: 213
Episode 276: reward: -152.044, steps: 127
Episode 277: reward: 143.173, steps: 325
Episode 278: reward: 151.889, steps: 359
Episode 279: reward: 211.556, steps: 400
Episode 280: reward: -11.192, steps: 264
Episode 281: reward: 208.996, steps: 342
Episode 282: reward: 241.341, steps: 294
Episode 283: reward: 159.975, steps: 273
Episode 284: reward: 151.853, steps: 379
Episode 285: reward: 205.587, steps: 286
Episode 286: reward: 224.513, steps: 300
Episode 287: reward: 141.051, steps: 334
Episode 288: reward: -40.778, steps: 201
Episode 289: reward: 130.661, steps: 496
Episode 290: reward: 216.440, steps: 376
Episode 291: reward: 193.365, steps: 390
Episode 292: reward: -111.570, steps: 224
Episode 293: reward: 224.259, steps: 290
Episode 294: reward: 170.777, steps: 364
Episode 295: reward: 181.329, steps: 288
Episode 296: reward: 213.810, steps: 335
Episode 297: reward: -95.314, steps: 211
Episode 298: reward: 90.182, steps: 447
Episode 299: reward: -39.550, steps: 179
Episode 300: reward: 128.587, steps: 321
Episode 301: reward: -146.404, steps: 135
Episode 302: reward: 189.689, steps: 264
Episode 303: reward: -125.026, steps: 351
Episode 304: reward: -74.078, steps: 251
Episode 305: reward: 228.309, steps: 335
Episode 306: reward: 191.063, steps: 277
Episode 307: reward: -304.166, steps: 157
Episode 308: reward: -91.658, steps: 146
Episode 309: reward: 203.842, steps: 414
Episode 310: reward: 144.553, steps: 342
Episode 311: reward: -123.038, steps: 191
Episode 312: reward: 144.816, steps: 448
Episode 313: reward: 114.918, steps: 654
Episode 314: reward: -68.413, steps: 179
Episode 315: reward: -108.999, steps: 203
Episode 316: reward: 172.061, steps: 383
Episode 317: reward: 195.616, steps: 287
Episode 318: reward: 14.767, steps: 200
Episode 319: reward: 239.839, steps: 364
Episode 320: reward: 212.830, steps: 366
Episode 321: reward: 227.342, steps: 459
Episode 322: reward: 193.906, steps: 288
Episode 323: reward: 228.865, steps: 299
Episode 324: reward: 35.863, steps: 200
Episode 325: reward: 205.814, steps: 378
Episode 326: reward: -153.174, steps: 115
Episode 327: reward: 232.958, steps: 305
Episode 328: reward: -9.879, steps: 417
Episode 329: reward: 182.263, steps: 347
Episode 330: reward: 225.064, steps: 492
Episode 331: reward: -88.363, steps: 297
Episode 332: reward: 143.749, steps: 445
Episode 333: reward: -100.753, steps: 310
Episode 334: reward: -124.986, steps: 224
Episode 335: reward: 225.271, steps: 249
Episode 336: reward: 220.696, steps: 269
Episode 337: reward: -133.021, steps: 197
Episode 338: reward: 207.734, steps: 257
Episode 339: reward: 214.780, steps: 320
Episode 340: reward: 7.136, steps: 249
Episode 341: reward: -95.566, steps: 215
Episode 342: reward: 153.148, steps: 314
Episode 343: reward: 190.058, steps: 317
Episode 344: reward: 200.644, steps: 295
Episode 345: reward: 189.171, steps: 378
Episode 346: reward: -55.589, steps: 320
Episode 347: reward: 234.959, steps: 283
Episode 348: reward: 242.300, steps: 244
Episode 349: reward: -103.472, steps: 221
Episode 350: reward: 248.007, steps: 263
Episode 351: reward: 243.590, steps: 319
Episode 352: reward: -73.733, steps: 190
Episode 353: reward: 206.074, steps: 270
Episode 354: reward: -24.702, steps: 182
Episode 355: reward: -122.488, steps: 197
Episode 356: reward: -63.079, steps: 177
Episode 357: reward: -101.384, steps: 202
Episode 358: reward: 243.839, steps: 306
Episode 359: reward: 180.125, steps: 314
Episode 360: reward: 221.937, steps: 303
Episode 361: reward: -91.089, steps: 212
Episode 362: reward: 200.988, steps: 304
Episode 363: reward: 113.672, steps: 432
Episode 364: reward: 88.676, steps: 1000
Episode 365: reward: 239.688, steps: 265
Episode 366: reward: 224.744, steps: 280
Episode 367: reward: 217.897, steps: 273
Episode 368: reward: 239.438, steps: 323
Episode 369: reward: -106.033, steps: 233
Episode 370: reward: 194.421, steps: 294
Episode 371: reward: -242.475, steps: 160
Episode 372: reward: 236.447, steps: 291
Episode 373: reward: -114.180, steps: 116
Episode 374: reward: -103.034, steps: 197
Episode 375: reward: 229.071, steps: 379
Episode 376: reward: -41.281, steps: 194
Episode 377: reward: -34.986, steps: 209
Episode 378: reward: 218.436, steps: 389
Episode 379: reward: -20.271, steps: 256
Episode 380: reward: 208.852, steps: 331
Episode 381: reward: -106.302, steps: 309
Episode 382: reward: 124.066, steps: 1000
Episode 383: reward: -135.585, steps: 230
Episode 384: reward: 218.857, steps: 504
Episode 385: reward: 0.024, steps: 209
Episode 386: reward: -20.361, steps: 220
Episode 387: reward: -91.143, steps: 198
Episode 388: reward: 224.170, steps: 304
Episode 389: reward: 199.258, steps: 331
Episode 390: reward: 195.458, steps: 284
Episode 391: reward: 156.702, steps: 323
Episode 392: reward: -141.819, steps: 420
Episode 393: reward: 164.089, steps: 468
Episode 394: reward: -148.524, steps: 209
Episode 395: reward: 152.260, steps: 309
Episode 396: reward: -31.118, steps: 212
Episode 397: reward: 221.504, steps: 281
Episode 398: reward: -62.313, steps: 220
Episode 399: reward: 228.978, steps: 347
Episode 400: reward: -28.877, steps: 317
Episode 401: reward: 227.242, steps: 298
Episode 402: reward: 37.919, steps: 214
Episode 403: reward: -67.056, steps: 165
Episode 404: reward: 209.439, steps: 703
Episode 405: reward: -156.232, steps: 219
Episode 406: reward: -101.527, steps: 204
Episode 407: reward: 206.774, steps: 331
Episode 408: reward: 138.329, steps: 335
Episode 409: reward: -85.614, steps: 299
Episode 410: reward: 229.665, steps: 287
Episode 411: reward: 245.589, steps: 246
Episode 412: reward: 215.253, steps: 264
Episode 413: reward: -63.417, steps: 209
Episode 414: reward: 225.063, steps: 357
Episode 415: reward: -146.254, steps: 142
Episode 416: reward: -128.789, steps: 213
Episode 417: reward: -12.049, steps: 243
Episode 418: reward: 198.334, steps: 313
Episode 419: reward: 24.363, steps: 1000
Episode 420: reward: 217.480, steps: 272
Episode 421: reward: -76.071, steps: 229
Episode 422: reward: -133.633, steps: 327
Episode 423: reward: 233.513, steps: 309
Episode 424: reward: 243.542, steps: 287
Episode 425: reward: -83.608, steps: 130
Episode 426: reward: -23.451, steps: 228
Episode 427: reward: -108.283, steps: 148
Episode 428: reward: 183.517, steps: 333
Episode 429: reward: -59.557, steps: 171
Episode 430: reward: 217.143, steps: 318
Episode 431: reward: -114.403, steps: 178
Episode 432: reward: 253.549, steps: 234
Episode 433: reward: 176.628, steps: 338
Episode 434: reward: -188.667, steps: 192
Episode 435: reward: 121.456, steps: 323
Episode 436: reward: 197.788, steps: 445
Episode 437: reward: 194.353, steps: 302
Episode 438: reward: 224.995, steps: 385
Episode 439: reward: 241.192, steps: 266
Episode 440: reward: 4.991, steps: 175
Episode 441: reward: 168.439, steps: 229
Episode 442: reward: -89.890, steps: 339
Episode 443: reward: -116.880, steps: 230
Episode 444: reward: -117.853, steps: 372
Episode 445: reward: 233.869, steps: 350
Episode 446: reward: 196.248, steps: 287
Episode 447: reward: 93.551, steps: 1000
Episode 448: reward: -128.971, steps: 200
Episode 449: reward: -132.731, steps: 217
Episode 450: reward: -219.691, steps: 298
Episode 451: reward: 130.577, steps: 321
Episode 452: reward: -106.463, steps: 385
Episode 453: reward: 222.681, steps: 440
Episode 454: reward: 211.787, steps: 314
Episode 455: reward: 165.278, steps: 281
Episode 456: reward: 178.826, steps: 306
Episode 457: reward: 220.293, steps: 353
Episode 458: reward: 155.215, steps: 473
Episode 459: reward: 241.088, steps: 310
Episode 460: reward: -90.143, steps: 188
Episode 461: reward: 155.298, steps: 298
Episode 462: reward: 173.343, steps: 309
Episode 463: reward: 241.620, steps: 349
Episode 464: reward: 189.967, steps: 278
Episode 465: reward: -120.915, steps: 205
Episode 466: reward: -90.745, steps: 224
Episode 467: reward: 94.830, steps: 365
Episode 468: reward: -97.216, steps: 199
Episode 469: reward: -58.016, steps: 199
Episode 470: reward: 192.625, steps: 590
Episode 471: reward: -122.476, steps: 142
Episode 472: reward: 10.915, steps: 254
Episode 473: reward: 123.972, steps: 445
Episode 474: reward: 249.955, steps: 339
Episode 475: reward: -21.496, steps: 367
Episode 476: reward: 10.164, steps: 220
Episode 477: reward: 243.072, steps: 313
Episode 478: reward: 227.797, steps: 292
Episode 479: reward: -121.150, steps: 120
Episode 480: reward: 175.222, steps: 487
Episode 481: reward: 219.443, steps: 334
Episode 482: reward: 233.411, steps: 308
Episode 483: reward: -59.805, steps: 229
Episode 484: reward: 233.325, steps: 326
Episode 485: reward: -10.076, steps: 213
Episode 486: reward: 206.624, steps: 316
Episode 487: reward: 148.333, steps: 333
Episode 488: reward: 191.207, steps: 308
Episode 489: reward: -135.999, steps: 322
Episode 490: reward: 217.591, steps: 334
Episode 491: reward: 209.711, steps: 343
Episode 492: reward: 185.234, steps: 319
Episode 493: reward: -51.115, steps: 250
Episode 494: reward: -220.639, steps: 190
Episode 495: reward: -6.243, steps: 246
Episode 496: reward: -76.190, steps: 224
Episode 497: reward: -117.117, steps: 196
Episode 498: reward: -144.016, steps: 132
Episode 499: reward: 174.737, steps: 421
Episode 500: reward: 215.919, steps: 487
Episode 501: reward: 191.266, steps: 385
Episode 502: reward: 155.447, steps: 676
Episode 503: reward: -75.741, steps: 254
Episode 504: reward: 72.171, steps: 1000
Episode 505: reward: 148.627, steps: 461
Episode 506: reward: -119.214, steps: 114
Episode 507: reward: 154.634, steps: 324
Episode 508: reward: 253.803, steps: 260
Episode 509: reward: -240.254, steps: 186
Episode 510: reward: 203.646, steps: 370
Episode 511: reward: 69.806, steps: 1000
Episode 512: reward: 210.610, steps: 351
Episode 513: reward: 229.721, steps: 323
Episode 514: reward: 238.236, steps: 357
Episode 515: reward: 211.455, steps: 304
Episode 516: reward: -103.318, steps: 128
Episode 517: reward: 217.617, steps: 400
Episode 518: reward: -85.576, steps: 190
Episode 519: reward: -4.425, steps: 371
Episode 520: reward: -121.958, steps: 119
Episode 521: reward: 222.640, steps: 301
Episode 522: reward: -24.124, steps: 245
Episode 523: reward: 220.539, steps: 435
Episode 524: reward: -25.771, steps: 258
Episode 525: reward: -35.698, steps: 246
Episode 526: reward: -105.673, steps: 218
Episode 527: reward: 237.902, steps: 292
Episode 528: reward: -34.570, steps: 207
Episode 529: reward: 187.114, steps: 334
Episode 530: reward: -131.273, steps: 198
Episode 531: reward: -142.762, steps: 121
Episode 532: reward: 197.816, steps: 444
Episode 533: reward: -70.555, steps: 249
Episode 534: reward: -118.182, steps: 253
Episode 535: reward: 213.432, steps: 311
Episode 536: reward: -112.388, steps: 215
Episode 537: reward: -31.898, steps: 199
Episode 538: reward: 242.040, steps: 302
Episode 539: reward: 131.497, steps: 385
Episode 540: reward: 101.998, steps: 252
Episode 541: reward: 137.169, steps: 301
Episode 542: reward: 225.275, steps: 352
Episode 543: reward: 204.771, steps: 310
Episode 544: reward: 228.142, steps: 351
Episode 545: reward: -76.200, steps: 231
Episode 546: reward: -129.406, steps: 212
Episode 547: reward: -192.456, steps: 160
Episode 548: reward: -159.659, steps: 290
Episode 549: reward: 244.639, steps: 296
Episode 550: reward: 203.117, steps: 356
Episode 551: reward: -116.062, steps: 189
Episode 552: reward: 218.117, steps: 337
Episode 553: reward: 255.897, steps: 279
Episode 554: reward: -11.488, steps: 209
Episode 555: reward: 248.058, steps: 279
Episode 556: reward: -26.849, steps: 204
Episode 557: reward: 232.400, steps: 321
Episode 558: reward: 221.885, steps: 342
Episode 559: reward: -75.890, steps: 212
Episode 560: reward: 226.536, steps: 465
Episode 561: reward: 190.350, steps: 428
Episode 562: reward: -33.168, steps: 226
Episode 563: reward: -148.708, steps: 132
Episode 564: reward: 156.108, steps: 597
Episode 565: reward: 159.641, steps: 348
Episode 566: reward: 132.394, steps: 364
Episode 567: reward: -21.854, steps: 214
Episode 568: reward: -53.514, steps: 248
Episode 569: reward: -126.720, steps: 222
Episode 570: reward: 7.591, steps: 294
Episode 571: reward: -65.544, steps: 241
Episode 572: reward: -104.055, steps: 195
Episode 573: reward: 207.430, steps: 414
Episode 574: reward: 105.412, steps: 356
Episode 575: reward: 232.674, steps: 359
Episode 576: reward: 207.864, steps: 343
Episode 577: reward: -105.322, steps: 226
Episode 578: reward: 233.644, steps: 266
Episode 579: reward: 189.355, steps: 411
Episode 580: reward: 216.813, steps: 427
Episode 581: reward: -74.441, steps: 157
Episode 582: reward: -144.121, steps: 319
Episode 583: reward: -80.145, steps: 207
Episode 584: reward: 237.207, steps: 263
Episode 585: reward: 214.601, steps: 436
Episode 586: reward: 224.726, steps: 318
Episode 587: reward: -63.861, steps: 156
Episode 588: reward: 218.858, steps: 287
Episode 589: reward: -42.961, steps: 193
Episode 590: reward: -36.740, steps: 327
Episode 591: reward: 134.786, steps: 213
Episode 592: reward: -124.812, steps: 337
Episode 593: reward: 136.224, steps: 321
Episode 594: reward: 218.353, steps: 294
Episode 595: reward: 183.954, steps: 261
Episode 596: reward: -113.101, steps: 214
Episode 597: reward: -232.219, steps: 363
Episode 598: reward: 227.082, steps: 436
Episode 599: reward: 227.150, steps: 449
Episode 600: reward: 199.214, steps: 393
Episode 601: reward: -157.092, steps: 148
Episode 602: reward: 118.586, steps: 229
Episode 603: reward: 154.576, steps: 306
Episode 604: reward: 236.477, steps: 325
Episode 605: reward: 209.355, steps: 296
Episode 606: reward: -120.541, steps: 223
Episode 607: reward: 236.986, steps: 311
Episode 608: reward: -24.244, steps: 252
Episode 609: reward: 240.443, steps: 278
Episode 610: reward: -43.443, steps: 273
Episode 611: reward: 228.633, steps: 271
Episode 612: reward: 123.768, steps: 448
Episode 613: reward: 226.293, steps: 398
Episode 614: reward: 179.248, steps: 743
Episode 615: reward: 217.212, steps: 405
Episode 616: reward: 221.297, steps: 290
Episode 617: reward: -70.290, steps: 221
Episode 618: reward: -113.840, steps: 200
Episode 619: reward: -123.290, steps: 131
Episode 620: reward: -14.007, steps: 253
Episode 621: reward: 183.389, steps: 283
Episode 622: reward: -133.139, steps: 137
Episode 623: reward: 206.147, steps: 310
Episode 624: reward: 228.121, steps: 334
Episode 625: reward: 131.477, steps: 271
Episode 626: reward: 75.636, steps: 1000
Episode 627: reward: 188.832, steps: 289
Episode 628: reward: 250.995, steps: 304
Episode 629: reward: 237.063, steps: 353
Episode 630: reward: 227.201, steps: 518
Episode 631: reward: 31.145, steps: 136
Episode 632: reward: 228.549, steps: 303
Episode 633: reward: 80.490, steps: 287
Episode 634: reward: 226.875, steps: 314
Episode 635: reward: 250.198, steps: 220
Episode 636: reward: 146.592, steps: 365
Episode 637: reward: 225.891, steps: 321
Episode 638: reward: 229.452, steps: 261
Episode 639: reward: 221.110, steps: 462
Episode 640: reward: -14.003, steps: 203
Episode 641: reward: 244.226, steps: 269
Episode 642: reward: -42.395, steps: 269
Episode 643: reward: -53.995, steps: 239
Episode 644: reward: 243.967, steps: 286
Episode 645: reward: 201.957, steps: 290
Episode 646: reward: -85.948, steps: 201
Episode 647: reward: 236.882, steps: 320
Episode 648: reward: -240.806, steps: 271
Episode 649: reward: 222.571, steps: 270
Episode 650: reward: 173.479, steps: 332
Episode 651: reward: 83.505, steps: 1000
Episode 652: reward: 170.438, steps: 240
Episode 653: reward: -159.225, steps: 164
Episode 654: reward: -124.520, steps: 199
Episode 655: reward: 218.262, steps: 288
Episode 656: reward: 204.497, steps: 295
Episode 657: reward: 203.303, steps: 242
Episode 658: reward: -94.756, steps: 282
Episode 659: reward: -181.380, steps: 137
Episode 660: reward: 159.122, steps: 383
Episode 661: reward: -164.913, steps: 188
Episode 662: reward: 177.292, steps: 266
Episode 663: reward: 99.024, steps: 528
Episode 664: reward: -103.439, steps: 226
Episode 665: reward: 116.611, steps: 329
Episode 666: reward: 226.290, steps: 362
Episode 667: reward: -122.315, steps: 231
Episode 668: reward: 167.979, steps: 325
Episode 669: reward: -122.752, steps: 214
Episode 670: reward: 133.529, steps: 334
Episode 671: reward: 208.954, steps: 280
Episode 672: reward: 206.863, steps: 315
Episode 673: reward: 144.969, steps: 251
Episode 674: reward: -18.278, steps: 200
Episode 675: reward: -98.203, steps: 112
Episode 676: reward: 240.811, steps: 266
Episode 677: reward: 139.527, steps: 216
Episode 678: reward: -24.154, steps: 213
Episode 679: reward: -3.742, steps: 241
Episode 680: reward: 168.798, steps: 699
Episode 681: reward: -90.154, steps: 233
Episode 682: reward: 241.186, steps: 430
Episode 683: reward: 237.132, steps: 259
Episode 684: reward: -37.382, steps: 215
Episode 685: reward: 227.301, steps: 279
Episode 686: reward: -2.048, steps: 175
Episode 687: reward: -73.275, steps: 223
Episode 688: reward: -101.411, steps: 194
Episode 689: reward: 101.189, steps: 413
Episode 690: reward: -123.627, steps: 237
Episode 691: reward: 243.724, steps: 293
Episode 692: reward: 237.627, steps: 299
Episode 693: reward: 261.807, steps: 232
Episode 694: reward: 168.993, steps: 244
Episode 695: reward: -91.048, steps: 289
Episode 696: reward: -88.849, steps: 190
Episode 697: reward: -83.321, steps: 202
Episode 698: reward: 200.692, steps: 302
Episode 699: reward: 174.828, steps: 296
Episode 700: reward: 225.121, steps: 364
Episode 701: reward: 217.536, steps: 323
Episode 702: reward: 211.163, steps: 303
Episode 703: reward: 1.911, steps: 339
Episode 704: reward: 158.462, steps: 376
Episode 705: reward: 231.210, steps: 396
Episode 706: reward: -36.466, steps: 233
Episode 707: reward: -56.618, steps: 244
Episode 708: reward: -17.753, steps: 201
Episode 709: reward: -60.335, steps: 178
Episode 710: reward: -136.723, steps: 137
Episode 711: reward: 216.536, steps: 263
Episode 712: reward: 200.714, steps: 295
Episode 713: reward: -100.190, steps: 212
Episode 714: reward: 152.328, steps: 339
Episode 715: reward: 177.614, steps: 373
Episode 716: reward: 222.463, steps: 396
Episode 717: reward: 229.381, steps: 398
Episode 718: reward: 222.732, steps: 316
Episode 719: reward: 239.252, steps: 307
Episode 720: reward: -38.912, steps: 198
Episode 721: reward: -27.192, steps: 257
Episode 722: reward: 260.259, steps: 247
Episode 723: reward: -66.660, steps: 248
Episode 724: reward: 73.322, steps: 358
Episode 725: reward: 235.248, steps: 266
Episode 726: reward: 245.345, steps: 250
Episode 727: reward: -139.145, steps: 142
Episode 728: reward: -114.109, steps: 192
Episode 729: reward: -61.490, steps: 250
Episode 730: reward: 221.709, steps: 352
Episode 731: reward: 210.848, steps: 385
Episode 732: reward: -132.081, steps: 145
Episode 733: reward: 223.638, steps: 335
Episode 734: reward: -10.889, steps: 295
Episode 735: reward: -145.333, steps: 204
Episode 736: reward: -110.351, steps: 192
Episode 737: reward: -16.599, steps: 314
Episode 738: reward: 218.867, steps: 564
Episode 739: reward: -122.810, steps: 147
Episode 740: reward: 184.832, steps: 352
Episode 741: reward: 245.944, steps: 320
Episode 742: reward: 239.195, steps: 282
Episode 743: reward: 229.653, steps: 293
Episode 744: reward: 226.165, steps: 280
Episode 745: reward: 198.801, steps: 363
Episode 746: reward: -76.093, steps: 215
Episode 747: reward: 4.624, steps: 282
Episode 748: reward: 241.952, steps: 291
Episode 749: reward: -5.529, steps: 418
Episode 750: reward: 216.357, steps: 250
Episode 751: reward: -141.066, steps: 342
Episode 752: reward: -179.247, steps: 309
Episode 753: reward: 233.673, steps: 364
Episode 754: reward: -5.014, steps: 237
Episode 755: reward: -121.443, steps: 123
Episode 756: reward: 111.572, steps: 531
Episode 757: reward: 235.509, steps: 313
Episode 758: reward: 185.015, steps: 274
Episode 759: reward: 158.834, steps: 419
Episode 760: reward: 236.930, steps: 295
Episode 761: reward: 145.941, steps: 362
Episode 762: reward: 208.578, steps: 429
Episode 763: reward: 173.141, steps: 302
Episode 764: reward: 189.711, steps: 305
Episode 765: reward: -67.901, steps: 216
Episode 766: reward: 232.236, steps: 287
Episode 767: reward: 231.192, steps: 299
Episode 768: reward: 242.763, steps: 279
Episode 769: reward: -31.126, steps: 187
Episode 770: reward: 218.362, steps: 260
Episode 771: reward: -141.666, steps: 134
Episode 772: reward: -63.452, steps: 285
Episode 773: reward: 108.977, steps: 300
Episode 774: reward: -64.701, steps: 238
Episode 775: reward: -107.704, steps: 242
Episode 776: reward: 174.063, steps: 327
Episode 777: reward: 222.260, steps: 342
Episode 778: reward: 210.728, steps: 328
Episode 779: reward: -114.421, steps: 186
Episode 780: reward: 205.308, steps: 353
Episode 781: reward: 205.631, steps: 334
Episode 782: reward: -134.129, steps: 144
Episode 783: reward: -108.472, steps: 191
Episode 784: reward: -67.210, steps: 202
Episode 785: reward: -157.346, steps: 193
Episode 786: reward: 212.785, steps: 297
Episode 787: reward: 167.533, steps: 302
Episode 788: reward: 236.484, steps: 278
Episode 789: reward: -2.137, steps: 184
Episode 790: reward: 224.488, steps: 312
Episode 791: reward: -22.639, steps: 218
Episode 792: reward: 124.232, steps: 601
Episode 793: reward: -159.624, steps: 220
Episode 794: reward: -110.888, steps: 344
Episode 795: reward: 116.882, steps: 347
Episode 796: reward: 5.115, steps: 236
Episode 797: reward: 231.839, steps: 249
Episode 798: reward: -267.804, steps: 229
Episode 799: reward: -140.842, steps: 295
Episode 800: reward: 221.230, steps: 275
Episode 801: reward: 214.895, steps: 235
Episode 802: reward: 27.292, steps: 181
Episode 803: reward: -111.626, steps: 116
Episode 804: reward: 113.959, steps: 517
Episode 805: reward: -102.571, steps: 302
Episode 806: reward: 246.791, steps: 317
Episode 807: reward: -255.111, steps: 170
Episode 808: reward: 219.484, steps: 344
Episode 809: reward: 199.429, steps: 349
Episode 810: reward: -95.113, steps: 221
Episode 811: reward: -131.618, steps: 196
Episode 812: reward: 196.414, steps: 349
Episode 813: reward: 160.231, steps: 313
Episode 814: reward: 246.051, steps: 317
Episode 815: reward: -29.704, steps: 214
Episode 816: reward: -16.243, steps: 188
Episode 817: reward: 201.406, steps: 322
Episode 818: reward: -110.414, steps: 204
Episode 819: reward: 238.193, steps: 257
Episode 820: reward: 146.261, steps: 283
Episode 821: reward: -75.442, steps: 213
Episode 822: reward: 223.041, steps: 528
Episode 823: reward: 206.824, steps: 420
Episode 824: reward: 232.279, steps: 291
Episode 825: reward: 217.064, steps: 280
Episode 826: reward: 191.908, steps: 331
Episode 827: reward: -184.378, steps: 130
Episode 828: reward: -138.049, steps: 329
Episode 829: reward: 148.919, steps: 516
Episode 830: reward: -58.352, steps: 245
Episode 831: reward: 222.390, steps: 425
Episode 832: reward: -49.014, steps: 229
Episode 833: reward: 98.373, steps: 1000
Episode 834: reward: 217.512, steps: 264
Episode 835: reward: -108.972, steps: 325
Episode 836: reward: -26.314, steps: 248
Episode 837: reward: -142.801, steps: 139
Episode 838: reward: -71.163, steps: 206
Episode 839: reward: -34.528, steps: 196
Episode 840: reward: -100.355, steps: 195
Episode 841: reward: 54.010, steps: 1000
Episode 842: reward: 147.022, steps: 304
Episode 843: reward: 258.558, steps: 296
Episode 844: reward: 198.747, steps: 276
Episode 845: reward: 230.440, steps: 326
Episode 846: reward: -47.477, steps: 208
Episode 847: reward: 174.996, steps: 306
Episode 848: reward: 228.717, steps: 497
Episode 849: reward: -121.541, steps: 160
Episode 850: reward: -155.223, steps: 218
Episode 851: reward: 212.106, steps: 320
Episode 852: reward: 235.546, steps: 353
Episode 853: reward: 195.917, steps: 368
Episode 854: reward: 228.166, steps: 312
Episode 855: reward: 222.412, steps: 362
Episode 856: reward: -144.124, steps: 113
Episode 857: reward: -215.796, steps: 160
Episode 858: reward: 229.888, steps: 287
Episode 859: reward: 226.750, steps: 253
Episode 860: reward: 153.854, steps: 315
Episode 861: reward: -124.896, steps: 204
Episode 862: reward: 216.874, steps: 294
Episode 863: reward: -108.986, steps: 208
Episode 864: reward: 168.654, steps: 568
Episode 865: reward: -29.788, steps: 196
Episode 866: reward: -53.019, steps: 214
Episode 867: reward: 218.586, steps: 320
Episode 868: reward: 201.725, steps: 438
Episode 869: reward: 113.715, steps: 267
Episode 870: reward: 225.721, steps: 379
Episode 871: reward: -6.646, steps: 318
Episode 872: reward: 228.819, steps: 332
Episode 873: reward: -93.070, steps: 185
Episode 874: reward: 204.204, steps: 373
Episode 875: reward: -104.152, steps: 212
Episode 876: reward: -105.618, steps: 139
Episode 877: reward: 171.770, steps: 275
Episode 878: reward: 204.028, steps: 243
Episode 879: reward: 233.046, steps: 271
Episode 880: reward: 234.711, steps: 278
Episode 881: reward: -174.679, steps: 164
Episode 882: reward: -116.004, steps: 187
Episode 883: reward: -110.751, steps: 322
Episode 884: reward: -129.711, steps: 241
Episode 885: reward: -32.335, steps: 211
Episode 886: reward: 120.894, steps: 560
Episode 887: reward: 240.443, steps: 277
Episode 888: reward: 145.278, steps: 526
Episode 889: reward: -129.874, steps: 231
Episode 890: reward: 158.718, steps: 353
Episode 891: reward: 239.333, steps: 286
Episode 892: reward: 161.385, steps: 347
Episode 893: reward: 114.886, steps: 301
Episode 894: reward: 222.373, steps: 453
Episode 895: reward: 102.904, steps: 420
Episode 896: reward: 217.244, steps: 306
Episode 897: reward: 222.021, steps: 408
Episode 898: reward: -75.819, steps: 196
Episode 899: reward: 215.490, steps: 334
Episode 900: reward: 237.828, steps: 286
Episode 901: reward: -176.672, steps: 202
Episode 902: reward: -98.127, steps: 211
Episode 903: reward: 5.906, steps: 236
Episode 904: reward: 191.694, steps: 562
Episode 905: reward: 222.812, steps: 387
Episode 906: reward: -34.971, steps: 175
Episode 907: reward: 224.998, steps: 279
Episode 908: reward: -22.227, steps: 227
Episode 909: reward: 149.518, steps: 306
Episode 910: reward: -32.666, steps: 211
Episode 911: reward: 213.763, steps: 266
Episode 912: reward: 236.327, steps: 279
Episode 913: reward: 183.904, steps: 292
Episode 914: reward: 217.234, steps: 397
Episode 915: reward: 224.334, steps: 277
Episode 916: reward: 201.273, steps: 392
Episode 917: reward: 178.923, steps: 379
Episode 918: reward: 228.964, steps: 281
Episode 919: reward: -79.954, steps: 242
Episode 920: reward: -18.449, steps: 197
Episode 921: reward: 218.737, steps: 304
Episode 922: reward: 205.742, steps: 347
Episode 923: reward: -124.648, steps: 200
Episode 924: reward: -151.177, steps: 232
Episode 925: reward: 148.586, steps: 288
Episode 926: reward: -163.354, steps: 185
Episode 927: reward: 233.002, steps: 259
Episode 928: reward: 202.350, steps: 309
Episode 929: reward: -204.001, steps: 136
Episode 930: reward: -36.570, steps: 197
Episode 931: reward: 192.633, steps: 352
Episode 932: reward: 236.892, steps: 367
Episode 933: reward: 179.329, steps: 339
Episode 934: reward: 240.618, steps: 272
Episode 935: reward: 238.096, steps: 599
Episode 936: reward: 224.280, steps: 298
Episode 937: reward: 139.940, steps: 314
Episode 938: reward: -12.646, steps: 245
Episode 939: reward: -38.116, steps: 237
Episode 940: reward: -106.398, steps: 341
Episode 941: reward: -12.786, steps: 243
Episode 942: reward: 239.616, steps: 295
Episode 943: reward: 220.149, steps: 315
Episode 944: reward: 247.605, steps: 294
Episode 945: reward: -57.483, steps: 287
Episode 946: reward: -135.119, steps: 229
Episode 947: reward: 201.225, steps: 312
Episode 948: reward: 72.825, steps: 1000
Episode 949: reward: 223.023, steps: 348
Episode 950: reward: -141.199, steps: 128
Episode 951: reward: 200.565, steps: 346
Episode 952: reward: -162.071, steps: 168
Episode 953: reward: -120.041, steps: 208
Episode 954: reward: 243.976, steps: 261
Episode 955: reward: 228.412, steps: 441
Episode 956: reward: 158.600, steps: 378
Episode 957: reward: 243.304, steps: 269
Episode 958: reward: 232.096, steps: 339
Episode 959: reward: 222.149, steps: 396
Episode 960: reward: 228.771, steps: 377
Episode 961: reward: 226.703, steps: 281
Episode 962: reward: 253.462, steps: 210
Episode 963: reward: 221.737, steps: 289
Episode 964: reward: -5.955, steps: 243
Episode 965: reward: -92.335, steps: 211
Episode 966: reward: 150.088, steps: 343
Episode 967: reward: 238.095, steps: 297
Episode 968: reward: 182.242, steps: 331
Episode 969: reward: 211.654, steps: 442
Episode 970: reward: 244.852, steps: 294
Episode 971: reward: -17.630, steps: 248
Episode 972: reward: 227.324, steps: 355
Episode 973: reward: 193.187, steps: 381
Episode 974: reward: 251.105, steps: 305
Episode 975: reward: 153.999, steps: 519
Episode 976: reward: -16.890, steps: 255
Episode 977: reward: 241.616, steps: 248
Episode 978: reward: 180.579, steps: 369
Episode 979: reward: 228.724, steps: 304
Episode 980: reward: -95.851, steps: 224
Episode 981: reward: -28.290, steps: 271
Episode 982: reward: -282.951, steps: 163
Episode 983: reward: 251.630, steps: 240
Episode 984: reward: -136.114, steps: 209
Episode 985: reward: 201.378, steps: 347
Episode 986: reward: 258.013, steps: 210
Episode 987: reward: -74.695, steps: 203
Episode 988: reward: 135.604, steps: 397
Episode 989: reward: -48.945, steps: 196
Episode 990: reward: -110.343, steps: 355
Episode 991: reward: -18.266, steps: 232
Episode 992: reward: 239.710, steps: 276
Episode 993: reward: 131.351, steps: 545
Episode 994: reward: 169.563, steps: 476
Episode 995: reward: -242.645, steps: 210
Episode 996: reward: 224.252, steps: 321
Episode 997: reward: 132.510, steps: 425
Episode 998: reward: -45.725, steps: 258
Episode 999: reward: 222.149, steps: 403
Episode 1000: reward: -125.605, steps: 133
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;test_history_dqn.pckl&#39;</span><span class="p">,</span><span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">test_his</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Average Reward Per Episode during test period for DQN Agent: &quot;</span><span class="p">,</span>\
      <span class="nb">sum</span><span class="p">(</span><span class="n">test_his</span><span class="p">[</span><span class="s1">&#39;episode_reward&#39;</span><span class="p">])</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">test_his</span><span class="p">[</span><span class="s1">&#39;episode_reward&#39;</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Average Reward Per Episode during test period for DQN Agent:  77.27019831736152
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;LunarLander-v2&#39;</span><span class="p">)</span>
<span class="n">nb_actions</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>

<span class="n">WINDOW_LENGTH</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">WINDOW_LENGTH</span><span class="p">,)</span> <span class="o">+</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">nb_actions</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>

<span class="n">memory</span> <span class="o">=</span> <span class="n">SequentialMemory</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">500000</span><span class="p">,</span> <span class="n">window_length</span><span class="o">=</span><span class="n">WINDOW_LENGTH</span><span class="p">)</span>

<span class="n">policy</span> <span class="o">=</span> <span class="n">EpsGreedyQPolicy</span><span class="p">()</span>

<span class="n">dqn</span> <span class="o">=</span> <span class="n">DQNAgent</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">nb_actions</span><span class="o">=</span><span class="n">nb_actions</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span> <span class="n">memory</span><span class="o">=</span><span class="n">memory</span><span class="p">,</span> \
               <span class="n">nb_steps_warmup</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=.</span><span class="mi">99</span><span class="p">,</span> <span class="n">target_model_update</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> \
               <span class="n">train_interval</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">delta_clip</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>

<span class="n">dqn</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=.</span><span class="mi">00025</span><span class="p">),</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mae&#39;</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre><span class="ansi-yellow-fg">WARN: gym.spaces.Box autodetected dtype as &lt;class &#39;numpy.float32&#39;&gt;. Please provide explicit dtype.</span>
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_2 (Flatten)          (None, 800)               0         
_________________________________________________________________
dense_5 (Dense)              (None, 512)               410112    
_________________________________________________________________
activation_5 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_6 (Dense)              (None, 64)                32832     
_________________________________________________________________
activation_6 (Activation)    (None, 64)                0         
_________________________________________________________________
dense_7 (Dense)              (None, 16)                1040      
_________________________________________________________________
activation_7 (Activation)    (None, 16)                0         
_________________________________________________________________
dense_8 (Dense)              (None, 4)                 68        
_________________________________________________________________
activation_8 (Activation)    (None, 4)                 0         
=================================================================
Total params: 444,052
Trainable params: 444,052
Non-trainable params: 0
_________________________________________________________________
None
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Load the Lunar Lander environment</span>
<span class="n">dqn</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s1">&#39;dqn_model1_weights.h5f&#39;</span><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1000</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">actions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="c1"># Run the game loop</span>
    <span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">dqn</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">get_recent_state</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">q_values</span> <span class="o">=</span> <span class="n">dqn</span><span class="o">.</span><span class="n">compute_q_values</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">dqn</span><span class="o">.</span><span class="n">test_policy</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">q_values</span><span class="p">)</span>
        <span class="n">a_star</span> <span class="o">=</span> <span class="n">heuristic</span><span class="p">(</span><span class="n">env</span><span class="p">,</span><span class="n">s</span><span class="p">)</span>
        <span class="n">actions</span> <span class="o">+=</span> <span class="p">[[</span><span class="n">a</span><span class="p">,</span><span class="n">a_star</span><span class="p">]]</span>
        
        <span class="c1"># Step on the game</span>
        <span class="n">s</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="c1">#env.render()</span>
        <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">r</span>
        <span class="c1">#if steps % 1 == 0 or done:</span>
        <span class="c1">#    print([&quot;{:+0.2f}&quot;.format(x) for x in s])</span>
        <span class="c1">#    print(&quot;step {} total_reward {:+0.2f}&quot;.format(steps, total_reward))</span>
        <span class="c1">#    print(&quot;reward:&quot;,r)</span>
        <span class="n">steps</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="n">done</span><span class="p">:</span> 
            <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">total_reward</span>
            <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="k">break</span>
            
<span class="c1">#env.close()</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;dqn2.pckl&#39;</span><span class="p">,</span><span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">([</span><span class="n">results</span><span class="p">,</span><span class="n">actions</span><span class="p">],</span> <span class="n">f</span><span class="p">)</span>
<span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

</div>
    </div>
  </div>
</body>

 


</html>
